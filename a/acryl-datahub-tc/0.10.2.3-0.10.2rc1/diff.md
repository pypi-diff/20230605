# Comparing `tmp/acryl-datahub-tc-0.10.2.3.tar.gz` & `tmp/acryl-datahub-tc-0.10.2rc1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/acryl-datahub-tc-0.10.2.3.tar", last modified: Mon Jun  5 18:19:02 2023, max compression
+gzip compressed data, was "dist/acryl-datahub-tc-0.10.2rc1.tar", last modified: Wed May 31 19:14:29 2023, max compression
```

## Comparing `acryl-datahub-tc-0.10.2.3.tar` & `acryl-datahub-tc-0.10.2rc1.tar`

### file list

```diff
@@ -1,645 +1,634 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/
--rw-r--r--   0 runner    (1001) docker     (123)    15743 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    11008 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/README.md
--rw-r--r--   0 runner    (1001) docker     (123)      928 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)     2059 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)    27640 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    15743 2023-06-05 18:19:01.000000 acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    25000 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-05 18:19:01.000000 acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)     6707 2023-06-05 18:19:01.000000 acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-05 18:19:01.000000 acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (123)    34621 2023-06-05 18:19:01.000000 acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       25 2023-06-05 18:19:01.000000 acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/
--rw-r--r--   0 runner    (1001) docker     (123)      576 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      106 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/circuit_breaker/
--rw-r--r--   0 runner    (1001) docker     (123)      268 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/circuit_breaker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5324 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/circuit_breaker/assertion_circuit_breaker.py
--rw-r--r--   0 runner    (1001) docker     (123)     1471 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/circuit_breaker/circuit_breaker.py
--rw-r--r--   0 runner    (1001) docker     (123)     2908 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/circuit_breaker/operation_circuit_breaker.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/corpgroup/
--rw-r--r--   0 runner    (1001) docker     (123)       63 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/corpgroup/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9873 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/corpgroup/corpgroup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/corpuser/
--rw-r--r--   0 runner    (1001) docker     (123)       60 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/corpuser/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5889 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/corpuser/corpuser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/datajob/
--rw-r--r--   0 runner    (1001) docker     (123)      116 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/datajob/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4265 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/datajob/dataflow.py
--rw-r--r--   0 runner    (1001) docker     (123)     7492 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/datajob/datajob.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/dataprocess/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/dataprocess/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14547 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/dataprocess/dataprocess_instance.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/graphql/
--rw-r--r--   0 runner    (1001) docker     (123)      104 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/graphql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2818 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/graphql/assertion.py
--rw-r--r--   0 runner    (1001) docker     (123)     1566 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/graphql/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5116 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/api/graphql/operation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2725 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/check_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)    24335 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/cli_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    16805 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/delete_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     7662 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/docker_check.py
--rw-r--r--   0 runner    (1001) docker     (123)    35178 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/docker_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     1431 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/get_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)    13161 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/ingest_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)      888 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/json_file.py
--rw-r--r--   0 runner    (1001) docker     (123)    12786 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/lite_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)    16497 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/migrate.py
--rw-r--r--   0 runner    (1001) docker     (123)     8936 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/migration_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3067 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/put_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     5509 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/quickstart_versioning.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/specific/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/specific/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1275 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/specific/file_loader.py
--rw-r--r--   0 runner    (1001) docker     (123)     1966 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/specific/group_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     1874 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/specific/user_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     1060 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/state_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)      489 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/telemetry.py
--rw-r--r--   0 runner    (1001) docker     (123)     7352 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/cli/timeline_cli.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/
--rw-r--r--   0 runner    (1001) docker     (123)      114 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      934 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/_config_enum.py
--rw-r--r--   0 runner    (1001) docker     (123)    10548 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/common.py
--rw-r--r--   0 runner    (1001) docker     (123)     3781 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/config_loader.py
--rw-r--r--   0 runner    (1001) docker     (123)     5594 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/git.py
--rw-r--r--   0 runner    (1001) docker     (123)      369 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/import_resolver.py
--rw-r--r--   0 runner    (1001) docker     (123)     2126 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/kafka.py
--rw-r--r--   0 runner    (1001) docker     (123)      476 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/pattern_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      768 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/pydantic_field_deprecation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2200 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/source_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     2363 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/time_window_config.py
--rw-r--r--   0 runner    (1001) docker     (123)      380 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/toml.py
--rw-r--r--   0 runner    (1001) docker     (123)      688 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/validate_field_removal.py
--rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/validate_field_rename.py
--rw-r--r--   0 runner    (1001) docker     (123)      867 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/validate_host_port.py
--rw-r--r--   0 runner    (1001) docker     (123)      301 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/configuration/yaml.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      292 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/aspect.py
--rw-r--r--   0 runner    (1001) docker     (123)     5761 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/kafka_emitter.py
--rw-r--r--   0 runner    (1001) docker     (123)    14740 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/mce_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     8579 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/mcp.py
--rw-r--r--   0 runner    (1001) docker     (123)     9185 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/mcp_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2523 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/mcp_patch_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)      706 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/request_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)    11246 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/rest_emitter.py
--rw-r--r--   0 runner    (1001) docker     (123)     3652 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/emitter/serialization_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)     6799 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/entrypoints.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      449 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/closeable.py
--rw-r--r--   0 runner    (1001) docker     (123)      886 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/committable.py
--rw-r--r--   0 runner    (1001) docker     (123)     2482 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/common.py
--rw-r--r--   0 runner    (1001) docker     (123)     3428 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/decorators.py
--rw-r--r--   0 runner    (1001) docker     (123)     1870 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/ingestion_job_checkpointing_provider_base.py
--rw-r--r--   0 runner    (1001) docker     (123)      608 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/pipeline_run_listener.py
--rw-r--r--   0 runner    (1001) docker     (123)     7150 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     4752 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/report.py
--rw-r--r--   0 runner    (1001) docker     (123)      994 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/report_helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)     4503 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/sink.py
--rw-r--r--   0 runner    (1001) docker     (123)     6074 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/source.py
--rw-r--r--   0 runner    (1001) docker     (123)      582 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     3316 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/workunit.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/extractor_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     1454 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/json_ref_patch.py
--rw-r--r--   0 runner    (1001) docker     (123)    24286 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/json_schema_util.py
--rw-r--r--   0 runner    (1001) docker     (123)     3649 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/mce_extractor.py
--rw-r--r--   0 runner    (1001) docker     (123)    13147 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/protobuf_util.py
--rw-r--r--   0 runner    (1001) docker     (123)    21985 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/schema_util.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/glossary/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/glossary/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8617 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/glossary/classification_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     2675 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/glossary/classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)      307 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/glossary/classifier_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     4813 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/glossary/datahub_classifier.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/graph/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    27718 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/graph/client.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/reporting/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/reporting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8506 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/reporting/datahub_ingestion_run_summary_provider.py
--rw-r--r--   0 runner    (1001) docker     (123)     1576 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/reporting/file_reporter.py
--rw-r--r--   0 runner    (1001) docker     (123)      310 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/reporting/reporting_provider_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/run/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/run/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1474 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/run/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    24179 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/run/pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3920 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/run/pipeline_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      557 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/blackhole.py
--rw-r--r--   0 runner    (1001) docker     (123)      591 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/console.py
--rw-r--r--   0 runner    (1001) docker     (123)     2838 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/datahub_kafka.py
--rw-r--r--   0 runner    (1001) docker     (123)     1991 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/datahub_lite.py
--rw-r--r--   0 runner    (1001) docker     (123)     8138 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/datahub_rest.py
--rw-r--r--   0 runner    (1001) docker     (123)     2743 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/file.py
--rw-r--r--   0 runner    (1001) docker     (123)      490 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/sink_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8283 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/aws_common.py
--rw-r--r--   0 runner    (1001) docker     (123)    47771 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/glue.py
--rw-r--r--   0 runner    (1001) docker     (123)     3892 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/s3_boto_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1694 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/s3_util.py
--rw-r--r--   0 runner    (1001) docker     (123)     3809 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1554 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    10383 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/feature_groups.py
--rw-r--r--   0 runner    (1001) docker     (123)    10165 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/job_classes.py
--rw-r--r--   0 runner    (1001) docker     (123)    35124 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/jobs.py
--rw-r--r--   0 runner    (1001) docker     (123)     9290 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    19207 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/azure/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/azure/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3706 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/azure/azure_common.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    51413 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/bigquery.py
--rw-r--r--   0 runner    (1001) docker     (123)    23560 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/bigquery_audit.py
--rw-r--r--   0 runner    (1001) docker     (123)    12861 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/bigquery_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4623 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/bigquery_report.py
--rw-r--r--   0 runner    (1001) docker     (123)    23505 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/bigquery_schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     1640 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    28666 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    12225 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)    37677 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/common/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1099 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/common/subtypes.py
--rw-r--r--   0 runner    (1001) docker     (123)    16634 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/confluent_schema_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)    26254 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/csv_enricher.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/data_lake_common/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/data_lake_common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      360 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/data_lake_common/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4599 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/data_lake_common/data_lake_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8707 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/data_lake_common/path_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/dbt/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/dbt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12796 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/dbt/dbt_cloud.py
--rw-r--r--   0 runner    (1001) docker     (123)    56371 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/dbt/dbt_common.py
--rw-r--r--   0 runner    (1001) docker     (123)    17244 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/dbt/dbt_core.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/delta_lake/
--rw-r--r--   0 runner    (1001) docker     (123)       71 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/delta_lake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3342 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/delta_lake/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2006 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/delta_lake/delta_lake_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      467 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/delta_lake/report.py
--rw-r--r--   0 runner    (1001) docker     (123)    12455 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/delta_lake/source.py
--rw-r--r--   0 runner    (1001) docker     (123)     1228 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/demo_data.py
--rw-r--r--   0 runner    (1001) docker     (123)    17398 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/elastic_search.py
--rw-r--r--   0 runner    (1001) docker     (123)    14671 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/feast.py
--rw-r--r--   0 runner    (1001) docker     (123)    16774 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/file.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/gcs/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/gcs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6703 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/gcs/gcs_source.py
--rw-r--r--   0 runner    (1001) docker     (123)      909 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/gcs/gcs_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    44577 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/ge_data_profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)     8830 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/ge_profiling_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/git/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/git/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2563 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/git/git_import.py
--rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/glue_profiling_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/iceberg/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/iceberg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    19281 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/iceberg/iceberg.py
--rw-r--r--   0 runner    (1001) docker     (123)     7823 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/iceberg/iceberg_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     9563 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/iceberg/iceberg_profiler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/identity/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/identity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    29532 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/identity/azure_ad.py
--rw-r--r--   0 runner    (1001) docker     (123)    32064 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/identity/okta.py
--rw-r--r--   0 runner    (1001) docker     (123)    17721 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/kafka.py
--rw-r--r--   0 runner    (1001) docker     (123)    46297 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/kafka_connect.py
--rw-r--r--   0 runner    (1001) docker     (123)      321 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/kafka_schema_registry_base.py
--rw-r--r--   0 runner    (1001) docker     (123)    17593 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/ldap.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    43879 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/looker_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     7536 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/looker_lib_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     2202 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/looker_query_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    53361 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/looker_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    24029 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/looker_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)    93092 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/lookml_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    22826 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/metabase.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/metadata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17202 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/metadata/business_glossary.py
--rw-r--r--   0 runner    (1001) docker     (123)     6717 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/metadata/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    30160 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/mode.py
--rw-r--r--   0 runner    (1001) docker     (123)    17143 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/mongodb.py
--rw-r--r--   0 runner    (1001) docker     (123)    43231 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/nifi.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    13322 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/openapi.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    13526 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/openapi_parser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/
--rw-r--r--   0 runner    (1001) docker     (123)       76 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16812 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2265 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/dataplatform_instance_resolver.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/data_classes.py
--rw-r--r--   0 runner    (1001) docker     (123)     1578 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/native_sql_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/parser.py
--rw-r--r--   0 runner    (1001) docker     (123)    31103 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/resolver.py
--rw-r--r--   0 runner    (1001) docker     (123)     6111 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/tree_function.py
--rw-r--r--   0 runner    (1001) docker     (123)     1052 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/validator.py
--rw-r--r--   0 runner    (1001) docker     (123)    17764 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/powerbi-lexical-grammar.rule
--rw-r--r--   0 runner    (1001) docker     (123)    45017 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/powerbi.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/rest_api_wrapper/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/rest_api_wrapper/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6558 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/rest_api_wrapper/data_classes.py
--rw-r--r--   0 runner    (1001) docker     (123)    29776 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/rest_api_wrapper/data_resolver.py
--rw-r--r--   0 runner    (1001) docker     (123)    16174 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/rest_api_wrapper/powerbi_api.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi_report_server/
--rw-r--r--   0 runner    (1001) docker     (123)      324 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi_report_server/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3689 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi_report_server/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)    20049 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi_report_server/report_server.py
--rw-r--r--   0 runner    (1001) docker     (123)    11684 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi_report_server/report_server_domain.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/profiling/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/profiling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/profiling/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    20413 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/pulsar.py
--rw-r--r--   0 runner    (1001) docker     (123)    32287 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redash.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      362 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/common.py
--rw-r--r--   0 runner    (1001) docker     (123)     5184 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/config.py
--rw-r--r--   0 runner    (1001) docker     (123)    17892 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     5550 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/profile.py
--rw-r--r--   0 runner    (1001) docker     (123)    17249 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/query.py
--rw-r--r--   0 runner    (1001) docker     (123)    33893 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/redshift.py
--rw-r--r--   0 runner    (1001) docker     (123)    12415 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/redshift_schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     1530 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/report.py
--rw-r--r--   0 runner    (1001) docker     (123)    15002 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/s3/
--rw-r--r--   0 runner    (1001) docker     (123)       56 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/s3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5095 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/s3/config.py
--rw-r--r--   0 runner    (1001) docker     (123)    20777 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/s3/profiling.py
--rw-r--r--   0 runner    (1001) docker     (123)      466 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/s3/report.py
--rw-r--r--   0 runner    (1001) docker     (123)    33367 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/s3/source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sagemaker_processors/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sagemaker_processors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    28464 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/salesforce.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15812 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema/json_schema.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      563 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/avro.py
--rw-r--r--   0 runner    (1001) docker     (123)      379 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2229 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/csv_tsv.py
--rw-r--r--   0 runner    (1001) docker     (123)     2103 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/json.py
--rw-r--r--   0 runner    (1001) docker     (123)     5760 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/object.py
--rw-r--r--   0 runner    (1001) docker     (123)     3426 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/parquet.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     8229 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_config.py
--rw-r--r--   0 runner    (1001) docker     (123)    28794 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_lineage_legacy.py
--rw-r--r--   0 runner    (1001) docker     (123)    22090 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_lineage_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)     6966 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)    35365 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_query.py
--rw-r--r--   0 runner    (1001) docker     (123)     3784 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_report.py
--rw-r--r--   0 runner    (1001) docker     (123)    19448 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     6141 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_tag.py
--rw-r--r--   0 runner    (1001) docker     (123)    18561 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_usage_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)    10735 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    62420 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)     1313 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/source_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9739 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/athena.py
--rw-r--r--   0 runner    (1001) docker     (123)    25177 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/clickhouse.py
--rw-r--r--   0 runner    (1001) docker     (123)     2346 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/druid.py
--rw-r--r--   0 runner    (1001) docker     (123)     1342 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/hana.py
--rw-r--r--   0 runner    (1001) docker     (123)     6481 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/hive.py
--rw-r--r--   0 runner    (1001) docker     (123)      737 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/mariadb.py
--rw-r--r--   0 runner    (1001) docker     (123)    11028 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/mssql.py
--rw-r--r--   0 runner    (1001) docker     (123)     2650 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/mysql.py
--rw-r--r--   0 runner    (1001) docker     (123)     2316 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/oauth_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     6923 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/oracle.py
--rw-r--r--   0 runner    (1001) docker     (123)    10999 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/postgres.py
--rw-r--r--   0 runner    (1001) docker     (123)     3606 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/presto.py
--rw-r--r--   0 runner    (1001) docker     (123)    33112 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/presto_on_hive.py
--rw-r--r--   0 runner    (1001) docker     (123)    45796 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/redshift.py
--rw-r--r--   0 runner    (1001) docker     (123)    44223 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     7105 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2731 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_generic.py
--rw-r--r--   0 runner    (1001) docker     (123)     6881 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_generic_profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)    11475 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_types.py
--rw-r--r--   0 runner    (1001) docker     (123)     7629 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    10551 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/trino.py
--rw-r--r--   0 runner    (1001) docker     (123)     4937 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/two_tier_sql_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    53109 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/vertica.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8797 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (123)     4220 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/entity_removal_state.py
--rw-r--r--   0 runner    (1001) docker     (123)      512 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/profiling_state.py
--rw-r--r--   0 runner    (1001) docker     (123)     4209 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/profiling_state_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     5544 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/redundant_run_skip_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)      143 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/sql_common_state.py
--rw-r--r--   0 runner    (1001) docker     (123)    13854 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/stale_entity_removal_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    15642 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/stateful_ingestion_base.py
--rw-r--r--   0 runner    (1001) docker     (123)      463 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/usage_common_state.py
--rw-r--r--   0 runner    (1001) docker     (123)     1271 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/use_case_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state_provider/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state_provider/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5226 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state_provider/datahub_ingestion_checkpointing_provider.py
--rw-r--r--   0 runner    (1001) docker     (123)      401 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state_provider/state_provider_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)    16218 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/superset.py
--rw-r--r--   0 runner    (1001) docker     (123)    93019 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/tableau.py
--rw-r--r--   0 runner    (1001) docker     (123)    15605 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/tableau_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     2284 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/tableau_constant.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6631 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4683 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)    14749 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/proxy.py
--rw-r--r--   0 runner    (1001) docker     (123)     8986 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/proxy_profiling.py
--rw-r--r--   0 runner    (1001) docker     (123)     5425 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/proxy_types.py
--rw-r--r--   0 runner    (1001) docker     (123)     1529 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/report.py
--rw-r--r--   0 runner    (1001) docker     (123)    25309 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/source.py
--rw-r--r--   0 runner    (1001) docker     (123)     8824 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/usage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/usage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9827 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/usage/clickhouse_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)    15306 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/usage/redshift_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)    10155 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/usage/starburst_trino_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     8442 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/usage/usage_common.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1654 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/bigquery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/csv_enricher.py
--rw-r--r--   0 runner    (1001) docker     (123)     5615 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/pulsar.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/sql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/sql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16134 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/sql/snowflake.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/usage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/usage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7702 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/usage/bigquery_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)      565 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/usage/snowflake_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1037 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/pulsar.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/sql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/sql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/sql/bigquery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1331 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/sql/snowflake.py
--rw-r--r--   0 runner    (1001) docker     (123)      229 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/time_window.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/usage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/usage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1374 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/usage/bigquery_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)      780 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/usage/snowflake_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3420 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_browse_path.py
--rw-r--r--   0 runner    (1001) docker     (123)     6849 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_ownership.py
--rw-r--r--   0 runner    (1001) docker     (123)     5605 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_properties.py
--rw-r--r--   0 runner    (1001) docker     (123)     5664 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_schema_tags.py
--rw-r--r--   0 runner    (1001) docker     (123)     6239 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_schema_terms.py
--rw-r--r--   0 runner    (1001) docker     (123)     4911 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_tags.py
--rw-r--r--   0 runner    (1001) docker     (123)     5707 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_terms.py
--rw-r--r--   0 runner    (1001) docker     (123)    10623 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/base_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6241 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/dataset_domain.py
--rw-r--r--   0 runner    (1001) docker     (123)     1598 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/dataset_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1323 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/mark_dataset_status.py
--rw-r--r--   0 runner    (1001) docker     (123)     1218 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/remove_dataset_ownership.py
--rw-r--r--   0 runner    (1001) docker     (123)      251 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/transform_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/integrations/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/integrations/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/integrations/great_expectations/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/integrations/great_expectations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    34843 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/integrations/great_expectations/action.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/lite/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/lite/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    32549 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/lite/duckdb_lite.py
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/lite/duckdb_lite_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2846 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/lite/lite_local.py
--rw-r--r--   0 runner    (1001) docker     (123)      286 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/lite/lite_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     1949 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/lite/lite_server.py
--rw-r--r--   0 runner    (1001) docker     (123)     4503 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/lite/lite_util.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)      197 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/events/
--rw-r--r--   0 runner    (1001) docker     (123)      257 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/events/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/access/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/access/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/access/token/
--rw-r--r--   0 runner    (1001) docker     (123)      277 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/access/token/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/
--rw-r--r--   0 runner    (1001) docker     (123)     1589 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/chart/
--rw-r--r--   0 runner    (1001) docker     (123)      807 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/chart/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/common/
--rw-r--r--   0 runner    (1001) docker     (123)     3634 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/common/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/common/fieldtransformer/
--rw-r--r--   0 runner    (1001) docker     (123)      355 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/common/fieldtransformer/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/container/
--rw-r--r--   0 runner    (1001) docker     (123)      469 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/container/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/
--rw-r--r--   0 runner    (1001) docker     (123)      615 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/
--rw-r--r--   0 runner    (1001) docker     (123)      828 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/azkaban/
--rw-r--r--   0 runner    (1001) docker     (123)      253 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/azkaban/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/
--rw-r--r--   0 runner    (1001) docker     (123)      535 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatform/
--rw-r--r--   0 runner    (1001) docker     (123)      341 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatform/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatforminstance/
--rw-r--r--   0 runner    (1001) docker     (123)      300 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatforminstance/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/
--rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/
--rw-r--r--   0 runner    (1001) docker     (123)     2204 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/domain/
--rw-r--r--   0 runner    (1001) docker     (123)      326 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/domain/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/events/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/events/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/events/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)      241 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/events/metadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/execution/
--rw-r--r--   0 runner    (1001) docker     (123)      734 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/execution/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/glossary/
--rw-r--r--   0 runner    (1001) docker     (123)      460 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/glossary/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/identity/
--rw-r--r--   0 runner    (1001) docker     (123)     1443 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/identity/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/
--rw-r--r--   0 runner    (1001) docker     (123)      556 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/
--rw-r--r--   0 runner    (1001) docker     (123)     3859 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/filter/
--rw-r--r--   0 runner    (1001) docker     (123)      491 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/filter/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/
--rw-r--r--   0 runner    (1001) docker     (123)     2317 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/ml/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/ml/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)     3151 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/
--rw-r--r--   0 runner    (1001) docker     (123)      932 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/
--rw-r--r--   0 runner    (1001) docker     (123)      860 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/v1/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/policy/
--rw-r--r--   0 runner    (1001) docker     (123)      876 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/policy/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/post/
--rw-r--r--   0 runner    (1001) docker     (123)      477 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/post/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/query/
--rw-r--r--   0 runner    (1001) docker     (123)      679 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/query/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/retention/
--rw-r--r--   0 runner    (1001) docker     (123)      561 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/retention/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/schema/
--rw-r--r--   0 runner    (1001) docker     (123)     2822 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/schema/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/secret/
--rw-r--r--   0 runner    (1001) docker     (123)      264 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/secret/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/settings/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/settings/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/settings/global/
--rw-r--r--   0 runner    (1001) docker     (123)      370 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/settings/global/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/step/
--rw-r--r--   0 runner    (1001) docker     (123)      288 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/step/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/tag/
--rw-r--r--   0 runner    (1001) docker     (123)      249 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/tag/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/telemetry/
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/telemetry/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/test/
--rw-r--r--   0 runner    (1001) docker     (123)      670 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/test/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/
--rw-r--r--   0 runner    (1001) docker     (123)      596 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/upgrade/
--rw-r--r--   0 runner    (1001) docker     (123)      380 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/upgrade/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/usage/
--rw-r--r--   0 runner    (1001) docker     (123)      561 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/usage/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/view/
--rw-r--r--   0 runner    (1001) docker     (123)      457 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/view/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)   433112 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schema.avsc
--rw-r--r--   0 runner    (1001) docker     (123)   666268 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schema_classes.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schemas/
--rw-r--r--   0 runner    (1001) docker     (123)   334535 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schemas/MetadataChangeEvent.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     8438 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schemas/MetadataChangeProposal.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      540 2023-06-05 18:18:58.000000 acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schemas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/py.typed
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/specific/
--rw-r--r--   0 runner    (1001) docker     (123)       88 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/specific/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      994 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/specific/custom_properties.py
--rw-r--r--   0 runner    (1001) docker     (123)     7642 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/specific/dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/telemetry/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/telemetry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      967 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/telemetry/stats.py
--rw-r--r--   0 runner    (1001) docker     (123)    11734 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/telemetry/telemetry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/upgrade/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/upgrade/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15375 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/upgrade/upgrade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      444 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/_markupsafe_compat.py
--rw-r--r--   0 runner    (1001) docker     (123)     3147 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/bigquery_sql_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1138 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/checkpoint_state_util.py
--rw-r--r--   0 runner    (1001) docker     (123)      459 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/config_clean.py
--rw-r--r--   0 runner    (1001) docker     (123)      468 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/dedup_list.py
--rw-r--r--   0 runner    (1001) docker     (123)      645 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/delayed_iter.py
--rw-r--r--   0 runner    (1001) docker     (123)    15987 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/file_backed_collections.py
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/global_warning_util.py
--rw-r--r--   0 runner    (1001) docker     (123)    10998 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/hive_schema_to_avro.py
--rw-r--r--   0 runner    (1001) docker     (123)     6609 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/logging_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     4614 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/lossy_collections.py
--rw-r--r--   0 runner    (1001) docker     (123)    10590 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     1564 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/memory_footprint.py
--rw-r--r--   0 runner    (1001) docker     (123)     3933 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/memory_leak_detector.py
--rw-r--r--   0 runner    (1001) docker     (123)      598 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/parsing_util.py
--rw-r--r--   0 runner    (1001) docker     (123)     1097 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/perf_timer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/registries/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/registries/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2452 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/registries/domain_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)      766 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sample_data.py
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/server_config_util.py
--rw-r--r--   0 runner    (1001) docker     (123)     4779 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/source_helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)      871 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sql_formatter.py
--rw-r--r--   0 runner    (1001) docker     (123)     6522 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sql_lineage_parser_impl.py
--rw-r--r--   0 runner    (1001) docker     (123)     5814 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sql_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)      456 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sql_parser_base.py
--rw-r--r--   0 runner    (1001) docker     (123)    14786 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sqlalchemy_query_combiner.py
--rw-r--r--   0 runner    (1001) docker     (123)     1990 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sqllineage_patch.py
--rw-r--r--   0 runner    (1001) docker     (123)     1390 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/stats_collections.py
--rw-r--r--   0 runner    (1001) docker     (123)      601 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/tee_io.py
--rw-r--r--   0 runner    (1001) docker     (123)      358 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/time.py
--rw-r--r--   0 runner    (1001) docker     (123)      970 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/type_annotations.py
--rw-r--r--   0 runner    (1001) docker     (123)      172 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/url_util.py
--rw-r--r--   0 runner    (1001) docker     (123)      984 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urn_encoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/corp_group_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/corpuser_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     2847 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/data_flow_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/data_job_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1097 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/data_platform_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/data_process_instance_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     3877 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/dataset_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1292 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/domain_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)       98 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/error.py
--rw-r--r--   0 runner    (1001) docker     (123)     1533 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/notebook_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1247 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/tag_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     5447 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     4176 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/urn_iter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/
--rw-r--r--   0 runner    (1001) docker     (123)     1073 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      291 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/_airflow_compat.py
--rw-r--r--   0 runner    (1001) docker     (123)      844 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/_airflow_shims.py
--rw-r--r--   0 runner    (1001) docker     (123)     3564 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/_lineage_core.py
--rw-r--r--   0 runner    (1001) docker     (123)    12902 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/_plugin.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/client/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/client/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    19475 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/client/airflow_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)      994 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/entities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1319 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/generic_recipe_sample_dag.py
--rw-r--r--   0 runner    (1001) docker     (123)     1348 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/lineage_backend_demo.py
--rw-r--r--   0 runner    (1001) docker     (123)     1414 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py
--rw-r--r--   0 runner    (1001) docker     (123)     2283 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/lineage_emission_dag.py
--rw-r--r--   0 runner    (1001) docker     (123)     1922 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/mysql_sample_dag.py
--rw-r--r--   0 runner    (1001) docker     (123)     3234 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/snowflake_sample_dag.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/hooks/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7231 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/hooks/datahub.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/lineage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/lineage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3286 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/lineage/datahub.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-05 18:19:02.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/datahub.py
--rw-r--r--   0 runner    (1001) docker     (123)     2900 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/datahub_assertion_operator.py
--rw-r--r--   0 runner    (1001) docker     (123)     2903 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/datahub_assertion_sensor.py
--rw-r--r--   0 runner    (1001) docker     (123)     3338 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/datahub_operation_operator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3606 2023-06-05 18:17:00.000000 acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/datahub_operation_sensor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/
+-rw-r--r--   0 runner    (1001) docker     (123)    15538 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    10870 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)      928 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)     2189 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)    27001 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    15538 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    24526 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     6651 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)    33655 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       25 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      106 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/
+-rw-r--r--   0 runner    (1001) docker     (123)      268 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5324 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/assertion_circuit_breaker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1471 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/circuit_breaker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2908 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/operation_circuit_breaker.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpgroup/
+-rw-r--r--   0 runner    (1001) docker     (123)       63 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpgroup/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9873 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpgroup/corpgroup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpuser/
+-rw-r--r--   0 runner    (1001) docker     (123)       60 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpuser/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5889 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpuser/corpuser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/
+-rw-r--r--   0 runner    (1001) docker     (123)      116 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4265 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/dataflow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7492 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/datajob.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/dataprocess/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/dataprocess/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14547 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/dataprocess/dataprocess_instance.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/
+-rw-r--r--   0 runner    (1001) docker     (123)      104 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2818 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/assertion.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1566 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5116 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/operation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2725 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/check_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24292 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/cli_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16533 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/delete_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7362 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/docker_check.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34107 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/docker_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1431 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/get_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13132 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/ingest_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)      888 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/json_file.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12786 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/lite_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16454 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/migrate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8936 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/migration_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3067 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/put_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5509 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/quickstart_versioning.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1275 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/file_loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1966 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/group_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1874 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/user_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1819 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/state_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)      489 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/telemetry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7352 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/timeline_cli.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/
+-rw-r--r--   0 runner    (1001) docker     (123)      114 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      934 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/_config_enum.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10548 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3770 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/config_loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5594 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/git.py
+-rw-r--r--   0 runner    (1001) docker     (123)      369 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/import_resolver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2105 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/kafka.py
+-rw-r--r--   0 runner    (1001) docker     (123)      386 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/pattern_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      768 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/pydantic_field_deprecation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2200 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/source_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2363 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/time_window_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)      380 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/toml.py
+-rw-r--r--   0 runner    (1001) docker     (123)      688 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_field_removal.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_field_rename.py
+-rw-r--r--   0 runner    (1001) docker     (123)      867 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_host_port.py
+-rw-r--r--   0 runner    (1001) docker     (123)      301 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/yaml.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      292 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/aspect.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5761 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/kafka_emitter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14740 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mce_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8579 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9187 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2523 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp_patch_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)      706 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/request_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11252 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/rest_emitter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3652 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/serialization_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6799 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/entrypoints.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      449 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/closeable.py
+-rw-r--r--   0 runner    (1001) docker     (123)      886 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/committable.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2753 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3428 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1870 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/ingestion_job_checkpointing_provider_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      608 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/pipeline_run_listener.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7150 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4752 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)      994 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/report_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4503 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/sink.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6074 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/source.py
+-rw-r--r--   0 runner    (1001) docker     (123)      582 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3316 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/workunit.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/extractor_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1454 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/json_ref_patch.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24286 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/json_schema_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3649 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/mce_extractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13147 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/protobuf_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21985 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/schema_util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6189 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/classification_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2675 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)      307 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/classifier_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4813 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/datahub_classifier.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/graph/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20384 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/graph/client.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8506 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/datahub_ingestion_run_summary_provider.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1576 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/file_reporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      310 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/reporting_provider_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1474 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24179 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3920 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/pipeline_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      557 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/blackhole.py
+-rw-r--r--   0 runner    (1001) docker     (123)      591 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/console.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2838 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_kafka.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1991 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_lite.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8138 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_rest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2743 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/file.py
+-rw-r--r--   0 runner    (1001) docker     (123)      490 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/sink_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8114 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/aws_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    47697 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/glue.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8485 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/path_spec.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3892 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/s3_boto_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1694 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/s3_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3809 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1554 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10383 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/feature_groups.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10165 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/job_classes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35124 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/jobs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9290 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19207 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/azure/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/azure/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3706 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/azure/azure_common.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    51101 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24168 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_audit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12893 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4316 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23505 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1640 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28643 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12225 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36592 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/common/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      980 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/common/subtypes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16833 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/confluent_schema_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26249 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/csv_enricher.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12796 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_cloud.py
+-rw-r--r--   0 runner    (1001) docker     (123)    56247 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17244 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_core.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/
+-rw-r--r--   0 runner    (1001) docker     (123)       71 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3342 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2006 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/delta_lake_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      467 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12467 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1228 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/demo_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17398 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/elastic_search.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14671 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/feast.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16770 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/file.py
+-rw-r--r--   0 runner    (1001) docker     (123)    44577 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ge_data_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8830 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ge_profiling_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/git/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/git/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2563 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/git/git_import.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/glue_profiling_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19281 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7823 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9563 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg_profiler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29532 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/azure_ad.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32064 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/okta.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17692 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/kafka.py
+-rw-r--r--   0 runner    (1001) docker     (123)    46297 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/kafka_connect.py
+-rw-r--r--   0 runner    (1001) docker     (123)      321 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/kafka_schema_registry_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17596 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ldap.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43834 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7480 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_lib_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2202 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_query_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    53361 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24029 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    81876 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/lookml_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22826 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metabase.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16789 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/business_glossary.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6717 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30160 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/mode.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17143 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/mongodb.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43231 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/nifi.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13322 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/openapi.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13526 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/openapi_parser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/
+-rw-r--r--   0 runner    (1001) docker     (123)       76 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13454 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2265 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/dataplatform_instance_resolver.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/data_classes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1578 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/native_sql_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29227 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/resolver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6111 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/tree_function.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1052 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/validator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17764 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/powerbi-lexical-grammar.rule
+-rw-r--r--   0 runner    (1001) docker     (123)    35205 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/powerbi.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4290 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/data_classes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27836 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/data_resolver.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13689 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/powerbi_api.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/
+-rw-r--r--   0 runner    (1001) docker     (123)      324 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3689 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20049 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/report_server.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11684 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/report_server_domain.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/profiling/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/profiling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/profiling/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20413 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/pulsar.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32287 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redash.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      362 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5163 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17491 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5607 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/profile.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17388 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/query.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34186 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/redshift.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12416 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/redshift_schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1389 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15002 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/
+-rw-r--r--   0 runner    (1001) docker     (123)       56 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5167 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4126 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/data_lake_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20777 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/profiling.py
+-rw-r--r--   0 runner    (1001) docker     (123)      466 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32051 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sagemaker_processors/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sagemaker_processors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27170 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/salesforce.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15812 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema/json_schema.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      563 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/avro.py
+-rw-r--r--   0 runner    (1001) docker     (123)      379 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2229 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/csv_tsv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2103 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/json.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5760 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/object.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3426 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/parquet.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7116 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29016 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_lineage_legacy.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21942 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_lineage_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6966 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33575 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_query.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3665 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19448 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6141 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_tag.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18561 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_usage_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10596 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    62420 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1313 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/source_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9739 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/athena.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25177 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/clickhouse.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2346 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/druid.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1342 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/hana.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6454 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/hive.py
+-rw-r--r--   0 runner    (1001) docker     (123)      737 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mariadb.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10973 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mssql.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2650 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mysql.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2316 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/oauth_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6923 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/oracle.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10476 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/postgres.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3606 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/presto.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33112 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/presto_on_hive.py
+-rw-r--r--   0 runner    (1001) docker     (123)    45775 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/redshift.py
+-rw-r--r--   0 runner    (1001) docker     (123)    44223 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7105 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2731 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_generic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6881 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_generic_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11475 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7629 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10551 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/trino.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4936 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/two_tier_sql_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)    53109 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/vertica.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8797 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4220 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/entity_removal_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)      512 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/profiling_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4209 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/profiling_state_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5544 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/redundant_run_skip_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)      143 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/sql_common_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13129 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/stale_entity_removal_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15642 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/stateful_ingestion_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      463 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/usage_common_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1271 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/use_case_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state_provider/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state_provider/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5226 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state_provider/datahub_ingestion_checkpointing_provider.py
+-rw-r--r--   0 runner    (1001) docker     (123)      401 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state_provider/state_provider_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14641 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/superset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    91756 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/tableau.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15651 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/tableau_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2284 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/tableau_constant.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3164 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11625 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/proxy.py
+-rw-r--r--   0 runner    (1001) docker     (123)      585 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19523 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9827 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/clickhouse_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15306 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/redshift_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10155 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/starburst_trino_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6281 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/usage_common.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1654 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/bigquery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/csv_enricher.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5615 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/pulsar.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/sql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/sql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16134 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/sql/snowflake.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7702 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/bigquery_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)      565 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/snowflake_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1037 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/pulsar.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/bigquery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1331 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/snowflake.py
+-rw-r--r--   0 runner    (1001) docker     (123)      229 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/time_window.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1374 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/bigquery_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)      780 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/snowflake_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3420 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_browse_path.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6849 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_ownership.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5605 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_properties.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5664 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_schema_tags.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6239 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_schema_terms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4911 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_tags.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5707 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_terms.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10623 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/base_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6241 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/dataset_domain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1598 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/dataset_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1323 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/mark_dataset_status.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1218 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/remove_dataset_ownership.py
+-rw-r--r--   0 runner    (1001) docker     (123)      251 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/transform_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/great_expectations/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/great_expectations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34843 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/great_expectations/action.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32549 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/duckdb_lite.py
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/duckdb_lite_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2846 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_local.py
+-rw-r--r--   0 runner    (1001) docker     (123)      286 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1949 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_server.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4503 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)      197 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/events/
+-rw-r--r--   0 runner    (1001) docker     (123)      257 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/events/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/access/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/access/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/access/token/
+-rw-r--r--   0 runner    (1001) docker     (123)      277 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/access/token/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/
+-rw-r--r--   0 runner    (1001) docker     (123)     1589 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/chart/
+-rw-r--r--   0 runner    (1001) docker     (123)      807 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/chart/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/common/
+-rw-r--r--   0 runner    (1001) docker     (123)     3456 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/common/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/common/fieldtransformer/
+-rw-r--r--   0 runner    (1001) docker     (123)      355 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/common/fieldtransformer/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/container/
+-rw-r--r--   0 runner    (1001) docker     (123)      469 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/container/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/
+-rw-r--r--   0 runner    (1001) docker     (123)      615 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/
+-rw-r--r--   0 runner    (1001) docker     (123)      828 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/azkaban/
+-rw-r--r--   0 runner    (1001) docker     (123)      253 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/azkaban/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/
+-rw-r--r--   0 runner    (1001) docker     (123)      535 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatform/
+-rw-r--r--   0 runner    (1001) docker     (123)      341 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatform/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatforminstance/
+-rw-r--r--   0 runner    (1001) docker     (123)      300 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatforminstance/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/
+-rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/
+-rw-r--r--   0 runner    (1001) docker     (123)     2204 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/domain/
+-rw-r--r--   0 runner    (1001) docker     (123)      326 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/domain/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/events/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/events/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/events/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)      241 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/events/metadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/execution/
+-rw-r--r--   0 runner    (1001) docker     (123)      734 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/execution/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/glossary/
+-rw-r--r--   0 runner    (1001) docker     (123)      460 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/glossary/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/identity/
+-rw-r--r--   0 runner    (1001) docker     (123)     1443 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/identity/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/
+-rw-r--r--   0 runner    (1001) docker     (123)      556 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/
+-rw-r--r--   0 runner    (1001) docker     (123)     3859 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/filter/
+-rw-r--r--   0 runner    (1001) docker     (123)      491 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/filter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/
+-rw-r--r--   0 runner    (1001) docker     (123)     2317 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ml/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ml/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)     3151 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/
+-rw-r--r--   0 runner    (1001) docker     (123)      932 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/
+-rw-r--r--   0 runner    (1001) docker     (123)      860 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/v1/
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/v1/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/policy/
+-rw-r--r--   0 runner    (1001) docker     (123)      876 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/policy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/post/
+-rw-r--r--   0 runner    (1001) docker     (123)      477 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/post/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/query/
+-rw-r--r--   0 runner    (1001) docker     (123)      679 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/query/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/retention/
+-rw-r--r--   0 runner    (1001) docker     (123)      561 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/retention/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/schema/
+-rw-r--r--   0 runner    (1001) docker     (123)     2822 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/schema/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/secret/
+-rw-r--r--   0 runner    (1001) docker     (123)      264 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/secret/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/settings/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/settings/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/settings/global/
+-rw-r--r--   0 runner    (1001) docker     (123)      370 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/settings/global/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/step/
+-rw-r--r--   0 runner    (1001) docker     (123)      288 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/step/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/tag/
+-rw-r--r--   0 runner    (1001) docker     (123)      249 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/tag/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/telemetry/
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/telemetry/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/test/
+-rw-r--r--   0 runner    (1001) docker     (123)      670 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/test/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/
+-rw-r--r--   0 runner    (1001) docker     (123)      596 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/upgrade/
+-rw-r--r--   0 runner    (1001) docker     (123)      380 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/upgrade/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/usage/
+-rw-r--r--   0 runner    (1001) docker     (123)      561 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/usage/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/view/
+-rw-r--r--   0 runner    (1001) docker     (123)      457 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/view/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)   421524 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schema.avsc
+-rw-r--r--   0 runner    (1001) docker     (123)   788555 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schema_classes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/
+-rw-r--r--   0 runner    (1001) docker     (123)   333312 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/MetadataChangeEvent.avsc
+-rw-r--r--   0 runner    (1001) docker     (123)     8438 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/MetadataChangeProposal.avsc
+-rw-r--r--   0 runner    (1001) docker     (123)      540 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/py.typed
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/specific/
+-rw-r--r--   0 runner    (1001) docker     (123)       88 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/specific/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      994 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/specific/custom_properties.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7642 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/specific/dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      967 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/stats.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11734 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/telemetry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/upgrade/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/upgrade/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15375 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/upgrade/upgrade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      444 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/_markupsafe_compat.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3147 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/bigquery_sql_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1138 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/checkpoint_state_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      459 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/config_clean.py
+-rw-r--r--   0 runner    (1001) docker     (123)      468 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/dedup_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)      645 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/delayed_iter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15335 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/file_backed_collections.py
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/global_warning_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10998 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/hive_schema_to_avro.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6609 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/logging_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4614 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/lossy_collections.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10590 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1564 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/memory_footprint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3933 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/memory_leak_detector.py
+-rw-r--r--   0 runner    (1001) docker     (123)      598 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/parsing_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1097 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/perf_timer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/registries/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/registries/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2452 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/registries/domain_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)      766 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sample_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/server_config_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4779 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/source_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)      871 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_formatter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6522 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_lineage_parser_impl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5814 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)      456 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_parser_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14786 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sqlalchemy_query_combiner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1990 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sqllineage_patch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1390 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/stats_collections.py
+-rw-r--r--   0 runner    (1001) docker     (123)      601 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/tee_io.py
+-rw-r--r--   0 runner    (1001) docker     (123)      358 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/time.py
+-rw-r--r--   0 runner    (1001) docker     (123)      970 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/type_annotations.py
+-rw-r--r--   0 runner    (1001) docker     (123)      172 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/url_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      984 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urn_encoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/corp_group_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/corpuser_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2847 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_flow_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_job_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1097 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_platform_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_process_instance_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3877 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/dataset_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1292 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/domain_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)       98 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/error.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1533 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/notebook_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1247 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/tag_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5447 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4176 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/urn_iter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/
+-rw-r--r--   0 runner    (1001) docker     (123)     1073 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      291 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_airflow_compat.py
+-rw-r--r--   0 runner    (1001) docker     (123)      844 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_airflow_shims.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3564 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_lineage_core.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12902 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_plugin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/client/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/client/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19475 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/client/airflow_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)      994 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/entities.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1319 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/generic_recipe_sample_dag.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1348 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_backend_demo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1414 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2283 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_emission_dag.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1922 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/mysql_sample_dag.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3234 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/snowflake_sample_dag.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6980 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/hooks/datahub.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/lineage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/lineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3286 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/lineage/datahub.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2900 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_assertion_operator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2903 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_assertion_sensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3338 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_operation_operator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3606 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_operation_sensor.py
```

### Comparing `acryl-datahub-tc-0.10.2.3/PKG-INFO` & `acryl-datahub-tc-0.10.2rc1/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,18 @@
 Metadata-Version: 2.1
 Name: acryl-datahub-tc
-Version: 0.10.2.3
+Version: 0.10.2rc1
 Summary: A CLI to work with DataHub metadata
 Home-page: https://datahubproject.io/
 License: Apache License 2.0
 Project-URL: Documentation, https://datahubproject.io/docs/
 Project-URL: Source, https://github.com/datahub-project/datahub
 Project-URL: Changelog, https://github.com/datahub-project/datahub/releases
 Description: # Introduction to Metadata Ingestion
         
-        <a
-            className='button button--primary button--lg'
-            href="https://datahubproject.io/integrations">
-            Find Integration Source
-        </a>
-        
         ## Integration Options
         
         DataHub supports both **push-based** and **pull-based** metadata integration.
         
         Push-based integrations allow you to emit metadata directly from your data systems when metadata changes, while pull-based integrations allow you to "crawl" or "ingest" metadata from the data systems by connecting to them and extracting metadata in a batch or incremental-batch manner. Supporting both mechanisms means that you can integrate with all your systems in the most flexible way possible.
         
         Examples of push-based integrations include [Airflow](../docs/lineage/airflow.md), [Spark](../metadata-integration/java/spark-lineage/README.md), [Great Expectations](./integration_docs/great-expectations.md) and [Protobuf Schemas](../metadata-integration/java/datahub-protobuf/README.md). This allows you to get low-latency metadata integration from the "active" agents in your data ecosystem. Examples of pull-based integrations include BigQuery, Snowflake, Looker, Tableau and many others.
@@ -282,15 +276,14 @@
 Provides-Extra: presto-on-hive
 Provides-Extra: pulsar
 Provides-Extra: redash
 Provides-Extra: redshift
 Provides-Extra: redshift-legacy
 Provides-Extra: redshift-usage-legacy
 Provides-Extra: s3
-Provides-Extra: gcs
 Provides-Extra: sagemaker
 Provides-Extra: salesforce
 Provides-Extra: snowflake
 Provides-Extra: snowflake-beta
 Provides-Extra: sqlalchemy
 Provides-Extra: superset
 Provides-Extra: tableau
```

### Comparing `acryl-datahub-tc-0.10.2.3/README.md` & `acryl-datahub-tc-0.10.2rc1/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,9 @@
 # Introduction to Metadata Ingestion
 
-<a
-    className='button button--primary button--lg'
-    href="https://datahubproject.io/integrations">
-    Find Integration Source
-</a>
-
 ## Integration Options
 
 DataHub supports both **push-based** and **pull-based** metadata integration.
 
 Push-based integrations allow you to emit metadata directly from your data systems when metadata changes, while pull-based integrations allow you to "crawl" or "ingest" metadata from the data systems by connecting to them and extracting metadata in a batch or incremental-batch manner. Supporting both mechanisms means that you can integrate with all your systems in the most flexible way possible.
 
 Examples of push-based integrations include [Airflow](../docs/lineage/airflow.md), [Spark](../metadata-integration/java/spark-lineage/README.md), [Great Expectations](./integration_docs/great-expectations.md) and [Protobuf Schemas](../metadata-integration/java/datahub-protobuf/README.md). This allows you to get low-latency metadata integration from the "active" agents in your data ecosystem. Examples of pull-based integrations include BigQuery, Snowflake, Looker, Tableau and many others.
```

### Comparing `acryl-datahub-tc-0.10.2.3/pyproject.toml` & `acryl-datahub-tc-0.10.2rc1/pyproject.toml`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/setup.cfg` & `acryl-datahub-tc-0.10.2rc1/setup.cfg`

 * *Files 8% similar despite different names*

```diff
@@ -65,17 +65,19 @@
 addopts = --cov=src --cov-report= --cov-config setup.cfg --strict-markers
 markers = 
 	airflow: marks tests related to airflow (deselect with '-m not airflow')
 	slow_unit: marks tests to only run slow unit tests (deselect with '-m not slow_unit')
 	integration: marks tests to only run in integration (deselect with '-m "not integration"')
 	integration_batch_1: mark tests to only run in batch 1 of integration tests. This is done mainly for parallelisation (deselect with '-m not integration_batch_1')
 	slow_integration: marks tests that are too slow to even run in integration (deselect with '-m "not slow_integration"')
+	performance: marks tests that are sparingly run to measure performance (deselect with '-m "not performance"')
 testpaths = 
 	tests/unit
 	tests/integration
+	tests/performance
 
 [coverage:run]
 
 [coverage:paths]
 source = 
 	src
 	*/site-packages
```

### Comparing `acryl-datahub-tc-0.10.2.3/setup.py` & `acryl-datahub-tc-0.10.2rc1/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,16 +21,15 @@
     # Typing extension should be >=3.10.0.2 ideally but we can't restrict due to Airflow 2.0.2 dependency conflict
     "typing_extensions>=3.7.4.3 ;  python_version < '3.8'",
     "typing_extensions>=3.10.0.2 ;  python_version >= '3.8'",
     "mypy_extensions>=0.4.3",
     # Actual dependencies.
     "typing-inspect",
     # pydantic 1.10.3 is incompatible with typing-extensions 4.1.1 - https://github.com/pydantic/pydantic/issues/4885
-    # pydantic 2 makes major, backwards-incompatible changes - https://github.com/pydantic/pydantic/issues/4887
-    "pydantic>=1.5.1,!=1.10.3,<2",
+    "pydantic>=1.5.1,!=1.10.3",
     "mixpanel>=4.9.0",
 }
 
 framework_common = {
     "click>=7.1.2",
     "click-default-group",
     "PyYAML",
@@ -118,22 +117,15 @@
     # https://github.com/great-expectations/great_expectations/pull/5382/files
     # datahub does not depend on traitlets directly but great expectations does.
     # https://github.com/ipython/traitlets/issues/741
     "traitlets<5.2.2",
     "greenlet",
 }
 
-sqllineage_lib = {
-    "sqllineage==1.3.6",
-    # We don't have a direct dependency on sqlparse but it is a dependency of sqllineage.
-    # As per https://github.com/reata/sqllineage/issues/361
-    # and https://github.com/reata/sqllineage/pull/360
-    # sqllineage has compat issues with sqlparse 0.4.4.
-    "sqlparse==0.4.3",
-}
+sqllineage_lib = "sqllineage==1.3.6"
 
 aws_common = {
     # AWS Python SDK
     "boto3",
     # Deal with a version incompatibility between botocore (used by boto3) and urllib3.
     # See https://github.com/boto/botocore/pull/2563.
     "botocore!=1.23.0",
@@ -148,15 +140,15 @@
     # Looker Python SDK
     "looker-sdk==23.0.0",
     # This version of lkml contains a fix for parsing lists in
     # LookML files with spaces between an item and the following comma.
     # See https://github.com/joshtemple/lkml/issues/73.
     "lkml>=1.3.0b5",
     "sql-metadata==2.2.2",
-    *sqllineage_lib,
+    sqllineage_lib,
     "GitPython>2",
 }
 
 bigquery_common = {
     # Google cloud logging library
     "google-cloud-logging<=3.5.0",
     "google-cloud-bigquery",
@@ -169,15 +161,15 @@
     "clickhouse-sqlalchemy>=0.1.8",
 }
 
 redshift_common = {
     "sqlalchemy-redshift",
     "psycopg2-binary",
     "GeoAlchemy2",
-    *sqllineage_lib,
+    sqllineage_lib,
     *path_spec_common,
 }
 
 snowflake_common = {
     # Snowflake plugin utilizes sql common
     *sql_common,
     # Required for all Snowflake sources.
@@ -210,15 +202,14 @@
     # Iceberg Python SDK
     "acryl-iceberg-legacy==0.0.4",
     "azure-identity==1.10.0",
 }
 
 s3_base = {
     *aws_common,
-    "more-itertools>=8.12.0",
     "parse>=1.19.0",
     "pyarrow>=6.0.1",
     "tableschema>=1.20.2",
     # ujson 5.2.0 has the JSONDecodeError exception type, which we need for error handling.
     "ujson>=5.2.0",
     "smart-open[s3]>=5.2.1",
     "moto[s3]",
@@ -238,18 +229,15 @@
 powerbi_report_server = {"requests", "requests_ntlm"}
 
 usage_common = {
     "sqlparse",
 }
 
 databricks_cli = {
-    "databricks-cli>=0.17.7",
-    "databricks-sdk>=0.1.1",
-    "pyspark",
-    "requests",
+    "databricks-cli==0.17.3",
 }
 
 # Note: for all of these, framework_common will be added.
 plugins: Dict[str, Set[str]] = {
     # Sink plugins.
     "datahub-kafka": kafka_common,
     "datahub-rest": rest_common,
@@ -263,31 +251,31 @@
         "apache-airflow >= 2.0.2",
         *rest_common,
     },
     "circuit-breaker": {
         "gql>=3.3.0",
         "gql[requests]>=3.3.0",
     },
-    "great-expectations": sql_common | sqllineage_lib,
+    "great-expectations": sql_common | {sqllineage_lib},
     # Source plugins
     # PyAthena is pinned with exact version because we use private method in PyAthena
     "athena": sql_common | {"PyAthena[SQLAlchemy]==2.4.1"},
     "azure-ad": set(),
     "bigquery": sql_common
     | bigquery_common
     | {
-        *sqllineage_lib,
+        sqllineage_lib,
         "sql_metadata",
         "sqlalchemy-bigquery>=1.4.1",
         "google-cloud-datacatalog-lineage==0.2.0",
     },
     "bigquery-beta": sql_common
     | bigquery_common
     | {
-        *sqllineage_lib,
+        sqllineage_lib,
         "sql_metadata",
         "sqlalchemy-bigquery>=1.4.1",
     },  # deprecated, but keeping the extra for backwards compatibility
     "clickhouse": sql_common | clickhouse_common,
     "clickhouse-usage": sql_common | usage_common | clickhouse_common,
     "datahub-lineage-file": set(),
     "datahub-business-glossary": set(),
@@ -329,56 +317,55 @@
     "iceberg": iceberg_common,
     "json-schema": set(),
     "kafka": {*kafka_common, *kafka_protobuf},
     "kafka-connect": sql_common | {"requests", "JPype1"},
     "ldap": {"python-ldap>=2.4"},
     "looker": looker_common,
     "lookml": looker_common,
-    "metabase": {"requests"} | sqllineage_lib,
-    "mode": {"requests", "tenacity>=8.0.1"} | sqllineage_lib,
+    "metabase": {"requests", sqllineage_lib},
+    "mode": {"requests", sqllineage_lib, "tenacity>=8.0.1"},
     "mongodb": {"pymongo[srv]>=3.11", "packaging"},
     "mssql": sql_common | {"sqlalchemy-pytds>=0.3"},
     "mssql-odbc": sql_common | {"pyodbc"},
     "mysql": sql_common | {"pymysql>=1.0.2"},
     # mariadb should have same dependency as mysql
     "mariadb": sql_common | {"pymysql>=1.0.2"},
     "okta": {"okta~=1.7.0"},
     "oracle": sql_common | {"cx_Oracle"},
     "postgres": sql_common | {"psycopg2-binary", "GeoAlchemy2"},
     "presto": sql_common | trino | {"acryl-pyhive[hive]>=0.6.12"},
     "presto-on-hive": sql_common
     | {"psycopg2-binary", "acryl-pyhive[hive]>=0.6.12", "pymysql>=1.0.2"},
     "pulsar": {"requests"},
-    "redash": {"redash-toolbelt", "sql-metadata"} | sqllineage_lib,
+    "redash": {"redash-toolbelt", "sql-metadata", sqllineage_lib},
     "redshift": sql_common | redshift_common | usage_common | {"redshift-connector"},
     "redshift-legacy": sql_common | redshift_common,
     "redshift-usage-legacy": sql_common | usage_common | redshift_common,
     "s3": {*s3_base, *data_lake_profiling},
-    "gcs": {*s3_base, *data_lake_profiling},
     "sagemaker": aws_common,
     "salesforce": {"simple-salesforce"},
     "snowflake": snowflake_common | usage_common,
     "snowflake-beta": (
         snowflake_common | usage_common
     ),  # deprecated, but keeping the extra for backwards compatibility
     "sqlalchemy": sql_common,
     "superset": {
         "requests",
         "sqlalchemy",
         "great_expectations",
         "greenlet",
     },
-    "tableau": {"tableauserverclient>=0.17.0"} | sqllineage_lib,
+    "tableau": {"tableauserverclient>=0.17.0", sqllineage_lib},
     "trino": sql_common | trino,
     "starburst-trino-usage": sql_common | usage_common | trino,
     "nifi": {"requests", "packaging"},
     "powerbi": microsoft_common | {"lark[regex]==1.1.4", "sqlparse"},
     "powerbi-report-server": powerbi_report_server,
     "vertica": sql_common | {"vertica-sqlalchemy-dialect[vertica-python]==0.0.1"},
-    "unity-catalog": databricks_cli | sqllineage_lib,
+    "unity-catalog": databricks_cli | {"requests"},
 }
 
 # This is mainly used to exclude plugins from the Docker image.
 all_exclude_plugins: Set[str] = {
     # SQL Server ODBC requires additional drivers, and so we don't want to keep
     # it included in the default "all" installation.
     "mssql-odbc",
@@ -417,15 +404,14 @@
     *framework_common,
     *mypy_stubs,
     *s3_base,
     # This is pinned only to avoid spurious errors in CI.
     # We should make an effort to keep it up to date.
     "black==22.12.0",
     "coverage>=5.1",
-    "faker>=18.4.0",
     "flake8>=3.8.3",  # DEPRECATION: Once we drop Python 3.7, we can pin to 6.x.
     "flake8-tidy-imports>=4.3.0",
     "flake8-bugbear==23.3.12",
     "isort>=5.7.0",
     "mypy==1.0.0",
     # pydantic 1.8.2 is incompatible with mypy 0.910.
     # See https://github.com/samuelcolvin/pydantic/pull/3175#issuecomment-995382910.
@@ -574,15 +560,14 @@
         "vertica = datahub.ingestion.source.sql.vertica:VerticaSource",
         "presto = datahub.ingestion.source.sql.presto:PrestoSource",
         "presto-on-hive = datahub.ingestion.source.sql.presto_on_hive:PrestoOnHiveSource",
         "pulsar = datahub.ingestion.source.pulsar:PulsarSource",
         "salesforce = datahub.ingestion.source.salesforce:SalesforceSource",
         "demo-data = datahub.ingestion.source.demo_data.DemoDataSource",
         "unity-catalog = datahub.ingestion.source.unity.source:UnityCatalogSource",
-        "gcs = datahub.ingestion.source.gcs.gcs_source:GCSSource",
     ],
     "datahub.ingestion.transformer.plugins": [
         "simple_remove_dataset_ownership = datahub.ingestion.transformer.remove_dataset_ownership:SimpleRemoveDatasetOwnership",
         "mark_dataset_status = datahub.ingestion.transformer.mark_dataset_status:MarkDatasetStatus",
         "set_dataset_browse_path = datahub.ingestion.transformer.add_dataset_browse_path:AddDatasetBrowsePathTransformer",
         "add_dataset_ownership = datahub.ingestion.transformer.add_dataset_ownership:AddDatasetOwnership",
         "simple_add_dataset_ownership = datahub.ingestion.transformer.add_dataset_ownership:SimpleAddDatasetOwnership",
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/PKG-INFO` & `acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,18 @@
 Metadata-Version: 2.1
 Name: acryl-datahub-tc
-Version: 0.10.2.3
+Version: 0.10.2rc1
 Summary: A CLI to work with DataHub metadata
 Home-page: https://datahubproject.io/
 License: Apache License 2.0
 Project-URL: Documentation, https://datahubproject.io/docs/
 Project-URL: Source, https://github.com/datahub-project/datahub
 Project-URL: Changelog, https://github.com/datahub-project/datahub/releases
 Description: # Introduction to Metadata Ingestion
         
-        <a
-            className='button button--primary button--lg'
-            href="https://datahubproject.io/integrations">
-            Find Integration Source
-        </a>
-        
         ## Integration Options
         
         DataHub supports both **push-based** and **pull-based** metadata integration.
         
         Push-based integrations allow you to emit metadata directly from your data systems when metadata changes, while pull-based integrations allow you to "crawl" or "ingest" metadata from the data systems by connecting to them and extracting metadata in a batch or incremental-batch manner. Supporting both mechanisms means that you can integrate with all your systems in the most flexible way possible.
         
         Examples of push-based integrations include [Airflow](../docs/lineage/airflow.md), [Spark](../metadata-integration/java/spark-lineage/README.md), [Great Expectations](./integration_docs/great-expectations.md) and [Protobuf Schemas](../metadata-integration/java/datahub-protobuf/README.md). This allows you to get low-latency metadata integration from the "active" agents in your data ecosystem. Examples of pull-based integrations include BigQuery, Snowflake, Looker, Tableau and many others.
@@ -282,15 +276,14 @@
 Provides-Extra: presto-on-hive
 Provides-Extra: pulsar
 Provides-Extra: redash
 Provides-Extra: redshift
 Provides-Extra: redshift-legacy
 Provides-Extra: redshift-usage-legacy
 Provides-Extra: s3
-Provides-Extra: gcs
 Provides-Extra: sagemaker
 Provides-Extra: salesforce
 Provides-Extra: snowflake
 Provides-Extra: snowflake-beta
 Provides-Extra: sqlalchemy
 Provides-Extra: superset
 Provides-Extra: tableau
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/SOURCES.txt` & `acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/SOURCES.txt`

 * *Files 0% similar despite different names*

```diff
@@ -151,14 +151,15 @@
 src/datahub/ingestion/source/superset.py
 src/datahub/ingestion/source/tableau.py
 src/datahub/ingestion/source/tableau_common.py
 src/datahub/ingestion/source/tableau_constant.py
 src/datahub/ingestion/source/aws/__init__.py
 src/datahub/ingestion/source/aws/aws_common.py
 src/datahub/ingestion/source/aws/glue.py
+src/datahub/ingestion/source/aws/path_spec.py
 src/datahub/ingestion/source/aws/s3_boto_utils.py
 src/datahub/ingestion/source/aws/s3_util.py
 src/datahub/ingestion/source/aws/sagemaker.py
 src/datahub/ingestion/source/aws/sagemaker_processors/__init__.py
 src/datahub/ingestion/source/aws/sagemaker_processors/common.py
 src/datahub/ingestion/source/aws/sagemaker_processors/feature_groups.py
 src/datahub/ingestion/source/aws/sagemaker_processors/job_classes.py
@@ -175,30 +176,23 @@
 src/datahub/ingestion/source/bigquery_v2/bigquery_schema.py
 src/datahub/ingestion/source/bigquery_v2/common.py
 src/datahub/ingestion/source/bigquery_v2/lineage.py
 src/datahub/ingestion/source/bigquery_v2/profiler.py
 src/datahub/ingestion/source/bigquery_v2/usage.py
 src/datahub/ingestion/source/common/__init__.py
 src/datahub/ingestion/source/common/subtypes.py
-src/datahub/ingestion/source/data_lake_common/__init__.py
-src/datahub/ingestion/source/data_lake_common/config.py
-src/datahub/ingestion/source/data_lake_common/data_lake_utils.py
-src/datahub/ingestion/source/data_lake_common/path_spec.py
 src/datahub/ingestion/source/dbt/__init__.py
 src/datahub/ingestion/source/dbt/dbt_cloud.py
 src/datahub/ingestion/source/dbt/dbt_common.py
 src/datahub/ingestion/source/dbt/dbt_core.py
 src/datahub/ingestion/source/delta_lake/__init__.py
 src/datahub/ingestion/source/delta_lake/config.py
 src/datahub/ingestion/source/delta_lake/delta_lake_utils.py
 src/datahub/ingestion/source/delta_lake/report.py
 src/datahub/ingestion/source/delta_lake/source.py
-src/datahub/ingestion/source/gcs/__init__.py
-src/datahub/ingestion/source/gcs/gcs_source.py
-src/datahub/ingestion/source/gcs/gcs_utils.py
 src/datahub/ingestion/source/git/__init__.py
 src/datahub/ingestion/source/git/git_import.py
 src/datahub/ingestion/source/iceberg/__init__.py
 src/datahub/ingestion/source/iceberg/iceberg.py
 src/datahub/ingestion/source/iceberg/iceberg_common.py
 src/datahub/ingestion/source/iceberg/iceberg_profiler.py
 src/datahub/ingestion/source/identity/__init__.py
@@ -244,14 +238,15 @@
 src/datahub/ingestion/source/redshift/query.py
 src/datahub/ingestion/source/redshift/redshift.py
 src/datahub/ingestion/source/redshift/redshift_schema.py
 src/datahub/ingestion/source/redshift/report.py
 src/datahub/ingestion/source/redshift/usage.py
 src/datahub/ingestion/source/s3/__init__.py
 src/datahub/ingestion/source/s3/config.py
+src/datahub/ingestion/source/s3/data_lake_utils.py
 src/datahub/ingestion/source/s3/profiling.py
 src/datahub/ingestion/source/s3/report.py
 src/datahub/ingestion/source/s3/source.py
 src/datahub/ingestion/source/sagemaker_processors/__init__.py
 src/datahub/ingestion/source/schema/__init__.py
 src/datahub/ingestion/source/schema/json_schema.py
 src/datahub/ingestion/source/schema_inference/__init__.py
@@ -310,21 +305,17 @@
 src/datahub/ingestion/source/state/usage_common_state.py
 src/datahub/ingestion/source/state/use_case_handler.py
 src/datahub/ingestion/source/state_provider/__init__.py
 src/datahub/ingestion/source/state_provider/datahub_ingestion_checkpointing_provider.py
 src/datahub/ingestion/source/state_provider/state_provider_registry.py
 src/datahub/ingestion/source/unity/__init__.py
 src/datahub/ingestion/source/unity/config.py
-src/datahub/ingestion/source/unity/profiler.py
 src/datahub/ingestion/source/unity/proxy.py
-src/datahub/ingestion/source/unity/proxy_profiling.py
-src/datahub/ingestion/source/unity/proxy_types.py
 src/datahub/ingestion/source/unity/report.py
 src/datahub/ingestion/source/unity/source.py
-src/datahub/ingestion/source/unity/usage.py
 src/datahub/ingestion/source/usage/__init__.py
 src/datahub/ingestion/source/usage/clickhouse_usage.py
 src/datahub/ingestion/source/usage/redshift_usage.py
 src/datahub/ingestion/source/usage/starburst_trino_usage.py
 src/datahub/ingestion/source/usage/usage_common.py
 src/datahub/ingestion/source_config/__init__.py
 src/datahub/ingestion/source_config/bigquery.py
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/entry_points.txt` & `acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/entry_points.txt`

 * *Files 2% similar despite different names*

```diff
@@ -32,15 +32,14 @@
 dbt-cloud = datahub.ingestion.source.dbt.dbt_cloud:DBTCloudSource
 delta-lake = datahub.ingestion.source.delta_lake:DeltaLakeSource
 demo-data = datahub.ingestion.source.demo_data.DemoDataSource
 druid = datahub.ingestion.source.sql.druid:DruidSource
 elasticsearch = datahub.ingestion.source.elastic_search:ElasticsearchSource
 feast = datahub.ingestion.source.feast:FeastRepositorySource
 file = datahub.ingestion.source.file:GenericFileSource
-gcs = datahub.ingestion.source.gcs.gcs_source:GCSSource
 glue = datahub.ingestion.source.aws.glue:GlueSource
 hana = datahub.ingestion.source.sql.hana:HanaSource
 hive = datahub.ingestion.source.sql.hive:HiveSource
 iceberg = datahub.ingestion.source.iceberg.iceberg:IcebergSource
 json-schema = datahub.ingestion.source.schema.json_schema:JsonSchemaSource
 kafka = datahub.ingestion.source.kafka:KafkaSource
 kafka-connect = datahub.ingestion.source.kafka_connect:KafkaConnectSource
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/acryl_datahub_tc.egg-info/requires.txt` & `acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/requires.txt`

 * *Files 21% similar despite different names*

```diff
@@ -1,2427 +1,2361 @@
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-typing-inspect
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-pydantic!=1.10.3,<2,>=1.5.1
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 mypy_extensions>=0.4.3
-mixpanel>=4.9.0
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+mixpanel>=4.9.0
+pydantic!=1.10.3,>=1.5.1
+ratelimiter
+humanfriendly
+jsonschema
 click-default-group
 toml>=0.10.0
+typing-inspect
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [:python_version < "3.8"]
 typing_extensions>=3.7.4.3
 
 [:python_version >= "3.8"]
 typing_extensions>=3.10.0.2
 
 [airflow]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-apache-airflow>=2.0.2
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-
-[all]
-jsonschema
 requests
-psycopg2-binary
-looker-sdk==23.0.0
-feast~=0.29.0
-tableauserverclient>=0.17.0
-termcolor>=1.0.0
-traitlets<5.2.2
-msal==1.16.0
-ratelimiter
-smart-open[s3]>=5.2.1
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
 Deprecated
-sqllineage==1.3.6
-acryl-datahub-classify==0.0.6
-sqlalchemy<2,>=1.3.24
-grpcio-tools<2,>=1.44.0
-PyYAML
-great_expectations
-snowflake-connector-python!=2.8.2,<3.0.0
-okta~=1.7.0
-sqlalchemy
-redash-toolbelt
-expandvars>=0.6.5
-python-ldap>=2.4
-acryl-pyhive[hive]>=0.6.12
-azure-identity==1.10.0
+ijson
+apache-airflow>=2.0.2
+termcolor>=1.0.0
+
+[all]
+pymysql>=1.0.2
+more-itertools>=8.12.0
+wcmatch
 sql_metadata
-pyarrow>=6.0.1
-SQLAlchemy<1.4.42
-GitPython>2
-sqlparse
-fastavro>=1.2.0
+google-cloud-datacatalog-lineage==0.2.0
+great_expectations
+GeoAlchemy2
+requests_ntlm
+acryl-pyhive[hive]>=0.6.13
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+PyAthena[sqlalchemy]==2.4.1
 clickhouse-sqlalchemy>=0.1.8
+azure-identity==1.10.0
+JPype1
+toml>=0.10.0
+requests
 moto[s3]
-ujson>=5.2.0
-boto3
-sql-metadata==2.2.2
-tenacity>=8.0.1
-cached_property
-elasticsearch==7.13.4
-sqlalchemy-pytds>=0.3
-pymongo[srv]>=3.11
-acryl-iceberg-legacy==0.0.4
+scipy>=1.7.2
+psutil>=5.8.0
+flask-openid>=1.3.0
+typeguard<3
+sqllineage==1.3.6
+smart-open[s3]>=5.2.1
+pandas
 snowflake-sqlalchemy!=1.2.5,>=1.2.4
-vertica-sqlalchemy-dialect[vertica-python]==0.0.1
-acryl-pyhive[hive]>=0.6.13
-botocore!=1.23.0
-lark[regex]==1.1.4
+progressbar2
+entrypoints
+msal
+parse>=1.19.0
+deltalake!=0.6.4,>=0.6.3
 gql[requests]>=3.3.0
-more-itertools>=8.12.0
-ijson
-redshift-connector
+greenlet
+google-cloud-bigquery
+avro<1.11,>=1.10.2
+click>=7.1.2
+sql-metadata==2.2.2
+jsonref
+acryl-datahub-classify==0.0.6
+tabulate
+aiohttp<4
+lkml>=1.3.0b5
+click-default-group
 grpcio<2,>=1.44.0
-JPype1
-wcmatch
-requests_ntlm
-pyspark
+google-cloud-logging<=3.5.0
+pymongo[srv]>=3.11
+sqlparse
+tableauserverclient>=0.17.0
+pyspark==3.0.3
 databricks-dbapi
-sqlalchemy-bigquery>=1.4.1
-sqlalchemy-redshift
-jsonref
-sqlparse==0.4.3
+tenacity>=8.0.1
+python-dateutil>=2.8.0
+snowflake-connector-python!=2.8.2,<3.0.0
+psycopg2-binary
+python-ldap>=2.4
 great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
-click>=7.1.2
+botocore!=1.23.0
+ijson
+pydeequ>=1.0.1
+termcolor>=1.0.0
+okta~=1.7.0
+elasticsearch==7.13.4
 packaging
-google-cloud-datacatalog-lineage==0.2.0
-progressbar2
-tabulate
+pydruid>=0.6.2
 cx_Oracle
-google-cloud-logging<=3.5.0
+docker
+tableschema>=1.20.2
 trino[sqlalchemy]!=0.317,>=0.308
-msal
-pydeequ>=1.0.1
-google-cloud-bigquery
-confluent_kafka>=1.5.0
-lkml>=1.3.0b5
-pymysql>=1.0.2
-sql-metadata
-databricks-sdk>=0.1.1
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-deltalake!=0.6.4,>=0.6.3
-requests_file
-greenlet
-pyspark==3.0.3
-GeoAlchemy2
-simple-salesforce
-toml>=0.10.0
-pydruid>=0.6.2
-aiohttp<4
+redash-toolbelt
 spacy==3.4.3
-typeguard<3
-docker
+acryl-iceberg-legacy==0.0.4
+ratelimiter
 humanfriendly
-gql>=3.3.0
-cryptography
-entrypoints
-flask-openid>=1.3.0
-avro<1.11,>=1.10.2
-databricks-cli>=0.17.7
-click-spinner
+feast~=0.29.0
+msal==1.16.0
+ujson>=5.2.0
+pyarrow>=6.0.1
+redshift-connector
+fastavro>=1.2.0
+boto3
+GitPython>2
 apache-airflow>=2.0.2
-scipy>=1.7.2
-python-dateutil>=2.8.0
-parse>=1.19.0
-tableschema>=1.20.2
+vertica-sqlalchemy-dialect[vertica-python]==0.0.1
+sql-metadata
+sqlalchemy-redshift
+databricks-cli==0.17.3
+cached_property
+looker-sdk==23.0.0
+expandvars>=0.6.5
+confluent_kafka>=1.5.0
+jsonschema
+acryl-pyhive[hive]>=0.6.12
+SQLAlchemy<1.4.42
+sqlalchemy-pytds>=0.3
+cryptography
+great-expectations<=0.15.50,>=0.15.12
+lark[regex]==1.1.4
+sqlalchemy
+grpcio-tools<2,>=1.44.0
 networkx>=2.6.2
-PyAthena[sqlalchemy]==2.4.1
-pandas
-psutil>=5.8.0
-click-default-group
+traitlets<5.2.2
+sqlalchemy-bigquery>=1.4.1
+sqlalchemy<2,>=1.3.24
+Deprecated
+gql>=3.3.0
+simple-salesforce
 
 [all:platform_machine != "aarch64" and platform_machine != "arm64"]
-hdbcli>=2.11.20
 sqlalchemy-hana>=0.5.0
+hdbcli>=2.11.20
 
 [all:platform_system != "Darwin" and (platform_machine == "aarch64" or platform_machine == "arm64")]
 confluent_kafka<1.9.0
 
 [athena]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+PyAthena[sqlalchemy]==2.4.1
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
+great-expectations<=0.15.50,>=0.15.12
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 greenlet
-PyAthena[sqlalchemy]==2.4.1
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [azure-ad]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [base]
-click>=7.1.2
-jsonschema
 packaging
-progressbar2
-tabulate
-termcolor>=1.0.0
-ratelimiter
-Deprecated
-PyYAML
+docker
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-toml>=0.10.0
-aiohttp<4
-docker
+ratelimiter
 humanfriendly
-cached_property
+toml>=0.10.0
+psutil>=5.8.0
+progressbar2
 entrypoints
 avro<1.11,>=1.10.2
-click-spinner
-ijson
-python-dateutil>=2.8.0
+cached_property
+click>=7.1.2
 jsonref
-psutil>=5.8.0
+tabulate
+aiohttp<4
+expandvars>=0.6.5
+jsonschema
 click-default-group
+python-dateutil>=2.8.0
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [bigquery]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
+more-itertools>=8.12.0
+jsonref
+sql_metadata
 packaging
 google-cloud-datacatalog-lineage==0.2.0
-progressbar2
 tabulate
-google-cloud-logging<=3.5.0
-sqlparse==0.4.3
-termcolor>=1.0.0
-traitlets<5.2.2
-google-cloud-bigquery
-ratelimiter
-Deprecated
-sqllineage==1.3.6
-sqlalchemy<2,>=1.3.24
-PyYAML
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
+PyYAML
+click-spinner
 requests_file
-greenlet
-expandvars>=0.6.5
-toml>=0.10.0
-aiohttp<4
-sql_metadata
-docker
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-click-spinner
-more-itertools>=8.12.0
+google-cloud-logging<=3.5.0
+toml>=0.10.0
 scipy>=1.7.2
-ijson
+psutil>=5.8.0
 python-dateutil>=2.8.0
+traitlets<5.2.2
+sqllineage==1.3.6
+progressbar2
 sqlalchemy-bigquery>=1.4.1
-jsonref
-psutil>=5.8.0
-click-default-group
+entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+greenlet
+google-cloud-bigquery
+termcolor>=1.0.0
 
 [bigquery-beta]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+more-itertools>=8.12.0
+jsonref
 sql_metadata
 packaging
-greenlet
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-tabulate
 google-cloud-logging<=3.5.0
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-sqlparse==0.4.3
-termcolor>=1.0.0
-traitlets<5.2.2
-google-cloud-bigquery
-ratelimiter
-click-spinner
-Deprecated
-more-itertools>=8.12.0
+toml>=0.10.0
 scipy>=1.7.2
-ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-sqllineage==1.3.6
+psutil>=5.8.0
 python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
+traitlets<5.2.2
+sqllineage==1.3.6
+progressbar2
 sqlalchemy-bigquery>=1.4.1
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+greenlet
+google-cloud-bigquery
+termcolor>=1.0.0
 
 [circuit-breaker]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-gql>=3.3.0
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-gql[requests]>=3.3.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+gql[requests]>=3.3.0
+gql>=3.3.0
+termcolor>=1.0.0
 
 [clickhouse]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-clickhouse-sqlalchemy>=0.1.8
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+clickhouse-sqlalchemy>=0.1.8
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
+great-expectations<=0.15.50,>=0.15.12
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [clickhouse-usage]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-sqlparse
-clickhouse-sqlalchemy>=0.1.8
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+clickhouse-sqlalchemy>=0.1.8
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
+great-expectations<=0.15.50,>=0.15.12
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+sqlparse
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [datahub-business-glossary]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [datahub-kafka]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-fastavro>=1.2.0
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
+docker
+aiohttp<4
+expandvars>=0.6.5
 confluent_kafka>=1.5.0
-ijson
-PyYAML
-python-dateutil>=2.8.0
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+fastavro>=1.2.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [datahub-kafka:platform_system != "Darwin" and (platform_machine == "aarch64" or platform_machine == "arm64")]
 confluent_kafka<1.9.0
 
 [datahub-lineage-file]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [datahub-lite]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
+docker
+aiohttp<4
+expandvars>=0.6.5
 fastapi
-ijson
-PyYAML
-duckdb
-python-dateutil>=2.8.0
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-uvicorn
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+duckdb
+python-dateutil>=2.8.0
+uvicorn
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [datahub-rest]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [dbt]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+ratelimiter
 humanfriendly
-tabulate
+toml>=0.10.0
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+botocore!=1.23.0
+progressbar2
 boto3
-cached_property
 entrypoints
-avro<1.11,>=1.10.2
-botocore!=1.23.0
-termcolor>=1.0.0
-ratelimiter
-click-spinner
 Deprecated
 ijson
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [dbt-cloud]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [delta-lake]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
+jsonref
+wcmatch
 packaging
-progressbar2
 tabulate
-termcolor>=1.0.0
-pydeequ>=1.0.1
-ratelimiter
-smart-open[s3]>=5.2.1
-Deprecated
-PyYAML
+docker
+aiohttp<4
+tableschema>=1.20.2
+expandvars>=0.6.5
 avro-gen3==0.7.10
-deltalake!=0.6.4,>=0.6.3
+PyYAML
+click-spinner
 requests_file
-pyspark==3.0.3
-expandvars>=0.6.5
+jsonschema
+click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-pyarrow>=6.0.1
 moto[s3]
-docker
-humanfriendly
 ujson>=5.2.0
+psutil>=5.8.0
+pyspark==3.0.3
+pyarrow>=6.0.1
+python-dateutil>=2.8.0
+smart-open[s3]>=5.2.1
+deltalake!=0.6.4,>=0.6.3
+botocore!=1.23.0
+progressbar2
 boto3
-cached_property
 entrypoints
-avro<1.11,>=1.10.2
-botocore!=1.23.0
-click-spinner
-more-itertools>=8.12.0
+Deprecated
 ijson
-python-dateutil>=2.8.0
+pydeequ>=1.0.1
 parse>=1.19.0
-tableschema>=1.20.2
-wcmatch
-jsonref
-psutil>=5.8.0
-click-default-group
+termcolor>=1.0.0
 
 [dev]
-jsonschema
+types-dataclasses
+more-itertools>=8.12.0
+types-PyMySQL
+acryl-pyhive[hive]>=0.6.13
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+clickhouse-sqlalchemy>=0.1.8
+mixpanel>=4.9.0
+types-Deprecated
+types-protobuf>=4.21.0.1
+toml>=0.10.0
+requests
+scipy>=1.7.2
+psutil>=5.8.0
 isort>=5.7.0
-types-click-spinner>=0.1.13.1
-termcolor>=1.0.0
-ratelimiter
 smart-open[s3]>=5.2.1
-Deprecated
-sqllineage==1.3.6
-acryl-datahub-classify==0.0.6
-sqlalchemy<2,>=1.3.24
-grpcio-tools<2,>=1.44.0
-duckdb
-pydantic>=1.9.0
-redash-toolbelt
-acryl-pyhive[hive]>=0.6.12
+types-pkg_resources
+progressbar2
 types-pytz
-pyarrow>=6.0.1
-sql_metadata
-azure-identity==1.10.0
-GitPython>2
-flake8>=3.8.3
-fastavro>=1.2.0
-ujson>=5.2.0
-mypy==1.0.0
-sql-metadata==2.2.2
-elasticsearch==7.13.4
-types-toml
-acryl-iceberg-legacy==0.0.4
-botocore!=1.23.0
-twine
-more-itertools>=8.12.0
-ijson
-redshift-connector
-great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
-google-cloud-logging<=3.5.0
-trino[sqlalchemy]!=0.317,>=0.308
-google-cloud-bigquery
-confluent_kafka>=1.5.0
-great-expectations<=0.15.50,>=0.15.12
-requests_file
+msal
+coverage>=5.1
 greenlet
+click>=7.1.2
+sql-metadata==2.2.2
+jsonref
+acryl-datahub-classify==0.0.6
 pytest-cov>=2.8.1
-jsonpickle
-types-ujson>=5.2.0
-docker
-humanfriendly
-types-pkg_resources
-databricks-cli>=0.17.7
-click-spinner
-fastapi
-pydantic!=1.10.3,<2,>=1.5.1
-parse>=1.19.0
-apache-airflow[snowflake]>=2.0.2
 requests-mock
-types-Deprecated
-freezegun
-click-default-group
-requests
-black==22.12.0
-psycopg2-binary
-virtualenv
-types-pyOpenSSL
-pytest-docker>=1.0.1
-looker-sdk==23.0.0
-tableauserverclient>=0.17.0
-traitlets<5.2.2
-msal==1.16.0
-PyYAML
+apache-airflow[snowflake]>=2.0.2
+types-termcolor>=1.0.0
 boto3-stubs[glue,s3,sagemaker,sts]
+sqlparse
+tableauserverclient>=0.17.0
+pyspark==3.0.3
 snowflake-connector-python!=2.8.2,<3.0.0
-okta~=1.7.0
-mixpanel>=4.9.0
-expandvars>=0.6.5
+freezegun
 python-ldap>=2.4
-SQLAlchemy<1.4.42
-types-python-dateutil
-sqlparse
-types-freezegun
-moto[s3]
-pytest-asyncio>=0.16.0
-clickhouse-sqlalchemy>=0.1.8
-snowflake-sqlalchemy!=1.2.5,>=1.2.4
-boto3
 types-requests>=2.28.11.6
+types-python-dateutil
+botocore!=1.23.0
+types-click==0.1.12
+ijson
+pydeequ>=1.0.1
+okta~=1.7.0
+packaging
+cx_Oracle
+tableschema>=1.20.2
+trino[sqlalchemy]!=0.317,>=0.308
+spacy==3.4.3
+ratelimiter
+msal==1.16.0
+black==22.12.0
+pyarrow>=6.0.1
+redshift-connector
+fastavro>=1.2.0
+flake8>=3.8.3
+types-click-spinner>=0.1.13.1
+sqlalchemy-redshift
 cached_property
-pytest>=6.2.2
-acryl-pyhive[hive]>=0.6.13
-lark[regex]==1.1.4
-faker>=18.4.0
-grpcio<2,>=1.44.0
+pydantic>=1.9.0
+looker-sdk==23.0.0
+expandvars>=0.6.5
+confluent_kafka>=1.5.0
+fastapi
+flake8-bugbear==23.3.12
+acryl-pyhive[hive]>=0.6.12
 mypy_extensions>=0.4.3
-wcmatch
+SQLAlchemy<1.4.42
+great-expectations<=0.15.50,>=0.15.12
+cryptography
+lark[regex]==1.1.4
+networkx>=2.6.2
+duckdb
+sqlalchemy<2,>=1.3.24
 requests_ntlm
-types-PyMySQL
-pyspark
-databricks-dbapi
-sqlalchemy-bigquery>=1.4.1
-sqlalchemy-redshift
-jsonref
-sqlparse==0.4.3
-click>=7.1.2
-types-dataclasses
-types-click==0.1.12
-packaging
 types-PyYAML
-google-cloud-datacatalog-lineage==0.2.0
-progressbar2
-tabulate
-cx_Oracle
-types-tabulate
-msal
-pydeequ>=1.0.1
-types-cachetools
-lkml>=1.3.0b5
-pymysql>=1.0.2
-sql-metadata
-databricks-sdk>=0.1.1
-avro-gen3==0.7.10
-deltalake!=0.6.4,>=0.6.3
 deepdiff
-pyspark==3.0.3
-GeoAlchemy2
-simple-salesforce
-toml>=0.10.0
-pydruid>=0.6.2
-aiohttp<4
-spacy==3.4.3
-typing-inspect
+pymysql>=1.0.2
+wcmatch
 types-six
-cryptography
+sql_metadata
+google-cloud-datacatalog-lineage==0.2.0
+GeoAlchemy2
+azure-identity==1.10.0
+moto[s3]
+types-pyOpenSSL
+pytest-asyncio>=0.16.0
+sqllineage==1.3.6
+pandas
+snowflake-sqlalchemy!=1.2.5,>=1.2.4
 entrypoints
+parse>=1.19.0
+deltalake!=0.6.4,>=0.6.3
+google-cloud-bigquery
 avro<1.11,>=1.10.2
-flake8-bugbear==23.3.12
-flake8-tidy-imports>=4.3.0
-coverage>=5.1
-build
-scipy>=1.7.2
-types-termcolor>=1.0.0
+tabulate
+aiohttp<4
+pytest>=6.2.2
+lkml>=1.3.0b5
+types-ujson>=5.2.0
+click-default-group
+twine
+grpcio<2,>=1.44.0
+google-cloud-logging<=3.5.0
+typing-inspect
+databricks-dbapi
 python-dateutil>=2.8.0
-tableschema>=1.20.2
-networkx>=2.6.2
-types-protobuf>=4.21.0.1
+types-cachetools
+psycopg2-binary
 uvicorn
-pandas
-psutil>=5.8.0
+great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
+types-freezegun
+types-toml
+mypy==1.0.0
+pytest-docker>=1.0.1
+termcolor>=1.0.0
+elasticsearch==7.13.4
+pydruid>=0.6.2
+docker
+build
+flake8-tidy-imports>=4.3.0
+redash-toolbelt
+acryl-iceberg-legacy==0.0.4
+pydantic!=1.10.3,>=1.5.1
+types-tabulate
+humanfriendly
+ujson>=5.2.0
+boto3
+GitPython>2
+sql-metadata
+databricks-cli==0.17.3
+jsonschema
+grpcio-tools<2,>=1.44.0
+jsonpickle
+traitlets<5.2.2
+virtualenv
+sqlalchemy-bigquery>=1.4.1
+Deprecated
+simple-salesforce
 
 [dev:platform_system != "Darwin" and (platform_machine == "aarch64" or platform_machine == "arm64")]
 confluent_kafka<1.9.0
 
 [dev:python_version < "3.8"]
 typing_extensions>=3.7.4.3
 
 [dev:python_version >= "3.8"]
 typing_extensions>=3.10.0.2
 
 [druid]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-pydruid>=0.6.2
+jsonref
 packaging
-progressbar2
+pydruid>=0.6.2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [elasticsearch]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+elasticsearch==7.13.4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-elasticsearch==7.13.4
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [feast]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-typeguard<3
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-flask-openid>=1.3.0
-feast~=0.29.0
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
+feast~=0.29.0
 toml>=0.10.0
-
-[gcs]
-click>=7.1.2
-jsonschema
-aiohttp<4
-pyarrow>=6.0.1
-packaging
-moto[s3]
+flask-openid>=1.3.0
+typeguard<3
+psutil>=5.8.0
+python-dateutil>=2.8.0
 progressbar2
-docker
-humanfriendly
-tabulate
-ujson>=5.2.0
-boto3
-cached_property
 entrypoints
-avro<1.11,>=1.10.2
-botocore!=1.23.0
-termcolor>=1.0.0
-pydeequ>=1.0.1
-ratelimiter
-click-spinner
 Deprecated
-smart-open[s3]>=5.2.1
-more-itertools>=8.12.0
 ijson
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-parse>=1.19.0
-tableschema>=1.20.2
-wcmatch
-requests_file
-pyspark==3.0.3
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [glue]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+ratelimiter
 humanfriendly
-tabulate
+toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+botocore!=1.23.0
+progressbar2
 boto3
-cached_property
 entrypoints
-avro<1.11,>=1.10.2
-botocore!=1.23.0
-termcolor>=1.0.0
-ratelimiter
-click-spinner
 Deprecated
 ijson
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [great-expectations]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-sqlparse==0.4.3
-termcolor>=1.0.0
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
-Deprecated
 sqllineage==1.3.6
-scipy>=1.7.2
-ijson
+progressbar2
+entrypoints
 sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
+Deprecated
+ijson
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [hana]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [hana:platform_machine != "aarch64" and platform_machine != "arm64"]
-hdbcli>=2.11.20
 sqlalchemy-hana>=0.5.0
+hdbcli>=2.11.20
 
 [hive]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
+docker
+aiohttp<4
+expandvars>=0.6.5
 acryl-pyhive[hive]>=0.6.13
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
-click-spinner
-Deprecated
-scipy>=1.7.2
-ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
 avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
+PyYAML
+click-spinner
 requests_file
-greenlet
-databricks-dbapi
-great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+databricks-dbapi
+python-dateutil>=2.8.0
+traitlets<5.2.2
+great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+greenlet
+termcolor>=1.0.0
 
 [iceberg]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-azure-identity==1.10.0
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-acryl-iceberg-legacy==0.0.4
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+azure-identity==1.10.0
+ratelimiter
+humanfriendly
+acryl-iceberg-legacy==0.0.4
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [integration-tests]
-requests
-packaging
-traitlets<5.2.2
-pydeequ>=1.0.1
-smart-open[s3]>=5.2.1
-sqllineage==1.3.6
-sqlalchemy<2,>=1.3.24
 pymysql>=1.0.2
-sql-metadata
-great-expectations<=0.15.50,>=0.15.12
-deltalake!=0.6.4,>=0.6.3
-greenlet
-pyspark==3.0.3
-redash-toolbelt
+wcmatch
+packaging
 pydruid>=0.6.2
-python-ldap>=2.4
+tableschema>=1.20.2
+acryl-pyhive[hive]>=0.6.13
 azure-identity==1.10.0
-pyarrow>=6.0.1
-moto[s3]
 clickhouse-sqlalchemy>=0.1.8
-gql>=3.3.0
-ujson>=5.2.0
-sqlalchemy-pytds>=0.3
-boto3
-pymongo[srv]>=3.11
+redash-toolbelt
+PyAthena[sqlalchemy]==2.4.1
 acryl-iceberg-legacy==0.0.4
-acryl-pyhive[hive]>=0.6.13
-botocore!=1.23.0
-gql[requests]>=3.3.0
-more-itertools>=8.12.0
+JPype1
+requests
+moto[s3]
 scipy>=1.7.2
+ujson>=5.2.0
+pyarrow>=6.0.1
+smart-open[s3]>=5.2.1
+sqllineage==1.3.6
+boto3
 parse>=1.19.0
-tableschema>=1.20.2
-JPype1
-wcmatch
+deltalake!=0.6.4,>=0.6.3
+greenlet
+gql[requests]>=3.3.0
+sql-metadata
+sqlalchemy-pytds>=0.3
+great-expectations<=0.15.50,>=0.15.12
+pymongo[srv]>=3.11
+pyspark==3.0.3
 databricks-dbapi
-PyAthena[sqlalchemy]==2.4.1
-sqlparse==0.4.3
+traitlets<5.2.2
+python-ldap>=2.4
 great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
+botocore!=1.23.0
+sqlalchemy<2,>=1.3.24
+gql>=3.3.0
+pydeequ>=1.0.1
 
 [integration-tests:platform_machine != "aarch64" and platform_machine != "arm64"]
-hdbcli>=2.11.20
 sqlalchemy-hana>=0.5.0
+hdbcli>=2.11.20
 
 [json-schema]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [kafka]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-fastavro>=1.2.0
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
+docker
+aiohttp<4
+expandvars>=0.6.5
 confluent_kafka>=1.5.0
-ijson
-grpcio-tools<2,>=1.44.0
-PyYAML
-python-dateutil>=2.8.0
 avro-gen3==0.7.10
-grpcio<2,>=1.44.0
-networkx>=2.6.2
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+grpcio<2,>=1.44.0
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+grpcio-tools<2,>=1.44.0
+networkx>=2.6.2
+python-dateutil>=2.8.0
+fastavro>=1.2.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [kafka-connect]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
+JPype1
+toml>=0.10.0
+requests
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-JPype1
-requests_file
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [kafka:platform_system != "Darwin" and (platform_machine == "aarch64" or platform_machine == "arm64")]
 confluent_kafka<1.9.0
 
 [ldap]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-python-ldap>=2.4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+python-ldap>=2.4
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [looker]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+sql-metadata==2.2.2
+jsonref
 packaging
-GitPython>2
-progressbar2
-docker
-humanfriendly
 tabulate
-sql-metadata==2.2.2
-cached_property
-entrypoints
-looker-sdk==23.0.0
-avro<1.11,>=1.10.2
-sqlparse==0.4.3
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-sqllineage==1.3.6
-ijson
+docker
+aiohttp<4
 lkml>=1.3.0b5
-PyYAML
-python-dateutil>=2.8.0
+expandvars>=0.6.5
+looker-sdk==23.0.0
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+sqllineage==1.3.6
+progressbar2
+entrypoints
+GitPython>2
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [lookml]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+sql-metadata==2.2.2
+jsonref
 packaging
-GitPython>2
-progressbar2
-docker
-humanfriendly
 tabulate
-sql-metadata==2.2.2
-cached_property
-entrypoints
-looker-sdk==23.0.0
-avro<1.11,>=1.10.2
-sqlparse==0.4.3
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-sqllineage==1.3.6
-ijson
+docker
+aiohttp<4
 lkml>=1.3.0b5
-PyYAML
-python-dateutil>=2.8.0
+expandvars>=0.6.5
+looker-sdk==23.0.0
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+sqllineage==1.3.6
+progressbar2
+entrypoints
+GitPython>2
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [mariadb]
+pymysql>=1.0.2
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-pymysql>=1.0.2
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [metabase]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-sqlparse==0.4.3
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-sqllineage==1.3.6
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+sqllineage==1.3.6
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [mode]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+ratelimiter
 humanfriendly
-tabulate
+toml>=0.10.0
+requests
+psutil>=5.8.0
 tenacity>=8.0.1
-cached_property
+python-dateutil>=2.8.0
+sqllineage==1.3.6
+progressbar2
 entrypoints
-avro<1.11,>=1.10.2
-sqlparse==0.4.3
-termcolor>=1.0.0
-ratelimiter
-click-spinner
 Deprecated
-sqllineage==1.3.6
 ijson
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [mongodb]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+ratelimiter
 humanfriendly
-tabulate
-cached_property
+toml>=0.10.0
 pymongo[srv]>=3.11
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
 Deprecated
 ijson
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [mssql]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
+docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
 sqlalchemy-pytds>=0.3
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-traitlets<5.2.2
 ratelimiter
-click-spinner
-Deprecated
+humanfriendly
+great-expectations<=0.15.50,>=0.15.12
+toml>=0.10.0
 scipy>=1.7.2
-ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
+psutil>=5.8.0
 python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
+traitlets<5.2.2
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [mssql-odbc]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
+docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
 click-spinner
-Deprecated
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
+toml>=0.10.0
 scipy>=1.7.2
-ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
+psutil>=5.8.0
 pyodbc
 python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
+traitlets<5.2.2
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [mysql]
+pymysql>=1.0.2
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-pymysql>=1.0.2
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [nifi]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [okta]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+okta~=1.7.0
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
-okta~=1.7.0
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [oracle]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+cx_Oracle
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-tabulate
-cx_Oracle
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [postgres]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-psycopg2-binary
+jsonref
 packaging
-greenlet
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
-click-spinner
-Deprecated
-scipy>=1.7.2
-ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
+docker
+aiohttp<4
 GeoAlchemy2
 expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+psycopg2-binary
+traitlets<5.2.2
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+greenlet
+termcolor>=1.0.0
 
 [powerbi]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-sqlparse
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-msal==1.16.0
-lark[regex]==1.1.4
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+lark[regex]==1.1.4
+ratelimiter
+humanfriendly
 toml>=0.10.0
+msal==1.16.0
+sqlparse
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [powerbi-report-server]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+ratelimiter
 humanfriendly
-tabulate
-cached_property
+toml>=0.10.0
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
 Deprecated
 ijson
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
 requests_ntlm
-requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [presto]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-acryl-pyhive[hive]>=0.6.12
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
+docker
+aiohttp<4
 trino[sqlalchemy]!=0.317,>=0.308
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
-click-spinner
-Deprecated
-scipy>=1.7.2
-ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
+expandvars>=0.6.5
 avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
+PyYAML
+click-spinner
 requests_file
-greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+acryl-pyhive[hive]>=0.6.12
+ratelimiter
+humanfriendly
+great-expectations<=0.15.50,>=0.15.12
 toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+greenlet
+termcolor>=1.0.0
 
 [presto-on-hive]
+pymysql>=1.0.2
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
+jsonref
+packaging
+tabulate
+docker
 aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
 acryl-pyhive[hive]>=0.6.12
+ratelimiter
+humanfriendly
+great-expectations<=0.15.50,>=0.15.12
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 psycopg2-binary
-packaging
+traitlets<5.2.2
 progressbar2
-docker
-humanfriendly
-tabulate
-cached_property
 entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
-click-spinner
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-pymysql>=1.0.2
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [pulsar]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [redash]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+redash-toolbelt
+ratelimiter
 humanfriendly
-tabulate
-cached_property
+toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+sqllineage==1.3.6
+progressbar2
 entrypoints
-avro<1.11,>=1.10.2
-sqlparse==0.4.3
-termcolor>=1.0.0
-ratelimiter
-click-spinner
 Deprecated
-sqllineage==1.3.6
 ijson
-PyYAML
 sql-metadata
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-requests_file
-redash-toolbelt
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [redshift]
-click>=7.1.2
-jsonschema
-psycopg2-binary
+wcmatch
 packaging
-progressbar2
-tabulate
-sqlparse==0.4.3
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
-Deprecated
-sqllineage==1.3.6
-sqlalchemy<2,>=1.3.24
-PyYAML
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 GeoAlchemy2
-greenlet
-expandvars>=0.6.5
-toml>=0.10.0
-aiohttp<4
-sqlparse
 docker
-humanfriendly
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
+avro-gen3==0.7.10
+PyYAML
 click-spinner
+requests_file
+ratelimiter
+humanfriendly
+toml>=0.10.0
 scipy>=1.7.2
-ijson
+psutil>=5.8.0
 redshift-connector
-python-dateutil>=2.8.0
+sqllineage==1.3.6
+progressbar2
+entrypoints
 parse>=1.19.0
-wcmatch
+greenlet
 sqlalchemy-redshift
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
 jsonref
-psutil>=5.8.0
+tabulate
+aiohttp<4
+expandvars>=0.6.5
+jsonschema
 click-default-group
+great-expectations<=0.15.50,>=0.15.12
+sqlparse
+python-dateutil>=2.8.0
+traitlets<5.2.2
+psycopg2-binary
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [redshift-legacy]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-psycopg2-binary
+jsonref
+wcmatch
 packaging
-greenlet
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-sqlparse==0.4.3
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
+docker
+aiohttp<4
+GeoAlchemy2
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
 click-spinner
-Deprecated
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
+toml>=0.10.0
 scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
 sqllineage==1.3.6
-ijson
+psycopg2-binary
+progressbar2
+entrypoints
 sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
+Deprecated
+ijson
 parse>=1.19.0
-great-expectations<=0.15.50,>=0.15.12
-wcmatch
-requests_file
-GeoAlchemy2
+greenlet
 sqlalchemy-redshift
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [redshift-usage-legacy]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-psycopg2-binary
+jsonref
+wcmatch
 packaging
-progressbar2
 tabulate
-sqlparse==0.4.3
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
-Deprecated
-sqllineage==1.3.6
-sqlalchemy<2,>=1.3.24
-PyYAML
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
+docker
+aiohttp<4
 GeoAlchemy2
-greenlet
 expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+scipy>=1.7.2
+psutil>=5.8.0
 sqlparse
-docker
-humanfriendly
-cached_property
+python-dateutil>=2.8.0
+traitlets<5.2.2
+sqllineage==1.3.6
+psycopg2-binary
+progressbar2
 entrypoints
-avro<1.11,>=1.10.2
-click-spinner
-scipy>=1.7.2
+sqlalchemy<2,>=1.3.24
+Deprecated
 ijson
-python-dateutil>=2.8.0
 parse>=1.19.0
-wcmatch
+greenlet
 sqlalchemy-redshift
-jsonref
-psutil>=5.8.0
-click-default-group
+termcolor>=1.0.0
 
 [s3]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-pyarrow>=6.0.1
+jsonref
+wcmatch
 packaging
-moto[s3]
-progressbar2
+tabulate
 docker
+aiohttp<4
+tableschema>=1.20.2
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+ratelimiter
 humanfriendly
-tabulate
+toml>=0.10.0
+moto[s3]
 ujson>=5.2.0
+psutil>=5.8.0
+pyarrow>=6.0.1
+pyspark==3.0.3
+python-dateutil>=2.8.0
+smart-open[s3]>=5.2.1
+botocore!=1.23.0
+progressbar2
 boto3
-cached_property
 entrypoints
-avro<1.11,>=1.10.2
-botocore!=1.23.0
-termcolor>=1.0.0
-pydeequ>=1.0.1
-ratelimiter
-click-spinner
 Deprecated
-smart-open[s3]>=5.2.1
-more-itertools>=8.12.0
 ijson
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
+pydeequ>=1.0.1
 parse>=1.19.0
-tableschema>=1.20.2
-wcmatch
-requests_file
-pyspark==3.0.3
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [sagemaker]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+ratelimiter
 humanfriendly
-tabulate
+toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+botocore!=1.23.0
+progressbar2
 boto3
-cached_property
 entrypoints
-avro<1.11,>=1.10.2
-botocore!=1.23.0
-termcolor>=1.0.0
-ratelimiter
-click-spinner
 Deprecated
 ijson
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [salesforce]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+ratelimiter
 humanfriendly
-tabulate
-cached_property
+toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
 Deprecated
 ijson
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-requests_file
 simple-salesforce
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [snowflake]
-click>=7.1.2
-jsonschema
 packaging
-progressbar2
-tabulate
-msal
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
-Deprecated
-acryl-datahub-classify==0.0.6
-sqlalchemy<2,>=1.3.24
-PyYAML
+docker
 avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-snowflake-connector-python!=2.8.2,<3.0.0
+PyYAML
+click-spinner
 requests_file
-greenlet
-expandvars>=0.6.5
-toml>=0.10.0
-aiohttp<4
-SQLAlchemy<1.4.42
 spacy==3.4.3
-sqlparse
-docker
+ratelimiter
 humanfriendly
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+pandas
 snowflake-sqlalchemy!=1.2.5,>=1.2.4
-cached_property
-cryptography
+progressbar2
 entrypoints
+msal
+greenlet
 avro<1.11,>=1.10.2
-pandas
-click-spinner
-scipy>=1.7.2
-ijson
-python-dateutil>=2.8.0
+cached_property
+click>=7.1.2
+acryl-datahub-classify==0.0.6
 jsonref
-psutil>=5.8.0
+tabulate
+aiohttp<4
+expandvars>=0.6.5
+SQLAlchemy<1.4.42
+jsonschema
+cryptography
 click-default-group
+great-expectations<=0.15.50,>=0.15.12
+sqlparse
+python-dateutil>=2.8.0
+snowflake-connector-python!=2.8.2,<3.0.0
+traitlets<5.2.2
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [snowflake-beta]
-click>=7.1.2
-jsonschema
 packaging
-progressbar2
-tabulate
-msal
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
-Deprecated
-acryl-datahub-classify==0.0.6
-sqlalchemy<2,>=1.3.24
-PyYAML
+docker
 avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-snowflake-connector-python!=2.8.2,<3.0.0
+PyYAML
+click-spinner
 requests_file
-greenlet
-expandvars>=0.6.5
-toml>=0.10.0
-aiohttp<4
-SQLAlchemy<1.4.42
 spacy==3.4.3
-sqlparse
-docker
+ratelimiter
 humanfriendly
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+pandas
 snowflake-sqlalchemy!=1.2.5,>=1.2.4
-cached_property
-cryptography
+progressbar2
 entrypoints
+msal
+greenlet
 avro<1.11,>=1.10.2
-pandas
-click-spinner
-scipy>=1.7.2
-ijson
-python-dateutil>=2.8.0
+cached_property
+click>=7.1.2
+acryl-datahub-classify==0.0.6
 jsonref
-psutil>=5.8.0
+tabulate
+aiohttp<4
+expandvars>=0.6.5
+SQLAlchemy<1.4.42
+jsonschema
+cryptography
 click-default-group
+great-expectations<=0.15.50,>=0.15.12
+sqlparse
+python-dateutil>=2.8.0
+snowflake-connector-python!=2.8.2,<3.0.0
+traitlets<5.2.2
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [sqlalchemy]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
 
 [starburst-trino-usage]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-sqlparse
-progressbar2
-docker
-humanfriendly
 tabulate
+docker
+aiohttp<4
 trino[sqlalchemy]!=0.317,>=0.308
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
-click-spinner
-Deprecated
-scipy>=1.7.2
-ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
+expandvars>=0.6.5
 avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
+PyYAML
+click-spinner
 requests_file
-greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+sqlparse
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+greenlet
+termcolor>=1.0.0
 
 [superset]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+jsonref
 packaging
-greenlet
-progressbar2
-docker
-humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
 great_expectations
-ijson
-PyYAML
-python-dateutil>=2.8.0
+tabulate
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-sqlalchemy
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+requests
+sqlalchemy
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+greenlet
+termcolor>=1.0.0
 
 [tableau]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-tableauserverclient>=0.17.0
-avro<1.11,>=1.10.2
-sqlparse==0.4.3
-termcolor>=1.0.0
-ratelimiter
-click-spinner
-Deprecated
-sqllineage==1.3.6
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+tableauserverclient>=0.17.0
+python-dateutil>=2.8.0
+sqllineage==1.3.6
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [trino]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
+docker
+aiohttp<4
 trino[sqlalchemy]!=0.317,>=0.308
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-termcolor>=1.0.0
-traitlets<5.2.2
-ratelimiter
-click-spinner
-Deprecated
-scipy>=1.7.2
-ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
+expandvars>=0.6.5
 avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
+PyYAML
+click-spinner
 requests_file
-greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+greenlet
+termcolor>=1.0.0
 
 [unity-catalog]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
-requests
+databricks-cli==0.17.3
+jsonref
 packaging
-progressbar2
-docker
-humanfriendly
 tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-sqlparse==0.4.3
-termcolor>=1.0.0
-databricks-cli>=0.17.7
-ratelimiter
-click-spinner
-Deprecated
-sqllineage==1.3.6
-ijson
-PyYAML
-python-dateutil>=2.8.0
+docker
+aiohttp<4
+expandvars>=0.6.5
 avro-gen3==0.7.10
-databricks-sdk>=0.1.1
-pyspark
+PyYAML
+click-spinner
 requests_file
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [vertica]
+avro<1.11,>=1.10.2
+cached_property
 click>=7.1.2
-jsonschema
-aiohttp<4
+jsonref
 packaging
-progressbar2
+tabulate
 docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
 humanfriendly
-tabulate
-cached_property
-entrypoints
-avro<1.11,>=1.10.2
-vertica-sqlalchemy-dialect[vertica-python]==0.0.1
-termcolor>=1.0.0
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-click-spinner
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-scipy>=1.7.2
 ijson
-sqlalchemy<2,>=1.3.24
-PyYAML
-python-dateutil>=2.8.0
-avro-gen3==0.7.10
-great-expectations<=0.15.50,>=0.15.12
-requests_file
+vertica-sqlalchemy-dialect[vertica-python]==0.0.1
 greenlet
-expandvars>=0.6.5
-jsonref
-psutil>=5.8.0
-click-default-group
-toml>=0.10.0
+termcolor>=1.0.0
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 import sys
 import warnings
 
 # Published at https://pypi.org/project/acryl-datahub/.
 __package_name__ = "acryl-datahub-tc"
-__version__ = "0.10.2.3"
+__version__ = "0.10.2.rc1"
+ 
 
 
 def is_dev_mode() -> bool:
     return __version__.endswith("dev0")
 
 
 def nice_version_name() -> str:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/circuit_breaker/assertion_circuit_breaker.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/assertion_circuit_breaker.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/circuit_breaker/circuit_breaker.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/circuit_breaker.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/circuit_breaker/operation_circuit_breaker.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/operation_circuit_breaker.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/corpgroup/corpgroup.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpgroup/corpgroup.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/corpuser/corpuser.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpuser/corpuser.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/datajob/dataflow.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/dataflow.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/datajob/datajob.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/datajob.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/entities/dataprocess/dataprocess_instance.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/dataprocess/dataprocess_instance.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/graphql/assertion.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/assertion.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/graphql/base.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/base.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/api/graphql/operation.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/operation.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/check_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/check_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/cli_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/cli_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -365,15 +365,14 @@
     platform: Optional[str],
     env: Optional[str] = None,
     entity_type: str = "dataset",
     search_query: str = "*",
     include_removed: bool = False,
     only_soft_deleted: Optional[bool] = None,
 ) -> Iterable[str]:
-    # TODO: Replace with DataHubGraph call
     session, gms_host = get_session_and_host()
     endpoint: str = "/entities?action=search"
     url = gms_host + endpoint
     filter_criteria = []
     entity_type_lower = entity_type.lower()
     if env and entity_type_lower != "container":
         filter_criteria.append({"field": "origin", "value": env, "condition": "EQUAL"})
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/delete_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/delete_cli.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 from dataclasses import dataclass
 from datetime import datetime
 from random import choices
 from typing import Any, Dict, List, Optional, Tuple
 
 import click
 import progressbar
-from click_default_group import DefaultGroup
 from requests import sessions
 from tabulate import tabulate
 
 from datahub.cli import cli_utils
 from datahub.emitter import rest_emitter
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.metadata.schema_classes import StatusClass, SystemMetadataClass
@@ -22,20 +21,14 @@
 logger = logging.getLogger(__name__)
 
 RUN_TABLE_COLUMNS = ["urn", "aspect name", "created at"]
 
 UNKNOWN_NUM_RECORDS = -1
 
 
-@click.group(cls=DefaultGroup, default="by-filter")
-def delete() -> None:
-    """Delete metadata from DataHub."""
-    pass
-
-
 @dataclass
 class DeletionResult:
     start_time: int = int(time.time() * 1000.0)
     end_time: int = 0
     num_records: int = 0
     num_timeseries_records: int = 0
     num_entities: int = 0
@@ -58,41 +51,20 @@
         self.num_entities += another_result.num_entities
         if another_result.sample_records:
             if not self.sample_records:
                 self.sample_records = []
             self.sample_records.extend(another_result.sample_records)
 
 
-@delete.command()
-@click.option(
-    "--registry-id", required=False, type=str, help="e.g. mycompany-dq-model:0.0.1"
-)
-@click.option(
-    "--soft/--hard",
-    required=False,
-    is_flag=True,
-    default=True,
-    help="specifies soft/hard deletion",
-)
-@click.option("-n", "--dry-run", required=False, is_flag=True)
 @telemetry.with_telemetry()
-def by_registry(
+def delete_for_registry(
     registry_id: str,
     soft: bool,
     dry_run: bool,
 ) -> DeletionResult:
-    """
-    Delete all metadata written using the given registry id and version pair.
-    """
-
-    if soft and not dry_run:
-        raise click.UsageError(
-            "Soft-deleting with a registry-id is not yet supported. Try --dry-run to see what you will be deleting, before issuing a hard-delete using the --hard flag"
-        )
-
     deletion_result = DeletionResult()
     deletion_result.num_entities = 1
     deletion_result.num_records = UNKNOWN_NUM_RECORDS  # Default is unknown
     registry_delete = {"registryId": registry_id, "dryRun": dry_run, "soft": soft}
     (
         structured_rows,
         entities_affected,
@@ -104,15 +76,15 @@
     deletion_result.num_entities = entities_affected
     deletion_result.num_records = aspects_affected
     deletion_result.sample_records = structured_rows
     deletion_result.end()
     return deletion_result
 
 
-@delete.command()
+@click.command()
 @click.option("--urn", required=False, type=str, help="the urn of the entity")
 @click.option(
     "-a",
     # option with `_` is inconsistent with rest of CLI but kept for backward compatibility
     "--aspect_name",
     "--aspect-name",
     required=False,
@@ -153,39 +125,41 @@
 )
 @click.option(
     "--end-time",
     required=False,
     type=click.DateTime(),
     help="the end time(only for timeseries aspects)",
 )
+@click.option("--registry-id", required=False, type=str)
 @click.option("-n", "--dry-run", required=False, is_flag=True)
 @click.option("--only-soft-deleted", required=False, is_flag=True, default=False)
 @upgrade.check_upgrade
 @telemetry.with_telemetry()
-def by_filter(
+def delete(
     urn: str,
     aspect_name: Optional[str],
     force: bool,
     soft: bool,
     env: str,
     platform: str,
     entity_type: str,
     query: str,
     start_time: Optional[datetime],
     end_time: Optional[datetime],
+    registry_id: str,
     dry_run: bool,
     only_soft_deleted: bool,
 ) -> None:
     """Delete metadata from datahub using a single urn or a combination of filters"""
 
     cli_utils.test_connectivity_complain_exit("delete")
     # one of these must be provided
-    if not urn and not platform and not env and not query:
+    if not urn and not platform and not env and not query and not registry_id:
         raise click.UsageError(
-            "You must provide one of urn / platform / env / query in order to delete entities."
+            "You must provide one of urn / platform / env / query / registry_id in order to delete entities."
         )
 
     include_removed: bool
     if soft:
         # For soft-delete include-removed does not make any sense
         include_removed = False
     else:
@@ -247,14 +221,23 @@
             if deletion_result.num_records == 0:
                 click.echo(f"Nothing deleted for {urn}")
             else:
                 click.echo(
                     f"Successfully deleted {urn}. {deletion_result.num_records} rows deleted"
                 )
 
+    elif registry_id:
+        # Registry-id based delete
+        if soft and not dry_run:
+            raise click.UsageError(
+                "Soft-deleting with a registry-id is not yet supported. Try --dry-run to see what you will be deleting, before issuing a hard-delete using the --hard flag"
+            )
+        deletion_result = delete_for_registry(
+            registry_id=registry_id, soft=soft, dry_run=dry_run
+        )
     else:
         # Filter based delete
         deletion_result = delete_with_filters(
             env=env,
             platform=platform,
             dry_run=dry_run,
             soft=soft,
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/docker_check.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/docker_check.py`

 * *Files 2% similar despite different names*

```diff
@@ -133,25 +133,14 @@
             for container in self.containers
             if container.status != ContainerStatus.OK
         ]
 
     def is_ok(self) -> bool:
         return not self.errors()
 
-    def needs_up(self) -> bool:
-        return any(
-            container.status
-            in {
-                ContainerStatus.EXITED_WITH_FAILURE,
-                ContainerStatus.DIED,
-                ContainerStatus.MISSING,
-            }
-            for container in self.containers
-        )
-
     def to_exception(
         self, header: str, footer: Optional[str] = None
     ) -> QuickstartError:
         message = f"{header}\n"
         for error in self.errors():
             message += f"- {error}\n"
         if footer:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/docker_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/docker_cli.py`

 * *Files 5% similar despite different names*

```diff
@@ -21,15 +21,14 @@
 from requests_file import FileAdapter
 
 from datahub.cli.cli_utils import DATAHUB_ROOT_FOLDER
 from datahub.cli.docker_check import (
     DATAHUB_COMPOSE_LEGACY_VOLUME_FILTERS,
     DATAHUB_COMPOSE_PROJECT_FILTER,
     DockerComposeVersionError,
-    QuickstartStatus,
     check_docker_quickstart,
     get_docker_client,
     run_quickstart_preflight_checks,
 )
 from datahub.cli.quickstart_versioning import QuickstartVersionMappingConfig
 from datahub.ingestion.run.pipeline import Pipeline
 from datahub.telemetry import telemetry
@@ -57,18 +56,14 @@
     "docker/quickstart/docker-compose.consumers-without-neo4j.quickstart.yml"
 )
 KAFKA_SETUP_QUICKSTART_COMPOSE_FILE = (
     "docker/quickstart/docker-compose.kafka-setup.quickstart.yml"
 )
 
 
-_QUICKSTART_MAX_WAIT_TIME = datetime.timedelta(minutes=10)
-_QUICKSTART_STATUS_CHECK_INTERVAL = datetime.timedelta(seconds=2)
-
-
 class Architectures(Enum):
     x86 = "x86"
     arm64 = "arm64"
     m1 = "m1"
     m2 = "m2"
 
 
@@ -715,29 +710,35 @@
             "been built locally",
             fg="red",
         )
 
     if build_locally:
         logger.info("Building docker images locally...")
         subprocess.run(
-            base_command + ["build", "--pull", "-q"],
+            [
+                *base_command,
+                "build",
+                "--pull",
+                "-q",
+            ],
             check=True,
             env=_docker_subprocess_env(),
         )
         logger.info("Finished building docker images!")
 
     # Start it up! (with retries)
     click.echo("\nStarting up DataHub...")
+    max_wait_time = datetime.timedelta(minutes=8)
     start_time = datetime.datetime.now()
-    status: Optional[QuickstartStatus] = None
+    sleep_interval = datetime.timedelta(seconds=2)
+    up_interval = datetime.timedelta(seconds=30)
     up_attempts = 0
-    while (datetime.datetime.now() - start_time) < _QUICKSTART_MAX_WAIT_TIME:
-        # We must run docker-compose up at least once.
-        # Beyond that, we should run it again if something goes wrong.
-        if up_attempts == 0 or (status and status.needs_up()):
+    while (datetime.datetime.now() - start_time) < max_wait_time:
+        # Attempt to run docker compose up every `up_interval`.
+        if (datetime.datetime.now() - start_time) > up_attempts * up_interval:
             if up_attempts > 0:
                 click.echo()
             subprocess.run(
                 base_command + ["up", "-d", "--remove-orphans"],
                 env=_docker_subprocess_env(),
             )
             up_attempts += 1
@@ -745,18 +746,16 @@
         # Check docker health every few seconds.
         status = check_docker_quickstart()
         if status.is_ok():
             break
 
         # Wait until next iteration.
         click.echo(".", nl=False)
-        time.sleep(_QUICKSTART_STATUS_CHECK_INTERVAL.total_seconds())
+        time.sleep(sleep_interval.total_seconds())
     else:
-        assert status
-
         # Falls through if the while loop doesn't exit via break.
         click.echo()
         with tempfile.NamedTemporaryFile(suffix=".log", delete=False) as log_file:
             ret = subprocess.run(
                 base_command + ["logs"],
                 stdout=subprocess.PIPE,
                 stderr=subprocess.STDOUT,
@@ -866,35 +865,14 @@
                 f"Fetching consumer docker-compose file {consumer_github_file} from GitHub"
             )
             # Download the quickstart docker-compose file from GitHub.
             quickstart_download_response = request_session.get(consumer_github_file)
             quickstart_download_response.raise_for_status()
             tmp_file.write(quickstart_download_response.content)
             logger.debug(f"Copied to {path}")
-    if kafka_setup:
-        kafka_setup_github_file = f"{base_url}/{KAFKA_SETUP_QUICKSTART_COMPOSE_FILE}"
-
-        default_kafka_compose_file = (
-            Path(DATAHUB_ROOT_FOLDER) / "quickstart/docker-compose.kafka-setup.yml"
-        )
-        with open(
-            default_kafka_compose_file, "wb"
-        ) if default_kafka_compose_file else tempfile.NamedTemporaryFile(
-            suffix=".yml", delete=False
-        ) as tmp_file:
-            path = pathlib.Path(tmp_file.name)
-            quickstart_compose_file_list.append(path)
-            click.echo(
-                f"Fetching consumer docker-compose file {kafka_setup_github_file} from GitHub"
-            )
-            # Download the quickstart docker-compose file from GitHub.
-            quickstart_download_response = request_session.get(kafka_setup_github_file)
-            quickstart_download_response.raise_for_status()
-            tmp_file.write(quickstart_download_response.content)
-            logger.debug(f"Copied to {path}")
 
 
 def valid_restore_options(
     restore: bool, restore_indices: bool, no_restore_indices: bool
 ) -> bool:
     if no_restore_indices and not restore:
         click.secho(
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/get_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/get_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/ingest_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/ingest_cli.py`

 * *Files 0% similar despite different names*

```diff
@@ -195,15 +195,14 @@
     )
 
     loop = asyncio.get_event_loop()
     loop.run_until_complete(run_func_check_upgrade(pipeline))
 
 
 def _test_source_connection(report_to: Optional[str], pipeline_config: dict) -> None:
-    connection_report = None
     try:
         connection_report = ConnectionManager().test_source_connection(pipeline_config)
         logger.info(connection_report.as_json())
         if report_to and report_to != "datahub":
             with open(report_to, "w") as out_fp:
                 out_fp.write(connection_report.as_json())
             logger.info(f"Wrote report successfully to {report_to}")
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/json_file.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/json_file.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/lite_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/lite_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/migrate.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/migrate.py`

 * *Files 1% similar despite different names*

```diff
@@ -136,16 +136,14 @@
     click.echo(
         f"Starting migration: platform:{platform}, instance={instance}, force={force}, dry-run={dry_run}"
     )
     run_id: str = f"migrate-{uuid.uuid4()}"
     migration_report = MigrationReport(run_id, dry_run, keep)
     system_metadata = SystemMetadataClass(runId=run_id)
 
-    rest_emitter = None # add default
-    
     if not dry_run:
         rest_emitter = DatahubRestEmitter(
             gms_server=cli_utils.get_session_and_host()[1]
         )
 
     urns_to_migrate = []
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/migration_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/migration_utils.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/put_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/put_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/quickstart_versioning.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/quickstart_versioning.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/specific/file_loader.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/file_loader.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/specific/group_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/group_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/specific/user_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/user_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/cli/timeline_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/timeline_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/_config_enum.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/_config_enum.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/config_loader.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/config_loader.py`

 * *Files 2% similar despite different names*

```diff
@@ -85,26 +85,26 @@
         elif config_file_path.suffix == ".toml":
             config_mech = TomlConfigurationMechanism()
         else:
             raise ConfigurationError(
                 f"Only .toml and .yml are supported. Cannot process file type {config_file_path.suffix}"
             )
         url_parsed = parse.urlparse(str(config_file))
-        if url_parsed.scheme in ("http", "https"):  # URLs will return http/https
+        if url_parsed.scheme in ("file", ""):  # Possibly a local file
+            if not config_file_path.is_file():
+                raise ConfigurationError(f"Cannot open config file {config_file_path}")
+            raw_config_file = config_file_path.read_text()
+        else:
             try:
                 response = requests.get(str(config_file))
                 raw_config_file = response.text
             except Exception as e:
                 raise ConfigurationError(
                     f"Cannot read remote file {config_file_path}, error:{e}"
                 )
-        else:
-            if not config_file_path.is_file():
-                raise ConfigurationError(f"Cannot open config file {config_file_path}")
-            raw_config_file = config_file_path.read_text()
 
     config_fp = io.StringIO(raw_config_file)
     raw_config = config_mech.load_config(config_fp)
     config = resolve_env_variables(raw_config)
     if squirrel_original_config:
         config[squirrel_field] = raw_config
     return config
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/git.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/git.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/kafka.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/kafka.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 
 
 class _KafkaConnectionConfig(ConfigModel):
     # bootstrap servers
     bootstrap: str = "localhost:9092"
 
     # schema registry location
-    schema_registry_url: str = "http://localhost:8080/schema-registry/api/"
+    schema_registry_url: str = "http://localhost:8081"
 
     schema_registry_config: dict = Field(
         default_factory=dict,
         description="Extra schema registry config serialized as JSON. These options will be passed into Kafka's SchemaRegistryClient. https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html?#schemaregistryclient",
     )
 
     client_timeout_seconds: int = Field(
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/pydantic_field_deprecation.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/pydantic_field_deprecation.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/source_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/source_common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/time_window_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/time_window_config.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/validate_field_removal.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_field_removal.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/validate_field_rename.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_field_rename.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/configuration/validate_host_port.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_host_port.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/emitter/kafka_emitter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/kafka_emitter.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/emitter/mce_builder.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mce_builder.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/emitter/mcp.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/emitter/mcp_builder.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp_builder.py`

 * *Files 1% similar despite different names*

```diff
@@ -108,15 +108,15 @@
     dataset_id: str
 
 
 class FolderKey(PlatformKey):
     folder_abs_path: str
 
 
-class BucketKey(PlatformKey):
+class S3BucketKey(PlatformKey):
     bucket_name: str
 
 
 class DatahubKeyJSONEncoder(json.JSONEncoder):
     # overload method default
     def default(self, obj: Any) -> Any:
         if hasattr(obj, "guid"):
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/emitter/mcp_patch_builder.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp_patch_builder.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/emitter/request_helper.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/request_helper.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/emitter/rest_emitter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/rest_emitter.py`

 * *Files 1% similar despite different names*

```diff
@@ -57,21 +57,21 @@
         connect_timeout_sec: Optional[float] = None,
         read_timeout_sec: Optional[float] = None,
         retry_status_codes: Optional[List[int]] = None,
         retry_methods: Optional[List[str]] = None,
         retry_max_times: Optional[int] = None,
         extra_headers: Optional[Dict[str, str]] = None,
         ca_certificate_path: Optional[str] = None,
+        server_telemetry_id: Optional[str] = None,
         disable_ssl_verification: bool = False,
     ):
-        if not gms_server:
-            raise ConfigurationError("gms server is required")
         self._gms_server = gms_server
         self._token = token
         self.server_config: Dict[str, Any] = {}
+        self.server_telemetry_id: str = ""
 
         self._session = requests.Session()
 
         self._session.headers.update(
             {
                 "X-RestLi-Protocol-Version": "2.0.0",
                 "Content-Type": "application/json",
@@ -84,15 +84,15 @@
             if system_auth is not None:
                 self._session.headers.update({"Authorization": system_auth})
 
         if extra_headers:
             self._session.headers.update(extra_headers)
 
         if ca_certificate_path:
-            self._session.cert = ca_certificate_path
+            self._session.verify = ca_certificate_path
 
         if disable_ssl_verification:
             self._session.verify = False
 
         if connect_timeout_sec:
             self._connect_timeout_sec = connect_timeout_sec
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/emitter/serialization_helper.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/serialization_helper.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/entrypoints.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/entrypoints.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/committable.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/committable.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/common.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,14 @@
 from abc import ABCMeta, abstractmethod
 from dataclasses import dataclass
 from typing import TYPE_CHECKING, Dict, Generic, Iterable, Optional, Tuple, TypeVar
 
+import requests
+
+from datahub.configuration.common import ConfigurationError
 from datahub.emitter.mce_builder import set_dataset_urn_to_lower
 from datahub.ingestion.api.committable import Committable
 from datahub.ingestion.graph.client import DatahubClientConfig, DataHubGraph
 
 if TYPE_CHECKING:
     from datahub.ingestion.run.pipeline import PipelineConfig
 
@@ -53,16 +56,20 @@
         self.run_id = run_id
         self.pipeline_name = pipeline_name
         self.dry_run_mode = dry_run
         self.preview_mode = preview_mode
         self.checkpointers: Dict[str, Committable] = {}
         try:
             self.graph = DataHubGraph(datahub_api) if datahub_api is not None else None
+        except (requests.exceptions.ConnectionError, ConfigurationError) as e:
+            raise Exception("Failed to connect to DataHub") from e
         except Exception as e:
-            raise Exception(f"Failed to connect to DataHub: {e}") from e
+            raise Exception(
+                "Failed to instantiate a valid DataHub Graph instance"
+            ) from e
 
         self._set_dataset_urn_to_lower_if_needed()
 
     def _set_dataset_urn_to_lower_if_needed(self) -> None:
         # TODO: Get rid of this function once lower-casing is the standard.
         if self.graph:
             server_config = self.graph.get_config()
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/decorators.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/decorators.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/ingestion_job_checkpointing_provider_base.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/ingestion_job_checkpointing_provider_base.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/pipeline_run_listener.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/pipeline_run_listener.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/registry.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/registry.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/report.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/report.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/report_helpers.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/report_helpers.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/sink.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/sink.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/source.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/transform.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/transform.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/api/workunit.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/workunit.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/json_ref_patch.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/json_ref_patch.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/json_schema_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/json_schema_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/mce_extractor.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/mce_extractor.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/protobuf_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/protobuf_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/extractor/schema_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/schema_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/glossary/classification_mixin.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/classification_mixin.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 import logging
-from dataclasses import dataclass, field
 from typing import Dict, List, Optional
 
 from datahub_classify.helper_classes import ColumnInfo, Metadata
 from typing_extensions import Protocol
 
 from datahub.configuration.common import ConfigurationError
 from datahub.emitter.mce_builder import get_sys_time, make_term_urn, make_user_urn
@@ -11,26 +10,14 @@
 from datahub.ingestion.glossary.classifier_registry import classifier_registry
 from datahub.metadata.com.linkedin.pegasus2avro.common import (
     AuditStamp,
     GlossaryTermAssociation,
     GlossaryTerms,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.schema import SchemaMetadata
-from datahub.utilities.lossy_collections import LossyDict, LossyList
-
-
-@dataclass
-class ClassificationReportMixin:
-    num_tables_classification_attempted: int = 0
-    num_tables_classification_failed: int = 0
-    num_tables_classified: int = 0
-
-    info_types_detected: LossyDict[str, LossyList[str]] = field(
-        default_factory=LossyDict
-    )
 
 
 class ClassificationSourceConfigProtocol(Protocol):
     classification: Optional[ClassificationConfig]
 
 
 class ClassificationSourceProtocol(Protocol):
@@ -42,50 +29,28 @@
     def logger(self) -> logging.Logger:
         ...
 
     @property
     def config(self) -> ClassificationSourceConfigProtocol:
         ...
 
-    @property
-    def report(self) -> ClassificationReportMixin:
-        ...
-
     def is_classification_enabled(self) -> bool:
         ...
 
     def is_classification_enabled_for_column(
         self, dataset_name: str, column_name: str
     ) -> bool:
         ...
 
     def is_classification_enabled_for_table(self, dataset_name: str) -> bool:
         ...
 
     def get_classifiers(self) -> List[Classifier]:
         ...
 
-    def get_columns_to_classify(
-        self,
-        dataset_name: str,
-        schema_metadata: SchemaMetadata,
-        sample_data: Dict[str, list],
-    ) -> List[ColumnInfo]:
-        ...
-
-    def extract_field_wise_terms(
-        self, field_terms: Dict[str, str], column_infos: List[ColumnInfo]
-    ) -> None:
-        ...
-
-    def populate_terms_in_schema_metadata(
-        self, schema_metadata: SchemaMetadata, field_terms: Dict[str, str]
-    ) -> None:
-        ...
-
 
 class ClassificationMixin:
     def is_classification_enabled(self: ClassificationSourceProtocol) -> bool:
         return (
             self.config.classification is not None
             and self.config.classification.enabled
             and len(self.config.classification.classifiers) > 0
@@ -136,109 +101,69 @@
     def classify_schema_fields(
         self: ClassificationSourceProtocol,
         dataset_name: str,
         schema_metadata: SchemaMetadata,
         sample_data: Dict[str, list],
     ) -> None:
         assert self.config.classification
-        column_infos = self.get_columns_to_classify(
-            dataset_name, schema_metadata, sample_data
-        )
-
-        if not column_infos:
-            self.logger.debug(
-                f"No columns in {dataset_name} considered for classification"
-            )
-            return None
-
-        self.report.num_tables_classification_attempted += 1
-        field_terms: Dict[str, str] = {}
-        try:
-            for classifier in self.classifiers:
-                column_info_with_proposals = classifier.classify(column_infos)
-                self.extract_field_wise_terms(field_terms, column_info_with_proposals)
-        except Exception:
-            self.report.num_tables_classification_failed += 1
-            raise
-
-        if field_terms:
-            self.report.num_tables_classified += 1
-            self.populate_terms_in_schema_metadata(schema_metadata, field_terms)
-
-    def populate_terms_in_schema_metadata(
-        self: ClassificationSourceProtocol,
-        schema_metadata: SchemaMetadata,
-        field_terms: Dict[str, str],
-    ) -> None:
-        for schema_field in schema_metadata.fields:
-            if schema_field.fieldPath in field_terms:
-                schema_field.glossaryTerms = GlossaryTerms(
-                    terms=[
-                        GlossaryTermAssociation(
-                            urn=make_term_urn(field_terms[schema_field.fieldPath])
-                        )
-                    ]
-                    # Keep existing terms if present
-                    + (
-                        schema_field.glossaryTerms.terms
-                        if schema_field.glossaryTerms
-                        else []
-                    ),
-                    auditStamp=AuditStamp(
-                        time=get_sys_time(), actor=make_user_urn("datahub")
-                    ),
-                )
-
-    def extract_field_wise_terms(
-        self: ClassificationSourceProtocol,
-        field_terms: Dict[str, str],
-        column_info_with_proposals: List[ColumnInfo],
-    ) -> None:
-        assert self.config.classification
-        for col_info in column_info_with_proposals:
-            if not col_info.infotype_proposals:
-                continue
-            infotype_proposal = max(
-                col_info.infotype_proposals, key=lambda p: p.confidence_level
-            )
-            self.report.info_types_detected.setdefault(
-                infotype_proposal.infotype, LossyList()
-            ).append(f"{col_info.metadata.dataset_name}.{col_info.metadata.name}")
-            field_terms[
-                col_info.metadata.name
-            ] = self.config.classification.info_type_to_term.get(
-                infotype_proposal.infotype, infotype_proposal.infotype
-            )
-
-    def get_columns_to_classify(
-        self: ClassificationSourceProtocol,
-        dataset_name: str,
-        schema_metadata: SchemaMetadata,
-        sample_data: Dict[str, list],
-    ) -> List[ColumnInfo]:
         column_infos: List[ColumnInfo] = []
 
-        for schema_field in schema_metadata.fields:
+        for field in schema_metadata.fields:
             if not self.is_classification_enabled_for_column(
-                dataset_name, schema_field.fieldPath
+                dataset_name, field.fieldPath
             ):
                 self.logger.debug(
-                    f"Skipping column {dataset_name}.{schema_field.fieldPath} from classification"
+                    f"Skipping column {dataset_name}.{field.fieldPath} from classification"
                 )
                 continue
             column_infos.append(
                 ColumnInfo(
                     metadata=Metadata(
                         {
-                            "Name": schema_field.fieldPath,
-                            "Description": schema_field.description,
-                            "DataType": schema_field.nativeDataType,
+                            "Name": field.fieldPath,
+                            "Description": field.description,
+                            "DataType": field.nativeDataType,
                             "Dataset_Name": dataset_name,
                         }
                     ),
-                    values=sample_data[schema_field.fieldPath]
-                    if schema_field.fieldPath in sample_data.keys()
+                    values=sample_data[field.fieldPath]
+                    if field.fieldPath in sample_data.keys()
                     else [],
                 )
             )
 
-        return column_infos
+        if not column_infos:
+            self.logger.debug(
+                f"No columns in {dataset_name} considered for classification"
+            )
+            return None
+
+        field_terms = {}
+        for classifier in self.classifiers:
+            column_info_with_proposals = classifier.classify(column_infos)
+            for col_info in column_info_with_proposals:
+                if not col_info.infotype_proposals:
+                    continue
+                infotype_proposal = max(
+                    col_info.infotype_proposals, key=lambda p: p.confidence_level
+                )
+                self.logger.info(
+                    f"Info Type Suggestion for Column {col_info.metadata.name} => {infotype_proposal.infotype} with {infotype_proposal.confidence_level} "
+                )
+                field_terms[
+                    col_info.metadata.name
+                ] = self.config.classification.info_type_to_term.get(
+                    infotype_proposal.infotype, infotype_proposal.infotype
+                )
+
+        for field in schema_metadata.fields:
+            if field.fieldPath in field_terms:
+                field.glossaryTerms = GlossaryTerms(
+                    terms=[
+                        GlossaryTermAssociation(
+                            urn=make_term_urn(field_terms[field.fieldPath])
+                        )
+                    ],
+                    auditStamp=AuditStamp(
+                        time=get_sys_time(), actor=make_user_urn("datahub")
+                    ),
+                )
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/glossary/classifier.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/classifier.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/glossary/datahub_classifier.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/datahub_classifier.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/graph/client.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/graph/client.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,54 +1,39 @@
 import json
 import logging
-import textwrap
-import time
 from dataclasses import dataclass
 from enum import Enum
 from json.decoder import JSONDecodeError
-from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Type, Union
+from typing import Any, Dict, Iterable, List, Optional, Type
 
 from avro.schema import RecordSchema
 from deprecated import deprecated
 from requests.adapters import Response
 from requests.models import HTTPError
-from typing_extensions import Literal
 
 from datahub.cli.cli_utils import get_boolean_env_variable, get_url_and_token
 from datahub.configuration.common import ConfigModel, GraphError, OperationalError
 from datahub.emitter.aspect import TIMESERIES_ASPECT_MAP
-from datahub.emitter.mce_builder import Aspect, make_data_platform_urn
-from datahub.emitter.mcp import MetadataChangeProposalWrapper
+from datahub.emitter.mce_builder import Aspect
 from datahub.emitter.rest_emitter import DatahubRestEmitter
 from datahub.emitter.serialization_helper import post_json_transform
-from datahub.ingestion.source.state.checkpoint import Checkpoint
 from datahub.metadata.schema_classes import (
-    ASPECT_NAME_MAP,
-    AspectBag,
     BrowsePathsClass,
     DatasetPropertiesClass,
     DatasetUsageStatisticsClass,
     DomainPropertiesClass,
     DomainsClass,
     GlobalTagsClass,
     GlossaryTermsClass,
     OwnershipClass,
     SchemaMetadataClass,
-    StatusClass,
-    SystemMetadataClass,
     TelemetryClientIdClass,
 )
 from datahub.utilities.urns.urn import Urn, guess_entity_type
 
-if TYPE_CHECKING:
-    from datahub.ingestion.source.state.entity_removal_state import (
-        GenericCheckpointState,
-    )
-
-
 logger = logging.getLogger(__name__)
 
 
 telemetry_enabled = get_boolean_env_variable("DATAHUB_TELEMETRY_ENABLED", True)
 
 
 class DatahubClientConfig(ConfigModel):
@@ -66,35 +51,14 @@
 
 
 # Alias for backwards compatibility.
 # DEPRECATION: Remove in v0.10.2.
 DataHubGraphConfig = DatahubClientConfig
 
 
-def _graphql_entity_type(entity_type: str) -> str:
-    """Convert the entity types into GraphQL "EntityType" enum values."""
-
-    # Hard-coded special cases.
-    if entity_type == "corpuser":
-        return "CORP_USER"
-
-    # Convert camelCase to UPPER_UNDERSCORE.
-    entity_type = (
-        "".join(["_" + c.lower() if c.isupper() else c for c in entity_type])
-        .lstrip("_")
-        .upper()
-    )
-
-    # Strip the "DATA_HUB_" prefix.
-    if entity_type.startswith("DATA_HUB_"):
-        entity_type = entity_type[len("DATA_HUB_") :]
-
-    return entity_type
-
-
 class DataHubGraph(DatahubRestEmitter):
     def __init__(self, config: DatahubClientConfig) -> None:
         self.config = config
         super().__init__(
             gms_server=self.config.server,
             token=self.config.token,
             connect_timeout_sec=self.config.timeout_sec,  # reuse timeout_sec for connect timeout
@@ -321,22 +285,22 @@
             {"field": k, "value": v, "condition": "EQUAL"}
             for k, v in filter_criteria_map.items()
         ]
         query_body = {
             "urn": entity_urn,
             "entity": guess_entity_type(entity_urn),
             "aspect": aspect_type.ASPECT_NAME,
-            "limit": 1,
+            "latestValue": True,
             "filter": {"or": [{"and": filter_criteria}]},
         }
         end_point = f"{self.config.server}/aspects?action=getTimeseriesAspectValues"
         resp: Dict = self._post_generic(end_point, query_body)
         values: list = resp.get("value", {}).get("values")
         if values:
-            assert len(values) == 1, len(values)
+            assert len(values) == 1
             aspect_json: str = values[0].get("aspect", {}).get("value")
             if aspect_json:
                 return aspect_type.from_obj(json.loads(aspect_json), tuples=False)
             else:
                 raise GraphError(
                     f"Failed to find {aspect_type} in response {aspect_json}"
                 )
@@ -350,29 +314,22 @@
             assert aspects, "if provided, aspects must be a non-empty list"
             endpoint = f"{endpoint}?aspects=List(" + ",".join(aspects) + ")"
 
         response = self._session.get(endpoint)
         response.raise_for_status()
         return response.json()
 
-    @deprecated(
-        reason="Use get_aspect for a single aspect or get_entity_semityped for a full entity."
-    )
     def get_aspects_for_entity(
         self,
         entity_urn: str,
         aspects: List[str],
         aspect_types: List[Type[Aspect]],
     ) -> Dict[str, Optional[Aspect]]:
         """
-        Get multiple aspects for an entity.
-
-        Deprecated in favor of `get_aspect` (single aspect) or `get_entity_semityped` (full
-        entity without manually specifying a list of aspects).
-
+        Get multiple aspects for an entity. To get a single aspect for an entity, use the `get_aspect_v2` method.
         Warning: Do not use this method to determine if an entity exists!
         This method will always return an entity, even if it doesn't exist. This is an issue with how DataHub server
         responds to these calls, and will be fixed automatically when the server-side issue is fixed.
 
         :param str entity_urn: The urn of the entity
         :param List[Type[Aspect]] aspect_type_list: List of aspect type classes being requested (e.g. [datahub.metadata.schema_classes.DatasetProperties])
         :param List[str] aspects_list: List of aspect names being requested (e.g. [schemaMetadata, datasetProperties])
@@ -384,73 +341,36 @@
         ), f"number of aspects requested ({len(aspects)}) should be the same as number of aspect types provided ({len(aspect_types)})"
 
         # TODO: generate aspects list from type classes
         response_json = self.get_entity_raw(entity_urn, aspects)
 
         result: Dict[str, Optional[Aspect]] = {}
         for aspect_type in aspect_types:
-            aspect_type_name = aspect_type.get_aspect_name()
+            record_schema = aspect_type.RECORD_SCHEMA
+            aspect_type_name = record_schema.props["Aspect"]["name"]
 
             aspect_json = response_json.get("aspects", {}).get(aspect_type_name)
             if aspect_json:
                 # need to apply a transform to the response to match rest.li and avro serialization
                 post_json_obj = post_json_transform(aspect_json)
                 result[aspect_type_name] = aspect_type.from_obj(post_json_obj["value"])
             else:
                 result[aspect_type_name] = None
 
         return result
 
-    def get_entity_semityped(self, entity_urn: str) -> AspectBag:
-        """Get all non-timeseries aspects for an entity (experimental).
-
-        This method is called "semityped" because it returns aspects as a dictionary of
-        properly typed objects. While the returned dictionary is constrained using a TypedDict,
-        the return type is still fairly loose.
-
-        Warning: Do not use this method to determine if an entity exists! This method will always return
-        something, even if the entity doesn't actually exist in DataHub.
-
-        :param entity_urn: The urn of the entity
-        :returns: A dictionary of aspect name to aspect value. If an aspect is not found, it will
-            not be present in the dictionary. The entity's key aspect will always be present.
-        """
-
-        response_json = self.get_entity_raw(entity_urn)
-
-        # Now, we parse the response into proper aspect objects.
-        result: AspectBag = {}
-        for aspect_name, aspect_json in response_json.get("aspects", {}).items():
-            aspect_type = ASPECT_NAME_MAP.get(aspect_name)
-            if aspect_type is None:
-                logger.warning(f"Ignoring unknown aspect type {aspect_name}")
-                continue
-
-            post_json_obj = post_json_transform(aspect_json)
-            aspect_value = aspect_type.from_obj(post_json_obj["value"])
-            result[aspect_name] = aspect_value  # type: ignore
-
-        return result
-
-    @property
-    def _search_endpoint(self):
+    def _get_search_endpoint(self):
         return f"{self.config.server}/entities?action=search"
 
-    @property
-    def _relationships_endpoint(self):
+    def _get_relationships_endpoint(self):
         return f"{self.config.server}/openapi/relationships/v1/"
 
-    @property
-    def _aspect_count_endpoint(self):
+    def _get_aspect_count_endpoint(self):
         return f"{self.config.server}/aspects?action=getCount"
 
-    @property
-    def _scroll_across_entities_endpoint(self):
-        return f"{self.config.server}/entities?action=scrollAcrossEntities"
-
     def get_domain_urn_by_name(self, domain_name: str) -> Optional[str]:
         """Retrieve a domain urn based on its name. Returns None if there is no match found"""
 
         filters = []
         filter_criteria = [
             {
                 "field": "name",
@@ -463,15 +383,15 @@
         search_body = {
             "input": "*",
             "entity": "domain",
             "start": 0,
             "count": 10,
             "filter": {"or": filters},
         }
-        results: Dict = self._post_generic(self._search_endpoint, search_body)
+        results: Dict = self._post_generic(self._get_search_endpoint(), search_body)
         num_entities = results.get("value", {}).get("numEntities", 0)
         if num_entities > 1:
             logger.warning(
                 f"Got {num_entities} results for domain name {domain_name}. Will return the first match."
             )
         entities_yielded: int = 0
         entities = []
@@ -483,15 +403,15 @@
 
     def get_container_urns_by_filter(
         self,
         env: Optional[str] = None,
         search_query: str = "*",
     ) -> Iterable[str]:
         """Return container urns that match based on query"""
-        url = self._search_endpoint
+        url = self._get_search_endpoint()
 
         container_filters = []
         for container_subtype in ["Database", "Schema", "Project", "Dataset"]:
             filter_criteria = []
 
             filter_criteria.append(
                 {
@@ -521,136 +441,26 @@
         logger.debug(f"Matched {num_entities} containers")
         entities_yielded: int = 0
         for x in results["value"]["entities"]:
             entities_yielded += 1
             logger.debug(f"yielding {x['entity']}")
             yield x["entity"]
 
-    def get_urns_by_filter(
-        self,
-        *,
-        entity_types: Optional[List[str]] = None,
-        platform: Optional[str] = None,
-        batch_size: int = 10000,
-    ) -> Iterable[str]:
-        """Fetch all urns that match the given filters.
-
-        Filters are combined conjunctively. If multiple filters are specified, the results will match all of them.
-        Note that specifying a platform filter will automatically exclude all entity types that do not have a platform.
-
-        :param entity_types: List of entity types to include. If None, all entity types will be returned.
-        :param platform: Platform to filter on. If None, all platforms will be returned.
-        """
-
-        types: Optional[List[str]] = None
-        if entity_types is not None:
-            if not entity_types:
-                raise ValueError("entity_types cannot be an empty list")
-
-            types = [_graphql_entity_type(entity_type) for entity_type in entity_types]
-
-        # Does not filter on env, because env is missing in dashboard / chart urns and custom properties
-        # For containers, use { field: "customProperties", values: ["instance=env}"], condition:EQUAL }
-        # For others, use { field: "origin", values: ["env"], condition:EQUAL }
-
-        andFilters = []
-        if platform:
-            andFilters += [
-                {
-                    "field": "platform.keyword",
-                    "values": [make_data_platform_urn(platform)],
-                    "condition": "EQUAL",
-                }
-            ]
-        orFilters = [{"and": andFilters}]
-
-        query = textwrap.dedent(
-            """
-            query scrollUrnsWithFilters(
-                $types: [EntityType!],
-                $orFilters: [AndFilterInput!],
-                $batchSize: Int!,
-                $scrollId: String) {
-
-                scrollAcrossEntities(input: {
-                    query: "*",
-                    count: $batchSize,
-                    scrollId: $scrollId,
-                    types: $types,
-                    orFilters: $orFilters,
-                    searchFlags: { skipHighlighting: true }
-                }) {
-                    nextScrollId
-                    searchResults {
-                        entity {
-                            urn
-                        }
-                    }
-                }
-            }
-            """
-        )
-
-        # Set scroll_id to False to enter while loop
-        scroll_id: Union[Literal[False], str, None] = False
-        while scroll_id is not None:
-            response = self.execute_graphql(
-                query,
-                variables={
-                    "types": types,
-                    "orFilters": orFilters,
-                    "batchSize": batch_size,
-                    "scrollId": scroll_id or None,
-                },
-            )
-            data = response["scrollAcrossEntities"]
-            scroll_id = data["nextScrollId"]
-            for entry in data["searchResults"]:
-                yield entry["entity"]["urn"]
-
-    def get_latest_pipeline_checkpoint(
-        self, pipeline_name: str, platform: str
-    ) -> Optional[Checkpoint["GenericCheckpointState"]]:
-        from datahub.ingestion.source.state.entity_removal_state import (
-            GenericCheckpointState,
-        )
-        from datahub.ingestion.source.state.stale_entity_removal_handler import (
-            StaleEntityRemovalHandler,
-        )
-        from datahub.ingestion.source.state_provider.datahub_ingestion_checkpointing_provider import (
-            DatahubIngestionCheckpointingProvider,
-        )
-
-        checkpoint_provider = DatahubIngestionCheckpointingProvider(self, "graph")
-        job_name = StaleEntityRemovalHandler.compute_job_id(platform)
-
-        raw_checkpoint = checkpoint_provider.get_latest_checkpoint(
-            pipeline_name, job_name
-        )
-        if not raw_checkpoint:
-            return None
-
-        return Checkpoint.create_from_checkpoint_aspect(
-            job_name=job_name,
-            checkpoint_aspect=raw_checkpoint,
-            state_class=GenericCheckpointState,
-        )
-
     def get_search_results(
         self, start: int = 0, count: int = 1, entity: str = "dataset"
     ) -> Dict:
         search_body = {"input": "*", "entity": entity, "start": start, "count": count}
-        results: Dict = self._post_generic(self._search_endpoint, search_body)
+        results: Dict = self._post_generic(self._get_search_endpoint(), search_body)
         return results
 
     def get_aspect_counts(self, aspect: str, urn_like: Optional[str] = None) -> int:
         args = {"aspect": aspect}
         if urn_like is not None:
             args["urnLike"] = urn_like
-        results = self._post_generic(self._aspect_count_endpoint, args)
+        results = self._post_generic(self._get_aspect_count_endpoint(), args)
         return results["value"]
 
     def execute_graphql(self, query: str, variables: Optional[Dict] = None) -> Dict:
         url = f"{self.config.server}/api/graphql"
         body: Dict = {
             "query": query,
         }
@@ -674,15 +484,15 @@
 
     def get_related_entities(
         self,
         entity_urn: str,
         relationship_types: List[str],
         direction: RelationshipDirection,
     ) -> Iterable[RelatedEntity]:
-        relationship_endpoint = self._relationships_endpoint
+        relationship_endpoint = self._get_relationships_endpoint()
         done = False
         start = 0
         while not done:
             response = self._get_generic(
                 url=relationship_endpoint,
                 params={
                     "urn": entity_urn,
@@ -697,27 +507,11 @@
                     relationship_type=related_entity["relationshipType"],
                 )
             done = response.get("count", 0) == 0 or response.get("count", 0) < len(
                 response.get("entities", [])
             )
             start = start + response.get("count", 0)
 
-    def soft_delete_urn(
-        self,
-        urn: str,
-        run_id: str = "soft-delete-urns",
-    ) -> None:
-        timestamp = int(time.time() * 1000)
-        self.emit_mcp(
-            MetadataChangeProposalWrapper(
-                entityUrn=urn,
-                aspect=StatusClass(removed=True),
-                systemMetadata=SystemMetadataClass(
-                    runId=run_id, lastObserved=timestamp
-                ),
-            )
-        )
-
 
 def get_default_graph() -> DataHubGraph:
     (url, token) = get_url_and_token()
     return DataHubGraph(DatahubClientConfig(server=url, token=token))
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/reporting/datahub_ingestion_run_summary_provider.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/datahub_ingestion_run_summary_provider.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/reporting/file_reporter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/file_reporter.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/run/connection.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/connection.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/run/pipeline.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/pipeline.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/run/pipeline_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/pipeline_config.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/blackhole.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/blackhole.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/console.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/console.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/datahub_kafka.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_kafka.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/datahub_lite.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_lite.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/datahub_rest.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_rest.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/sink/file.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/file.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/aws_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/aws_common.py`

 * *Files 6% similar despite different names*

```diff
@@ -55,15 +55,15 @@
 
     assumed_role_object = sts_client.assume_role(
         **assume_role_args,
     )
     return dict(assumed_role_object["Credentials"])
 
 
-AUTODETECT_CREDENTIALS_DOC_LINK = "Can be auto-detected, see [the AWS boto3 docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) for details."
+AUTODETECT_CREDENTIALS_DOC_LINK = "Can be auto-detected, see https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html for details."
 
 
 class AwsConnectionConfig(ConfigModel):
     """
     Common AWS credentials config.
 
     Currently used by:
@@ -93,19 +93,19 @@
         default=None,
         description="Named AWS profile to use. Only used if access key / secret are unset. If not set the default will be used",
     )
     aws_region: str = Field(description="AWS region code.")
 
     aws_endpoint_url: Optional[str] = Field(
         default=None,
-        description="The AWS service endpoint. This is normally [constructed automatically](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html), but can be overridden here.",
+        description="Autodetected. See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html",
     )
     aws_proxy: Optional[Dict[str, str]] = Field(
         default=None,
-        description="A set of proxy configs to use with AWS. See the [botocore.config](https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html) docs for details.",
+        description="Autodetected. See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html",
     )
 
     def _normalized_aws_roles(self) -> List[AwsAssumeRoleConfig]:
         if not self.aws_role:
             return []
         elif isinstance(self.aws_role, str):
             return [AwsAssumeRoleConfig(RoleArn=self.aws_role)]
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/glue.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/glue.py`

 * *Files 2% similar despite different names*

```diff
@@ -130,32 +130,28 @@
         default=True, description="Whether to extract Glue transform jobs."
     )
     ignore_unsupported_connectors: Optional[bool] = Field(
         default=True,
         description="Whether to ignore unsupported connectors. If disabled, an error will be raised.",
     )
     emit_s3_lineage: bool = Field(
-        default=False, description="Whether to emit S3-to-Glue lineage."
+        default=False, description=" Whether to emit S3-to-Glue lineage."
     )
     glue_s3_lineage_direction: str = Field(
         default="upstream",
         description="If `upstream`, S3 is upstream to Glue. If `downstream` S3 is downstream to Glue.",
     )
     domain: Dict[str, AllowDenyPattern] = Field(
         default=dict(),
         description="regex patterns for tables to filter to assign domain_key. ",
     )
     catalog_id: Optional[str] = Field(
         default=None,
         description="The aws account id where the target glue catalog lives. If None, datahub will ingest glue in aws caller's account.",
     )
-    ignore_resource_links: Optional[bool] = Field(
-        default=False,
-        description="If set to True, ignore database resource links.",
-    )
     use_s3_bucket_tags: Optional[bool] = Field(
         default=False,
         description="If an S3 Buckets Tags should be created for the Tables ingested by Glue. Please Note that this will not apply tags to any folders ingested, only the files.",
     )
     use_s3_object_tags: Optional[bool] = Field(
         default=False,
         description="If an S3 Objects Tags should be created for the Tables ingested by Glue.",
@@ -634,67 +630,70 @@
                     ),
                 ],
             )
         )
 
         return MetadataWorkUnit(id=f'{job_name}-{node["Id"]}', mce=mce)
 
-    def get_all_databases(self) -> Iterable[Mapping[str, Any]]:
-        # see https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue/paginator/GetDatabases.html
-        paginator = self.glue_client.get_paginator("get_databases")
-
-        if self.source_config.catalog_id:
-            paginator_response = paginator.paginate(
-                CatalogId=self.source_config.catalog_id
-            )
-        else:
-            paginator_response = paginator.paginate()
+    def get_all_tables_and_databases(
+        self,
+    ) -> Tuple[Dict, List[Dict]]:
+        def get_tables_from_database(database_name: str) -> List[dict]:
+            new_tables = []
 
-        for page in paginator_response:
-            yield from page["DatabaseList"]
+            # see https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.get_tables
+            paginator = self.glue_client.get_paginator("get_tables")
 
-    def get_tables_from_database(self, database_name: str) -> Iterable[Dict]:
-        # see https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue/paginator/GetTables.html
-        paginator = self.glue_client.get_paginator("get_tables")
-
-        if self.source_config.catalog_id:
-            paginator_response = paginator.paginate(
-                DatabaseName=database_name, CatalogId=self.source_config.catalog_id
-            )
-        else:
-            paginator_response = paginator.paginate(DatabaseName=database_name)
+            if self.source_config.catalog_id:
+                paginator_response = paginator.paginate(
+                    DatabaseName=database_name, CatalogId=self.source_config.catalog_id
+                )
+            else:
+                paginator_response = paginator.paginate(DatabaseName=database_name)
 
-        for page in paginator_response:
-            yield from page["TableList"]
+            for page in paginator_response:
+                new_tables += page["TableList"]
 
-    def get_all_databases_and_tables(
-        self,
-    ) -> Tuple[Dict, List[Dict]]:
-        all_databases = self.get_all_databases()
+            return new_tables
+
+        def get_databases() -> List[Mapping[str, Any]]:
+            databases = []
+
+            # see https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.get_databases
+            paginator = self.glue_client.get_paginator("get_databases")
+
+            if self.source_config.catalog_id:
+                paginator_response = paginator.paginate(
+                    CatalogId=self.source_config.catalog_id
+                )
+            else:
+                paginator_response = paginator.paginate()
+
+            for page in paginator_response:
+                for db in page["DatabaseList"]:
+                    if self.source_config.database_pattern.allowed(db["Name"]):
+                        databases.append(db)
+
+            return databases
 
-        if self.source_config.ignore_resource_links:
-            all_databases = [
-                database
-                for database in all_databases
-                if "TargetDatabase" not in database
-            ]
+        all_databases = get_databases()
 
-        allowed_databases = {
+        databases = {
             database["Name"]: database
             for database in all_databases
             if self.source_config.database_pattern.allowed(database["Name"])
         }
 
-        all_tables = [
+        all_tables: List[dict] = [
             table
-            for database_name in allowed_databases
-            for table in self.get_tables_from_database(database_name)
+            for databaseName in databases.keys()
+            for table in get_tables_from_database(databaseName)
         ]
 
-        return allowed_databases, all_tables
+        return databases, all_tables
 
     def get_lineage_if_enabled(
         self, mce: MetadataChangeEventClass
     ) -> Optional[MetadataChangeProposalWrapper]:
         if self.source_config.emit_s3_lineage:
             # extract dataset properties aspect
             dataset_properties: Optional[
@@ -939,15 +938,15 @@
         return auto_stale_entity_removal(
             self.stale_entity_removal_handler,
             auto_status_aspect(self.get_workunits_internal()),
         )
 
     def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         database_seen = set()
-        databases, tables = self.get_all_databases_and_tables()
+        databases, tables = self.get_all_tables_and_databases()
 
         for table in tables:
             database_name = table["DatabaseName"]
             table_name = table["Name"]
             full_table_name = f"{database_name}.{table_name}"
             self.report.report_table_scanned()
             if not self.source_config.database_pattern.allowed(
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/s3_boto_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/s3_boto_utils.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/s3_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/s3_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/feature_groups.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/feature_groups.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/job_classes.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/job_classes.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/jobs.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/jobs.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/lineage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/lineage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/aws/sagemaker_processors/models.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/models.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/azure/azure_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/azure/azure_common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/bigquery.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery.py`

 * *Files 2% similar despite different names*

```diff
@@ -488,34 +488,49 @@
             )
             if self.config.include_external_url
             else None,
             tags=tags_joined,
         )
 
     def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
+        logger.info("Getting projects")
         conn: bigquery.Client = get_bigquery_client(self.config)
         self.add_config_to_report()
 
         projects = self._get_projects(conn)
-        if not projects:
+        if len(projects) == 0:
+            logger.error(
+                "Get projects didn't return any project. "
+                "Maybe resourcemanager.projects.get permission is missing for the service account. "
+                "You can assign predefined roles/bigquery.metadataViewer role to your service account."
+            )
+            self.report.report_failure(
+                "metadata-extraction",
+                "Get projects didn't return any project. "
+                "Maybe resourcemanager.projects.get permission is missing for the service account. "
+                "You can assign predefined roles/bigquery.metadataViewer role to your service account.",
+            )
             return
 
         for project_id in projects:
+            if not self.config.project_id_pattern.allowed(project_id.id):
+                self.report.report_dropped(project_id.id)
+                continue
             logger.info(f"Processing project: {project_id.id}")
-            self.report.set_ingestion_stage(project_id.id, "Metadata Extraction")
+            self.report.set_project_state(project_id.id, "Metadata Extraction")
             yield from self._process_project(conn, project_id)
 
         if self._should_ingest_usage():
             yield from self.usage_extractor.run(
                 [p.id for p in projects], self.table_refs
             )
 
         if self._should_ingest_lineage():
             for project in projects:
-                self.report.set_ingestion_stage(project.id, "Lineage Extraction")
+                self.report.set_project_state(project.id, "Lineage Extraction")
                 yield from self.generate_lineage(project.id)
 
     def get_workunits(self) -> Iterable[MetadataWorkUnit]:
         return auto_materialize_referenced_tags(
             auto_stale_entity_removal(
                 self.stale_entity_removal_handler,
                 auto_workunit_reporter(
@@ -565,45 +580,35 @@
                 self.redundant_run_skip_handler.update_state(
                     start_time_millis=datetime_to_ts_millis(self.config.start_time),
                     end_time_millis=datetime_to_ts_millis(self.config.end_time),
                 )
         return True
 
     def _get_projects(self, conn: bigquery.Client) -> List[BigqueryProject]:
-        logger.info("Getting projects")
         if self.config.project_ids or self.config.project_id:
             project_ids = self.config.project_ids or [self.config.project_id]  # type: ignore
             return [
                 BigqueryProject(id=project_id, name=project_id)
                 for project_id in project_ids
             ]
         else:
-            return list(self._get_project_list(conn))
-
-    def _get_project_list(self, conn: bigquery.Client) -> Iterable[BigqueryProject]:
-        try:
-            projects = BigQueryDataDictionary.get_projects(conn)
-        except Exception as e:
-            logger.error(f"Error getting projects. {e}", exc_info=True)
-            projects = []
-
-        if not projects:  # Report failure on exception and if empty list is returned
-            self.report.report_failure(
-                "metadata-extraction",
-                "Get projects didn't return any project. "
-                "Maybe resourcemanager.projects.get permission is missing for the service account. "
-                "You can assign predefined roles/bigquery.metadataViewer role to your service account.",
-            )
-            return []
-
-        for project in projects:
-            if self.config.project_id_pattern.allowed(project.id):
-                yield project
-            else:
-                self.report.report_dropped(project.id)
+            try:
+                return BigQueryDataDictionary.get_projects(conn)
+            except Exception as e:
+                # TODO: Merge with error logging in `get_workunits_internal`
+                trace = traceback.format_exc()
+                logger.error(
+                    f"Get projects didn't return any project. Maybe resourcemanager.projects.get permission is missing for the service account. You can assign predefined roles/bigquery.metadataViewer role to your service account. The error was: {e}"
+                )
+                logger.error(trace)
+                self.report.report_failure(
+                    "metadata-extraction",
+                    f"Get projects didn't return any project. Maybe resourcemanager.projects.get permission is missing for the service account. You can assign predefined roles/bigquery.metadataViewer role to your service account. The error was: {e} Stacktrace: {trace}",
+                )
+                return []
 
     def _process_project(
         self, conn: bigquery.Client, bigquery_project: BigqueryProject
     ) -> Iterable[MetadataWorkUnit]:
         db_tables: Dict[str, List[BigqueryTable]] = {}
         db_views: Dict[str, List[BigqueryView]] = {}
 
@@ -662,15 +667,15 @@
                     "metadata-extraction",
                     f"{project_id}.{bigquery_dataset.name} - {error_message} - {trace}",
                 )
                 continue
 
         if self.config.profiling.enabled:
             logger.info(f"Starting profiling project {project_id}")
-            self.report.set_ingestion_stage(project_id, "Profiling")
+            self.report.set_project_state(project_id, "Profiling")
             yield from self.profiler.get_workunits(
                 project_id=project_id,
                 tables=db_tables,
             )
 
     def generate_lineage(self, project_id: str) -> Iterable[MetadataWorkUnit]:
         logger.info(f"Generate lineage for {project_id}")
@@ -718,56 +723,35 @@
     ) -> Iterable[MetadataWorkUnit]:
         dataset_name = bigquery_dataset.name
 
         yield from self.gen_dataset_containers(
             dataset_name, project_id, bigquery_dataset.labels
         )
 
-        columns = None
-        if self.config.include_tables or self.config.include_views:
-            columns = BigQueryDataDictionary.get_columns_for_dataset(
-                conn,
-                project_id=project_id,
-                dataset_name=dataset_name,
-                column_limit=self.config.column_limit,
-                run_optimized_column_query=self.config.run_optimized_column_query,
-            )
+        columns = BigQueryDataDictionary.get_columns_for_dataset(
+            conn,
+            project_id=project_id,
+            dataset_name=dataset_name,
+            column_limit=self.config.column_limit,
+            run_optimized_column_query=self.config.run_optimized_column_query,
+        )
 
         if self.config.include_tables:
             db_tables[dataset_name] = list(
                 self.get_tables_for_dataset(conn, project_id, dataset_name)
             )
 
             for table in db_tables[dataset_name]:
                 table_columns = columns.get(table.name, []) if columns else []
                 yield from self._process_table(
                     table=table,
                     columns=table_columns,
                     project_id=project_id,
                     dataset_name=dataset_name,
                 )
-        elif self.config.include_table_lineage or self.config.include_usage_statistics:
-            # Need table_refs to calculate lineage and usage
-            for table_item in conn.list_tables(f"{project_id}.{dataset_name}"):
-                identifier = BigqueryTableIdentifier(
-                    project_id=project_id,
-                    dataset=dataset_name,
-                    table=table_item.table_id,
-                )
-                if not self.config.table_pattern.allowed(identifier.raw_table_name()):
-                    self.report.report_dropped(identifier.raw_table_name())
-                    continue
-                try:
-                    self.table_refs.add(
-                        str(BigQueryTableRef(identifier).get_sanitized_table_ref())
-                    )
-                except Exception as e:
-                    logger.warning(
-                        f"Could not create table ref for {table_item.path}: {e}"
-                    )
 
         if self.config.include_views:
             db_views[dataset_name] = list(
                 BigQueryDataDictionary.get_views_for_dataset(
                     conn, project_id, dataset_name, self.config.profiling.enabled
                 )
             )
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/bigquery_audit.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_audit.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,43 +9,59 @@
 
 from datahub.emitter.mce_builder import make_dataset_urn
 from datahub.utilities.parsing_util import (
     get_first_missing_key,
     get_first_missing_key_any,
 )
 
-BQ_FILTER_RULE_TEMPLATE = "BQ_FILTER_RULE_TEMPLATE"
-
 BQ_AUDIT_V2 = {
-    BQ_FILTER_RULE_TEMPLATE: """
+    "BQ_FILTER_REGEX_ALLOW_TEMPLATE": """protoPayload.metadata.jobChange.job.jobStats.queryStats.referencedTables =~ "projects/.*/datasets/.*/tables/{table_allow_pattern}"
+""".strip(
+        "\t \n"
+    ),
+    "BQ_FILTER_REGEX_DENY_TEMPLATE": """
+    {logical_operator}
+            NOT (
+                protoPayload.metadata.jobChange.job.jobStats.queryStats.referencedTables =~ "projects/.*/datasets/.*/tables/{table_deny_pattern}"
+            )
+""".strip(
+        "\t \n"
+    ),
+    "BQ_FILTER_RULE_TEMPLATE": """
 resource.type=("bigquery_project" OR "bigquery_dataset")
 AND
-timestamp >= "{start_time}"
-AND
-timestamp < "{end_time}"
-AND
 (
     (
         protoPayload.methodName=
             (
                 "google.cloud.bigquery.v2.JobService.Query"
                 OR
                 "google.cloud.bigquery.v2.JobService.InsertJob"
             )
         AND
         protoPayload.metadata.jobChange.job.jobStatus.jobState="DONE"
         AND NOT protoPayload.metadata.jobChange.job.jobStatus.errorResult:*
         AND protoPayload.metadata.jobChange.job.jobStats.queryStats.referencedTables:*
         AND NOT protoPayload.metadata.jobChange.job.jobStats.queryStats.referencedTables =~ "projects/.*/datasets/.*/tables/__TABLES__|__TABLES_SUMMARY__|INFORMATION_SCHEMA.*"
+         AND (
+            {allow_regex}
+            {deny_regex}
+         OR
+            protoPayload.metadata.tableDataRead.reason = "JOB"
+        )
     )
     OR
     (
         protoPayload.metadata.tableDataRead:*
     )
 )
+AND
+timestamp >= "{start_time}"
+AND
+timestamp < "{end_time}"
 """.strip(
         "\t \n"
     ),
 }
 
 AuditLogEntry = Any
 
@@ -162,17 +178,16 @@
         return self.get_table_name()
 
 
 @dataclass(frozen=True, order=True)
 class BigQueryTableRef:
     # Handle table time travel. See https://cloud.google.com/bigquery/docs/time-travel
     # See https://cloud.google.com/bigquery/docs/table-decorators#time_decorators
-    SNAPSHOT_TABLE_REGEX: ClassVar[Pattern[str]] = re.compile(
-        "^(.+)@(-?\\d{1,13})(-(-?\\d{1,13})?)?$"
-    )
+    # Handling for @0 and @-TIME_OFFSET is apparently missing
+    SNAPSHOT_TABLE_REGEX: ClassVar[Pattern[str]] = re.compile(r"^(.+)@(\d{13})$")
 
     table_identifier: BigqueryTableIdentifier
 
     @classmethod
     def from_bigquery_table(cls, table: BigqueryTableIdentifier) -> "BigQueryTableRef":
         return cls(
             BigqueryTableIdentifier(table.project_id, table.dataset, table.table)
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/bigquery_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_config.py`

 * *Files 3% similar despite different names*

```diff
@@ -98,19 +98,15 @@
     # The inheritance hierarchy is wonky here, but these options need modifications.
     project_id: Optional[str] = Field(
         default=None,
         description="[deprecated] Use project_id_pattern or project_ids instead.",
     )
     project_ids: List[str] = Field(
         default_factory=list,
-        description=(
-            "Ingests specified project_ids. Use this property if you want to specify what projects to ingest or "
-            "don't want to give project resourcemanager.projects.list to your service account. "
-            "Overrides `project_id_pattern`."
-        ),
+        description="Ingests specified project_ids. Use this property if you only want to ingest one project and don't want to give project resourcemanager.projects.list to your service account.",
     )
 
     project_on_behalf: Optional[str] = Field(
         default=None,
         description="[Advanced] The BigQuery project in which queries are executed. Will be passed when creating a job. If not passed, falls back to the project associated with the service account.",
     )
 
@@ -193,15 +189,15 @@
         hidden_from_docs=True,
         default=False,
         description="Run optimized column query to get column information. This is an experimental feature and may not work for all cases.",
     )
 
     file_backed_cache_size: int = Field(
         hidden_from_docs=True,
-        default=2000,
+        default=200,
         description="Maximum number of entries for the in-memory caches of FileBacked data structures.",
     )
 
     def __init__(self, **data: Any):
         super().__init__(**data)
 
         if self.credential:
@@ -264,17 +260,18 @@
                 "Please update `dataset_pattern` to match against fully qualified schema name `<project_id>.<dataset_name>` and set config `match_fully_qualified_names : True`."
                 "Current default `match_fully_qualified_names: False` is only to maintain backward compatibility. "
                 "The config option `match_fully_qualified_names` will be deprecated in future and the default behavior will assume `match_fully_qualified_names: True`."
             )
         return values
 
     def get_table_pattern(self, pattern: List[str]) -> str:
-        return "|".join(pattern) if pattern else ""
+        return "|".join(pattern) if self.table_pattern else ""
 
-    def get_sql_alchemy_url(self) -> str:
+    # TODO: remove run_on_compute when the legacy bigquery source will be deprecated
+    def get_sql_alchemy_url(self, run_on_compute: bool = False) -> str:
         if self.project_on_behalf:
             return f"bigquery://{self.project_on_behalf}"
         # When project_id is not set, we will attempt to detect the project ID
         # based on the credentials or environment variables.
         # See https://github.com/mxmzdlv/pybigquery#authentication.
         return "bigquery://"
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/bigquery_report.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_report.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import collections
 import dataclasses
 import logging
 from dataclasses import dataclass, field
-from datetime import datetime, timezone
+from datetime import datetime
 from typing import Counter, Dict, List, Optional
 
 import pydantic
 
 from datahub.ingestion.source.sql.sql_generic_profiler import ProfilingSqlReport
 from datahub.utilities.lossy_collections import LossyDict, LossyList
 from datahub.utilities.perf_timer import PerfTimer
@@ -66,36 +66,31 @@
     deny_pattern: Optional[str] = None
     num_usage_workunits_emitted: int = 0
     total_query_log_entries: int = 0
     num_read_events: int = 0
     num_query_events: int = 0
     num_filtered_read_events: int = 0
     num_filtered_query_events: int = 0
-    num_usage_query_hash_collisions: int = 0
     num_operational_stats_workunits_emitted: int = 0
     read_reasons_stat: Counter[str] = dataclasses.field(
         default_factory=collections.Counter
     )
     operation_types_stat: Counter[str] = dataclasses.field(
         default_factory=collections.Counter
     )
-    usage_state_size: Optional[str] = None
-    ingestion_stage: Optional[str] = None
-    ingestion_stage_durations: Dict[str, str] = field(default_factory=TopKDict)
+    current_project_status: Optional[str] = None
 
-    _timer: Optional[PerfTimer] = field(
+    timer: Optional[PerfTimer] = field(
         default=None, init=False, repr=False, compare=False
     )
 
-    def set_ingestion_stage(self, project: str, stage: str) -> None:
-        if self._timer:
-            elapsed = f"{self._timer.elapsed_seconds():.2f}"
+    def set_project_state(self, project: str, stage: str) -> None:
+        if self.timer:
             logger.info(
-                f"Time spent in stage <{self.ingestion_stage}>: {elapsed} seconds"
+                f"Time spent in stage <{self.current_project_status}>: "
+                f"{self.timer.elapsed_seconds():.2f} seconds"
             )
-            if self.ingestion_stage:
-                self.ingestion_stage_durations[self.ingestion_stage] = elapsed
         else:
-            self._timer = PerfTimer()
+            self.timer = PerfTimer()
 
-        self.ingestion_stage = f"{project}: {stage} at {datetime.now(timezone.utc)}"
-        self._timer.start()
+        self.current_project_status = f"{project}: {stage} at {datetime.now()}"
+        self.timer.start()
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/bigquery_schema.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_schema.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/lineage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/lineage.py`

 * *Files 2% similar despite different names*

```diff
@@ -446,45 +446,40 @@
                 e.destinationTable.table_identifier.dataset
             ) or not self.config.table_pattern.allowed(
                 e.destinationTable.table_identifier.get_table_name()
             ):
                 self.report.num_skipped_lineage_entries_not_allowed[e.project_id] += 1
                 continue
 
-            lineage_from_event: Set[LineageEdge] = set()
-
             destination_table_str = str(e.destinationTable)
             has_table = False
             for ref_table in e.referencedTables:
                 if str(ref_table) != destination_table_str:
-                    lineage_from_event.add(
+                    lineage_map[destination_table_str].add(
                         LineageEdge(
                             table=str(ref_table),
                             auditStamp=e.end_time
                             if e.end_time
                             else datetime.now(tz=timezone.utc),
                         )
                     )
                     has_table = True
             has_view = False
             for ref_view in e.referencedViews:
                 if str(ref_view) != destination_table_str:
-                    lineage_from_event.add(
+                    lineage_map[destination_table_str].add(
                         LineageEdge(
                             table=str(ref_view),
                             auditStamp=e.end_time
                             if e.end_time
                             else datetime.now(tz=timezone.utc),
                         )
                     )
                     has_view = True
-
-            if not lineage_from_event:
-                self.report.num_skipped_lineage_entries_other[e.project_id] += 1
-            elif self.config.lineage_use_sql_parser and has_table and has_view:
+            if self.config.lineage_use_sql_parser and has_table and has_view:
                 # If there is a view being referenced then bigquery sends both the view as well as underlying table
                 # in the references. There is no distinction between direct/base objects accessed. So doing sql parsing
                 # to ensure we only use direct objects accessed for lineage
                 try:
                     parser = BigQuerySQLParser(
                         e.query,
                         self.config.sql_parser_use_external_process,
@@ -497,22 +492,23 @@
                     logger.debug(
                         f"Sql Parser failed on query: {e.query}. It won't cause any issue except table/view lineage can't be detected reliably. The error was {ex}."
                     )
                     self.report.num_lineage_entries_sql_parser_failure[
                         e.project_id
                     ] += 1
                     continue
+                curr_lineage = lineage_map[destination_table_str]
                 new_lineage = set()
-                for lineage in lineage_from_event:
+                for lineage in curr_lineage:
                     name = lineage.table.split("/")[-1]
                     if name in referenced_objs:
                         new_lineage.add(lineage)
-                lineage_from_event = new_lineage
-
-            lineage_map[destination_table_str].update(lineage_from_event)
+                lineage_map[destination_table_str] = new_lineage
+            if not (has_table or has_view):
+                self.report.num_skipped_lineage_entries_other[e.project_id] += 1
 
         logger.info("Exiting create lineage map function")
         return lineage_map
 
     def parse_view_lineage(
         self, project: str, dataset: str, view: BigqueryView
     ) -> Optional[List[BigqueryTableIdentifier]]:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/profiler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/profiler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/bigquery_v2/usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/usage.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,9 @@
-import hashlib
 import json
 import logging
-import os
 import textwrap
 import time
 import uuid
 from dataclasses import dataclass
 from datetime import datetime
 from typing import (
     Any,
@@ -16,27 +14,25 @@
     Iterator,
     List,
     Optional,
     Tuple,
     Union,
 )
 
-import humanfriendly
 from google.cloud.bigquery import Client as BigQueryClient
 from google.cloud.logging_v2.client import Client as GCPLoggingClient
 from ratelimiter import RateLimiter
 
 from datahub.configuration.time_window_config import get_time_bucket
 from datahub.emitter.mce_builder import make_user_urn
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.closeable import Closeable
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.bigquery_v2.bigquery_audit import (
     BQ_AUDIT_V2,
-    BQ_FILTER_RULE_TEMPLATE,
     AuditEvent,
     AuditLogEntry,
     BigQueryAuditMetadata,
     BigQueryTableRef,
     QueryEvent,
     ReadEvent,
 )
@@ -44,18 +40,15 @@
 from datahub.ingestion.source.bigquery_v2.bigquery_report import BigQueryV2Report
 from datahub.ingestion.source.bigquery_v2.common import (
     BQ_DATE_SHARD_FORMAT,
     BQ_DATETIME_FORMAT,
     _make_gcp_logging_client,
     get_bigquery_client,
 )
-from datahub.ingestion.source.usage.usage_common import (
-    TOTAL_BUDGET_FOR_QUERY_LIST,
-    make_usage_workunit,
-)
+from datahub.ingestion.source.usage.usage_common import make_usage_workunit
 from datahub.metadata.schema_classes import OperationClass, OperationTypeClass
 from datahub.utilities.file_backed_collections import ConnectionWrapper, FileBackedDict
 from datahub.utilities.perf_timer import PerfTimer
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 
@@ -82,45 +75,45 @@
     "ALTER_TABLE": OperationTypeClass.ALTER,
     "ALTER_VIEW": OperationTypeClass.ALTER,
     "ALTER_MATERIALIZED_VIEW": OperationTypeClass.ALTER,
     "ALTER_SCHEMA": OperationTypeClass.ALTER,
 }
 
 READ_STATEMENT_TYPES: List[str] = ["SELECT"]
-STRING_ENCODING = "utf-8"
-MAX_QUERY_LENGTH = TOTAL_BUDGET_FOR_QUERY_LIST
 
 
 @dataclass(frozen=True, order=True)
 class OperationalDataMeta:
     statement_type: str
     last_updated_timestamp: int
     actor_email: str
     custom_type: Optional[str] = None
 
 
 def bigquery_audit_metadata_query_template(
     dataset: str,
     use_date_sharded_tables: bool,
+    table_allow_filter: Optional[str] = None,
     limit: Optional[int] = None,
 ) -> str:
     """
     Receives a dataset (with project specified) and returns a query template that is used to query exported
     v2 AuditLogs containing protoPayloads of type BigQueryAuditMetadata.
     :param dataset: the dataset to query against in the form of $PROJECT.$DATASET
     :param use_date_sharded_tables: whether to read from date sharded audit log tables or time partitioned audit log
            tables
+    :param table_allow_filter: regex used to filter on log events that contain the wanted datasets
     :param limit: maximum number of events to query for
     :return: a query template, when supplied start_time and end_time, can be used to query audit logs from BigQuery
     """
-    allow_filter = """
+    allow_filter = f"""
       AND EXISTS (SELECT *
               from UNNEST(JSON_EXTRACT_ARRAY(protopayload_auditlog.metadataJson,
                                              "$.jobChange.job.jobStats.queryStats.referencedTables")) AS x
-              where REGEXP_CONTAINS(x, r'(projects/.*/datasets/.*/tables/.*)'))
+              where REGEXP_CONTAINS(x, r'(projects/.*/datasets/.*/tables/{table_allow_filter if table_allow_filter else ".*"})'))
     """
 
     limit_text = f"limit {limit}" if limit else ""
 
     shard_condition = ""
     if use_date_sharded_tables:
         from_table = f"`{dataset}.cloudaudit_googleapis_com_data_access_*`"
@@ -160,15 +153,14 @@
     return textwrap.dedent(query)
 
 
 class BigQueryUsageState(Closeable):
     read_events: FileBackedDict[ReadEvent]
     query_events: FileBackedDict[QueryEvent]
     column_accesses: FileBackedDict[Tuple[str, str]]
-    queries: FileBackedDict[str]
 
     def __init__(self, config: BigQueryV2Config):
         self.conn = ConnectionWrapper()
         self.read_events = FileBackedDict[ReadEvent](
             shared_connection=self.conn,
             tablename="read_events",
             extra_columns={
@@ -176,66 +168,45 @@
                 "name": lambda e: e.jobName,
                 "timestamp": lambda e: get_time_bucket(
                     e.timestamp, config.bucket_duration
                 ),
                 "user": lambda e: e.actor_email,
             },
             cache_max_size=config.file_backed_cache_size,
-            # Evict entire cache to reduce db calls.
-            cache_eviction_batch_size=max(int(config.file_backed_cache_size * 0.9), 1),
-            delay_index_creation=True,
-            should_compress_value=True,
         )
         # Keyed by job_name
         self.query_events = FileBackedDict[QueryEvent](
             shared_connection=self.conn,
             tablename="query_events",
             extra_columns={
                 "query": lambda e: e.query,
                 "is_read": lambda e: int(e.statementType in READ_STATEMENT_TYPES),
             },
             cache_max_size=config.file_backed_cache_size,
-            cache_eviction_batch_size=max(int(config.file_backed_cache_size * 0.9), 1),
-            delay_index_creation=True,
-            should_compress_value=True,
         )
         # Created just to store column accesses in sqlite for JOIN
         self.column_accesses = FileBackedDict[Tuple[str, str]](
             shared_connection=self.conn,
             tablename="column_accesses",
             extra_columns={"read_event": lambda p: p[0], "field": lambda p: p[1]},
             cache_max_size=config.file_backed_cache_size,
-            cache_eviction_batch_size=max(int(config.file_backed_cache_size * 0.9), 1),
-            delay_index_creation=True,
         )
-        self.queries = FileBackedDict[str](cache_max_size=config.file_backed_cache_size)
 
     def close(self) -> None:
         self.read_events.close()
         self.query_events.close()
         self.column_accesses.close()
         self.conn.close()
 
-        self.queries.close()
-
-    def create_indexes(self) -> None:
-        self.read_events.create_indexes()
-        self.query_events.create_indexes()
-        self.column_accesses.create_indexes()
-
     def standalone_events(self) -> Iterable[AuditEvent]:
-        query = """
-        SELECT r.value, q.value
-        FROM read_events r
-        LEFT JOIN query_events q ON r.name = q.key
-        """
-        for read_value, query_value in self.read_events.sql_query_iterator(query):
-            read_event = self.read_events.deserializer(read_value)
+        for read_event in self.read_events.values():
             query_event = (
-                self.query_events.deserializer(query_value) if query_value else None
+                self.query_events.get(read_event.jobName)
+                if read_event.jobName
+                else None
             )
             yield AuditEvent(read_event=read_event, query_event=query_event)
         for _, query_event in self.query_events.items_snapshot("NOT is_read"):
             yield AuditEvent(query_event=query_event)
 
     @staticmethod
     def usage_statistics_query(top_n: int) -> str:
@@ -243,28 +214,28 @@
         SELECT a.timestamp, a.resource, a.query_count, b.query_freq, c.user_freq, d.column_freq FROM (
             SELECT
                 r.timestamp,
                 r.resource,
                 COUNT(q.query) query_count
             FROM
                 read_events r
-                INNER JOIN query_events q ON r.name = q.key
+                LEFT JOIN query_events q ON r.name = q.key
             GROUP BY r.timestamp, r.resource
         ) a
         LEFT JOIN (
             SELECT timestamp, resource, json_group_array(json_array(query, query_count)) as query_freq FROM (
                 SELECT
                     r.timestamp,
                     r.resource,
                     q.query,
                     COUNT(r.key) as query_count,
                     ROW_NUMBER() over (PARTITION BY r.timestamp, r.resource, q.query ORDER BY COUNT(r.key) DESC, q.query) as rank
                 FROM
                     read_events r
-                    INNER JOIN query_events q ON r.name = q.key
+                    LEFT JOIN query_events q ON r.name = q.key
                 GROUP BY r.timestamp, r.resource, q.query
                 ORDER BY r.timestamp, r.resource, query_count DESC, q.query
             ) WHERE rank <= {top_n}
             GROUP BY timestamp, resource
         ) b ON a.timestamp = b.timestamp AND a.resource = b.resource
         LEFT JOIN (
             SELECT timestamp, resource, json_group_array(json_array(user, user_count)) as user_freq FROM (
@@ -313,29 +284,19 @@
             query, refs=[self.query_events, self.column_accesses]
         )
         for row in rows:
             yield self.UsageStatistic(
                 timestamp=row["timestamp"],
                 resource=row["resource"],
                 query_count=row["query_count"],
-                query_freq=json.loads(row["query_freq"] or "[]"),
-                user_freq=json.loads(row["user_freq"] or "[]"),
-                column_freq=json.loads(row["column_freq"] or "[]"),
+                query_freq=json.loads(row["query_freq"]),
+                user_freq=json.loads(row["user_freq"]),
+                column_freq=json.loads(row["column_freq"]),
             )
 
-    def report_disk_usage(self, report: BigQueryV2Report) -> None:
-        report.usage_state_size = str(
-            {
-                "main": humanfriendly.format_size(os.path.getsize(self.conn.filename)),
-                "queries": humanfriendly.format_size(
-                    os.path.getsize(self.queries._conn.filename)
-                ),
-            }
-        )
-
 
 class BigQueryUsageExtractor:
     """
     This plugin extracts the following:
     * Statistics on queries issued and tables and columns accessed (excludes views)
     * Aggregation of these statistics into buckets, by day or hour granularity
 
@@ -343,16 +304,14 @@
     1. Depending on the compliance policies setup for the bigquery instance, sometimes logging.read permission is not sufficient. In that case, use either admin or private log viewer permission.
     :::
     """
 
     def __init__(self, config: BigQueryV2Config, report: BigQueryV2Report):
         self.config: BigQueryV2Config = config
         self.report: BigQueryV2Report = report
-        # Replace hash of query with uuid if there are hash conflicts
-        self.uuid_to_query: Dict[str, str] = {}
 
     def _is_table_allowed(self, table_ref: Optional[BigQueryTableRef]) -> bool:
         return (
             table_ref is not None
             and self.config.dataset_pattern.allowed(table_ref.table_identifier.dataset)
             and self.config.table_pattern.allowed(table_ref.table_identifier.table)
         )
@@ -365,24 +324,21 @@
 
     def _run(
         self, events: Iterable[AuditEvent], table_refs: Collection[str]
     ) -> Iterable[MetadataWorkUnit]:
         try:
             with BigQueryUsageState(self.config) as usage_state:
                 self._ingest_events(events, table_refs, usage_state)
-                usage_state.create_indexes()
-                usage_state.report_disk_usage(self.report)
 
                 if self.config.usage.include_operational_stats:
                     yield from self._generate_operational_workunits(
                         usage_state, table_refs
                     )
 
                 yield from self._generate_usage_workunits(usage_state)
-                usage_state.report_disk_usage(self.report)
         except Exception as e:
             logger.error("Error processing usage", exc_info=True)
             self.report.report_warning("usage-ingestion", str(e))
 
     def _ingest_events(
         self,
         events: Iterable[AuditEvent],
@@ -402,15 +358,15 @@
                 )
                 self._report_error("store-event", e)
         logger.info(f"Total number of events aggregated = {num_aggregated}.")
 
     def _generate_operational_workunits(
         self, usage_state: BigQueryUsageState, table_refs: Collection[str]
     ) -> Iterable[MetadataWorkUnit]:
-        self.report.set_ingestion_stage("*", "Usage Extraction Operational Stats")
+        self.report.set_project_state("All", "Usage Extraction Operational Stats")
         for audit_event in usage_state.standalone_events():
             try:
                 operational_wu = self._create_operation_workunit(
                     audit_event, table_refs
                 )
                 if operational_wu:
                     yield operational_wu
@@ -421,42 +377,31 @@
                     exc_info=True,
                 )
                 self._report_error("operation-workunit", e)
 
     def _generate_usage_workunits(
         self, usage_state: BigQueryUsageState
     ) -> Iterable[MetadataWorkUnit]:
-        self.report.set_ingestion_stage("*", "Usage Extraction Usage Aggregation")
+        self.report.set_project_state("All", "Usage Extraction Usage Aggregation")
         top_n = (
             self.config.usage.top_n_queries
             if self.config.usage.include_top_n_queries
             else 0
         )
         for entry in usage_state.usage_statistics(top_n=top_n):
             try:
-                query_freq = [
-                    (
-                        self.uuid_to_query.get(
-                            query_hash, usage_state.queries[query_hash]
-                        ),
-                        count,
-                    )
-                    for query_hash, count in entry.query_freq
-                ]
                 yield make_usage_workunit(
                     bucket_start_time=datetime.fromisoformat(entry.timestamp),
                     resource=BigQueryTableRef.from_string_name(entry.resource),
                     query_count=entry.query_count,
-                    query_freq=query_freq,
+                    query_freq=entry.query_freq,
                     user_freq=entry.user_freq,
                     column_freq=entry.column_freq,
                     bucket_duration=self.config.bucket_duration,
-                    resource_urn_builder=lambda resource: resource.to_urn(
-                        self.config.env
-                    ),
+                    urn_builder=lambda resource: resource.to_urn(self.config.env),
                     top_n_queries=self.config.usage.top_n_queries,
                     format_sql_queries=self.config.usage.format_sql_queries,
                 )
                 self.report.num_usage_workunits_emitted += 1
             except Exception as e:
                 logger.warning(
                     f"Unable to generate usage workunit for bucket {entry.timestamp}, {entry.resource}",
@@ -467,15 +412,15 @@
     def _get_usage_events(self, projects: Iterable[str]) -> Iterable[AuditEvent]:
         if self.config.use_exported_bigquery_audit_metadata:
             projects = ["*"]  # project_id not used when using exported metadata
 
         for project_id in projects:
             with PerfTimer() as timer:
                 try:
-                    self.report.set_ingestion_stage(
+                    self.report.set_project_state(
                         project_id, "Usage Extraction Ingestion"
                     )
                     yield from self._get_parsed_bigquery_log_events(project_id)
                 except Exception as e:
                     logger.error(
                         f"Error getting usage events for project {project_id}",
                         exc_info=True,
@@ -495,15 +440,15 @@
     ) -> bool:
         """Stores a usage event in `usage_state` and returns if an event was successfully processed."""
         if event.read_event and (
             self.config.start_time <= event.read_event.timestamp < self.config.end_time
         ):
             resource = event.read_event.resource
             if str(resource) not in table_refs:
-                logger.debug(f"Skipping non-existent {resource} from usage")
+                logger.info(f"Skipping non-existent {resource} from usage")
                 self.report.num_usage_resources_dropped += 1
                 self.report.report_dropped(str(resource))
                 return False
             elif resource.is_temporary_table([self.config.temp_table_dataset_prefix]):
                 logger.debug(f"Dropping temporary table {resource}")
                 self.report.report_dropped(str(resource))
                 return False
@@ -511,31 +456,22 @@
             # Use uuid keys to store all entries -- no overwriting
             key = str(uuid.uuid4())
             usage_state.read_events[key] = event.read_event
             for field_read in event.read_event.fieldsRead:
                 usage_state.column_accesses[str(uuid.uuid4())] = key, field_read
             return True
         elif event.query_event and event.query_event.job_name:
-            query = event.query_event.query[:MAX_QUERY_LENGTH]
-            query_hash = hashlib.md5(query.encode(STRING_ENCODING)).hexdigest()
-            if usage_state.queries.get(query_hash, query) != query:
-                key = str(uuid.uuid4())
-                self.uuid_to_query[key] = query
-                event.query_event.query = key
-                self.report.num_usage_query_hash_collisions += 1
-            else:
-                usage_state.queries[query_hash] = query
-                event.query_event.query = query_hash
             usage_state.query_events[event.query_event.job_name] = event.query_event
             return True
         return False
 
     def _get_exported_bigquery_audit_metadata(
         self,
         bigquery_client: BigQueryClient,
+        allow_filter: str,
         limit: Optional[int] = None,
     ) -> Iterable[BigQueryAuditMetadata]:
         if self.config.bigquery_audit_metadata_datasets is None:
             return
 
         corrected_start_time = self.config.start_time - self.config.max_query_duration
         start_time = corrected_start_time.strftime(BQ_DATETIME_FORMAT)
@@ -551,14 +487,15 @@
             logger.info(
                 f"Start loading log entries from BigQueryAuditMetadata in {dataset}"
             )
 
             query = bigquery_audit_metadata_query_template(
                 dataset,
                 self.config.use_date_sharded_audit_log_tables,
+                allow_filter,
                 limit=limit,
             ).format(
                 start_time=start_time,
                 end_time=end_time,
                 start_date=start_date,
                 end_date=end_date,
             )
@@ -572,14 +509,15 @@
                     yield from query_job
             else:
                 yield from query_job
 
     def _get_bigquery_log_entries_via_gcp_logging(
         self, client: GCPLoggingClient, limit: Optional[int] = None
     ) -> Iterable[AuditLogEntry]:
+        self.report.total_query_log_entries = 0
 
         filter = self._generate_filter(BQ_AUDIT_V2)
         logger.debug(filter)
 
         list_entries: Iterable[AuditLogEntry]
         rate_limiter: Optional[RateLimiter] = None
         if self.config.rate_limit:
@@ -614,25 +552,57 @@
         )
 
     def _generate_filter(self, audit_templates: Dict[str, str]) -> str:
         # We adjust the filter values a bit, since we need to make sure that the join
         # between query events and read events is complete. For example, this helps us
         # handle the case where the read happens within our time range but the query
         # completion event is delayed and happens after the configured end time.
+        # Can safely access the first index of the allow list as it by default contains ".*"
+        use_allow_filter = self.config.table_pattern and (
+            len(self.config.table_pattern.allow) > 1
+            or self.config.table_pattern.allow[0] != ".*"
+        )
+        use_deny_filter = self.config.table_pattern and self.config.table_pattern.deny
+        allow_regex = (
+            audit_templates["BQ_FILTER_REGEX_ALLOW_TEMPLATE"].format(
+                table_allow_pattern=self.config.get_table_pattern(
+                    self.config.table_pattern.allow
+                )
+            )
+            if use_allow_filter
+            else ""
+        )
+        deny_regex = (
+            audit_templates["BQ_FILTER_REGEX_DENY_TEMPLATE"].format(
+                table_deny_pattern=self.config.get_table_pattern(
+                    self.config.table_pattern.deny
+                ),
+                logical_operator="AND" if use_allow_filter else "",
+            )
+            if use_deny_filter
+            else ("" if use_allow_filter else "FALSE")
+        )
 
+        logger.debug(
+            f"use_allow_filter={use_allow_filter}, use_deny_filter={use_deny_filter}, "
+            f"allow_regex={allow_regex}, deny_regex={deny_regex}"
+        )
         start_time = (self.config.start_time - self.config.max_query_duration).strftime(
             BQ_DATETIME_FORMAT
         )
         self.report.log_entry_start_time = start_time
         end_time = (self.config.end_time + self.config.max_query_duration).strftime(
             BQ_DATETIME_FORMAT
         )
         self.report.log_entry_end_time = end_time
-        filter = audit_templates[BQ_FILTER_RULE_TEMPLATE].format(
-            start_time=start_time, end_time=end_time, allow_regex="", deny_regex="FALSE"
+        filter = audit_templates["BQ_FILTER_RULE_TEMPLATE"].format(
+            start_time=start_time,
+            end_time=end_time,
+            allow_regex=allow_regex,
+            deny_regex=deny_regex,
         )
         return filter
 
     @staticmethod
     def _get_destination_table(event: AuditEvent) -> Optional[BigQueryTableRef]:
         if (
             not event.read_event
@@ -788,14 +758,15 @@
 
         return custom_properties
 
     def _parse_bigquery_log_entry(
         self, entry: Union[AuditLogEntry, BigQueryAuditMetadata]
     ) -> Optional[AuditEvent]:
         event: Optional[Union[ReadEvent, QueryEvent]] = None
+
         missing_read_entry = ReadEvent.get_missing_key_entry(entry)
         if missing_read_entry is None:
             event = ReadEvent.from_entry(entry, self.config.debug_include_full_payloads)
             if not self._is_table_allowed(event.resource):
                 self.report.num_filtered_read_events += 1
                 return None
 
@@ -867,14 +838,17 @@
         self, project_id: str, limit: Optional[int] = None
     ) -> Iterable[AuditEvent]:
         parse_fn: Callable[[Any], Optional[AuditEvent]]
         if self.config.use_exported_bigquery_audit_metadata:
             bq_client = get_bigquery_client(self.config)
             entries = self._get_exported_bigquery_audit_metadata(
                 bigquery_client=bq_client,
+                allow_filter=self.config.get_table_pattern(
+                    self.config.table_pattern.allow
+                ),
                 limit=limit,
             )
             parse_fn = self._parse_exported_bigquery_audit_metadata
         else:
             logging_client = _make_gcp_logging_client(
                 project_id, self.config.extra_client_options
             )
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/common/subtypes.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/common/subtypes.py`

 * *Files 15% similar despite different names*

```diff
@@ -10,34 +10,31 @@
     # System-Specific SubTypes
     LOOKER_EXPLORE = "Explore"
     ELASTIC_INDEX_TEMPLATE = "Index Template"
     ELASTIC_INDEX = "Index"
     ELASTIC_DATASTREAM = "Datastream"
     SALESFORCE_CUSTOM_OBJECT = "Custom Object"
     SALESFORCE_STANDARD_OBJECT = "Object"
-    POWERBI_DATASET_TABLE = "PowerBI Dataset Table"
 
 
 class DatasetContainerSubTypes(str, Enum):
     # Generic SubTypes
     DATABASE = "Database"
     SCHEMA = "Schema"
     # System-Specific SubTypes
     PRESTO_CATALOG = "Catalog"
     BIGQUERY_PROJECT = "Project"
     BIGQUERY_DATASET = "Dataset"
     DATABRICKS_METASTORE = "Metastore"
-    FOLDER = "Folder"
+    S3_FOLDER = "Folder"
     S3_BUCKET = "S3 bucket"
-    GCS_BUCKET = "GCS bucket"
 
 
 class BIContainerSubTypes(str, Enum):
     LOOKER_FOLDER = "Folder"
     TABLEAU_WORKBOOK = "Workbook"
     POWERBI_WORKSPACE = "Workspace"
-    POWERBI_DATASET = "PowerBI Dataset"
 
 
 class BIAssetSubTypes(str, Enum):
     # Generic SubTypes
     REPORT = "Report"
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/confluent_schema_registry.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/confluent_schema_registry.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 import json
 import logging
 from dataclasses import dataclass
 from hashlib import md5
 from typing import Any, List, Optional, Set, Tuple
 
+import confluent_kafka
 import jsonref
 from confluent_kafka.schema_registry.schema_registry_client import (
     RegisteredSchema,
     Schema,
     SchemaReference,
-    SchemaRegistryClient,
 )
 
 from datahub.ingestion.extractor import protobuf_util, schema_util
 from datahub.ingestion.extractor.json_schema_util import JsonSchemaTranslator
 from datahub.ingestion.extractor.protobuf_util import ProtobufSchema
 from datahub.ingestion.source.kafka import KafkaSourceConfig, KafkaSourceReport
 from datahub.ingestion.source.kafka_schema_registry_base import KafkaSchemaRegistryBase
@@ -41,19 +41,22 @@
     """
 
     def __init__(
         self, source_config: KafkaSourceConfig, report: KafkaSourceReport
     ) -> None:
         self.source_config: KafkaSourceConfig = source_config
         self.report: KafkaSourceReport = report
-        self.schema_registry_client = SchemaRegistryClient(
-            {
-                "url": source_config.connection.schema_registry_url,
-                **source_config.connection.schema_registry_config,
-            }
+        # Use the fully qualified name for SchemaRegistryClient to make it mock patchable for testing.
+        self.schema_registry_client = (
+            confluent_kafka.schema_registry.schema_registry_client.SchemaRegistryClient(
+                {
+                    "url": source_config.connection.schema_registry_url,
+                    **source_config.connection.schema_registry_config,
+                }
+            )
         )
         self.known_schema_registry_subjects: List[str] = []
         try:
             self.known_schema_registry_subjects.extend(
                 self.schema_registry_client.get_subjects()
             )
         except Exception as e:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/csv_enricher.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/csv_enricher.py`

 * *Files 1% similar despite different names*

```diff
@@ -542,32 +542,32 @@
     def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         # As per https://stackoverflow.com/a/49150749/5004662, we want to use
         # the 'utf-8-sig' encoding to handle any BOM character that may be
         # present in the file. Excel is known to add a BOM to CSV files.
         # As per https://stackoverflow.com/a/63508823/5004662,
         # this is also safe with normal files that don't have a BOM.
         parsed_location = parse.urlparse(self.config.filename)
-        if parsed_location.scheme in ("http", "https"):
+        if parsed_location.scheme in ("file", ""):
+            with open(
+                pathlib.Path(self.config.filename), mode="r", encoding="utf-8-sig"
+            ) as f:
+                rows = list(csv.DictReader(f, delimiter=self.config.delimiter))
+        else:
             try:
                 resp = requests.get(self.config.filename)
                 decoded_content = resp.content.decode("utf-8-sig")
                 rows = list(
                     csv.DictReader(
                         decoded_content.splitlines(), delimiter=self.config.delimiter
                     )
                 )
             except Exception as e:
                 raise ConfigurationError(
                     f"Cannot read remote file {self.config.filename}, error:{e}"
                 )
-        else:
-            with open(
-                pathlib.Path(self.config.filename), mode="r", encoding="utf-8-sig"
-            ) as f:
-                rows = list(csv.DictReader(f, delimiter=self.config.delimiter))
 
         for row in rows:
             # We need the resource to move forward
             if not row["resource"]:
                 continue
 
             is_resource_row: bool = not row["subresource"]
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/data_lake_common/data_lake_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/data_lake_utils.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,34 +1,30 @@
 import logging
 from typing import Iterable, List, Optional
 
 from datahub.emitter.mcp_builder import (
-    BucketKey,
     FolderKey,
     KeyType,
     PlatformKey,
+    S3BucketKey,
     add_dataset_to_container,
     gen_containers,
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.aws.s3_util import (
     get_bucket_name,
     get_bucket_relative_path,
     is_s3_uri,
 )
 from datahub.ingestion.source.common.subtypes import DatasetContainerSubTypes
-from datahub.ingestion.source.gcs.gcs_utils import get_gcs_bucket_name, is_gcs_uri
 
 # hide annoying debug errors from py4j
 logging.getLogger("py4j").setLevel(logging.ERROR)
 logger: logging.Logger = logging.getLogger(__name__)
 
-PLATFORM_S3 = "s3"
-PLATFORM_GCS = "gcs"
-
 
 class ContainerWUCreator:
     processed_containers: List[str]
 
     def __init__(self, platform, platform_instance, env):
         self.processed_containers = []
         self.platform = platform
@@ -61,45 +57,34 @@
             platform=self.platform,
             instance=self.instance,
             backcompat_instance_for_guid=self.env,
             folder_abs_path=abs_path,
         )
 
     def gen_bucket_key(self, name):
-        return BucketKey(
-            platform=self.platform,
+        return S3BucketKey(
+            platform="s3",
             instance=self.instance,
             backcompat_instance_for_guid=self.env,
             bucket_name=name,
         )
 
-    def get_bucket_name(self, path):
-        if is_s3_uri(path):
-            return get_bucket_name(path)
-        elif is_gcs_uri(path):
-            return get_gcs_bucket_name(path)
-
     def create_container_hierarchy(
-        self, path: str, dataset_urn: str
+        self, path: str, is_s3: bool, dataset_urn: str
     ) -> Iterable[MetadataWorkUnit]:
         logger.debug(f"Creating containers for {dataset_urn}")
         base_full_path = path
         parent_key = None
-        if self.platform in (PLATFORM_S3, PLATFORM_GCS):
-            bucket_name = self.get_bucket_name(path)
+        if is_s3_uri(path):
+            bucket_name = get_bucket_name(path)
             bucket_key = self.gen_bucket_key(bucket_name)
-
             yield from self.create_emit_containers(
                 container_key=bucket_key,
                 name=bucket_name,
-                sub_types=[
-                    DatasetContainerSubTypes.S3_BUCKET
-                    if self.platform == "s3"
-                    else DatasetContainerSubTypes.GCS_BUCKET
-                ],
+                sub_types=[DatasetContainerSubTypes.S3_BUCKET],
                 parent_container_key=None,
             )
             parent_key = bucket_key
             base_full_path = get_bucket_relative_path(path)
 
         parent_folder_path = (
             base_full_path[: base_full_path.rfind("/")]
@@ -114,23 +99,23 @@
             )
             return
 
         for folder in parent_folder_path.split("/"):
             abs_path = folder
             if parent_key:
                 prefix: str = ""
-                if isinstance(parent_key, BucketKey):
+                if isinstance(parent_key, S3BucketKey):
                     prefix = parent_key.bucket_name
                 elif isinstance(parent_key, FolderKey):
                     prefix = parent_key.folder_abs_path
                 abs_path = prefix + "/" + folder
             folder_key = self.gen_folder_key(abs_path)
             yield from self.create_emit_containers(
                 container_key=folder_key,
                 name=folder,
-                sub_types=[DatasetContainerSubTypes.FOLDER],
+                sub_types=[DatasetContainerSubTypes.S3_FOLDER],
                 parent_container_key=parent_key,
             )
             parent_key = folder_key
 
         assert parent_key is not None
         yield from add_dataset_to_container(parent_key, dataset_urn)
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/data_lake_common/path_spec.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/path_spec.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,30 +7,29 @@
 import pydantic
 from cached_property import cached_property
 from pydantic.fields import Field
 from wcmatch import pathlib
 
 from datahub.configuration.common import ConfigModel
 from datahub.ingestion.source.aws.s3_util import is_s3_uri
-from datahub.ingestion.source.gcs.gcs_utils import is_gcs_uri
 
 # hide annoying debug errors from py4j
 logging.getLogger("py4j").setLevel(logging.ERROR)
 logger: logging.Logger = logging.getLogger(__name__)
 
 SUPPORTED_FILE_TYPES: List[str] = ["csv", "tsv", "json", "parquet", "avro"]
 SUPPORTED_COMPRESSIONS: List[str] = ["gz", "bz2"]
 
 
 class PathSpec(ConfigModel):
     class Config:
         arbitrary_types_allowed = True
 
     include: str = Field(
-        description="Path to table. Name variable `{table}` is used to mark the folder with dataset. In absence of `{table}`, file level dataset will be created. Check below examples for more details."
+        description="Path to table (s3 or local file system). Name variable {table} is used to mark the folder with dataset. In absence of {table}, file level dataset will be created. Check below examples for more details."
     )
     exclude: Optional[List[str]] = Field(
         default=None,
         description="list of paths in glob pattern which will be excluded while scanning for the datasets",
     )
     file_types: List[str] = Field(
         default=SUPPORTED_FILE_TYPES,
@@ -107,26 +106,25 @@
                     raise ValueError(
                         f"file type {file_type} not in supported file types. Please specify one from {SUPPORTED_FILE_TYPES}"
                     )
             return v
 
     @pydantic.validator("default_extension")
     def validate_default_extension(cls, v):
-        if v is not None and v not in SUPPORTED_FILE_TYPES:
+        if v not in SUPPORTED_FILE_TYPES:
             raise ValueError(
                 f"default extension {v} not in supported default file extension. Please specify one from {SUPPORTED_FILE_TYPES}"
             )
         return v
 
     @pydantic.validator("sample_files", always=True)
     def turn_off_sampling_for_non_s3(cls, v, values):
         is_s3 = is_s3_uri(values.get("include") or "")
-        is_gcs = is_gcs_uri(values.get("include") or "")
-        if not is_s3 and not is_gcs:
-            # Sampling only makes sense on s3 and gcs currently
+        if not is_s3:
+            # Sampling only makes sense on s3 currently
             v = False
         return v
 
     @pydantic.validator("exclude", each_item=True)
     def no_named_fields_in_exclude(cls, v: str) -> str:
         if len(parse.compile(v).named_fields) != 0:
             raise ValueError(
@@ -158,18 +156,14 @@
         return v
 
     @cached_property
     def is_s3(self):
         return is_s3_uri(self.include)
 
     @cached_property
-    def is_gcs(self):
-        return is_gcs_uri(self.include)
-
-    @cached_property
     def compiled_include(self):
         parsable_include = PathSpec.get_parsable_include(self.include)
         logger.debug(f"parsable_include: {parsable_include}")
         compiled_include = parse.compile(parsable_include)
         logger.debug(f"Setting compiled_include: {compiled_include}")
         return compiled_include
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/dbt/dbt_cloud.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_cloud.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/dbt/dbt_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_common.py`

 * *Files 0% similar despite different names*

```diff
@@ -1348,17 +1348,15 @@
     ) -> Optional[MetadataWorkUnit]:
         if not node.node_type:
             return None
 
         subtypes: List[str] = [node.node_type.capitalize()]
         if node.materialization == "table":
             subtypes.append(DatasetSubTypes.TABLE)
-
-        if node.node_type == "model" or node.node_type == "snapshot":
-            # We need to add the view subtype so that the view properties tab shows up in the UI.
+        elif node.materialization == "view":
             subtypes.append(DatasetSubTypes.VIEW)
 
         return MetadataChangeProposalWrapper(
             entityUrn=node_datahub_urn,
             aspect=SubTypesClass(typeNames=subtypes),
         ).as_workunit()
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/dbt/dbt_core.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_core.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/delta_lake/config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/config.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/delta_lake/delta_lake_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/delta_lake_utils.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/delta_lake/source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/source.py`

 * *Files 1% similar despite different names*

```diff
@@ -24,21 +24,21 @@
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.aws.s3_boto_utils import get_s3_tags, list_folders_path
 from datahub.ingestion.source.aws.s3_util import (
     get_bucket_name,
     get_key_prefix,
     strip_s3_prefix,
 )
-from datahub.ingestion.source.data_lake_common.data_lake_utils import ContainerWUCreator
 from datahub.ingestion.source.delta_lake.config import DeltaLakeSourceConfig
 from datahub.ingestion.source.delta_lake.delta_lake_utils import (
     get_file_count,
     read_delta_table,
 )
 from datahub.ingestion.source.delta_lake.report import DeltaLakeSourceReport
+from datahub.ingestion.source.s3.data_lake_utils import ContainerWUCreator
 from datahub.ingestion.source.schema_inference.csv_tsv import tableschema_type_map
 from datahub.metadata.com.linkedin.pegasus2avro.common import Status
 from datahub.metadata.com.linkedin.pegasus2avro.metadata.snapshot import DatasetSnapshot
 from datahub.metadata.com.linkedin.pegasus2avro.mxe import MetadataChangeEvent
 from datahub.metadata.com.linkedin.pegasus2avro.schema import (
     SchemaField,
     SchemaFieldDataType,
@@ -274,15 +274,15 @@
                 dataset_snapshot.aspects.append(s3_tags)
         mce = MetadataChangeEvent(proposedSnapshot=dataset_snapshot)
         wu = MetadataWorkUnit(id=delta_table.metadata().id, mce=mce)
         self.report.report_workunit(wu)
         yield wu
 
         container_wus = self.container_WU_creator.create_container_hierarchy(
-            browse_path, dataset_urn
+            browse_path, self.source_config.is_s3, dataset_urn
         )
         for wu in container_wus:
             self.report.report_workunit(wu)
             yield wu
 
         yield from self._create_operation_aspect_wu(delta_table, dataset_urn)
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/demo_data.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/demo_data.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/elastic_search.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/elastic_search.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/feast.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/feast.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/file.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/file.py`

 * *Files 1% similar despite different names*

```diff
@@ -243,15 +243,31 @@
         if self.fp:
             self.fp.close()
         super().close()
 
     def _iterate_file(self, path: str) -> Iterable[Tuple[int, Any]]:
         self.report.current_file_name = path
         path_parsed = parse.urlparse(path)
-        if path_parsed.scheme not in ("http", "https"):  # A local file
+        if path_parsed.scheme not in ("file", ""):  # A remote file
+            try:
+                response = requests.get(path)
+                parse_start_time = datetime.datetime.now()
+                data = response.json()
+            except Exception as e:
+                raise ConfigurationError(f"Cannot read remote file {path}, error:{e}")
+            if not isinstance(data, list):
+                data = [data]
+            parse_end_time = datetime.datetime.now()
+            self.report.add_parse_time(parse_end_time - parse_start_time)
+            self.report.current_file_size = len(response.content)
+            self.report.current_file_elements_read = 0
+            for i, obj in enumerate(data):
+                yield i, obj
+                self.report.current_file_elements_read += 1
+        else:
             self.report.current_file_size = os.path.getsize(path)
             if self.config.read_mode == FileReadMode.AUTO:
                 file_read_mode = (
                     FileReadMode.BATCH
                     if self.report.current_file_size
                     < self.config._minsize_for_streaming_mode_in_bytes
                     else FileReadMode.STREAM
@@ -294,30 +310,14 @@
                 for row in ijson.items(parse_stream, "item", use_float=True):
                     parse_end_time = datetime.datetime.now()
                     self.report.add_parse_time(parse_end_time - parse_start_time)
                     rows_yielded += 1
                     self.report.current_file_elements_read += 1
                     yield rows_yielded, row
                     parse_start_time = datetime.datetime.now()
-        else:
-            try:
-                response = requests.get(path)
-                parse_start_time = datetime.datetime.now()
-                data = response.json()
-            except Exception as e:
-                raise ConfigurationError(f"Cannot read remote file {path}, error:{e}")
-            if not isinstance(data, list):
-                data = [data]
-            parse_end_time = datetime.datetime.now()
-            self.report.add_parse_time(parse_end_time - parse_start_time)
-            self.report.current_file_size = len(response.content)
-            self.report.current_file_elements_read = 0
-            for i, obj in enumerate(data):
-                yield i, obj
-                self.report.current_file_elements_read += 1
 
         self.report.files_completed.append(path)
         self.report.num_files_completed += 1
         self.report.total_bytes_read_completed_files += self.report.current_file_size
         self.report.reset_current_file_stats()
 
     def iterate_mce_file(self, path: str) -> Iterator[MetadataChangeEvent]:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/ge_data_profiler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ge_data_profiler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/ge_profiling_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ge_profiling_config.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/git/git_import.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/git/git_import.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/glue_profiling_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/glue_profiling_config.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/iceberg/iceberg.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/iceberg/iceberg_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg_common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/iceberg/iceberg_profiler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg_profiler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/identity/azure_ad.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/azure_ad.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/identity/okta.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/okta.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/kafka.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/kafka.py`

 * *Files 1% similar despite different names*

```diff
@@ -97,15 +97,15 @@
         default=False,
         description="Disables warnings reported for non-AVRO/Protobuf value or key schemas if set.",
     )
     disable_topic_record_naming_strategy: bool = pydantic.Field(
         default=False,
         description="Disables the utilization of the TopicRecordNameStrategy for Schema Registry subjects. For more information, visit: https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#handling-differences-between-preregistered-and-client-derived-schemas:~:text=io.confluent.kafka.serializers.subject.TopicRecordNameStrategy",
     )
-    
+
 
 @dataclass
 class KafkaSourceReport(StaleEntityRemovalSourceReport):
     topics_scanned: int = 0
     filtered: List[str] = field(default_factory=list)
 
     def report_topic_scanned(self, topic: str) -> None:
@@ -138,16 +138,15 @@
     @classmethod
     def create_schema_registry(
         cls, config: KafkaSourceConfig, report: KafkaSourceReport
     ) -> KafkaSchemaRegistryBase:
         try:
             schema_registry_class: Type = import_path(config.schema_registry_class)
             return schema_registry_class.create(config, report)
-        except Exception as e:
-            logger.debug(e, exc_info=e)
+        except (ImportError, AttributeError):
             raise ImportError(config.schema_registry_class)
 
     def __init__(self, config: KafkaSourceConfig, ctx: PipelineContext):
         super().__init__(config, ctx)
         self.source_config: KafkaSourceConfig = config
         self.consumer: confluent_kafka.Consumer = confluent_kafka.Consumer(
             {
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/kafka_connect.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/kafka_connect.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/ldap.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ldap.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 """LDAP Source"""
 import dataclasses
+import re
 from typing import Any, Dict, Iterable, List, Optional
 
 import ldap
 from ldap.controls import SimplePagedResultsControl
 from pydantic.fields import Field
 
 from datahub.configuration.common import ConfigurationError
@@ -410,16 +411,16 @@
         return MetadataChangeEvent(proposedSnapshot=user_snapshot)
 
     def build_corp_group_mce(self, attrs: dict) -> Optional[MetadataChangeEvent]:
         """Creates a MetadataChangeEvent for LDAP groups."""
         cn = attrs.get(self.config.group_attrs_map["urn"])
         if cn:
             full_name = cn[0].decode()
-            admins = parse_users(attrs, self.config.group_attrs_map["admins"])
-            members = parse_users(attrs, self.config.group_attrs_map["members"])
+            admins = parse_from_attrs(attrs, self.config.group_attrs_map["admins"])
+            members = parse_from_attrs(attrs, self.config.group_attrs_map["members"])
             email = (
                 attrs[self.config.group_attrs_map["email"]][0].decode()
                 if self.config.group_attrs_map["email"] in attrs
                 else full_name
             )
             description = (
                 attrs[self.config.group_attrs_map["description"]][0].decode()
@@ -452,37 +453,37 @@
         return self.report
 
     def close(self) -> None:
         self.ldap_client.unbind()
         super().close()
 
 
-def parse_users(attrs: Dict[str, Any], filter_key: str) -> List[str]:
-    """Converts a list of LDAP DNs to Datahub corpuser strings."""
+def parse_from_attrs(attrs: Dict[str, Any], filter_key: str) -> List[str]:
+    """Converts a list of LDAP formats to Datahub corpuser strings."""
     if filter_key in attrs:
         return [
-            f"urn:li:corpuser:{parse_ldap_dn(ldap_user)}"
+            f"urn:li:corpuser:{strip_ldap_info(ldap_user)}"
             for ldap_user in attrs[filter_key]
         ]
     return []
 
 
+def strip_ldap_info(input_clean: bytes) -> str:
+    """Converts a b'uid=username,ou=Groups,dc=internal,dc=machines'
+    format to username"""
+    return input_clean.decode().split(",")[0].lstrip("uid=")
+
+
 def parse_groups(attrs: Dict[str, Any], filter_key: str) -> List[str]:
-    """Converts a list of LDAP DNs to Datahub corpgroup strings"""
+    """Converts a list of LDAP groups to Datahub corpgroup strings"""
     if filter_key in attrs:
         return [
-            f"urn:li:corpGroup:{parse_ldap_dn(ldap_group)}"
+            f"urn:li:corpGroup:{strip_ldap_group_cn(ldap_group)}"
             for ldap_group in attrs[filter_key]
         ]
     return []
 
 
-def parse_ldap_dn(input_clean: bytes) -> str:
-    """
-    Converts an LDAP DN of format b'cn=group_name,ou=Groups,dc=internal,dc=machines'
-    or b'uid=username,ou=Groups,dc=internal,dc=machines' to group name or username.
-    Inputs which are not valid LDAP DNs are simply decoded and returned as strings.
-    """
-    if ldap.dn.is_dn(input_clean):
-        return ldap.dn.str2dn(input_clean, flags=ldap.DN_FORMAT_LDAPV3)[0][0][1]
-    else:
-        return input_clean.decode()
+def strip_ldap_group_cn(input_clean: bytes) -> str:
+    """Converts a b'cn=group_name,ou=Groups,dc=internal,dc=machines'
+    format to group name"""
+    return re.sub("cn=", "", input_clean.decode().split(",")[0], flags=re.IGNORECASE)
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/looker_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_common.py`

 * *Files 1% similar despite different names*

```diff
@@ -1161,17 +1161,14 @@
     looker_api_wrapper: LookerAPI
     fields: str = ",".join(["id", "email", "display_name", "first_name", "last_name"])
 
     def __init__(self, looker_api: LookerAPI):
         self.looker_api_wrapper = looker_api
 
     def get_by_id(self, id_: str) -> Optional[LookerUser]:
-        if not id_:
-            return None
-
         logger.debug(f"Will get user {id_}")
 
         raw_user: Optional[User] = self.looker_api_wrapper.get_user(
             str(id_), user_fields=self.fields
         )
         if raw_user is None:
             return None
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/looker_lib_wrapper.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_lib_wrapper.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 from looker_sdk.rtl.transport import TransportOptions
 from looker_sdk.sdk.api40.models import (
     Dashboard,
     DashboardBase,
     DBConnection,
     Folder,
     LookmlModel,
-    LookmlModelExplore,
     User,
     WriteQuery,
 )
 from pydantic import BaseModel, Field
 
 from datahub.configuration import ConfigModel
 from datahub.configuration.common import ConfigurationError
@@ -142,15 +141,15 @@
         self.client_stats.dashboard_calls += 1
         return self.client.dashboard(
             dashboard_id=dashboard_id,
             fields=self.__fields_mapper(fields),
             transport_options=self.transport_options,
         )
 
-    def lookml_model_explore(self, model: str, explore_name: str) -> LookmlModelExplore:
+    def lookml_model_explore(self, model, explore_name):
         self.client_stats.explore_calls += 1
         return self.client.lookml_model_explore(
             model, explore_name, transport_options=self.transport_options
         )
 
     @lru_cache(maxsize=1000)
     def folder_ancestors(
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/looker_query_model.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_query_model.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/looker_source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_source.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/looker_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/looker/lookml_source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/lookml_source.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,28 +1,16 @@
-import copy
 import glob
 import itertools
 import logging
 import pathlib
 import re
 import tempfile
 from dataclasses import dataclass, field as dataclass_field, replace
 from datetime import datetime, timedelta
-from typing import (
-    Any,
-    ClassVar,
-    Dict,
-    Iterable,
-    List,
-    Optional,
-    Set,
-    Tuple,
-    Type,
-    Union,
-)
+from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Type, Union
 
 import lkml
 import lkml.simple
 import pydantic
 from looker_sdk.error import SDKError
 from looker_sdk.sdk.api40.models import DBConnection
 from pydantic import root_validator, validator
@@ -245,18 +233,14 @@
     process_isolation_for_sql_parsing: bool = Field(
         False,
         description="When enabled, sql parsing will be executed in a separate process to prevent memory leaks.",
     )
     stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = Field(
         default=None, description=""
     )
-    process_refinements: bool = Field(
-        False,
-        description="When enabled, looker refinement will be processed to adapt an existing view.",
-    )
 
     @validator("connection_to_platform_map", pre=True)
     def convert_string_to_connection_def(cls, conn_map):
         # Previous version of config supported strings in connection map. This upconverts strings to ConnectionMap
         for key in conn_map:
             if isinstance(conn_map[key], str):
                 platform = conn_map[key]
@@ -681,254 +665,14 @@
         )
         if viewfile is None:
             return None
 
         return replace(viewfile, connection=connection)
 
 
-class LookerRefinementResolver:
-    """
-    Refinements are a way to "edit" an existing view or explore.
-    Refer: https://cloud.google.com/looker/docs/lookml-refinements
-
-    A refinement to an existing view/explore is only applied if it's refinement is reachable from include files in a model.
-    For refinement applied order please refer: https://cloud.google.com/looker/docs/lookml-refinements#refinements_are_applied_in_order
-    """
-
-    REFINEMENT_PREFIX: ClassVar[str] = "+"
-    DIMENSIONS: ClassVar[str] = "dimensions"
-    MEASURES: ClassVar[str] = "measures"
-    DIMENSION_GROUPS: ClassVar[str] = "dimension_groups"
-    NAME: ClassVar[str] = "name"
-    EXTENDS: ClassVar[str] = "extends"
-    EXTENDS_ALL: ClassVar[str] = "extends__all"
-
-    looker_model: LookerModel
-    looker_viewfile_loader: LookerViewFileLoader
-    connection_definition: LookerConnectionDefinition
-    source_config: LookMLSourceConfig
-    reporter: LookMLSourceReport
-    view_refinement_cache: Dict[
-        str, dict
-    ]  # Map of view-name as key, and it is raw view dictionary after applying refinement process
-    explore_refinement_cache: Dict[
-        str, dict
-    ]  # Map of explore-name as key, and it is raw view dictionary after applying refinement process
-
-    def __init__(
-        self,
-        looker_model: LookerModel,
-        looker_viewfile_loader: LookerViewFileLoader,
-        connection_definition: LookerConnectionDefinition,
-        source_config: LookMLSourceConfig,
-        reporter: LookMLSourceReport,
-    ):
-        self.looker_model = looker_model
-        self.looker_viewfile_loader = looker_viewfile_loader
-        self.connection_definition = connection_definition
-        self.source_config = source_config
-        self.reporter = reporter
-        self.view_refinement_cache = {}
-        self.explore_refinement_cache = {}
-
-    @staticmethod
-    def is_refinement(view_name: str) -> bool:
-        return view_name.startswith(LookerRefinementResolver.REFINEMENT_PREFIX)
-
-    @staticmethod
-    def merge_column(
-        original_dict: dict, refinement_dict: dict, key: str
-    ) -> List[dict]:
-        """
-        Merge a dimension/measure/other column with one from a refinement.
-        This follows the process documented at https://help.looker.com/hc/en-us/articles/4419773929107-LookML-refinements
-        """
-        merge_column: List[dict] = []
-        original_value: List[dict] = original_dict.get(key, [])
-        refine_value: List[dict] = refinement_dict.get(key, [])
-        # name is required field, not going to be None
-        original_column_map = {
-            column[LookerRefinementResolver.NAME]: column for column in original_value
-        }
-        refine_column_map = {
-            column[LookerRefinementResolver.NAME]: column for column in refine_value
-        }
-        for existing_column_name in original_column_map:
-            existing_column = original_column_map[existing_column_name]
-            refine_column = refine_column_map.get(existing_column_name)
-            if refine_column is not None:
-                existing_column.update(refine_column)
-
-            merge_column.append(existing_column)
-
-        # merge any remaining column from refine_column_map
-        for new_column_name in refine_column_map:
-            if new_column_name not in original_column_map:
-                merge_column.append(refine_column_map[new_column_name])
-
-        return merge_column
-
-    @staticmethod
-    def merge_and_set_column(
-        new_raw_view: dict, refinement_view: dict, key: str
-    ) -> None:
-        merged_column = LookerRefinementResolver.merge_column(
-            new_raw_view, refinement_view, key
-        )
-        if merged_column:
-            new_raw_view[key] = merged_column
-
-    @staticmethod
-    def merge_refinements(raw_view: dict, refinement_views: List[dict]) -> dict:
-        """
-        Iterate over refinement_views and merge parameter of each view with raw_view.
-        Detail of merging order can be found at https://cloud.google.com/looker/docs/lookml-refinements
-        """
-        new_raw_view: dict = copy.deepcopy(raw_view)
-
-        for refinement_view in refinement_views:
-            # Merge dimension and measure
-            # TODO: low priority: handle additive parameters
-            # https://cloud.google.com/looker/docs/lookml-refinements#some_parameters_are_additive
-
-            # Merge Dimension
-            LookerRefinementResolver.merge_and_set_column(
-                new_raw_view, refinement_view, LookerRefinementResolver.DIMENSIONS
-            )
-            # Merge Measure
-            LookerRefinementResolver.merge_and_set_column(
-                new_raw_view, refinement_view, LookerRefinementResolver.MEASURES
-            )
-            # Merge Dimension Group
-            LookerRefinementResolver.merge_and_set_column(
-                new_raw_view, refinement_view, LookerRefinementResolver.DIMENSION_GROUPS
-            )
-
-        return new_raw_view
-
-    def get_refinements(self, views: List[dict], view_name: str) -> List[dict]:
-        """
-        Refinement syntax for view and explore are same.
-        This function can be used to filter out view/explore refinement from raw dictionary list
-        """
-        view_refinement_name: str = self.REFINEMENT_PREFIX + view_name
-        refined_views: List[dict] = []
-
-        for raw_view in views:
-            if view_refinement_name == raw_view[LookerRefinementResolver.NAME]:
-                refined_views.append(raw_view)
-
-        return refined_views
-
-    def get_refinement_from_model_includes(self, view_name: str) -> List[dict]:
-        refined_views: List[dict] = []
-
-        for include in self.looker_model.resolved_includes:
-            included_looker_viewfile = self.looker_viewfile_loader.load_viewfile(
-                include.include,
-                include.project,
-                self.connection_definition,
-                self.reporter,
-            )
-
-            if not included_looker_viewfile:
-                continue
-
-            refined_views.extend(
-                self.get_refinements(included_looker_viewfile.views, view_name)
-            )
-
-        return refined_views
-
-    def should_skip_processing(self, raw_view_name: str) -> bool:
-        if LookerRefinementResolver.is_refinement(raw_view_name):
-            return True
-
-        if self.source_config.process_refinements is False:
-            return True
-
-        return False
-
-    def apply_view_refinement(self, raw_view: dict) -> dict:
-        """
-        Looker process the lkml file in include order and merge the all refinement to original view.
-        """
-        assert raw_view.get(LookerRefinementResolver.NAME) is not None
-
-        raw_view_name: str = raw_view[LookerRefinementResolver.NAME]
-
-        if self.should_skip_processing(raw_view_name):
-            return raw_view
-
-        if raw_view_name in self.view_refinement_cache:
-            logger.debug(f"Returning applied refined view {raw_view_name} from cache")
-            return self.view_refinement_cache[raw_view_name]
-
-        logger.debug(f"Processing refinement for view {raw_view_name}")
-
-        refinement_views: List[dict] = self.get_refinement_from_model_includes(
-            raw_view_name
-        )
-
-        self.view_refinement_cache[raw_view_name] = self.merge_refinements(
-            raw_view, refinement_views
-        )
-
-        return self.view_refinement_cache[raw_view_name]
-
-    @staticmethod
-    def add_extended_explore(
-        raw_explore: dict, refinement_explores: List[Dict]
-    ) -> None:
-        extended_explores: Set[str] = set()
-        for view in refinement_explores:
-            extends = list(
-                itertools.chain.from_iterable(
-                    view.get(
-                        LookerRefinementResolver.EXTENDS,
-                        view.get(LookerRefinementResolver.EXTENDS_ALL, []),
-                    )
-                )
-            )
-            extended_explores.update(extends)
-
-        if extended_explores:  # if it is not empty then add to the original view
-            raw_explore[LookerRefinementResolver.EXTENDS] = list(extended_explores)
-
-    def apply_explore_refinement(self, raw_view: dict) -> dict:
-        """
-        In explore refinement `extends` parameter is additive.
-        Refer looker refinement document: https://cloud.google.com/looker/docs/lookml-refinements#additive
-        """
-        assert raw_view.get(LookerRefinementResolver.NAME) is not None
-
-        raw_view_name: str = raw_view[LookerRefinementResolver.NAME]
-
-        if self.should_skip_processing(raw_view_name):
-            return raw_view
-
-        if raw_view_name in self.explore_refinement_cache:
-            logger.debug(
-                f"Returning applied refined explore {raw_view_name} from cache"
-            )
-            return self.explore_refinement_cache[raw_view_name]
-
-        logger.debug(f"Processing refinement for explore {raw_view_name}")
-
-        refinement_explore: List[dict] = self.get_refinements(
-            self.looker_model.explores, raw_view_name
-        )
-
-        self.add_extended_explore(raw_view, refinement_explore)
-
-        self.explore_refinement_cache[raw_view_name] = raw_view
-
-        return self.explore_refinement_cache[raw_view_name]
-
-
 VIEW_LANGUAGE_LOOKML: str = "lookml"
 VIEW_LANGUAGE_SQL: str = "sql"
 
 
 def _find_view_from_resolved_includes(
     connection: Optional[LookerConnectionDefinition],
     resolved_includes: List[ProjectInclude],
@@ -1073,15 +817,14 @@
         cls,
         project_name: str,
         model_name: str,
         looker_view: dict,
         connection: LookerConnectionDefinition,
         looker_viewfile: LookerViewFile,
         looker_viewfile_loader: LookerViewFileLoader,
-        looker_refinement_resolver: LookerRefinementResolver,
         reporter: LookMLSourceReport,
         max_file_snippet_length: int,
         parse_table_names_from_sql: bool = False,
         sql_parser_path: str = "datahub.utilities.sql_parser.DefaultSQLParser",
         extract_col_level_lineage: bool = False,
         populate_sql_logic_in_descriptions: bool = False,
         process_isolation_for_sql_parsing: bool = False,
@@ -1092,15 +835,14 @@
         # so we resolve this field while taking that into account.
         sql_table_name: Optional[str] = LookerView.get_including_extends(
             view_name=view_name,
             looker_view=looker_view,
             connection=connection,
             looker_viewfile=looker_viewfile,
             looker_viewfile_loader=looker_viewfile_loader,
-            looker_refinement_resolver=looker_refinement_resolver,
             field="sql_table_name",
             reporter=reporter,
         )
 
         # Some sql_table_name fields contain quotes like: optimizely."group", just remove the quotes
         sql_table_name = (
             sql_table_name.replace('"', "").replace("`", "")
@@ -1109,15 +851,14 @@
         )
         derived_table = LookerView.get_including_extends(
             view_name=view_name,
             looker_view=looker_view,
             connection=connection,
             looker_viewfile=looker_viewfile,
             looker_viewfile_loader=looker_viewfile_loader,
-            looker_refinement_resolver=looker_refinement_resolver,
             field="derived_table",
             reporter=reporter,
         )
 
         dimensions = cls._get_fields(
             looker_view.get("dimensions", []),
             ViewFieldType.DIMENSION,
@@ -1336,49 +1077,47 @@
 
     @classmethod
     def resolve_extends_view_name(
         cls,
         connection: LookerConnectionDefinition,
         looker_viewfile: LookerViewFile,
         looker_viewfile_loader: LookerViewFileLoader,
-        looker_refinement_resolver: LookerRefinementResolver,
         target_view_name: str,
         reporter: LookMLSourceReport,
     ) -> Optional[dict]:
         # The view could live in the same file.
         for raw_view in looker_viewfile.views:
             raw_view_name = raw_view["name"]
             if raw_view_name == target_view_name:
-                return looker_refinement_resolver.apply_view_refinement(raw_view)
+                return raw_view
 
         # Or, it could live in one of the imports.
         view = _find_view_from_resolved_includes(
             connection,
             looker_viewfile.resolved_includes,
             looker_viewfile_loader,
             target_view_name,
             reporter,
         )
         if view:
-            return looker_refinement_resolver.apply_view_refinement(view[1])
+            return view[1]
         else:
             logger.warning(
                 f"failed to resolve view {target_view_name} included from {looker_viewfile.absolute_file_path}"
             )
             return None
 
     @classmethod
     def get_including_extends(
         cls,
         view_name: str,
         looker_view: dict,
         connection: LookerConnectionDefinition,
         looker_viewfile: LookerViewFile,
         looker_viewfile_loader: LookerViewFileLoader,
-        looker_refinement_resolver: LookerRefinementResolver,
         field: str,
         reporter: LookMLSourceReport,
     ) -> Optional[Any]:
         extends = list(
             itertools.chain.from_iterable(
                 looker_view.get("extends", looker_view.get("extends__all", []))
             )
@@ -1388,20 +1127,15 @@
         if field in looker_view:
             return looker_view[field]
 
         # Then, check the views this extends, following Looker's precedence rules.
         for extend in reversed(extends):
             assert extend != view_name, "a view cannot extend itself"
             extend_view = LookerView.resolve_extends_view_name(
-                connection,
-                looker_viewfile,
-                looker_viewfile_loader,
-                looker_refinement_resolver,
-                extend,
-                reporter,
+                connection, looker_viewfile, looker_viewfile_loader, extend, reporter
             )
             if not extend_view:
                 raise NameError(
                     f"failed to resolve extends view {extend} in view {view_name} of file {looker_viewfile.absolute_file_path}"
                 )
             if field in extend_view:
                 return extend_view[field]
@@ -1804,15 +1538,15 @@
         with tempfile.TemporaryDirectory("lookml_tmp") as tmp_dir:
             # Clone the base_folder if necessary.
             if not self.source_config.base_folder:
                 assert self.source_config.git_info
                 # we don't have a base_folder, so we need to clone the repo and process it locally
                 start_time = datetime.now()
                 git_clone = GitClone(tmp_dir)
-                # Github info deploy key is always populated
+                # github info deploy key is always populated
                 assert self.source_config.git_info.deploy_key
                 assert self.source_config.git_info.repo_ssh_locator
                 checkout_dir = git_clone.clone(
                     ssh_key=self.source_config.git_info.deploy_key,
                     repo_url=self.source_config.git_info.repo_ssh_locator,
                     branch=self.source_config.git_info.branch_for_clone,
                 )
@@ -1988,47 +1722,30 @@
                 self.reporter.report_warning(
                     f"model-{model_name}",
                     f"Failed to load connection {model.connection}. Check your API key permissions.",
                 )
                 self.reporter.report_models_dropped(model_name)
                 continue
 
-            explore_reachable_views: Set[str] = set()
-            looker_refinement_resolver: LookerRefinementResolver = (
-                LookerRefinementResolver(
-                    looker_model=model,
-                    connection_definition=connectionDefinition,
-                    looker_viewfile_loader=viewfile_loader,
-                    source_config=self.source_config,
-                    reporter=self.reporter,
-                )
-            )
+            explore_reachable_views: Set[ProjectInclude] = set()
             if self.source_config.emit_reachable_views_only:
                 model_explores_map = {d["name"]: d for d in model.explores}
                 for explore_dict in model.explores:
                     try:
-                        if LookerRefinementResolver.is_refinement(explore_dict["name"]):
-                            continue
-
-                        explore_dict = (
-                            looker_refinement_resolver.apply_explore_refinement(
-                                explore_dict
-                            )
-                        )
                         explore: LookerExplore = LookerExplore.from_dict(
                             model_name,
                             explore_dict,
                             model.resolved_includes,
                             viewfile_loader,
                             self.reporter,
                             model_explores_map,
                         )
                         if explore.upstream_views:
                             for view_name in explore.upstream_views:
-                                explore_reachable_views.add(view_name.include)
+                                explore_reachable_views.add(view_name)
                     except Exception as e:
                         self.reporter.report_warning(
                             f"{model}.explores",
                             f"failed to process {explore_dict} due to {e}. Run with --debug for full stacktrace",
                         )
                         logger.debug("Failed to process explore", exc_info=e)
 
@@ -2048,76 +1765,66 @@
                 logger.debug(f"Attempting to load view file: {include}")
                 looker_viewfile = viewfile_loader.load_viewfile(
                     path=include.include,
                     project_name=include.project,
                     connection=connectionDefinition,
                     reporter=self.reporter,
                 )
-
                 if looker_viewfile is not None:
                     for raw_view in looker_viewfile.views:
-                        raw_view_name = raw_view["name"]
-                        if LookerRefinementResolver.is_refinement(raw_view_name):
-                            continue
-
                         if (
                             self.source_config.emit_reachable_views_only
-                            and raw_view_name not in explore_reachable_views
+                            and ProjectInclude(_BASE_PROJECT_NAME, raw_view["name"])
+                            not in explore_reachable_views
                         ):
                             logger.debug(
-                                f"view {raw_view_name} is not reachable from an explore, skipping.."
+                                f"view {raw_view['name']} is not reachable from an explore, skipping.."
+                            )
+                            self.reporter.report_unreachable_view_dropped(
+                                raw_view["name"]
                             )
-                            self.reporter.report_unreachable_view_dropped(raw_view_name)
                             continue
 
                         self.reporter.report_views_scanned()
                         try:
-                            raw_view = looker_refinement_resolver.apply_view_refinement(
-                                raw_view=raw_view,
-                            )
                             maybe_looker_view = LookerView.from_looker_dict(
                                 include.project
                                 if include.project != _BASE_PROJECT_NAME
                                 else project_name,
                                 model_name,
                                 raw_view,
                                 connectionDefinition,
                                 looker_viewfile,
                                 viewfile_loader,
-                                looker_refinement_resolver,
                                 self.reporter,
                                 self.source_config.max_file_snippet_length,
                                 self.source_config.parse_table_names_from_sql,
                                 self.source_config.sql_parser,
                                 self.source_config.extract_column_level_lineage,
                                 self.source_config.populate_sql_logic_for_missing_descriptions,
                                 process_isolation_for_sql_parsing=self.source_config.process_isolation_for_sql_parsing,
                             )
                         except Exception as e:
                             self.reporter.report_warning(
                                 include.include,
                                 f"unable to load Looker view {raw_view}: {repr(e)}",
                             )
                             continue
-
                         if maybe_looker_view:
                             if self.source_config.view_pattern.allowed(
                                 maybe_looker_view.id.view_name
                             ):
                                 view_connection_mapping = view_connection_map.get(
                                     maybe_looker_view.id.view_name
                                 )
                                 if not view_connection_mapping:
                                     view_connection_map[
                                         maybe_looker_view.id.view_name
                                     ] = (model_name, model.connection)
                                     # first time we are discovering this view
-                                    logger.debug(
-                                        f"Generating MCP for view {raw_view['name']}"
-                                    )
                                     mce = self._build_dataset_mce(maybe_looker_view)
                                     workunit = MetadataWorkUnit(
                                         id=f"lookml-view-{maybe_looker_view.id}",
                                         mce=mce,
                                     )
                                     processed_view_files.add(include.include)
                                     self.reporter.report_workunit(workunit)
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/metabase.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metabase.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/metadata/business_glossary.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/business_glossary.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,44 +1,41 @@
 import logging
 import pathlib
 import time
 from dataclasses import dataclass, field
-from typing import Any, Dict, Iterable, List, Optional, TypeVar, Union
+from typing import Any, Dict, Iterable, List, Optional, Union
 
 from pydantic import validator
 from pydantic.fields import Field
 
 import datahub.metadata.schema_classes as models
 from datahub.configuration.common import ConfigModel
 from datahub.configuration.config_loader import load_config_file
 from datahub.emitter.mce_builder import datahub_guid, make_group_urn, make_user_urn
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
-from datahub.ingestion.api.decorators import (
+from datahub.ingestion.api.decorators import (  # SourceCapability,; capability,
     SupportStatus,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.source import Source, SourceReport
-from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.api.workunit import MetadataWorkUnit, UsageStatsWorkUnit
 from datahub.ingestion.graph.client import DataHubGraph
 from datahub.utilities.registries.domain_registry import DomainRegistry
-from datahub.utilities.source_helpers import (
-    auto_status_aspect,
-    auto_workunit,
-    auto_workunit_reporter,
-)
+from datahub.utilities.source_helpers import auto_workunit_reporter
 from datahub.utilities.urn_encoder import UrnEncoder
 
 logger = logging.getLogger(__name__)
 
-GlossaryNodeInterface = TypeVar(
-    "GlossaryNodeInterface", "GlossaryNodeConfig", "BusinessGlossaryConfig"
-)
+valid_status: models.StatusClass = models.StatusClass(removed=False)
+
+# This needed to map path presents in inherits, contains, values, and related_terms to terms' optional id
+path_vs_id: Dict[str, Optional[str]] = {}
 
 
 class Owners(ConfigModel):
     users: Optional[List[str]]
     groups: Optional[List[str]]
 
 
@@ -59,54 +56,51 @@
     contains: Optional[List[str]]
     values: Optional[List[str]]
     related_terms: Optional[List[str]]
     custom_properties: Optional[Dict[str, str]]
     knowledge_links: Optional[List[KnowledgeCard]]
     domain: Optional[str]
 
-    # Private fields.
-    _urn: str
-
 
 class GlossaryNodeConfig(ConfigModel):
     id: Optional[str]
     name: str
     description: str
     owners: Optional[Owners]
-    terms: Optional[List["GlossaryTermConfig"]]
+    terms: Optional[List[GlossaryTermConfig]]
     nodes: Optional[List["GlossaryNodeConfig"]]
     knowledge_links: Optional[List[KnowledgeCard]]
 
-    # Private fields.
-    _urn: str
+
+GlossaryNodeConfig.update_forward_refs()
 
 
 class DefaultConfig(ConfigModel):
     """Holds defaults for populating fields in glossary terms"""
 
-    source: Optional[str]
+    source: str
     owners: Owners
     url: Optional[str] = None
-    source_type: str = "INTERNAL"
+    source_type: Optional[str] = "INTERNAL"
 
 
 class BusinessGlossarySourceConfig(ConfigModel):
     file: Union[str, pathlib.Path] = Field(
         description="File path or URL to business glossary file to ingest."
     )
     enable_auto_id: bool = Field(
-        description="Generate guid urns instead of a plaintext path urn with the node/term's hierarchy.",
+        description="Generate id field from GlossaryNode and GlossaryTerm's name field",
         default=False,
     )
 
 
 class BusinessGlossaryConfig(DefaultConfig):
     version: str
-    terms: Optional[List["GlossaryTermConfig"]]
-    nodes: Optional[List["GlossaryNodeConfig"]]
+    nodes: Optional[List[GlossaryNodeConfig]]
+    terms: Optional[List[GlossaryTermConfig]]
 
     @validator("version")
     def version_must_be_1(cls, v):
         if v != "1":
             raise ValueError("Only version 1 is supported")
         return v
 
@@ -168,37 +162,37 @@
             for o in owners.groups
         ]
     return models.OwnershipClass(owners=owners_meta)
 
 
 def get_mces(
     glossary: BusinessGlossaryConfig,
-    path_vs_id: Dict[str, str],
     ingestion_config: BusinessGlossarySourceConfig,
     ctx: PipelineContext,
 ) -> Iterable[Union[MetadataChangeProposalWrapper, models.MetadataChangeEventClass]]:
+    path: List[str] = []
     root_owners = get_owners(glossary.owners)
 
     if glossary.nodes:
         for node in glossary.nodes:
             yield from get_mces_from_node(
                 node,
-                path_vs_id=path_vs_id,
+                path + [node.name],
                 parentNode=None,
                 parentOwners=root_owners,
                 defaults=glossary,
                 ingestion_config=ingestion_config,
                 ctx=ctx,
             )
 
     if glossary.terms:
         for term in glossary.terms:
             yield from get_mces_from_term(
                 term,
-                path_vs_id=path_vs_id,
+                path + [term.name],
                 parentNode=None,
                 parentOwnership=root_owners,
                 defaults=glossary,
                 ingestion_config=ingestion_config,
                 ctx=ctx,
             )
 
@@ -239,63 +233,64 @@
     term_urn: str, domain_aspect: models.DomainsClass
 ) -> MetadataChangeProposalWrapper:
     return MetadataChangeProposalWrapper(entityUrn=term_urn, aspect=domain_aspect)
 
 
 def get_mces_from_node(
     glossaryNode: GlossaryNodeConfig,
-    path_vs_id: Dict[str, str],
+    path: List[str],
     parentNode: Optional[str],
     parentOwners: models.OwnershipClass,
     defaults: DefaultConfig,
     ingestion_config: BusinessGlossarySourceConfig,
     ctx: PipelineContext,
 ) -> Iterable[Union[MetadataChangeProposalWrapper, models.MetadataChangeEventClass]]:
-    node_urn = glossaryNode._urn
-
+    node_urn = make_glossary_node_urn(
+        path, glossaryNode.id, ingestion_config.enable_auto_id
+    )
     node_info = models.GlossaryNodeInfoClass(
         definition=glossaryNode.description,
         parentNode=parentNode,
         name=glossaryNode.name,
     )
     node_owners = parentOwners
     if glossaryNode.owners is not None:
         assert glossaryNode.owners is not None
         node_owners = get_owners(glossaryNode.owners)
 
     node_snapshot = models.GlossaryNodeSnapshotClass(
         urn=node_urn,
-        aspects=[node_info, node_owners],
+        aspects=[node_info, node_owners, valid_status],
     )
     yield get_mce_from_snapshot(node_snapshot)
 
     if glossaryNode.knowledge_links is not None:
         mcp: Optional[MetadataChangeProposalWrapper] = make_institutional_memory_mcp(
             node_urn, glossaryNode.knowledge_links
         )
         if mcp is not None:
             yield mcp
 
     if glossaryNode.nodes:
         for node in glossaryNode.nodes:
             yield from get_mces_from_node(
                 node,
-                path_vs_id=path_vs_id,
+                path + [node.name],
                 parentNode=node_urn,
                 parentOwners=node_owners,
                 defaults=defaults,
                 ingestion_config=ingestion_config,
                 ctx=ctx,
             )
 
     if glossaryNode.terms:
         for term in glossaryNode.terms:
             yield from get_mces_from_term(
                 glossaryTerm=term,
-                path_vs_id=path_vs_id,
+                path=path + [term.name],
                 parentNode=node_urn,
                 parentOwnership=node_owners,
                 defaults=defaults,
                 ingestion_config=ingestion_config,
                 ctx=ctx,
             )
 
@@ -314,36 +309,37 @@
         domains=[domain_registry.get_domain_urn(domain) for domain in domains]
     )
     return domain_class
 
 
 def get_mces_from_term(
     glossaryTerm: GlossaryTermConfig,
-    path_vs_id: Dict[str, str],
+    path: List[str],
     parentNode: Optional[str],
     parentOwnership: models.OwnershipClass,
     defaults: DefaultConfig,
     ingestion_config: BusinessGlossarySourceConfig,
     ctx: PipelineContext,
 ) -> Iterable[Union[models.MetadataChangeEventClass, MetadataChangeProposalWrapper]]:
-    term_urn = glossaryTerm._urn
-
+    term_urn = make_glossary_term_urn(
+        path, glossaryTerm.id, ingestion_config.enable_auto_id
+    )
     aspects: List[
         Union[
             models.GlossaryTermInfoClass,
             models.GlossaryRelatedTermsClass,
             models.OwnershipClass,
-            models.GlossaryTermKeyClass,
             models.StatusClass,
+            models.GlossaryTermKeyClass,
             models.BrowsePathsClass,
         ]
     ] = []
     term_info = models.GlossaryTermInfoClass(
         definition=glossaryTerm.description,
-        termSource=glossaryTerm.term_source
+        termSource=glossaryTerm.term_source  # type: ignore
         if glossaryTerm.term_source is not None
         else defaults.source_type,
         sourceRef=glossaryTerm.source_ref
         if glossaryTerm.source_ref
         else defaults.source,
         sourceUrl=glossaryTerm.source_url if glossaryTerm.source_url else defaults.url,
         parentNode=parentNode,
@@ -432,54 +428,35 @@
         mcp: Optional[MetadataChangeProposalWrapper] = make_institutional_memory_mcp(
             term_urn, glossaryTerm.knowledge_links
         )
         if mcp is not None:
             yield mcp
 
 
-def materialize_all_node_urns(
-    glossary: BusinessGlossaryConfig, enable_auto_id: bool
-) -> None:
-    """After this runs, all nodes will have an id value that is a valid urn."""
-
-    def _process_child_terms(
-        parent_node: GlossaryNodeInterface, path: List[str]
-    ) -> None:
-        for term in parent_node.terms or []:
-            term._urn = make_glossary_term_urn(
-                path + [term.name], term.id, enable_auto_id
-            )
-
-        for node in parent_node.nodes or []:
-            node._urn = make_glossary_node_urn(
-                path + [node.name], node.id, enable_auto_id
-            )
-            _process_child_terms(node, path + [node.name])
-
-    _process_child_terms(glossary, [])
-
+def populate_path_vs_id(glossary: BusinessGlossaryConfig) -> None:
+    path: List[str] = []
 
-def populate_path_vs_id(glossary: BusinessGlossaryConfig) -> Dict[str, str]:
-    # This needed to map paths present in inherits, contains, values, and related_terms to term's
-    # urn, if one was manually specified.
-    path_vs_id: Dict[str, str] = {}
+    def _process_child_terms(parent_node: GlossaryNodeConfig, path: List[str]) -> None:
+        path_vs_id[".".join(path + [parent_node.name])] = parent_node.id
 
-    def _process_child_terms(
-        parent_node: GlossaryNodeInterface, path: List[str]
-    ) -> None:
-        for term in parent_node.terms or []:
-            path_vs_id[".".join(path + [term.name])] = term._urn
+        if parent_node.terms:
+            for term in parent_node.terms:
+                path_vs_id[".".join(path + [parent_node.name] + [term.name])] = term.id
+
+        if parent_node.nodes:
+            for node in parent_node.nodes:
+                _process_child_terms(node, path + [parent_node.name])
 
-        for node in parent_node.nodes or []:
-            path_vs_id[".".join(path + [node.name])] = node._urn
-            _process_child_terms(node, path + [node.name])
-
-    _process_child_terms(glossary, [])
+    if glossary.nodes:
+        for node in glossary.nodes:
+            _process_child_terms(node, path)
 
-    return path_vs_id
+    if glossary.terms:
+        for term in glossary.terms:
+            path_vs_id[".".join(path + [term.name])] = term.id
 
 
 @platform_name("Business Glossary")
 @config_class(BusinessGlossarySourceConfig)
 @support_status(SupportStatus.CERTIFIED)
 @dataclass
 class BusinessGlossaryFileSource(Source):
@@ -491,40 +468,32 @@
     report: SourceReport = field(default_factory=SourceReport)
 
     @classmethod
     def create(cls, config_dict, ctx):
         config = BusinessGlossarySourceConfig.parse_obj(config_dict)
         return cls(ctx, config)
 
-    @classmethod
     def load_glossary_config(
-        cls, file_name: Union[str, pathlib.Path]
+        self, file_name: Union[str, pathlib.Path]
     ) -> BusinessGlossaryConfig:
         config = load_config_file(file_name)
         glossary_cfg = BusinessGlossaryConfig.parse_obj(config)
         return glossary_cfg
 
-    def get_workunits(self) -> Iterable[MetadataWorkUnit]:
-        return auto_workunit_reporter(
-            self.report,
-            auto_status_aspect(
-                self.get_workunits_internal(),
-            ),
-        )
+    def get_workunits(self) -> Iterable[Union[MetadataWorkUnit, UsageStatsWorkUnit]]:
+        return auto_workunit_reporter(self.report, self.get_workunits_internal())
 
     def get_workunits_internal(
         self,
-    ) -> Iterable[MetadataWorkUnit]:
+    ) -> Iterable[Union[MetadataWorkUnit, UsageStatsWorkUnit]]:
         glossary_config = self.load_glossary_config(self.config.file)
-
-        materialize_all_node_urns(glossary_config, self.config.enable_auto_id)
-        path_vs_id = populate_path_vs_id(glossary_config)
-
-        for event in auto_workunit(
-            get_mces(
-                glossary_config, path_vs_id, ingestion_config=self.config, ctx=self.ctx
-            )
+        populate_path_vs_id(glossary_config)
+        for event in get_mces(
+            glossary_config, ingestion_config=self.config, ctx=self.ctx
         ):
-            yield event
+            if isinstance(event, models.MetadataChangeEventClass):
+                yield MetadataWorkUnit(f"{event.proposedSnapshot.urn}", mce=event)
+            elif isinstance(event, MetadataChangeProposalWrapper):
+                yield event.as_workunit()
 
     def get_report(self):
         return self.report
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/metadata/lineage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/lineage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/mode.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/mode.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/mongodb.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/mongodb.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/nifi.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/nifi.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/openapi.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/openapi.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/openapi_parser.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/openapi_parser.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/config.py`

 * *Files 12% similar despite different names*

```diff
@@ -31,15 +31,14 @@
     PBIAccessToken = "PBIAccessToken"
     DASHBOARD_LIST = "DASHBOARD_LIST"
     TILE_LIST = "TILE_LIST"
     REPORT_LIST = "REPORT_LIST"
     PAGE_BY_REPORT = "PAGE_BY_REPORT"
     DATASET_GET = "DATASET_GET"
     DATASET_LIST = "DATASET_LIST"
-    WORKSPACE_MODIFIED_LIST = "WORKSPACE_MODIFIED_LIST"
     REPORT_GET = "REPORT_GET"
     DATASOURCE_GET = "DATASOURCE_GET"
     TILE_GET = "TILE_GET"
     ENTITY_USER_LIST = "ENTITY_USER_LIST"
     SCAN_CREATE = "SCAN_CREATE"
     SCAN_GET = "SCAN_GET"
     SCAN_RESULT_GET = "SCAN_RESULT_GET"
@@ -67,33 +66,26 @@
     BROWSERPATH = "browsePaths"
     DASHBOARD_INFO = "dashboardInfo"
     DATAPLATFORM_INSTANCE = "dataPlatformInstance"
     DATASET = "dataset"
     DATASETS = "datasets"
     DATASET_KEY = "datasetKey"
     DATASET_PROPERTIES = "datasetProperties"
-    VIEW_PROPERTIES = "viewProperties"
-    SCHEMA_METADATA = "schemaMetadata"
-    SUBTYPES = "subTypes"
     VALUE = "value"
     ENTITY = "ENTITY"
     ID = "id"
     HTTP_RESPONSE_TEXT = "HttpResponseText"
     HTTP_RESPONSE_STATUS_CODE = "HttpResponseStatusCode"
     NAME = "name"
     DISPLAY_NAME = "displayName"
     CURRENT_VALUE = "currentValue"
     ORDER = "order"
     IDENTIFIER = "identifier"
     EMAIL_ADDRESS = "emailAddress"
     PRINCIPAL_TYPE = "principalType"
-    DATASET_USER_ACCESS_RIGHT = "datasetUserAccessRight"
-    REPORT_USER_ACCESS_RIGHT = "reportUserAccessRight"
-    DASHBOARD_USER_ACCESS_RIGHT = "dashboardUserAccessRight"
-    GROUP_USER_ACCESS_RIGHT = "groupUserAccessRight"
     GRAPH_ID = "graphId"
     WORKSPACES = "workspaces"
     TITLE = "title"
     EMBED_URL = "embedUrl"
     ACCESS_TOKEN = "access_token"
     IS_READ_ONLY = "isReadOnly"
     WEB_URL = "webUrl"
@@ -144,18 +136,14 @@
     )
 
     AMAZON_REDSHIFT = DataPlatformPair(
         powerbi_data_platform_name="AmazonRedshift",
         datahub_data_platform_name="redshift",
     )
 
-    DATABRICK_SQL = DataPlatformPair(
-        powerbi_data_platform_name="Databricks", datahub_data_platform_name="databricks"
-    )
-
 
 @dataclass
 class PowerBiDashboardSourceReport(StaleEntityRemovalSourceReport):
     dashboards_scanned: int = 0
     charts_scanned: int = 0
     filtered_dashboards: List[str] = dataclass_field(default_factory=list)
     filtered_charts: List[str] = dataclass_field(default_factory=list)
@@ -192,37 +180,15 @@
         default=None,
         description="DataHub platform instance name. To generate correct urn for upstream dataset, this should match "
         "with platform instance name used in ingestion"
         "recipe of other datahub sources.",
     )
     env: str = pydantic.Field(
         default=DEFAULT_ENV,
-        description="The environment that all assets produced by DataHub platform ingestion source belong to",
-    )
-
-
-class OwnershipMapping(ConfigModel):
-    create_corp_user: bool = pydantic.Field(
-        default=True, description="Whether ingest PowerBI user as Datahub Corpuser"
-    )
-    use_powerbi_email: bool = pydantic.Field(
-        default=False,
-        description="Use PowerBI User email to ingest as corpuser, default is powerbi user identifier",
-    )
-    remove_email_suffix: bool = pydantic.Field(
-        default=False,
-        description="Remove PowerBI User email suffix for example, @acryl.io",
-    )
-    dataset_configured_by_as_owner: bool = pydantic.Field(
-        default=False,
-        description="Take PBI dataset configuredBy as dataset owner if exist",
-    )
-    owner_criteria: Optional[List[str]] = pydantic.Field(
-        default=None,
-        description="Need to have certain authority to qualify as owner for example ['ReadWriteReshareExplore','Owner','Admin']",
+        description="The environment that the platform is located in. It is default to PROD",
     )
 
 
 class PowerBiDashboardSourceConfig(
     StatefulIngestionConfigBase, DatasetSourceConfigMixin
 ):
     platform_name: str = pydantic.Field(
@@ -277,77 +243,40 @@
     client_id: str = pydantic.Field(description="Azure app client identifier")
     # Azure app client secret
     client_secret: str = pydantic.Field(description="Azure app client secret")
     # timeout for meta-data scanning
     scan_timeout: int = pydantic.Field(
         default=60, description="timeout for PowerBI metadata scanning"
     )
-    scan_batch_size: int = pydantic.Field(
-        default=1,
-        gt=0,
-        le=100,
-        description="batch size for sending workspace_ids to PBI, 100 is the limit",
-    )
-    workspace_id_as_urn_part: bool = pydantic.Field(
-        default=False,
-        description="Highly recommend changing this to True, as you can have the same workspace name"
-        "To maintain backward compatability, this is set to False which uses workspace name",
-    )
     # Enable/Disable extracting ownership information of Dashboard
     extract_ownership: bool = pydantic.Field(
         default=False,
         description="Whether ownership should be ingested. Admin API access is required if this setting is enabled. "
         "Note that enabling this may overwrite owners that you've added inside DataHub's web application.",
     )
     # Enable/Disable extracting report information
     extract_reports: bool = pydantic.Field(
         default=True, description="Whether reports should be ingested"
     )
-    # Configure ingestion of ownership
-    ownership: OwnershipMapping = pydantic.Field(
-        default=OwnershipMapping(),
-        description="Configure how is ownership ingested",
-    )
-    modified_since: Optional[str] = pydantic.Field(
-        description="Get only recently modified workspaces based on modified_since datetime '2023-02-10T00:00:00.0000000Z', excludePersonalWorkspaces and excludeInActiveWorkspaces limit to last 30 days",
-    )
-    extract_dashboards: bool = pydantic.Field(
-        default=True,
-        description="Whether to ingest PBI Dashboard and Tiles as Datahub Dashboard and Chart",
-    )
-    # Enable/Disable extracting dataset schema
-    extract_dataset_schema: bool = pydantic.Field(
-        default=False,
-        description="Whether to ingest PBI Dataset Table columns and measures",
-    )
     # Enable/Disable extracting lineage information of PowerBI Dataset
     extract_lineage: bool = pydantic.Field(
         default=True,
         description="Whether lineage should be ingested between X and Y. Admin API access is required if this setting is enabled",
     )
     # Enable/Disable extracting endorsements to tags. Please notice this may overwrite
     # any existing tags defined to those entities
     extract_endorsements_to_tags: bool = pydantic.Field(
         default=False,
         description="Whether to extract endorsements to tags, note that this may overwrite existing tags. Admin API "
         "access is required is this setting is enabled",
     )
-    filter_dataset_endorsements: AllowDenyPattern = pydantic.Field(
-        default=AllowDenyPattern.allow_all(),
-        description="Filter and ingest datasets which are 'Certified' or 'Promoted' endorsement. If both are added, dataset which are 'Certified' or 'Promoted' will be ingested . Default setting allows all dataset to be ingested",
-    )
     # Enable/Disable extracting workspace information to DataHub containers
     extract_workspaces_to_containers: bool = pydantic.Field(
         default=True, description="Extract workspaces to DataHub containers"
     )
-    # Enable/Disable grouping PBI dataset tables into Datahub container (PBI Dataset)
-    extract_datasets_to_containers: bool = pydantic.Field(
-        default=False,
-        description="PBI tables will be grouped under a Datahub Container, the container reflect a PBI Dataset",
-    )
     # Enable/Disable extracting lineage information from PowerBI Native query
     native_query_parsing: bool = pydantic.Field(
         default=True,
         description="Whether PowerBI native query should be parsed to extract lineage",
     )
 
     # convert PowerBI dataset URN to lower-case
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/dataplatform_instance_resolver.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/dataplatform_instance_resolver.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/data_classes.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/data_classes.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/native_sql_parser.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/native_sql_parser.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/parser.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/parser.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/resolver.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/resolver.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,15 +10,14 @@
     DataPlatformPair,
     PowerBiDashboardSourceReport,
     SupportedDataPlatform,
 )
 from datahub.ingestion.source.powerbi.m_query import native_sql_parser, tree_function
 from datahub.ingestion.source.powerbi.m_query.data_classes import (
     TRACE_POWERBI_MQUERY_PARSER,
-    AbstractIdentifierAccessor,
     DataAccessFunctionDetail,
     IdentifierAccessor,
 )
 from datahub.ingestion.source.powerbi.rest_api_wrapper.data_classes import Table
 
 logger = logging.getLogger(__name__)
 
@@ -516,56 +515,14 @@
                 full_name=f"{db_name}.{schema_name}.{table_name}",
                 datasource_server=server,
                 data_platform_pair=self.get_platform_pair(),
             )
         ]
 
 
-class DatabrickDataPlatformTableCreator(AbstractDataPlatformTableCreator):
-    def create_dataplatform_tables(
-        self, data_access_func_detail: DataAccessFunctionDetail
-    ) -> List[DataPlatformTable]:
-        logger.debug(
-            f"Processing Databrick data-access function detail {data_access_func_detail}"
-        )
-        value_dict = {}
-        temp_accessor: Optional[
-            Union[IdentifierAccessor, AbstractIdentifierAccessor]
-        ] = data_access_func_detail.identifier_accessor
-        while temp_accessor:
-            if isinstance(temp_accessor, IdentifierAccessor):
-                value_dict[temp_accessor.items["Kind"]] = temp_accessor.items["Name"]
-                if temp_accessor.next is not None:
-                    temp_accessor = temp_accessor.next
-                else:
-                    break
-            else:
-                logger.debug(
-                    "expecting instance to be IdentifierAccessor, please check if parsing is done properly"
-                )
-                return []
-
-        db_name: str = value_dict["Database"]
-        schema_name: str = value_dict["Schema"]
-        table_name: str = value_dict["Table"]
-        server, _ = self.get_db_detail_from_argument(data_access_func_detail.arg_list)
-
-        return [
-            DataPlatformTable(
-                name=table_name,
-                full_name=f"{db_name}.{schema_name}.{table_name}",
-                datasource_server=server if server else "",
-                data_platform_pair=self.get_platform_pair(),
-            )
-        ]
-
-    def get_platform_pair(self) -> DataPlatformPair:
-        return SupportedDataPlatform.DATABRICK_SQL.value
-
-
 class DefaultThreeStepDataAccessSources(AbstractDataPlatformTableCreator, ABC):
     def get_datasource_server(
         self, arguments: List[str], data_access_func_detail: DataAccessFunctionDetail
     ) -> str:
         return tree_function.strip_char_from_list([arguments[0]])[0]
 
     def create_dataplatform_tables(
@@ -750,25 +707,19 @@
 
 class FunctionName(Enum):
     NATIVE_QUERY = "Value.NativeQuery"
     POSTGRESQL_DATA_ACCESS = "PostgreSQL.Database"
     ORACLE_DATA_ACCESS = "Oracle.Database"
     SNOWFLAKE_DATA_ACCESS = "Snowflake.Databases"
     MSSQL_DATA_ACCESS = "Sql.Database"
-    DATABRICK_DATA_ACCESS = "Databricks.Catalogs"
     GOOGLE_BIGQUERY_DATA_ACCESS = "GoogleBigQuery.Database"
     AMAZON_REDSHIFT_DATA_ACCESS = "AmazonRedshift.Database"
 
 
 class SupportedResolver(Enum):
-    DATABRICK_QUERY = (
-        DatabrickDataPlatformTableCreator,
-        FunctionName.DATABRICK_DATA_ACCESS,
-    )
-
     POSTGRES_SQL = (
         PostgresDataPlatformTableCreator,
         FunctionName.POSTGRESQL_DATA_ACCESS,
     )
 
     ORACLE = (
         OracleDataPlatformTableCreator,
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/tree_function.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/tree_function.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/m_query/validator.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/validator.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/powerbi-lexical-grammar.rule` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/powerbi-lexical-grammar.rule`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/powerbi.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/powerbi.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,34 +1,32 @@
 #########################################################
 #
 # Meta Data Ingestion From the Power BI Source
 #
 #########################################################
+
 import logging
-from typing import Iterable, List, Optional, Set, Tuple, Union
+from typing import Iterable, List, Optional, Tuple
 
 import datahub.emitter.mce_builder as builder
 import datahub.ingestion.source.powerbi.rest_api_wrapper.data_classes as powerbi_data_classes
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
-from datahub.emitter.mcp_builder import PlatformKey, gen_containers
+from datahub.emitter.mcp_builder import gen_containers
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.source import SourceReport
 from datahub.ingestion.api.workunit import MetadataWorkUnit
-from datahub.ingestion.source.common.subtypes import (
-    BIContainerSubTypes,
-    DatasetSubTypes,
-)
+from datahub.ingestion.source.common.subtypes import BIContainerSubTypes
 from datahub.ingestion.source.powerbi.config import (
     Constant,
     PlatformDetail,
     PowerBiDashboardSourceConfig,
     PowerBiDashboardSourceReport,
 )
 from datahub.ingestion.source.powerbi.dataplatform_instance_resolver import (
@@ -55,27 +53,22 @@
     ContainerClass,
     CorpUserKeyClass,
     DashboardInfoClass,
     DashboardKeyClass,
     DatasetLineageTypeClass,
     DatasetPropertiesClass,
     GlobalTagsClass,
-    OtherSchemaClass,
     OwnerClass,
     OwnershipClass,
     OwnershipTypeClass,
-    SchemaFieldClass,
-    SchemaFieldDataTypeClass,
-    SchemaMetadataClass,
     StatusClass,
     SubTypesClass,
     TagAssociationClass,
     UpstreamClass,
     UpstreamLineageClass,
-    ViewPropertiesClass,
 )
 from datahub.utilities.dedup_list import deduplicate_list
 from datahub.utilities.source_helpers import (
     auto_stale_entity_removal,
     auto_status_aspect,
     auto_workunit_reporter,
 )
@@ -106,16 +99,14 @@
         config: PowerBiDashboardSourceConfig,
         reporter: PowerBiDashboardSourceReport,
         dataplatform_instance_resolver: AbstractDataPlatformInstanceResolver,
     ):
         self.__config = config
         self.__reporter = reporter
         self.__dataplatform_instance_resolver = dataplatform_instance_resolver
-        self.processed_datasets: Set[powerbi_data_classes.PowerBIDataset] = set()
-        self.workspace_key: PlatformKey
 
     @staticmethod
     def urn_to_lowercase(value: str, flag: bool) -> str:
         if flag is True:
             return value.lower()
 
         return value
@@ -155,26 +146,14 @@
                 PLATFORM=self.__config.platform_name,
                 ENTITY_URN=mcp.entityUrn,
                 ASPECT_NAME=mcp.aspectName,
             ),
             mcp=mcp,
         )
 
-    def extract_dataset_schema(
-        self, table: powerbi_data_classes.Table, ds_urn: str
-    ) -> List[MetadataChangeProposalWrapper]:
-        schema_metadata = self.to_datahub_schema(table)
-        schema_mcp = self.new_mcp(
-            entity_type=Constant.DATASET,
-            entity_urn=ds_urn,
-            aspect_name=Constant.SCHEMA_METADATA,
-            aspect=schema_metadata,
-        )
-        return [schema_mcp]
-
     def extract_lineage(
         self, table: powerbi_data_classes.Table, ds_urn: str
     ) -> List[MetadataChangeProposalWrapper]:
         mcps: List[MetadataChangeProposalWrapper] = []
 
         # table.dataset should always be set, but we check it just in case.
         parameters = table.dataset.parameters if table.dataset else {}
@@ -223,136 +202,46 @@
                 entityUrn=ds_urn,
                 aspect=upstream_lineage,
             )
             mcps.append(mcp)
 
         return mcps
 
-    def create_datahub_owner_urn(self, user: str) -> str:
-        """
-        Create corpuser urn from PowerBI (configured by| modified by| created by) user
-        """
-        if self.__config.ownership.remove_email_suffix:
-            return builder.make_user_urn(user.split("@")[0])
-        return builder.make_user_urn(f"users.{user}")
-
-    def to_datahub_schema_field(
-        self,
-        field: Union[powerbi_data_classes.Column, powerbi_data_classes.Measure],
-    ) -> SchemaFieldClass:
-        data_type = field.dataType
-        if isinstance(field, powerbi_data_classes.Measure):
-            description = (
-                f"{field.expression} {field.description}"
-                if field.description
-                else field.expression
-            )
-        elif field.description:
-            description = field.description
-        else:
-            description = None
-
-        schema_field = SchemaFieldClass(
-            fieldPath=f"{field.name}",
-            type=SchemaFieldDataTypeClass(type=field.datahubDataType),
-            nativeDataType=data_type,
-            description=description,
-        )
-        return schema_field
-
-    def to_datahub_schema(
-        self,
-        table: powerbi_data_classes.Table,
-    ) -> SchemaMetadataClass:
-
-        fields = []
-        table_fields = (
-            [self.to_datahub_schema_field(column) for column in table.columns]
-            if table.columns
-            else []
-        )
-        measure_fields = (
-            [self.to_datahub_schema_field(measure) for measure in table.measures]
-            if table.measures
-            else []
-        )
-        fields.extend(table_fields)
-        fields.extend(measure_fields)
-
-        schema_metadata = SchemaMetadataClass(
-            schemaName=table.name,
-            platform=self.__config.platform_urn,
-            version=0,
-            hash="",
-            platformSchema=OtherSchemaClass(rawSchema=""),
-            fields=fields,
-        )
-
-        return schema_metadata
-
     def to_datahub_dataset(
         self,
         dataset: Optional[powerbi_data_classes.PowerBIDataset],
         workspace: powerbi_data_classes.Workspace,
     ) -> List[MetadataChangeProposalWrapper]:
         """
         Map PowerBi dataset to datahub dataset. Here we are mapping each table of PowerBi Dataset to Datahub dataset.
         In PowerBi Tile would be having single dataset, However corresponding Datahub's chart might have many input sources.
         """
 
         dataset_mcps: List[MetadataChangeProposalWrapper] = []
         if dataset is None:
             return dataset_mcps
-        if not any(
-            [
-                self.__config.filter_dataset_endorsements.allowed(tag)
-                for tag in (dataset.tags or [""])
-            ]
-        ):
-            logger.debug(
-                "Returning empty dataset_mcps as no dataset tag matched with filter_dataset_endorsements"
-            )
-            return dataset_mcps
 
         logger.debug(
             f"Mapping dataset={dataset.name}(id={dataset.id}) to datahub dataset"
         )
 
         for table in dataset.tables:
-            self.processed_datasets.add(dataset)
             # Create a URN for dataset
             ds_urn = builder.make_dataset_urn_with_platform_instance(
                 platform=self.__config.platform_name,
                 name=self.assets_urn_to_lowercase(table.full_name),
                 platform_instance=self.__config.platform_instance,
                 env=self.__config.env,
             )
 
             logger.debug(f"{Constant.Dataset_URN}={ds_urn}")
             # Create datasetProperties mcp
-            if table.expression:
-                view_properties = ViewPropertiesClass(
-                    materialized=False,
-                    viewLogic=table.expression,
-                    viewLanguage="m_query",
-                )
-                view_prop_mcp = self.new_mcp(
-                    entity_type=Constant.DATASET,
-                    entity_urn=ds_urn,
-                    aspect_name=Constant.VIEW_PROPERTIES,
-                    aspect=view_properties,
-                )
-                dataset_mcps.extend([view_prop_mcp])
             ds_properties = DatasetPropertiesClass(
                 name=table.name,
                 description=dataset.description,
-                externalUrl=dataset.webUrl,
-                customProperties={
-                    "datasetId": dataset.id,
-                },
             )
 
             info_mcp = self.new_mcp(
                 entity_type=Constant.DATASET,
                 entity_urn=ds_urn,
                 aspect_name=Constant.DATASET_PROPERTIES,
                 aspect=ds_properties,
@@ -361,57 +250,23 @@
             # Remove status mcp
             status_mcp = self.new_mcp(
                 entity_type=Constant.DATASET,
                 entity_urn=ds_urn,
                 aspect_name=Constant.STATUS,
                 aspect=StatusClass(removed=False),
             )
-            if self.__config.extract_dataset_schema:
-                dataset_mcps.extend(self.extract_dataset_schema(table, ds_urn))
-
-            subtype_mcp = self.new_mcp(
-                entity_type=Constant.DATASET,
-                entity_urn=ds_urn,
-                aspect_name=Constant.SUBTYPES,
-                aspect=SubTypesClass(
-                    typeNames=[
-                        DatasetSubTypes.POWERBI_DATASET_TABLE,
-                        DatasetSubTypes.VIEW,
-                    ]
-                ),
-            )
-            # normally the person who configure the dataset will be the most accurate person for ownership
-            if (
-                self.__config.extract_ownership
-                and self.__config.ownership.dataset_configured_by_as_owner
-                and dataset.configuredBy
-            ):
-                # Dashboard Ownership
-                user_urn = self.create_datahub_owner_urn(dataset.configuredBy)
-                owner_class = OwnerClass(owner=user_urn, type=OwnershipTypeClass.NONE)
-                # Dashboard owner MCP
-                ownership = OwnershipClass(owners=[owner_class])
-                owner_mcp = self.new_mcp(
-                    entity_type=Constant.DATASET,
-                    entity_urn=ds_urn,
-                    aspect_name=Constant.OWNERSHIP,
-                    aspect=ownership,
-                )
-                dataset_mcps.extend([owner_mcp])
-
-            dataset_mcps.extend([info_mcp, status_mcp, subtype_mcp])
+            dataset_mcps.extend([info_mcp, status_mcp])
 
             if self.__config.extract_lineage is True:
                 dataset_mcps.extend(self.extract_lineage(table, ds_urn))
 
             self.append_container_mcp(
                 dataset_mcps,
                 workspace,
                 ds_urn,
-                dataset,
             )
 
             self.append_tag_mcp(
                 dataset_mcps,
                 ds_urn,
                 Constant.DATASET,
                 dataset.tags,
@@ -651,65 +506,42 @@
         return list_of_mcps
 
     def append_container_mcp(
         self,
         list_of_mcps: List[MetadataChangeProposalWrapper],
         workspace: powerbi_data_classes.Workspace,
         entity_urn: str,
-        dataset: Optional[powerbi_data_classes.PowerBIDataset] = None,
     ) -> None:
-        if self.__config.extract_datasets_to_containers and isinstance(
-            dataset, powerbi_data_classes.PowerBIDataset
-        ):
-            container_key = dataset.get_dataset_key(self.__config.platform_name)
-        elif self.__config.extract_workspaces_to_containers:
+        if self.__config.extract_workspaces_to_containers:
             container_key = workspace.get_workspace_key(
                 platform_name=self.__config.platform_name,
                 platform_instance=self.__config.platform_instance,
-                workspace_id_as_urn_part=self.__config.workspace_id_as_urn_part,
             )
-        else:
-            return None
+            container_urn = builder.make_container_urn(
+                guid=container_key.guid(),
+            )
 
-        container_urn = builder.make_container_urn(
-            guid=container_key.guid(),
-        )
-        mcp = MetadataChangeProposalWrapper(
-            changeType=ChangeTypeClass.UPSERT,
-            entityUrn=entity_urn,
-            aspect=ContainerClass(container=f"{container_urn}"),
-        )
-        list_of_mcps.append(mcp)
+            mcp = MetadataChangeProposalWrapper(
+                changeType=ChangeTypeClass.UPSERT,
+                entityUrn=entity_urn,
+                aspect=ContainerClass(container=f"{container_urn}"),
+            )
+            list_of_mcps.append(mcp)
 
     def generate_container_for_workspace(
         self, workspace: powerbi_data_classes.Workspace
     ) -> Iterable[MetadataWorkUnit]:
-        self.workspace_key = workspace.get_workspace_key(
-            platform_name=self.__config.platform_name,
-            workspace_id_as_urn_part=self.__config.workspace_id_as_urn_part,
-        )
+        workspace_key = workspace.get_workspace_key(self.__config.platform_name)
         container_work_units = gen_containers(
-            container_key=self.workspace_key,
+            container_key=workspace_key,
             name=workspace.name,
             sub_types=[BIContainerSubTypes.POWERBI_WORKSPACE],
         )
         return container_work_units
 
-    def generate_container_for_dataset(
-        self, dataset: powerbi_data_classes.PowerBIDataset
-    ) -> Iterable[MetadataWorkUnit]:
-        dataset_key = dataset.get_dataset_key(self.__config.platform_name)
-        container_work_units = gen_containers(
-            container_key=dataset_key,
-            name=dataset.name if dataset.name else dataset.id,
-            parent_container_key=self.workspace_key,
-            sub_types=[BIContainerSubTypes.POWERBI_DATASET],
-        )
-        return container_work_units
-
     def append_tag_mcp(
         self,
         list_of_mcps: List[MetadataChangeProposalWrapper],
         entity_urn: str,
         entity_type: str,
         tags: List[str],
     ) -> None:
@@ -728,19 +560,16 @@
         """
         Map PowerBi user to datahub user
         """
 
         logger.debug(f"Mapping user {user.displayName}(id={user.id}) to datahub's user")
 
         # Create an URN for user
-        user_id = user.get_urn_part(
-            use_email=self.__config.ownership.use_powerbi_email,
-            remove_email_suffix=self.__config.ownership.remove_email_suffix,
-        )
-        user_urn = builder.make_user_urn(user_id)
+        user_urn = builder.make_user_urn(user.get_urn_part())
+
         user_key = CorpUserKeyClass(username=user.id)
 
         user_key_mcp = self.new_mcp(
             entity_type=Constant.CORP_USER,
             entity_urn=user_urn,
             aspect_name=Constant.CORP_USER_KEY,
             aspect=user_key,
@@ -750,34 +579,15 @@
 
     def to_datahub_users(
         self, users: List[powerbi_data_classes.User]
     ) -> List[MetadataChangeProposalWrapper]:
         user_mcps = []
 
         for user in users:
-            if user:
-                user_rights = [
-                    user.datasetUserAccessRight,
-                    user.reportUserAccessRight,
-                    user.dashboardUserAccessRight,
-                    user.groupUserAccessRight,
-                ]
-                if (
-                    user.principalType == "User"
-                    and self.__config.ownership.owner_criteria
-                    and len(
-                        set(user_rights) & set(self.__config.ownership.owner_criteria)
-                    )
-                    > 0
-                ):
-                    user_mcps.extend(self.to_datahub_user(user))
-                elif self.__config.ownership.owner_criteria is None:
-                    user_mcps.extend(self.to_datahub_user(user))
-                else:
-                    continue
+            user_mcps.extend(self.to_datahub_user(user))
 
         return user_mcps
 
     def to_datahub_chart(
         self,
         tiles: List[powerbi_data_classes.Tile],
         workspace: powerbi_data_classes.Workspace,
@@ -828,16 +638,15 @@
         # Lets convert dashboard to datahub dashboard
         dashboard_mcps: List[
             MetadataChangeProposalWrapper
         ] = self.to_datahub_dashboard_mcp(dashboard, workspace, chart_mcps, user_mcps)
 
         # Now add MCPs in sequence
         mcps.extend(ds_mcps)
-        if self.__config.ownership.create_corp_user:
-            mcps.extend(user_mcps)
+        mcps.extend(user_mcps)
         mcps.extend(chart_mcps)
         mcps.extend(dashboard_mcps)
 
         # Convert MCP to work_units
         work_units = map(self._to_work_unit, mcps)
         # Return set of work_unit
         return deduplicate_list([wu for wu in work_units if wu is not None])
@@ -1057,16 +866,15 @@
         chart_mcps = self.pages_to_chart(report.pages, workspace, ds_mcps)
 
         # Let's convert report to datahub dashboard
         report_mcps = self.report_to_dashboard(workspace, report, chart_mcps, user_mcps)
 
         # Now add MCPs in sequence
         mcps.extend(ds_mcps)
-        if self.__config.ownership.create_corp_user:
-            mcps.extend(user_mcps)
+        mcps.extend(user_mcps)
         mcps.extend(chart_mcps)
         mcps.extend(report_mcps)
 
         # Convert MCP to work_units
         work_units = map(self._to_work_unit, mcps)
         return work_units
 
@@ -1122,17 +930,16 @@
         )
 
     @classmethod
     def create(cls, config_dict, ctx):
         config = PowerBiDashboardSourceConfig.parse_obj(config_dict)
         return cls(config, ctx)
 
-    def get_allowed_workspaces(self) -> List[powerbi_data_classes.Workspace]:
+    def get_allowed_workspaces(self) -> Iterable[powerbi_data_classes.Workspace]:
         all_workspaces = self.powerbi_client.get_workspaces()
-
         allowed_wrk = [
             workspace
             for workspace in all_workspaces
             if self.source_config.workspace_id_pattern.allowed(workspace.id)
         ]
 
         logger.info(f"Number of workspaces = {len(all_workspaces)}")
@@ -1152,111 +959,61 @@
             if key not in powerbi_data_platforms:
                 raise ValueError(f"PowerBI DataPlatform {key} is not supported")
 
         logger.debug(
             f"Dataset lineage would get ingested for data-platform = {self.source_config.dataset_type_mapping}"
         )
 
-    def get_workspace_workunit(
-        self, workspace: powerbi_data_classes.Workspace
-    ) -> Iterable[MetadataWorkUnit]:
-        if self.source_config.extract_workspaces_to_containers:
-            workspace_workunits = self.mapper.generate_container_for_workspace(
-                workspace
-            )
-
-            for workunit in workspace_workunits:
-                # Return workunit to Datahub Ingestion framework
-                yield workunit
-        for dashboard in workspace.dashboards:
-            try:
-                # Fetch PowerBi users for dashboards
-                dashboard.users = self.powerbi_client.get_dashboard_users(dashboard)
-                # Increase dashboard and tiles count in report
-                self.reporter.report_dashboards_scanned()
-                self.reporter.report_charts_scanned(count=len(dashboard.tiles))
-            except Exception as e:
-                message = f"Error ({e}) occurred while loading dashboard {dashboard.displayName}(id={dashboard.id}) tiles."
-
-                logger.exception(message, e)
-                self.reporter.report_warning(dashboard.id, message)
-            # Convert PowerBi Dashboard and child entities to Datahub work unit to ingest into Datahub
-            workunits = self.mapper.to_datahub_work_units(dashboard, workspace)
-            for workunit in workunits:
-                # Return workunit to Datahub Ingestion framework
-                yield workunit
-
-        for report in workspace.reports:
-            for work_unit in self.mapper.report_to_datahub_work_units(
-                report, workspace
-            ):
-                yield work_unit
-
-        for dataset in self.mapper.processed_datasets:
-            if self.source_config.extract_datasets_to_containers:
-                dataset_workunits = self.mapper.generate_container_for_dataset(dataset)
-                for workunit in dataset_workunits:
-                    yield workunit
-
     def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         """
         Datahub Ingestion framework invoke this method
         """
         logger.info("PowerBi plugin execution is started")
         # Validate dataset type mapping
         self.validate_dataset_type_mapping()
         # Fetch PowerBi workspace for given workspace identifier
+        for workspace in self.get_allowed_workspaces():
+            logger.info(f"Scanning workspace id: {workspace.id}")
+            self.powerbi_client.fill_workspace(workspace, self.reporter)
+
+            if self.source_config.extract_workspaces_to_containers:
+                workspace_workunits = self.mapper.generate_container_for_workspace(
+                    workspace
+                )
 
-        allowed_workspaces = self.get_allowed_workspaces()
-        workspaces_len = len(allowed_workspaces)
-
-        batch_size = (
-            self.source_config.scan_batch_size
-        )  # 100 is the maximum allowed for powerbi scan
-        num_batches = (workspaces_len + batch_size - 1) // batch_size
-        batches = [
-            allowed_workspaces[i * batch_size : (i + 1) * batch_size]
-            for i in range(num_batches)
-        ]
-        for batch_workspaces in batches:
-            for workspace in self.powerbi_client.fill_workspaces(
-                batch_workspaces, self.reporter
-            ):
-                logger.info(f"Processing workspace id: {workspace.id}")
+                for workunit in workspace_workunits:
+                    # Return workunit to Datahub Ingestion framework
+                    yield workunit
+            for dashboard in workspace.dashboards:
+                try:
+                    # Fetch PowerBi users for dashboards
+                    dashboard.users = self.powerbi_client.get_dashboard_users(dashboard)
+                    # Increase dashboard and tiles count in report
+                    self.reporter.report_dashboards_scanned()
+                    self.reporter.report_charts_scanned(count=len(dashboard.tiles))
+                except Exception as e:
+                    message = f"Error ({e}) occurred while loading dashboard {dashboard.displayName}(id={dashboard.id}) tiles."
+
+                    logger.exception(message, e)
+                    self.reporter.report_warning(dashboard.id, message)
+                # Convert PowerBi Dashboard and child entities to Datahub work unit to ingest into Datahub
+                workunits = self.mapper.to_datahub_work_units(dashboard, workspace)
+                for workunit in workunits:
+                    # Return workunit to Datahub Ingestion framework
+                    yield workunit
 
-                if self.source_config.modified_since:
-                    # As modified_workspaces is not idempotent, hence we checkpoint for each powerbi workspace
-                    # Because job_id is used as dictionary key, we have to set a new job_id
-                    # Refer to https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/source/state/stateful_ingestion_base.py#L390
-                    self.stale_entity_removal_handler.set_job_id(workspace.id)
-                    self.register_stateful_ingestion_usecase_handler(
-                        self.stale_entity_removal_handler
-                    )
-
-                    yield from auto_stale_entity_removal(
-                        self.stale_entity_removal_handler,
-                        auto_workunit_reporter(
-                            self.reporter,
-                            auto_status_aspect(self.get_workspace_workunit(workspace)),
-                        ),
-                    )
-                else:
-                    # Maintain backward compatibility
-                    yield from self.get_workspace_workunit(workspace)
+            for report in workspace.reports:
+                for work_unit in self.mapper.report_to_datahub_work_units(
+                    report, workspace
+                ):
+                    yield work_unit
 
     def get_workunits(self) -> Iterable[MetadataWorkUnit]:
-        # As modified_workspaces is not idempotent, hence auto_stale_entity_removal is run later for each workspace_id
-        # This will result in creating checkpoint for each workspace_id
-        if self.source_config.modified_since:
-            return self.get_workunits_internal()
-        else:
-            # Since we only run for a fixed list of workspace_ids
-            # This will result in one checkpoint for the list of configured workspace_ids
-            return auto_stale_entity_removal(
-                self.stale_entity_removal_handler,
-                auto_workunit_reporter(
-                    self.reporter, auto_status_aspect(self.get_workunits_internal())
-                ),
-            )
+        return auto_stale_entity_removal(
+            self.stale_entity_removal_handler,
+            auto_workunit_reporter(
+                self.reporter, auto_status_aspect(self.get_workunits_internal())
+            ),
+        )
 
     def get_report(self) -> SourceReport:
         return self.reporter
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/rest_api_wrapper/data_resolver.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/data_resolver.py`

 * *Files 4% similar despite different names*

```diff
@@ -497,22 +497,21 @@
         Constant.TILE_LIST: "{POWERBI_ADMIN_BASE_URL}/dashboards/{DASHBOARD_ID}/tiles",
         Constant.REPORT_LIST: "{POWERBI_ADMIN_BASE_URL}/groups/{WORKSPACE_ID}/reports",
         Constant.SCAN_GET: "{POWERBI_ADMIN_BASE_URL}/workspaces/scanStatus/{SCAN_ID}",
         Constant.SCAN_RESULT_GET: "{POWERBI_ADMIN_BASE_URL}/workspaces/scanResult/{SCAN_ID}",
         Constant.SCAN_CREATE: "{POWERBI_ADMIN_BASE_URL}/workspaces/getInfo",
         Constant.ENTITY_USER_LIST: "{POWERBI_ADMIN_BASE_URL}/{ENTITY}/{ENTITY_ID}/users",
         Constant.DATASET_LIST: "{POWERBI_ADMIN_BASE_URL}/groups/{WORKSPACE_ID}/datasets",
-        Constant.WORKSPACE_MODIFIED_LIST: "{POWERBI_ADMIN_BASE_URL}/workspaces/modified",
     }
 
-    def create_scan_job(self, workspace_ids: List[str]) -> str:
+    def create_scan_job(self, workspace_id: str) -> str:
         """
         Create scan job on PowerBI for the workspace
         """
-        request_body = {"workspaces": workspace_ids}
+        request_body = {"workspaces": [workspace_id]}
 
         scan_create_endpoint = AdminAPIResolver.API_ENDPOINTS[Constant.SCAN_CREATE]
         scan_create_endpoint = scan_create_endpoint.format(
             POWERBI_ADMIN_BASE_URL=DataResolverBase.ADMIN_BASE_URL
         )
 
         logger.debug(
@@ -641,20 +640,14 @@
         users: List[User] = [
             User(
                 id=instance.get(Constant.IDENTIFIER),
                 displayName=instance.get(Constant.DISPLAY_NAME),
                 emailAddress=instance.get(Constant.EMAIL_ADDRESS),
                 graphId=instance.get(Constant.GRAPH_ID),
                 principalType=instance.get(Constant.PRINCIPAL_TYPE),
-                datasetUserAccessRight=instance.get(Constant.DATASET_USER_ACCESS_RIGHT),
-                reportUserAccessRight=instance.get(Constant.REPORT_USER_ACCESS_RIGHT),
-                dashboardUserAccessRight=instance.get(
-                    Constant.DASHBOARD_USER_ACCESS_RIGHT
-                ),
-                groupUserAccessRight=instance.get(Constant.GROUP_USER_ACCESS_RIGHT),
             )
             for instance in users_dict
         ]
 
         return users
 
     def get_scan_result(self, scan_id: str) -> Optional[dict]:
@@ -682,15 +675,15 @@
             or len(res.json().get("workspaces")) == 0
         ):
             logger.warning(
                 f"Scan result is not available for scan identifier = {scan_id}"
             )
             return None
 
-        return res.json()
+        return res.json()["workspaces"][0]
 
     def get_groups_endpoint(self) -> str:
         return f"{AdminAPIResolver.ADMIN_BASE_URL}/groups"
 
     def get_dashboards_endpoint(self, workspace: Workspace) -> str:
         dashboard_list_endpoint: str = self.API_ENDPOINTS[Constant.DASHBOARD_LIST]
         # Replace place holders
@@ -742,52 +735,12 @@
 
         raw_instance: dict = response_dict[Constant.VALUE][0]
         return new_powerbi_dataset(workspace_id, raw_instance)
 
     def _get_pages_by_report(self, workspace: Workspace, report_id: str) -> List[Page]:
         return []  # Report pages are not available in Admin API
 
-    def get_modified_workspaces(self, modified_since: str) -> List[str]:
-        """
-        Get list of modified workspaces
-        """
-        modified_workspaces_endpoint = self.API_ENDPOINTS[
-            Constant.WORKSPACE_MODIFIED_LIST
-        ].format(
-            POWERBI_ADMIN_BASE_URL=DataResolverBase.ADMIN_BASE_URL,
-        )
-        parameters: Dict[str, Any] = {
-            "excludePersonalWorkspaces": True,
-            "excludeInActiveWorkspaces": True,
-            "modifiedSince": modified_since,
-        }
-
-        res = self._request_session.get(
-            modified_workspaces_endpoint,
-            params=parameters,
-            headers=self.get_authorization_header(),
-        )
-        if res.status_code == 400:
-            error_msg_json = res.json()
-            if (
-                error_msg_json.get("error")
-                and error_msg_json["error"]["code"] == "InvalidRequest"
-            ):
-                raise ConfigurationError(
-                    "Please check if modified_since is within last 30 days."
-                )
-            else:
-                raise ConfigurationError(
-                    f"Please resolve the following error: {res.text}"
-                )
-        res.raise_for_status()
-
-        # Return scan_id of Scan created for the given workspace
-        workspace_ids = [row["id"] for row in res.json()]
-        logger.debug("modified workspace_ids: {}".format(workspace_ids))
-        return workspace_ids
-
     def get_dataset_parameters(
         self, workspace_id: str, dataset_id: str
     ) -> Dict[str, str]:
         logger.debug("Get dataset parameter is unsupported in Admin API")
         return {}
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi/rest_api_wrapper/powerbi_api.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/powerbi_api.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,25 +1,22 @@
 import json
 import logging
 import sys
-from typing import Any, Dict, Iterable, List, Optional, cast
+from typing import Any, Dict, List, Optional, cast
 
 import requests
 
 from datahub.ingestion.source.powerbi.config import (
     Constant,
     PowerBiDashboardSourceConfig,
     PowerBiDashboardSourceReport,
 )
 from datahub.ingestion.source.powerbi.rest_api_wrapper import data_resolver
 from datahub.ingestion.source.powerbi.rest_api_wrapper.data_classes import (
-    FIELD_TYPE_MAPPING,
-    Column,
     Dashboard,
-    Measure,
     PowerBIDataset,
     Report,
     Table,
     User,
     Workspace,
 )
 from datahub.ingestion.source.powerbi.rest_api_wrapper.data_resolver import (
@@ -182,18 +179,14 @@
 
         fill_ownership()
         fill_tags()
 
         return reports
 
     def get_workspaces(self) -> List[Workspace]:
-        if self.__config.modified_since:
-            workspaces = self.get_modified_workspaces()
-            return workspaces
-
         groups: List[dict] = []
         try:
             groups = self._get_resolver().get_groups()
         except:
             self.log_http_error(message="Unable to fetch list of workspaces")
 
         workspaces = [
@@ -207,45 +200,24 @@
                 dashboard_endorsements={},
                 scan_result={},
             )
             for workspace in groups
         ]
         return workspaces
 
-    def get_modified_workspaces(self) -> List[Workspace]:
-        workspaces = []
-        try:
-            modified_workspace_ids = self._get_resolver().get_modified_workspaces(
-                self.__config.modified_since
-            )
-            workspaces = [
-                Workspace(
-                    id=workspace_id,
-                    name="",
-                    datasets={},
-                    dashboards=[],
-                    reports=[],
-                    report_endorsements={},
-                    dashboard_endorsements={},
-                    scan_result={},
-                )
-                for workspace_id in modified_workspace_ids
-            ]
-        except:
-            self.log_http_error(message="Unable to fetch list of modified workspaces")
-        return workspaces
-
-    def _get_scan_result(self, workspace_ids: List[str]) -> Any:
+    def _get_scan_result(self, workspace: Workspace) -> Any:
         scan_id: Optional[str] = None
         try:
             scan_id = self.__admin_api_resolver.create_scan_job(
-                workspace_ids=workspace_ids
+                workspace_id=workspace.id
             )
         except:
-            e = self.log_http_error(message=f"Unable to fetch get scan result.")
+            e = self.log_http_error(
+                message=f"Unable to fetch dataset lineage for {workspace.name}({workspace.id})."
+            )
             if data_resolver.is_permission_error(cast(Exception, e)):
                 logger.warning(
                     "Dataset lineage can not be ingestion because this user does not have access to the PowerBI Admin "
                     "API. "
                 )
             return None
 
@@ -341,74 +313,37 @@
                     Table(
                         name=table[Constant.NAME],
                         full_name="{}.{}".format(
                             dataset_name.replace(" ", "_"),
                             table[Constant.NAME].replace(" ", "_"),
                         ),
                         expression=expression,
-                        columns=[
-                            Column(
-                                **column,
-                                datahubDataType=FIELD_TYPE_MAPPING.get(
-                                    column["dataType"], FIELD_TYPE_MAPPING["Null"]
-                                ),
-                            )
-                            for column in table.get("columns", [])
-                        ],
-                        measures=[
-                            Measure(**measure) for measure in table.get("measures", [])
-                        ],
                         dataset=dataset_instance,
                     )
                 )
 
         return dataset_map
 
-    def _fill_metadata_from_scan_result(
-        self, workspaces: List[Workspace]
-    ) -> List[Workspace]:
-        workspace_ids = [workspace.id for workspace in workspaces]
-        scan_result = self._get_scan_result(workspace_ids)
-        if not scan_result:
-            return workspaces
-
-        workspaces = []
-        for workspace_metadata in scan_result["workspaces"]:
-            cur_workspace = Workspace(
-                id=workspace_metadata["id"],
-                name=workspace_metadata["name"],
-                datasets={},
-                dashboards=[],
-                reports=[],
-                report_endorsements={},
-                dashboard_endorsements={},
-                scan_result={},
-            )
-            cur_workspace.scan_result = workspace_metadata
-            cur_workspace.datasets = self._get_workspace_datasets(
-                cur_workspace.scan_result
+    def _fill_metadata_from_scan_result(self, workspace: Workspace) -> None:
+        workspace.scan_result = self._get_scan_result(workspace)
+        workspace.datasets = self._get_workspace_datasets(workspace.scan_result)
+        # Fetch endorsements tag if it is enabled from configuration
+        if self.__config.extract_endorsements_to_tags is False:
+            logger.info(
+                "Skipping endorsements tag as extract_endorsements_to_tags is set to "
+                "false "
             )
+            return
 
-            # Fetch endorsements tag if it is enabled from configuration
-            if self.__config.extract_endorsements_to_tags:
-                cur_workspace.dashboard_endorsements = self._get_dashboard_endorsements(
-                    cur_workspace.scan_result
-                )
-                cur_workspace.report_endorsements = self._get_report_endorsements(
-                    cur_workspace.scan_result
-                )
-            else:
-                logger.info(
-                    "Skipping endorsements tag as extract_endorsements_to_tags is set to "
-                    "false "
-                )
-
-            workspaces.append(cur_workspace)
-
-        return workspaces
+        workspace.dashboard_endorsements = self._get_dashboard_endorsements(
+            workspace.scan_result
+        )
+        workspace.report_endorsements = self._get_report_endorsements(
+            workspace.scan_result
+        )
 
     def _fill_regular_metadata_detail(self, workspace: Workspace) -> None:
         def fill_dashboards() -> None:
             workspace.dashboards = self._get_resolver().get_dashboards(workspace)
             # set tiles of Dashboard
             for dashboard in workspace.dashboards:
                 dashboard.tiles = self._get_resolver().get_tiles(
@@ -428,22 +363,20 @@
                 logger.info(
                     "Skipping tag retrieval for dashboard as extract_endorsements_to_tags is set to false"
                 )
                 return
             for dashboard in workspace.dashboards:
                 dashboard.tags = workspace.dashboard_endorsements.get(dashboard.id, [])
 
-        if self.__config.extract_dashboards:
-            fill_dashboards()
-
+        fill_dashboards()
         fill_reports()
         fill_dashboard_tags()
 
     # flake8: noqa: C901
-    def fill_workspaces(
-        self, workspaces: List[Workspace], reporter: PowerBiDashboardSourceReport
-    ) -> Iterable[Workspace]:
-        workspaces = self._fill_metadata_from_scan_result(workspaces=workspaces)
-        # First try to fill the admin detail as some regular metadata contains lineage to admin metadata
-        for workspace in workspaces:
-            self._fill_regular_metadata_detail(workspace=workspace)
-        return workspaces
+    def fill_workspace(
+        self, workspace: Workspace, reporter: PowerBiDashboardSourceReport
+    ) -> None:
+        self._fill_metadata_from_scan_result(
+            workspace=workspace
+        )  # First try to fill the admin detail as some regular metadata contains lineage to admin metadata
+
+        self._fill_regular_metadata_detail(workspace=workspace)
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi_report_server/constants.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/constants.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi_report_server/report_server.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/report_server.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/powerbi_report_server/report_server_domain.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/report_server_domain.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/profiling/common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/profiling/common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/pulsar.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/pulsar.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redash.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redash.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/config.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,16 +3,16 @@
 
 from pydantic import root_validator
 from pydantic.fields import Field
 
 from datahub.configuration import ConfigModel
 from datahub.configuration.pydantic_field_deprecation import pydantic_field_deprecated
 from datahub.configuration.source_common import DatasetLineageProviderConfigBase
-from datahub.ingestion.source.data_lake_common.path_spec import PathSpec
-from datahub.ingestion.source.sql.postgres import BasePostgresConfig
+from datahub.ingestion.source.aws.path_spec import PathSpec
+from datahub.ingestion.source.sql.postgres import PostgresConfig
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulLineageConfigMixin,
     StatefulProfilingConfigMixin,
     StatefulUsageConfigMixin,
 )
 from datahub.ingestion.source.usage.usage_common import BaseUsageConfig
 
@@ -56,15 +56,15 @@
     email_domain: Optional[str] = Field(
         default=None,
         description="Email domain of your organisation so users can be displayed on UI appropriately.",
     )
 
 
 class RedshiftConfig(
-    BasePostgresConfig,
+    PostgresConfig,
     DatasetLineageProviderConfigBase,
     S3DatasetLineageProviderConfigBase,
     RedshiftUsageConfig,
     StatefulLineageConfigMixin,
     StatefulProfilingConfigMixin,
 ):
     database: str = Field(default="dev", description="database")
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/lineage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/lineage.py`

 * *Files 5% similar despite different names*

```diff
@@ -103,25 +103,15 @@
 
     def _get_sources_from_query(self, db_name: str, query: str) -> List[LineageDataset]:
         sources: List[LineageDataset] = list()
 
         parser = LineageRunner(query)
 
         for table in parser.source_tables:
-            split = str(table).split(".")
-            if len(split) == 3:
-                db_name, source_schema, source_table = split
-            elif len(split) == 2:
-                source_schema, source_table = split
-            else:
-                raise ValueError(
-                    f"Invalid table name {table} in query {query}. "
-                    f"Expected format: [db_name].[schema].[table] or [schema].[table] or [table]."
-                )
-
+            source_schema, source_table = str(table).split(".")
             if source_schema == "<default>":
                 source_schema = str(self.config.default_schema)
 
             source = LineageDataset(
                 platform=LineageDatasetPlatform.REDSHIFT,
                 path=f"{db_name}.{source_schema}.{source_table}",
             )
@@ -155,27 +145,30 @@
                 lineage_type.VIEW_DDL_SQL_PARSING,
             }
             and ddl is not None
         ):
             try:
                 sources = self._get_sources_from_query(db_name=db_name, query=ddl)
             except Exception as e:
-                logger.warning(
-                    f"Error parsing query {ddl} for getting lineage. Error was {e}."
+                self.warn(
+                    logger,
+                    "parsing-query",
+                    f"Error parsing query {ddl} for getting lineage ."
+                    f"\nError was {e}.",
                 )
-                self.report.num_lineage_dropped_query_parser += 1
         else:
             if lineage_type == lineage_type.COPY and filename is not None:
                 platform = LineageDatasetPlatform.S3
                 path = filename.strip()
                 if urlparse(path).scheme != "s3":
-                    logger.warning(
-                        "Only s3 source supported with copy. The source was: {path}."
+                    self.warn(
+                        logger,
+                        "non-s3-lineage",
+                        f"Only s3 source supported with copy. The source was: {path}.",
                     )
-                    self.report.num_lineage_dropped_not_support_copy_path += 1
                     return sources
                 path = strip_s3_prefix(self._get_s3_path(path))
             elif source_schema is not None and source_table is not None:
                 platform = LineageDatasetPlatform.REDSHIFT
                 path = f"{db_name}.{source_schema}.{source_table}"
             else:
                 return []
@@ -319,18 +312,19 @@
                 # Filtering out tables which does not exist in Redshift
                 # It was deleted in the meantime or query parser did not capture well the table name
                 if (
                     db not in all_tables
                     or schema not in all_tables[db]
                     or not any(table == t.name for t in all_tables[db][schema])
                 ):
-                    logger.debug(
-                        f"{source.path} missing table, dropping from lineage.",
+                    self.warn(
+                        logger,
+                        "missing-table",
+                        f"{source.path} missing table",
                     )
-                    self.report.num_lineage_tables_dropped += 1
                     continue
 
             targe_source.append(source)
         return targe_source
 
     def populate_lineage(
         self,
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/profile.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/profile.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import dataclasses
 import logging
 from datetime import datetime
 from typing import Dict, Iterable, List, Optional, Union, cast
 
 from datahub.emitter.mce_builder import make_dataset_urn_with_platform_instance
-from datahub.emitter.mcp import MetadataChangeProposalWrapper
+from datahub.emitter.mcp_builder import wrap_aspect_as_workunit
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.ge_data_profiler import GEProfilerRequest
 from datahub.ingestion.source.redshift.config import RedshiftConfig
 from datahub.ingestion.source.redshift.redshift_schema import (
     RedshiftTable,
     RedshiftView,
 )
@@ -45,27 +45,29 @@
     def get_workunits(
         self,
         tables: Union[
             Dict[str, Dict[str, List[RedshiftTable]]],
             Dict[str, Dict[str, List[RedshiftView]]],
         ],
     ) -> Iterable[MetadataWorkUnit]:
+
         # Extra default SQLAlchemy option for better connection pooling and threading.
         # https://docs.sqlalchemy.org/en/14/core/pooling.html#sqlalchemy.pool.QueuePool.params.max_overflow
         if self.config.profiling.enabled:
             self.config.options.setdefault(
                 "max_overflow", self.config.profiling.max_workers
             )
 
         for db in tables.keys():
             profile_requests = []
             for schema in tables.get(db, {}).keys():
                 if not self.config.schema_pattern.allowed(schema):
                     continue
                 for table in tables[db].get(schema, {}):
+
                     # Emit the profile work unit
                     profile_request = self.get_redshift_profile_request(
                         table, schema, db
                     )
                     if profile_request is not None:
                         profile_requests.append(profile_request)
 
@@ -94,17 +96,20 @@
 
                 # We don't add to the profiler state if we only do table level profiling as it always happens
                 if self.state_handler and not request.profile_table_level_only:
                     self.state_handler.add_to_state(
                         dataset_urn, int(datetime.now().timestamp() * 1000)
                     )
 
-                yield MetadataChangeProposalWrapper(
-                    entityUrn=dataset_urn, aspect=profile
-                ).as_workunit()
+                yield wrap_aspect_as_workunit(
+                    "dataset",
+                    dataset_urn,
+                    "datasetProfile",
+                    profile,
+                )
 
     def get_redshift_profile_request(
         self,
         table: Union[RedshiftTable, RedshiftView],
         schema_name: str,
         db_name: str,
     ) -> Optional[RedshiftProfilerRequest]:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/query.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/query.py`

 * *Files 3% similar despite different names*

```diff
@@ -6,42 +6,45 @@
 class RedshiftQuery:
     list_databases: str = """SELECT datname FROM pg_database
         WHERE (datname <> ('padb_harvest')::name)
         AND (datname <> ('template0')::name)
         AND (datname <> ('template1')::name)
         """
 
-    list_schemas: str = """SELECT distinct n.nspname AS "schema_name",
-        'local' as schema_type,
+    list_schemas: str = """SELECT database_name,
+        schema_name,
+        schema_type,
+        -- setting user_name to null as we don't use it now now and it breaks backward compatibility due to additional permission need
+        -- usename as schema_owner_name,
         null as schema_owner_name,
-        '' as schema_option,
-        null as external_database
-        FROM pg_catalog.pg_class c
-        LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
-        JOIN pg_catalog.pg_user u ON u.usesysid = c.relowner
-        WHERE c.relkind IN ('r','v','m','S','f')
-        AND   n.nspname !~ '^pg_'
-        AND   n.nspname != 'information_schema'
+        schema_option,
+        NULL::varchar(255) as external_database
+        FROM SVV_REDSHIFT_SCHEMAS as s
+        -- inner join pg_catalog.pg_user_info as i on i.usesysid = s.schema_owner
+        where schema_name !~ '^pg_'
+        AND   schema_name != 'information_schema'
 UNION ALL
-SELECT  schemaname as schema_name,
+SELECT null as database_name,
+        schemaname as schema_name,
         CASE s.eskind
             WHEN '1' THEN 'GLUE'
             WHEN '2' THEN 'HIVE'
             WHEN '3' THEN 'POSTGRES'
             WHEN '4' THEN 'REDSHIFT'
             ELSE 'OTHER'
         END as schema_type,
         -- setting user_name to null as we don't use it now now and it breaks backward compatibility due to additional permission need
         -- usename as schema_owner_name,
         null as schema_owner_name,
         esoptions as schema_option,
         databasename as external_database
         FROM SVV_EXTERNAL_SCHEMAS as s
         -- inner join pg_catalog.pg_user_info as i on i.usesysid = s.esowner
-        ORDER BY SCHEMA_NAME;
+        ORDER BY database_name,
+            SCHEMA_NAME;
         """
 
     list_tables: str = """
  SELECT  CASE c.relkind
                 WHEN 'r' THEN 'TABLE'
                 WHEN 'v' THEN 'VIEW'
                 WHEN 'm' THEN 'MATERIALIZED VIEW'
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/redshift.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/redshift.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 
 from datahub.emitter.mce_builder import (
     make_data_platform_urn,
     make_dataset_urn_with_platform_instance,
     make_tag_urn,
 )
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
+from datahub.emitter.mcp_builder import wrap_aspect_as_workunit
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
     config_class,
     platform_name,
@@ -68,15 +69,19 @@
 )
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StaleEntityRemovalHandler,
 )
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulIngestionSourceBase,
 )
-from datahub.metadata.com.linkedin.pegasus2avro.common import SubTypes, TimeStamp
+from datahub.metadata.com.linkedin.pegasus2avro.common import (
+    Status,
+    SubTypes,
+    TimeStamp,
+)
 from datahub.metadata.com.linkedin.pegasus2avro.dataset import (
     DatasetProperties,
     ViewProperties,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.schema import (
     ArrayType,
     BooleanType,
@@ -335,27 +340,23 @@
 
     @staticmethod
     def get_redshift_connection(
         config: RedshiftConfig,
     ) -> redshift_connector.Connection:
         client_options = config.extra_client_options
         host, port = config.host_port.split(":")
-        conn = redshift_connector.connect(
+        return redshift_connector.connect(
             host=host,
             port=int(port),
             user=config.username,
             database=config.database,
             password=config.password.get_secret_value() if config.password else None,
             **client_options,
         )
 
-        conn.autocommit = True
-
-        return conn
-
     def get_workunits(self) -> Iterable[MetadataWorkUnit]:
         return auto_stale_entity_removal(
             self.stale_entity_removal_handler,
             auto_workunit_reporter(
                 self.report,
                 auto_status_aspect(self.get_workunits_internal()),
             ),
@@ -494,14 +495,15 @@
                             + 1
                         )
 
             if self.config.include_views:
                 logger.info("process views")
                 if schema.name in self.db_views[schema.database]:
                     for view in self.db_views[schema.database][schema.name]:
+                        logger.info(f"View: {view}")
                         view.columns = schema_columns[schema.name].get(view.name, [])
                         yield from self._process_view(
                             table=view, database=database, schema=schema
                         )
 
                         self.report.view_processed[report_key] = (
                             self.report.view_processed.get(
@@ -609,17 +611,21 @@
         )
         if view.ddl:
             view_properties_aspect = ViewProperties(
                 materialized=view.type == "VIEW_MATERIALIZED",
                 viewLanguage="SQL",
                 viewLogic=view.ddl,
             )
-            yield MetadataChangeProposalWrapper(
-                entityUrn=dataset_urn, aspect=view_properties_aspect
-            ).as_workunit()
+            wu = wrap_aspect_as_workunit(
+                "dataset",
+                dataset_urn,
+                "viewProperties",
+                view_properties_aspect,
+            )
+            yield wu
 
     # TODO: Remove to common?
     def gen_schema_fields(self, columns: List[RedshiftColumn]) -> List[SchemaField]:
         schema_fields: List[SchemaField] = []
 
         for col in columns:
             tags: List[TagAssociationClass] = []
@@ -662,17 +668,18 @@
             schemaName=dataset_name,
             platform=make_data_platform_urn(self.platform),
             version=0,
             hash="",
             platformSchema=MySqlDDL(tableSchema=""),
             fields=self.gen_schema_fields(table.columns),
         )
-        yield MetadataChangeProposalWrapper(
-            entityUrn=dataset_urn, aspect=schema_metadata
-        ).as_workunit()
+        wu = wrap_aspect_as_workunit(
+            "dataset", dataset_urn, "schemaMetadata", schema_metadata
+        )
+        yield wu
 
     # TODO: Move to common
     def gen_dataset_workunits(
         self,
         table: Union[RedshiftTable, RedshiftView],
         database: str,
         schema: str,
@@ -682,14 +689,17 @@
     ) -> Iterable[MetadataWorkUnit]:
         datahub_dataset_name = f"{database}.{schema}.{table.name}"
         dataset_urn = make_dataset_urn_with_platform_instance(
             platform=self.platform,
             name=datahub_dataset_name,
             platform_instance=self.config.platform_instance,
         )
+        status = Status(removed=False)
+        wu = wrap_aspect_as_workunit("dataset", dataset_urn, "status", status)
+        yield wu
 
         yield from self.gen_schema_metadata(
             dataset_urn, table, str(datahub_dataset_name)
         )
 
         dataset_properties = DatasetProperties(
             name=table.name,
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/redshift_schema.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/redshift_schema.py`

 * *Files 0% similar despite different names*

```diff
@@ -159,15 +159,15 @@
     @staticmethod
     def get_tables_and_views(
         conn: redshift_connector.Connection,
     ) -> Tuple[Dict[str, List[RedshiftTable]], Dict[str, List[RedshiftView]]]:
         tables: Dict[str, List[RedshiftTable]] = {}
         views: Dict[str, List[RedshiftView]] = {}
 
-        # This query needs to run separately as we can't join with the main query because it works with
+        # This query needs to run separately as we can't join witht the main query because it works with
         # driver only functions.
         enriched_table = RedshiftDataDictionary.enrich_tables(conn)
 
         cur = RedshiftDataDictionary.get_query_result(conn, RedshiftQuery.list_tables)
         field_names = [i[0] for i in cur.description]
         db_tables = cur.fetchall()
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/report.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/report.py`

 * *Files 22% similar despite different names*

```diff
@@ -20,13 +20,10 @@
         default_factory=TopKDict
     )
     lineage_mem_size: Dict[str, str] = field(default_factory=TopKDict)
     tables_in_mem_size: Dict[str, str] = field(default_factory=TopKDict)
     views_in_mem_size: Dict[str, str] = field(default_factory=TopKDict)
     num_operational_stats_skipped: int = 0
     num_usage_stat_skipped: int = 0
-    num_lineage_tables_dropped: int = 0
-    num_lineage_dropped_query_parser: int = 0
-    num_lineage_dropped_not_support_copy_path: int = 0
 
     def report_dropped(self, key: str) -> None:
         self.filtered.append(key)
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/redshift/usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/s3/config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/config.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,28 +1,32 @@
 import logging
 from typing import Any, Dict, List, Optional, Union
 
 import pydantic
 from pydantic.fields import Field
 
 from datahub.configuration.common import AllowDenyPattern
-from datahub.configuration.source_common import DatasetSourceConfigMixin
+from datahub.configuration.source_common import (
+    EnvConfigMixin,
+    PlatformInstanceConfigMixin,
+)
 from datahub.configuration.validate_field_rename import pydantic_renamed_field
 from datahub.ingestion.source.aws.aws_common import AwsConnectionConfig
-from datahub.ingestion.source.data_lake_common.config import PathSpecsConfigMixin
-from datahub.ingestion.source.data_lake_common.path_spec import PathSpec
+from datahub.ingestion.source.aws.path_spec import PathSpec
 from datahub.ingestion.source.s3.profiling import DataLakeProfilerConfig
 
 # hide annoying debug errors from py4j
 logging.getLogger("py4j").setLevel(logging.ERROR)
 logger: logging.Logger = logging.getLogger(__name__)
 
 
-class DataLakeSourceConfig(DatasetSourceConfigMixin, PathSpecsConfigMixin):
-
+class DataLakeSourceConfig(PlatformInstanceConfigMixin, EnvConfigMixin):
+    path_specs: List[PathSpec] = Field(
+        description="List of PathSpec. See [below](#path-spec) the details about PathSpec"
+    )
     platform: str = Field(
         default="",
         description="The platform that this source connects to (either 's3' or 'file'). "
         "If not specified, the platform will be inferred from the path_specs.",
     )
     aws_config: Optional[AwsConnectionConfig] = Field(
         default=None, description="AWS configuration"
@@ -31,15 +35,15 @@
     # Whether or not to create in datahub from the s3 bucket
     use_s3_bucket_tags: Optional[bool] = Field(
         None, description="Whether or not to create tags in datahub from the s3 bucket"
     )
     # Whether or not to create in datahub from the s3 object
     use_s3_object_tags: Optional[bool] = Field(
         None,
-        description="Whether or not to create tags in datahub from the s3 object",
+        description="# Whether or not to create tags in datahub from the s3 object",
     )
 
     # Whether to update the table schema when schema in files within the partitions are updated
     update_schema_on_partition_file_updates: Optional[bool] = Field(
         default=False,
         description="Whether to update the table schema when schema in files within the partitions are updated.",
     )
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/s3/profiling.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/profiling.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/s3/source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/source.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,14 @@
 import pathlib
 import re
 from collections import OrderedDict
 from datetime import datetime
 from typing import Any, Dict, Iterable, List, Optional, Tuple
 
 import pydeequ
-from more_itertools import peekable
 from pydeequ.analyzers import AnalyzerContext
 from pyspark.conf import SparkConf
 from pyspark.sql import SparkSession
 from pyspark.sql.dataframe import DataFrame
 from pyspark.sql.types import (
     ArrayType,
     BinaryType,
@@ -31,14 +30,15 @@
     StructField,
     StructType,
     TimestampType,
 )
 from pyspark.sql.utils import AnalysisException
 from smart_open import open as smart_open
 
+import datahub.ingestion.source.s3.config
 from datahub.emitter.mce_builder import (
     make_data_platform_urn,
     make_dataset_urn_with_platform_instance,
 )
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
@@ -54,16 +54,16 @@
 from datahub.ingestion.source.aws.s3_boto_utils import get_s3_tags, list_folders
 from datahub.ingestion.source.aws.s3_util import (
     get_bucket_name,
     get_bucket_relative_path,
     get_key_prefix,
     strip_s3_prefix,
 )
-from datahub.ingestion.source.data_lake_common.data_lake_utils import ContainerWUCreator
 from datahub.ingestion.source.s3.config import DataLakeSourceConfig, PathSpec
+from datahub.ingestion.source.s3.data_lake_utils import ContainerWUCreator
 from datahub.ingestion.source.s3.profiling import _SingleTableProfiler
 from datahub.ingestion.source.s3.report import DataLakeSourceReport
 from datahub.ingestion.source.schema_inference import avro, csv_tsv, json, parquet
 from datahub.metadata.com.linkedin.pegasus2avro.common import Status
 from datahub.metadata.com.linkedin.pegasus2avro.metadata.snapshot import DatasetSnapshot
 from datahub.metadata.com.linkedin.pegasus2avro.mxe import MetadataChangeEvent
 from datahub.metadata.com.linkedin.pegasus2avro.schema import (
@@ -207,17 +207,14 @@
 
     Schemas for Parquet and Avro files are extracted as provided.
 
     Schemas for schemaless formats (CSV, TSV, JSON) are inferred. For CSV and TSV files, we consider the first 100 rows by default, which can be controlled via the `max_rows` recipe parameter (see [below](#config-details))
     JSON file schemas are inferred on the basis of the entire file (given the difficulty in extracting only the first few objects of the file), which may impact performance.
     We are working on using iterator-based JSON parsers to avoid reading in the entire JSON object.
 
-    To ingest datasets from your data lake, you need to provide the dataset path format specifications using `path_specs` configuration in ingestion recipe.
-    Refer section [Path Specs](https://datahubproject.io/docs/generated/ingestion/sources/s3/#path-specs) for examples.
-
     Note that because the profiling is run with PySpark, we require Spark 3.0.3 with Hadoop 3.2 to be installed (see [compatibility](#compatibility) for more details). If profiling, make sure that permissions for **s3a://** access are set because Spark and Hadoop use the s3a:// protocol to interface with AWS (schema inference outside of profiling requires s3:// access).
     Enabling profiling will slow down ingestion runs.
     """
 
     source_config: DataLakeSourceConfig
     report: DataLakeSourceReport
     profiling_times_taken: List[float]
@@ -376,15 +373,15 @@
             return None
         logger.debug(f"dataframe read for file {file} with row count {df.count()}")
         # replace periods in names because they break PyDeequ
         # see https://mungingdata.com/pyspark/avoid-dots-periods-column-names/
         return df.toDF(*(c.replace(".", "_") for c in df.columns))
 
     def get_fields(self, table_data: TableData, path_spec: PathSpec) -> List:
-        if self.is_s3_platform():
+        if table_data.is_s3:
             if self.source_config.aws_config is None:
                 raise ValueError("AWS config is required for S3 file sources")
 
             s3_client = self.source_config.aws_config.get_s3_client(
                 self.source_config.verify_ssl
             )
 
@@ -393,19 +390,18 @@
             )
         else:
             file = open(table_data.full_path, "rb")
 
         fields = []
 
         extension = pathlib.Path(table_data.full_path).suffix
-        from datahub.ingestion.source.data_lake_common.path_spec import (
-            SUPPORTED_COMPRESSIONS,
-        )
-
-        if path_spec.enable_compression and (extension[1:] in SUPPORTED_COMPRESSIONS):
+        if path_spec.enable_compression and (
+            extension[1:]
+            in datahub.ingestion.source.aws.path_spec.SUPPORTED_COMPRESSIONS
+        ):
             # Removing the compression extension and using the one before that like .json.gz -> .json
             extension = pathlib.Path(table_data.full_path).with_suffix("").suffix
         if extension == "" and path_spec.default_extension:
             extension = f".{path_spec.default_extension}"
 
         try:
             if extension == ".parquet":
@@ -519,15 +515,15 @@
 
     def ingest_table(
         self, table_data: TableData, path_spec: PathSpec
     ) -> Iterable[MetadataWorkUnit]:
         logger.info(f"Extracting table schema from file: {table_data.full_path}")
         browse_path: str = (
             strip_s3_prefix(table_data.table_path)
-            if self.is_s3_platform()
+            if table_data.is_s3
             else table_data.table_path.strip("/")
         )
 
         data_platform_urn = make_data_platform_urn(self.source_config.platform)
         logger.info(f"Creating dataset urn with name: {browse_path}")
         dataset_urn = make_dataset_urn_with_platform_instance(
             self.source_config.platform,
@@ -543,15 +539,15 @@
 
         customProperties: Optional[Dict[str, str]] = None
         if not path_spec.sample_files:
             customProperties = {
                 "number_of_files": str(table_data.number_of_files),
                 "size_in_bytes": str(table_data.size_in_bytes),
             }
-            if self.is_s3_platform():
+            if table_data.is_s3:
                 customProperties["table_path"] = str(table_data.table_path)
 
         dataset_properties = DatasetPropertiesClass(
             description="",
             name=table_data.display_name,
             customProperties=customProperties,
         )
@@ -592,15 +588,15 @@
 
         mce = MetadataChangeEvent(proposedSnapshot=dataset_snapshot)
         wu = MetadataWorkUnit(id=table_data.table_path, mce=mce)
         self.report.report_workunit(wu)
         yield wu
 
         container_wus = self.container_WU_creator.create_container_hierarchy(
-            table_data.table_path, dataset_urn
+            table_data.table_path, table_data.is_s3, dataset_urn
         )
         for wu in container_wus:
             self.report.report_workunit(wu)
             yield wu
 
         if self.source_config.profiling.enabled:
             yield from self.get_table_profile(table_data, dataset_urn)
@@ -621,15 +617,15 @@
         self, path_spec: PathSpec, path: str, timestamp: datetime, size: int
     ) -> TableData:
         logger.debug(f"Getting table data for path: {path}")
         table_name, table_path = path_spec.extract_table_name_and_path(path)
         table_data = None
         table_data = TableData(
             display_name=table_name,
-            is_s3=self.is_s3_platform(),
+            is_s3=path_spec.is_s3,
             full_path=path,
             partitions=None,
             timestamp=timestamp,
             table_path=table_path,
             number_of_files=1,
             size_in_bytes=size,
         )
@@ -646,28 +642,14 @@
             bucket_name, folder_split[0], self.source_config.aws_config
         )
         for folder in folders:
             yield from self.resolve_templated_folders(
                 bucket_name, f"{folder}{folder_split[1]}"
             )
 
-    def get_dir_to_process(self, bucket_name: str, folder: str) -> str:
-        iterator = list_folders(
-            bucket_name=bucket_name,
-            prefix=folder,
-            aws_config=self.source_config.aws_config,
-        )
-        iterator = peekable(iterator)
-        if iterator:
-            for item in iterator:
-                last = item
-            return self.get_dir_to_process(bucket_name=bucket_name, folder=last + "/")
-        else:
-            return folder
-
     def s3_browser(self, path_spec: PathSpec) -> Iterable[Tuple[str, datetime, int]]:
         if self.source_config.aws_config is None:
             raise ValueError("aws_config not set. Cannot browse s3")
         s3 = self.source_config.aws_config.get_s3_resource(
             self.source_config.verify_ssl
         )
         bucket_name = get_bucket_name(path_spec.include)
@@ -701,40 +683,33 @@
             for folder in self.resolve_templated_folders(
                 bucket_name, get_bucket_relative_path(include[:table_index])
             ):
                 for f in list_folders(
                     bucket_name, f"{folder}", self.source_config.aws_config
                 ):
                     logger.info(f"Processing folder: {f}")
-                    dir_to_process = self.get_dir_to_process(
-                        bucket_name=bucket_name, folder=f
-                    )
-                    logger.info(f"Getting files from folder: {dir_to_process}")
-                    dir_to_process = dir_to_process.rstrip("\\")
+
                     for obj in (
-                        bucket.objects.filter(Prefix=f"{dir_to_process}")
+                        bucket.objects.filter(Prefix=f"{f}")
                         .page_size(PAGE_SIZE)
                         .limit(SAMPLE_SIZE)
                     ):
-                        s3_path = self.create_s3_path(obj.bucket_name, obj.key)
-                        logger.debug(f"Sampling file: {s3_path}")
+                        s3_path = f"s3://{obj.bucket_name}/{obj.key}"
+                        logger.debug(f"Samping file: {s3_path}")
                         yield s3_path, obj.last_modified, obj.size,
         else:
             logger.debug(
                 "No template in the pathspec can't do sampling, fallbacking to do full scan"
             )
             path_spec.sample_files = False
             for obj in bucket.objects.filter(Prefix=prefix).page_size(PAGE_SIZE):
-                s3_path = self.create_s3_path(obj.bucket_name, obj.key)
+                s3_path = f"s3://{obj.bucket_name}/{obj.key}"
                 logger.debug(f"Path: {s3_path}")
                 yield s3_path, obj.last_modified, obj.size,
 
-    def create_s3_path(self, bucket_name: str, key: str) -> str:
-        return f"s3://{bucket_name}/{key}"
-
     def local_browser(self, path_spec: PathSpec) -> Iterable[Tuple[str, datetime, int]]:
         prefix = self.get_prefix(path_spec.include)
         if os.path.isfile(prefix):
             logger.debug(f"Scanning single local file: {prefix}")
             yield prefix, datetime.utcfromtimestamp(
                 os.path.getmtime(prefix)
             ), os.path.getsize(prefix)
@@ -754,15 +729,15 @@
             self.source_config.env,
         )
         with PerfTimer() as timer:
             assert self.source_config.path_specs
             for path_spec in self.source_config.path_specs:
                 file_browser = (
                     self.s3_browser(path_spec)
-                    if self.is_s3_platform()
+                    if self.source_config.platform == "s3"
                     else self.local_browser(path_spec)
                 )
                 table_dict: Dict[str, TableData] = {}
                 for file, timestamp, size in file_browser:
                     if not path_spec.allowed(file):
                         continue
                     table_data = self.extract_table_data(
@@ -834,12 +809,9 @@
                     "total_time_taken": stats.discretize(total_time_taken),
                     "count": stats.discretize(len(self.profiling_times_taken)),
                     "platform": self.source_config.platform,
                     **time_percentiles,
                 },
             )
 
-    def is_s3_platform(self):
-        return self.source_config.platform == "s3"
-
     def get_report(self):
         return self.report
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/salesforce.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/salesforce.py`

 * *Files 2% similar despite different names*

```diff
@@ -57,15 +57,14 @@
 
 logger = logging.getLogger(__name__)
 
 
 class SalesforceAuthType(Enum):
     USERNAME_PASSWORD = "USERNAME_PASSWORD"
     DIRECT_ACCESS_TOKEN = "DIRECT_ACCESS_TOKEN"
-    JSON_WEB_TOKEN = "JSON_WEB_TOKEN"
 
 
 class SalesforceProfilingConfig(ConfigModel):
     enabled: bool = Field(
         default=False,
         description="Whether profiling should be done. Supports only table-level profiling at this stage",
     )
@@ -77,20 +76,14 @@
     platform = "salesforce"
 
     auth: SalesforceAuthType = SalesforceAuthType.USERNAME_PASSWORD
 
     # Username, Password Auth
     username: Optional[str] = Field(description="Salesforce username")
     password: Optional[str] = Field(description="Password for Salesforce user")
-    consumer_key: Optional[str] = Field(
-        description="Consumer key for Salesforce JSON web token access"
-    )
-    private_key: Optional[str] = Field(
-        description="Private key as a string for Salesforce JSON web token access"
-    )
     security_token: Optional[str] = Field(
         description="Security token for Salesforce username"
     )
     # client_id, client_secret not required
 
     # Direct - Instance URL, Access Token Auth
     instance_url: Optional[str] = Field(
@@ -233,34 +226,14 @@
                     username=self.config.username,
                     password=self.config.password,
                     security_token=self.config.security_token,
                     session=self.session,
                     domain="test" if self.config.is_sandbox else None,
                 )
 
-            elif self.config.auth is SalesforceAuthType.JSON_WEB_TOKEN:
-                logger.debug("Json Web Token provided in the config")
-                assert (
-                    self.config.username is not None
-                ), "Config username is required for JSON_WEB_TOKEN auth"
-                assert (
-                    self.config.consumer_key is not None
-                ), "Config consumer_key is required for JSON_WEB_TOKEN auth"
-                assert (
-                    self.config.private_key is not None
-                ), "Config private_key is required for JSON_WEB_TOKEN auth"
-
-                self.sf = Salesforce(
-                    username=self.config.username,
-                    consumer_key=self.config.consumer_key,
-                    privatekey=self.config.private_key,
-                    session=self.session,
-                    domain="test" if self.config.is_sandbox else None,
-                )
-
         except Exception as e:
             logger.error(e)
             raise ConfigurationError("Salesforce login failed") from e
         else:
             # List all REST API versions and use latest one
             versions_url = "https://{instance}/services/data/".format(
                 instance=self.sf.sf_instance,
@@ -390,22 +363,20 @@
         return MetadataChangeProposalWrapper(
             entityUrn=datasetUrn, aspect=dataPlatformInstance
         ).as_workunit()
 
     def get_operation_workunit(
         self, customObject: dict, datasetUrn: str
     ) -> Iterable[WorkUnit]:
-        reported_time: int = int(time.time() * 1000)
-
         if customObject.get("CreatedBy") and customObject.get("CreatedDate"):
             timestamp = self.get_time_from_salesforce_timestamp(
                 customObject["CreatedDate"]
             )
             operation = OperationClass(
-                timestampMillis=reported_time,
+                timestampMillis=timestamp,
                 operationType=OperationTypeClass.CREATE,
                 lastUpdatedTimestamp=timestamp,
                 actor=builder.make_user_urn(customObject["CreatedBy"]["Username"]),
             )
 
             yield MetadataChangeProposalWrapper(
                 entityUrn=datasetUrn, aspect=operation
@@ -418,15 +389,15 @@
             if customObject.get("LastModifiedBy") and customObject.get(
                 "LastModifiedDate"
             ):
                 timestamp = self.get_time_from_salesforce_timestamp(
                     customObject["LastModifiedDate"]
                 )
                 operation = OperationClass(
-                    timestampMillis=reported_time,
+                    timestampMillis=timestamp,
                     operationType=OperationTypeClass.ALTER,
                     lastUpdatedTimestamp=timestamp,
                     actor=builder.make_user_urn(
                         customObject["LastModifiedBy"]["Username"]
                     ),
                 )
                 yield MetadataChangeProposalWrapper(
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema/json_schema.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema/json_schema.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/avro.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/avro.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/csv_tsv.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/csv_tsv.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/json.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/json.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/object.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/object.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/schema_inference/parquet.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/parquet.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/constants.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/constants.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_config.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import logging
 from enum import Enum
-from typing import Dict, List, Optional, cast
+from typing import Dict, Optional, cast
 
 from pydantic import Field, SecretStr, root_validator, validator
 
 from datahub.configuration.common import AllowDenyPattern
-from datahub.configuration.pattern_utils import UUID_REGEX
 from datahub.configuration.validate_field_removal import pydantic_removed_field
 from datahub.configuration.validate_field_rename import pydantic_renamed_field
 from datahub.ingestion.glossary.classifier import ClassificationConfig
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulProfilingConfigMixin,
     StatefulUsageConfigMixin,
 )
@@ -17,26 +16,14 @@
     BaseSnowflakeConfig,
     SnowflakeConfig,
 )
 from datahub.ingestion.source_config.usage.snowflake_usage import SnowflakeUsageConfig
 
 logger = logging.Logger(__name__)
 
-# FIVETRAN creates temporary tables in schema named FIVETRAN_xxx_STAGING.
-# Ref - https://support.fivetran.com/hc/en-us/articles/1500003507122-Why-Is-There-an-Empty-Schema-Named-Fivetran-staging-in-the-Destination-
-#
-# DBT incremental models create temporary tables ending with __dbt_tmp
-# Ref - https://discourse.getdbt.com/t/handling-bigquery-incremental-dbt-tmp-tables/7540
-DEFAULT_UPSTREAMS_DENY_LIST = [
-    r".*\.FIVETRAN_.*_STAGING\..*",  # fivetran
-    r".*__DBT_TMP$",  # dbt
-    rf".*\.SEGMENT_{UUID_REGEX}",  # segment
-    rf".*\.STAGING_.*_{UUID_REGEX}",  # stitch
-]
-
 
 class TagOption(str, Enum):
     with_lineage = "with_lineage"
     without_lineage = "without_lineage"
     skip = "skip"
 
 
@@ -93,37 +80,27 @@
     )
 
     use_legacy_lineage_method: bool = Field(
         default=True,
         description="Whether to use the legacy lineage computation method. If set to False, ingestion uses new optimised lineage extraction method that requires less ingestion process memory.",
     )
 
-    validate_upstreams_against_patterns: bool = Field(
-        default=True,
-        description="Whether to validate upstream snowflake tables against allow-deny patterns",
-    )
-
-    tag_pattern: AllowDenyPattern = Field(
-        default=AllowDenyPattern.allow_all(),
-        description="List of regex patterns for tags to include in ingestion. Only used if `extract_tags` is enabled.",
-    )
-
-    upstreams_deny_pattern: List[str] = Field(
-        default=DEFAULT_UPSTREAMS_DENY_LIST,
-        description="[Advanced] Regex patterns for upstream tables to filter in ingestion. Specify regex to match the entire table name in database.schema.table format. Defaults are to set in such a way to ignore the temporary staging tables created by known ETL tools. Not used if `use_legacy_lineage_method=True`",
-    )
-
     @validator("include_column_lineage")
     def validate_include_column_lineage(cls, v, values):
         if not values.get("include_table_lineage") and v:
             raise ValueError(
                 "include_table_lineage must be True for include_column_lineage to be set."
             )
         return v
 
+    tag_pattern: AllowDenyPattern = Field(
+        default=AllowDenyPattern.allow_all(),
+        description="List of regex patterns for tags to include in ingestion. Only used if `extract_tags` is enabled.",
+    )
+
     @root_validator(pre=False)
     def validate_unsupported_configs(cls, values: Dict) -> Dict:
         value = values.get("include_read_operational_stats")
         if value is not None and value:
             raise ValueError(
                 "include_read_operational_stats is not supported. Set `include_read_operational_stats` to False.",
             )
@@ -154,22 +131,23 @@
             values.get("profiling") is not None and values["profiling"].enabled
         )
         delete_detection_enabled = (
             values.get("stateful_ingestion") is not None
             and values["stateful_ingestion"].enabled
             and values["stateful_ingestion"].remove_stale_metadata
         )
+        include_table_lineage = values.get("include_table_lineage")
 
         # TODO: Allow lineage extraction and profiling irrespective of basic schema extraction,
-        # as it seems possible with some refactor
+        # as it seems possible with some refractor
         if not include_technical_schema and any(
-            [include_profiles, delete_detection_enabled]
+            [include_profiles, delete_detection_enabled, include_table_lineage]
         ):
             raise ValueError(
-                "Cannot perform Deletion Detection or Profiling without extracting snowflake technical schema. Set `include_technical_schema` to True or disable Deletion Detection and Profiling."
+                "Can not perform Deletion Detection, Lineage Extraction, Profiling without extracting snowflake technical schema.  Set `include_technical_schema` to True or disable Deletion Detection, Lineage Extraction, Profiling."
             )
 
         return values
 
     def get_sql_alchemy_url(
         self,
         database: Optional[str] = None,
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_lineage_legacy.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_lineage_legacy.py`

 * *Files 2% similar despite different names*

```diff
@@ -224,40 +224,44 @@
             with PerfTimer() as timer:
                 self._populate_lineage()
                 self.report.table_lineage_query_secs = timer.elapsed_seconds()
 
     def get_table_upstream_workunits(self, discovered_tables):
         if self.config.include_table_lineage:
             for dataset_name in discovered_tables:
-                dataset_urn = builder.make_dataset_urn_with_platform_instance(
-                    self.platform,
-                    dataset_name,
-                    self.config.platform_instance,
-                    self.config.env,
-                )
-                upstream_lineage = self._get_upstream_lineage_info(dataset_name)
-                if upstream_lineage is not None:
-                    yield MetadataChangeProposalWrapper(
-                        entityUrn=dataset_urn, aspect=upstream_lineage
-                    ).as_workunit()
+                if self._is_dataset_pattern_allowed(
+                    dataset_name, SnowflakeObjectDomain.TABLE
+                ):
+                    dataset_urn = builder.make_dataset_urn_with_platform_instance(
+                        self.platform,
+                        dataset_name,
+                        self.config.platform_instance,
+                        self.config.env,
+                    )
+                    upstream_lineage = self._get_upstream_lineage_info(dataset_name)
+                    if upstream_lineage is not None:
+                        yield MetadataChangeProposalWrapper(
+                            entityUrn=dataset_urn, aspect=upstream_lineage
+                        ).as_workunit()
 
     def get_view_upstream_workunits(self, discovered_views):
         if self.config.include_view_lineage:
             for view_name in discovered_views:
-                dataset_urn = builder.make_dataset_urn_with_platform_instance(
-                    self.platform,
-                    view_name,
-                    self.config.platform_instance,
-                    self.config.env,
-                )
-                upstream_lineage = self._get_upstream_lineage_info(view_name)
-                if upstream_lineage is not None:
-                    yield MetadataChangeProposalWrapper(
-                        entityUrn=dataset_urn, aspect=upstream_lineage
-                    ).as_workunit()
+                if self._is_dataset_pattern_allowed(view_name, "view"):
+                    dataset_urn = builder.make_dataset_urn_with_platform_instance(
+                        self.platform,
+                        view_name,
+                        self.config.platform_instance,
+                        self.config.env,
+                    )
+                    upstream_lineage = self._get_upstream_lineage_info(view_name)
+                    if upstream_lineage is not None:
+                        yield MetadataChangeProposalWrapper(
+                            entityUrn=dataset_urn, aspect=upstream_lineage
+                        ).as_workunit()
 
     def _get_upstream_lineage_info(
         self, dataset_name: str
     ) -> Optional[UpstreamLineage]:
         lineage = self._lineage_map[dataset_name]
         external_lineage = self._external_lineage_map[dataset_name]
         if not (lineage.upstreamTables or lineage.columnLineages or external_lineage):
@@ -438,15 +442,15 @@
         upstream_table_name = self.get_dataset_identifier_from_qualified_name(
             db_row["UPSTREAM_TABLE_NAME"]
         )
         if not self._is_dataset_pattern_allowed(
             key, SnowflakeObjectDomain.TABLE
         ) or not (
             self._is_dataset_pattern_allowed(
-                upstream_table_name, SnowflakeObjectDomain.TABLE, is_upstream=True
+                upstream_table_name, SnowflakeObjectDomain.TABLE
             )
         ):
             return
         self._lineage_map[key].update_lineage(
             # (<upstream_table_name>, <json_list_of_upstream_columns>, <json_list_of_downstream_columns>)
             SnowflakeUpstreamTable.from_dict(
                 upstream_table_name,
@@ -492,15 +496,15 @@
             db_row["DOWNSTREAM_VIEW"]
         )
 
         if not self._is_dataset_pattern_allowed(
             dataset_name=view_name,
             dataset_type=db_row["REFERENCING_OBJECT_DOMAIN"],
         ) or not self._is_dataset_pattern_allowed(
-            view_upstream, db_row["REFERENCED_OBJECT_DOMAIN"], is_upstream=True
+            view_upstream, db_row["REFERENCED_OBJECT_DOMAIN"]
         ):
             return
             # key is the downstream view name
         self._lineage_map[view_name].update_lineage(
             # (<upstream_table_name>, <empty_json_list_of_upstream_table_columns>, <empty_json_list_of_downstream_view_columns>)
             SnowflakeUpstreamTable.from_dict(view_upstream, None, None),
             self.config.include_column_lineage,
@@ -546,15 +550,15 @@
         view_name: str = self.get_dataset_identifier_from_qualified_name(
             db_row["VIEW_NAME"]
         )
         downstream_table: str = self.get_dataset_identifier_from_qualified_name(
             db_row["DOWNSTREAM_TABLE_NAME"]
         )
         if not self._is_dataset_pattern_allowed(
-            view_name, db_row["VIEW_DOMAIN"], is_upstream=True
+            view_name, db_row["VIEW_DOMAIN"]
         ) or not self._is_dataset_pattern_allowed(
             downstream_table, db_row["DOWNSTREAM_TABLE_DOMAIN"]
         ):
             return
 
             # Capture view->downstream table lineage.
         self._lineage_map[downstream_table].update_lineage(
@@ -647,15 +651,15 @@
     ) -> List[str]:
         column_upstreams = []
         for upstream_col in fine_upstream.inputColumns:
             if (
                 upstream_col.objectName
                 and upstream_col.columnName
                 and self._is_dataset_pattern_allowed(
-                    upstream_col.objectName, upstream_col.objectDomain, is_upstream=True
+                    upstream_col.objectName, upstream_col.objectDomain
                 )
             ):
                 upstream_dataset_name = self.get_dataset_identifier_from_qualified_name(
                     upstream_col.objectName
                 )
                 upstream_dataset_urn = builder.make_dataset_urn_with_platform_instance(
                     self.platform,
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_lineage_v2.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_lineage_v2.py`

 * *Files 0% similar despite different names*

```diff
@@ -336,15 +336,14 @@
 
     def _fetch_upstream_lineages_for_tables(self):
         query: str = SnowflakeQuery.table_to_table_lineage_history_v2(
             start_time_millis=int(self.config.start_time.timestamp() * 1000)
             if not self.config.ignore_start_time_lineage
             else 0,
             end_time_millis=int(self.config.end_time.timestamp() * 1000),
-            upstreams_deny_pattern=self.config.upstreams_deny_pattern,
             include_view_lineage=self.config.include_view_lineage,
             include_column_lineage=self.config.include_column_lineage,
         )
         try:
             for db_row in self.query(query):
                 yield db_row
         except Exception as e:
@@ -371,15 +370,15 @@
         return upstreams
 
     def _process_add_single_upstream(self, upstreams, upstream_table):
         upstream_name = self.get_dataset_identifier_from_qualified_name(
             upstream_table["upstream_object_name"]
         )
         if upstream_name and self._is_dataset_pattern_allowed(
-            upstream_name, upstream_table["upstream_object_domain"], is_upstream=True
+            upstream_name, upstream_table["upstream_object_domain"]
         ):
             upstreams.append(
                 UpstreamClass(
                     dataset=builder.make_dataset_urn_with_platform_instance(
                         self.platform,
                         upstream_name,
                         self.config.platform_instance,
@@ -483,17 +482,15 @@
     ) -> List[str]:
         column_upstreams = []
         for upstream_col in fine_upstream.inputColumns:
             if (
                 upstream_col.objectName
                 and upstream_col.columnName
                 and self._is_dataset_pattern_allowed(
-                    upstream_col.objectName,
-                    upstream_col.objectDomain,
-                    is_upstream=True,
+                    upstream_col.objectName, upstream_col.objectDomain
                 )
             ):
                 upstream_dataset_name = self.get_dataset_identifier_from_qualified_name(
                     upstream_col.objectName
                 )
                 upstream_dataset_urn = builder.make_dataset_urn_with_platform_instance(
                     self.platform,
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_profiler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_profiler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_query.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_query.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,31 +1,10 @@
-from typing import List, Optional
+from typing import Optional
 
 from datahub.ingestion.source.snowflake.constants import SnowflakeObjectDomain
-from datahub.ingestion.source.snowflake.snowflake_config import (
-    DEFAULT_UPSTREAMS_DENY_LIST,
-)
-
-
-def create_deny_regex_sql_filter(
-    upstreams_deny_pattern: List[str], filter_cols: List[str]
-) -> str:
-    upstream_sql_filter = (
-        " AND ".join(
-            [
-                (f"NOT RLIKE({col_name},'{regexp}','i')")
-                for col_name in filter_cols
-                for regexp in upstreams_deny_pattern
-            ]
-        )
-        if upstreams_deny_pattern
-        else ""
-    )
-
-    return upstream_sql_filter
 
 
 class SnowflakeQuery:
     ACCESS_HISTORY_TABLE_VIEW_DOMAINS_FILTER = (
         "("
         f"'{SnowflakeObjectDomain.TABLE.capitalize()}',"
         f"'{SnowflakeObjectDomain.EXTERNAL_TABLE.capitalize()}',"
@@ -443,38 +422,28 @@
             PARTITION BY view_name,
             downstream_table_name {", downstream_table_columns" if include_column_lineage else ""}
             ORDER BY
               query_start_time DESC
           ) = 1
         """
 
-    # Note on use of `upstreams_deny_pattern` to ignore temporary tables:
-    # Snowflake access history may include temporary tables in DIRECT_OBJECTS_ACCESSED and
-    # OBJECTS_MODIFIED->columns->directSources. We do not need these temporary tables and filter these in the query.
     @staticmethod
     def table_to_table_lineage_history_v2(
         start_time_millis: int,
         end_time_millis: int,
         include_view_lineage: bool = True,
         include_column_lineage: bool = True,
-        upstreams_deny_pattern: List[str] = DEFAULT_UPSTREAMS_DENY_LIST,
     ) -> str:
         if include_column_lineage:
             return SnowflakeQuery.table_upstreams_with_column_lineage(
-                start_time_millis,
-                end_time_millis,
-                upstreams_deny_pattern,
-                include_view_lineage,
+                start_time_millis, end_time_millis, include_view_lineage
             )
         else:
             return SnowflakeQuery.table_upstreams_only(
-                start_time_millis,
-                end_time_millis,
-                upstreams_deny_pattern,
-                include_view_lineage,
+                start_time_millis, end_time_millis, include_view_lineage
             )
 
     @staticmethod
     def view_dependencies_v2() -> str:
         return """
         SELECT
             ARRAY_UNIQUE_AGG(
@@ -695,28 +664,21 @@
             basic_usage_counts.bucket_start_time
         """
 
     @staticmethod
     def table_upstreams_with_column_lineage(
         start_time_millis: int,
         end_time_millis: int,
-        upstreams_deny_pattern: List[str],
         include_view_lineage: bool = True,
     ) -> str:
         allowed_upstream_table_domains = (
             SnowflakeQuery.ACCESS_HISTORY_TABLE_VIEW_DOMAINS_FILTER
             if include_view_lineage
             else SnowflakeQuery.ACCESS_HISTORY_TABLE_DOMAINS_FILTER
         )
-
-        upstream_sql_filter = create_deny_regex_sql_filter(
-            upstreams_deny_pattern,
-            ["upstream_table_name", "upstream_column_table_name"],
-        )
-
         return f"""
         WITH column_lineage_history AS (
             SELECT
                 r.value : "objectName" :: varchar AS upstream_table_name,
                 r.value : "objectDomain" :: varchar AS upstream_table_domain,
                 w.value : "objectName" :: varchar AS downstream_table_name,
                 w.value : "objectDomain" :: varchar AS downstream_table_domain,
@@ -727,33 +689,32 @@
                 t.query_start_time AS query_start_time,
                 t.query_id AS query_id
             FROM
                 (SELECT * from snowflake.account_usage.access_history) t,
                 lateral flatten(input => t.DIRECT_OBJECTS_ACCESSED) r,
                 lateral flatten(input => t.OBJECTS_MODIFIED) w,
                 lateral flatten(input => w.value : "columns", outer => true) wcols,
-                lateral flatten(input => wcols.value : "directSources", outer => true) wcols_directSources
+                lateral flatten(input => wcols.value : "directSourceColumns", outer => true) wcols_directSources
             WHERE
                 r.value : "objectId" IS NOT NULL
                 AND w.value : "objectId" IS NOT NULL
                 AND w.value : "objectName" NOT LIKE '%.GE_TMP_%'
                 AND w.value : "objectName" NOT LIKE '%.GE_TEMP_%'
                 AND t.query_start_time >= to_timestamp_ltz({start_time_millis}, 3)
                 AND t.query_start_time < to_timestamp_ltz({end_time_millis}, 3)
                 AND upstream_table_domain in {allowed_upstream_table_domains}
                 AND downstream_table_domain = '{SnowflakeObjectDomain.TABLE.capitalize()}'
-                {("AND " + upstream_sql_filter) if upstream_sql_filter else ""}
             ),
         column_upstream_jobs AS (
             SELECT
                 downstream_table_name,
                 downstream_column_name,
                 ANY_VALUE(query_start_time),
                 query_id,
-                ARRAY_UNIQUE_AGG(
+                ARRAY_AGG(
                     OBJECT_CONSTRUCT(
                         'object_name', upstream_column_table_name,
                         'object_domain', upstream_column_object_domain,
                         'column_name', upstream_column_name
                     )
                 ) as upstream_columns_for_job
             FROM
@@ -782,48 +743,39 @@
             ANY_VALUE(h.downstream_table_domain) AS "DOWNSTREAM_TABLE_DOMAIN",
             ARRAY_UNIQUE_AGG(
                 OBJECT_CONSTRUCT(
                     'upstream_object_name', h.upstream_table_name,
                     'upstream_object_domain', h.upstream_table_domain
                 )
             ) AS "UPSTREAM_TABLES",
-            ARRAY_UNIQUE_AGG(
+            ARRAY_AGG(
                 OBJECT_CONSTRUCT(
                 'column_name', column_upstreams.downstream_column_name,
                 'upstreams', column_upstreams.upstreams
                 )
             ) AS "UPSTREAM_COLUMNS"
             FROM
                 column_lineage_history h
             LEFT JOIN column_upstreams column_upstreams
                 on h.downstream_table_name = column_upstreams.downstream_table_name
             GROUP BY
                 h.downstream_table_name
-            ORDER BY
-                h.downstream_table_name
         """
 
-    # See Note on temporary tables above.
     @staticmethod
     def table_upstreams_only(
         start_time_millis: int,
         end_time_millis: int,
-        upstreams_deny_pattern: List[str],
         include_view_lineage: bool = True,
     ) -> str:
         allowed_upstream_table_domains = (
             SnowflakeQuery.ACCESS_HISTORY_TABLE_VIEW_DOMAINS_FILTER
             if include_view_lineage
             else SnowflakeQuery.ACCESS_HISTORY_TABLE_DOMAINS_FILTER
         )
-
-        upstream_sql_filter = create_deny_regex_sql_filter(
-            upstreams_deny_pattern,
-            ["upstream_table_name"],
-        )
         return f"""
             WITH table_lineage_history AS (
                 SELECT
                     r.value:"objectName"::varchar AS upstream_table_name,
                     r.value:"objectDomain"::varchar AS upstream_table_domain,
                     r.value:"columns" AS upstream_table_columns,
                     w.value:"objectName"::varchar AS downstream_table_name,
@@ -838,25 +790,22 @@
                 AND w.value:"objectId" IS NOT NULL
                 AND w.value:"objectName" NOT LIKE '%.GE_TMP_%'
                 AND w.value:"objectName" NOT LIKE '%.GE_TEMP_%'
                 AND t.query_start_time >= to_timestamp_ltz({start_time_millis}, 3)
                 AND t.query_start_time < to_timestamp_ltz({end_time_millis}, 3)
                 AND upstream_table_domain in {allowed_upstream_table_domains}
                 AND downstream_table_domain = '{SnowflakeObjectDomain.TABLE.capitalize()}'
-                {("AND " + upstream_sql_filter) if upstream_sql_filter else ""}
                 )
             SELECT
                 downstream_table_name AS "DOWNSTREAM_TABLE_NAME",
                 ANY_VALUE(downstream_table_domain) as "DOWNSTREAM_TABLE_DOMAIN",
                 ARRAY_UNIQUE_AGG(
                     OBJECT_CONSTRUCT(
                         'upstream_object_name', upstream_table_name,
                         'upstream_object_domain', upstream_table_domain
                     )
                 ) as "UPSTREAM_TABLES"
                 FROM
                     table_lineage_history
                 GROUP BY
                     downstream_table_name
-                ORDER BY
-                    downstream_table_name
             """
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_report.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_report.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,21 +1,18 @@
 from dataclasses import dataclass, field
 from typing import Dict, MutableSet, Optional
 
-from datahub.ingestion.glossary.classification_mixin import ClassificationReportMixin
 from datahub.ingestion.source.snowflake.constants import SnowflakeEdition
 from datahub.ingestion.source.sql.sql_generic_profiler import ProfilingSqlReport
 from datahub.ingestion.source_report.sql.snowflake import SnowflakeReport
 from datahub.ingestion.source_report.usage.snowflake_usage import SnowflakeUsageReport
 
 
 @dataclass
-class SnowflakeV2Report(
-    SnowflakeReport, SnowflakeUsageReport, ProfilingSqlReport, ClassificationReportMixin
-):
+class SnowflakeV2Report(SnowflakeReport, SnowflakeUsageReport, ProfilingSqlReport):
     account_locator: Optional[str] = None
     region: Optional[str] = None
 
     schemas_scanned: int = 0
     databases_scanned: int = 0
     tags_scanned: int = 0
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_schema.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_schema.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_tag.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_tag.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_usage_v2.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_usage_v2.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -99,18 +99,15 @@
             raise Exception(f"Unknown snowflake region {region}")
         return cloud, cloud_region_id
 
     def _is_dataset_pattern_allowed(
         self: SnowflakeCommonProtocol,
         dataset_name: Optional[str],
         dataset_type: Optional[str],
-        is_upstream: bool = False,
     ) -> bool:
-        if is_upstream and not self.config.validate_upstreams_against_patterns:
-            return True
         if not dataset_type or not dataset_name:
             return True
         dataset_params = dataset_name.split(".")
         if dataset_type.lower() not in (
             SnowflakeObjectDomain.TABLE,
             SnowflakeObjectDomain.EXTERNAL_TABLE,
             SnowflakeObjectDomain.VIEW,
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/snowflake/snowflake_v2.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_v2.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/source_registry.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/source_registry.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/athena.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/athena.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/clickhouse.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/clickhouse.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/druid.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/druid.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/hana.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/hana.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/hive.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/hive.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,19 +15,19 @@
     SupportStatus,
     capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.extractor import schema_util
-from datahub.ingestion.source.sql.sql_common import register_custom_type
-from datahub.ingestion.source.sql.two_tier_sql_source import (
-    TwoTierSQLAlchemyConfig,
-    TwoTierSQLAlchemySource,
+from datahub.ingestion.source.sql.sql_common import (
+    SQLAlchemySource,
+    register_custom_type,
 )
+from datahub.ingestion.source.sql.sql_config import BasicSQLAlchemyConfig
 from datahub.metadata.com.linkedin.pegasus2avro.schema import (
     DateTypeClass,
     NullTypeClass,
     NumberTypeClass,
     SchemaField,
     TimeTypeClass,
 )
@@ -86,15 +86,15 @@
     DatabricksPyhiveDialect.get_columns = dbapi_get_columns_patched
 except ModuleNotFoundError:
     pass
 except Exception as e:
     logger.warning(f"Failed to patch method due to {e}")
 
 
-class HiveConfig(TwoTierSQLAlchemyConfig):
+class HiveConfig(BasicSQLAlchemyConfig):
     # defaults
     scheme = Field(default="hive", hidden_from_docs=True)
 
     # Hive SQLAlchemy connector returns views as tables.
     # See https://github.com/dropbox/PyHive/blob/b21c507a24ed2f2b0cf15b0b6abb1c43f31d3ee0/pyhive/sqlalchemy_hive.py#L270-L273.
     # Disabling views helps us prevent this duplication.
     include_views = Field(
@@ -109,15 +109,15 @@
 
 
 @platform_name("Hive")
 @config_class(HiveConfig)
 @support_status(SupportStatus.CERTIFIED)
 @capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
 @capability(SourceCapability.DOMAINS, "Supported via the `domain` config field")
-class HiveSource(TwoTierSQLAlchemySource):
+class HiveSource(SQLAlchemySource):
     """
     This plugin extracts the following:
 
     - Metadata for databases, schemas, and tables
     - Column types associated with each table
     - Detailed table and storage information
     - Table, row, and column statistics via optional SQL profiling.
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/mariadb.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mariadb.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/mssql.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mssql.py`

 * *Files 1% similar despite different names*

```diff
@@ -240,20 +240,19 @@
                     "SELECT name FROM master.sys.databases WHERE name NOT IN \
                   ('master', 'model', 'msdb', 'tempdb', 'Resource', \
                        'distribution' , 'reportserver', 'reportservertempdb'); "
                 )
                 for db in databases:
                     if self.config.database_pattern.allowed(db["name"]):
                         url = self.config.get_sql_alchemy_url(current_db=db["name"])
-                        with create_engine(
-                            url, **self.config.options
-                        ).connect() as conn:
-                            inspector = inspect(conn)
-                            self.current_database = db["name"]
-                            yield inspector
+                        inspector = inspect(
+                            create_engine(url, **self.config.options).connect()
+                        )
+                        self.current_database = db["name"]
+                        yield inspector
 
     def get_identifier(
         self, *, schema: str, entity: str, inspector: Inspector, **kwargs: Any
     ) -> str:
         regular = f"{schema}.{entity}"
         if self.config.database:
             if self.config.database_alias:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/mysql.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mysql.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/oauth_generator.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/oauth_generator.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/oracle.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/oracle.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/postgres.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/postgres.py`

 * *Files 4% similar despite different names*

```diff
@@ -93,42 +93,30 @@
     # so pydantic is able to parse the tuple using parse_obj
     source_table: str
     source_schema: str
     dependent_view: str
     dependent_schema: str
 
 
-class BasePostgresConfig(BasicSQLAlchemyConfig):
+class PostgresConfig(BasicSQLAlchemyConfig):
+    # defaults
     scheme = Field(default="postgresql+psycopg2", description="database scheme")
     schema_pattern = Field(default=AllowDenyPattern(deny=["information_schema"]))
-
-
-class PostgresConfig(BasePostgresConfig):
     include_view_lineage = Field(
         default=False, description="Include table lineage for views"
     )
 
     database_pattern: AllowDenyPattern = Field(
         default=AllowDenyPattern.allow_all(),
-        description=(
-            "Regex patterns for databases to filter in ingestion. "
-            "Note: this is not used if `database` or `sqlalchemy_uri` are provided."
-        ),
+        description="Regex patterns for databases to filter in ingestion.",
     )
     database: Optional[str] = Field(
         default=None,
         description="database (catalog). If set to Null, all databases will be considered for ingestion.",
     )
-    initial_database: Optional[str] = Field(
-        default="postgres",
-        description=(
-            "Initial database used to query for the list of databases, when ingesting multiple databases. "
-            "Note: this is not used if `database` or `sqlalchemy_uri` are provided."
-        ),
-    )
 
 
 @platform_name("Postgres")
 @config_class(PostgresConfig)
 @support_status(SupportStatus.CERTIFIED)
 @capability(SourceCapability.DOMAINS, "Enabled by default")
 @capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
@@ -152,36 +140,35 @@
 
     @classmethod
     def create(cls, config_dict, ctx):
         config = PostgresConfig.parse_obj(config_dict)
         return cls(config, ctx)
 
     def get_inspectors(self) -> Iterable[Inspector]:
-        # Note: get_sql_alchemy_url will choose `sqlalchemy_uri` over the passed in database
-        url = self.config.get_sql_alchemy_url(
-            database=self.config.database or self.config.initial_database
-        )
+        # This method can be overridden in the case that you want to dynamically
+        # run on multiple databases.
+        url = self.config.get_sql_alchemy_url()
         logger.debug(f"sql_alchemy_url={url}")
         engine = create_engine(url, **self.config.options)
         with engine.connect() as conn:
-            if self.config.database or self.config.sqlalchemy_uri:
+            if self.config.database and self.config.database != "":
                 inspector = inspect(conn)
                 yield inspector
             else:
                 # pg_database catalog -  https://www.postgresql.org/docs/current/catalog-pg-database.html
                 # exclude template databases - https://www.postgresql.org/docs/current/manage-ag-templatedbs.html
                 databases = conn.execute(
                     "SELECT datname from pg_database where datname not in ('template0', 'template1')"
                 )
                 for db in databases:
-                    if not self.config.database_pattern.allowed(db["datname"]):
-                        continue
-                    url = self.config.get_sql_alchemy_url(database=db["datname"])
-                    with create_engine(url, **self.config.options).connect() as conn:
-                        inspector = inspect(conn)
+                    if self.config.database_pattern.allowed(db["datname"]):
+                        url = self.config.get_sql_alchemy_url(database=db["datname"])
+                        inspector = inspect(
+                            create_engine(url, **self.config.options).connect()
+                        )
                         yield inspector
 
     def get_workunits(self) -> Iterable[Union[MetadataWorkUnit, SqlWorkUnit]]:
         yield from super().get_workunits()
 
         for inspector in self.get_inspectors():
             if self.config.include_view_lineage:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/presto.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/presto.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/presto_on_hive.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/presto_on_hive.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/redshift.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/redshift.py`

 * *Files 1% similar despite different names*

```diff
@@ -28,17 +28,17 @@
     SupportStatus,
     capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.aws.path_spec import PathSpec
 from datahub.ingestion.source.aws.s3_util import strip_s3_prefix
-from datahub.ingestion.source.data_lake_common.path_spec import PathSpec
-from datahub.ingestion.source.sql.postgres import BasePostgresConfig
+from datahub.ingestion.source.sql.postgres import PostgresConfig
 from datahub.ingestion.source.sql.sql_common import (
     SQLAlchemySource,
     SQLSourceReport,
     SqlWorkUnit,
 )
 
 # TRICKY: it's necessary to import the Postgres source because
@@ -120,15 +120,15 @@
 
     s3_lineage_config: Optional[S3LineageProviderConfig] = Field(
         default=None, description="Common config for S3 lineage generation"
     )
 
 
 class RedshiftConfig(
-    BasePostgresConfig,
+    PostgresConfig,
     BaseTimeWindowConfig,
     DatasetLineageProviderConfigBase,
     DatasetS3LineageProviderConfigBase,
 ):
     def get_identifier(self, schema: str, table: str) -> str:
         regular = f"{schema}.{table}"
         if self.database_alias:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_config.py`

 * *Files 1% similar despite different names*

```diff
@@ -121,15 +121,15 @@
             raise ValueError("host_port and schema or connect_uri required.")
 
         return self.sqlalchemy_uri or make_sqlalchemy_uri(
             self.scheme,
             self.username,
             self.password.get_secret_value() if self.password is not None else None,
             self.host_port,
-            database or self.database,
+            self.database or database,
             uri_opts=uri_opts,
         )
 
 
 def make_sqlalchemy_uri(
     scheme: str,
     username: Optional[str],
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_generic.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_generic.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_generic_profiler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_generic_profiler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_types.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_types.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/sql_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_utils.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/trino.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/trino.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/two_tier_sql_source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/two_tier_sql_source.py`

 * *Files 4% similar despite different names*

```diff
@@ -102,17 +102,18 @@
             if self.config.database and self.config.database != "":
                 databases = [self.config.database]
             else:
                 databases = inspector.get_schema_names()
             for db in databases:
                 if self.config.database_pattern.allowed(db):
                     url = self.config.get_sql_alchemy_url(current_db=db)
-                    with create_engine(url, **self.config.options).connect() as conn:
-                        inspector = inspect(conn)
-                        yield inspector
+                    inspector = inspect(
+                        create_engine(url, **self.config.options).connect()
+                    )
+                    yield inspector
 
     def gen_schema_containers(
         self,
         schema: str,
         database: str,
         extra_properties: Optional[Dict[str, Any]] = None,
     ) -> Iterable[MetadataWorkUnit]:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/sql/vertica.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/vertica.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/checkpoint.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/checkpoint.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/entity_removal_state.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/entity_removal_state.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/profiling_state.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/profiling_state.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/profiling_state_handler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/profiling_state_handler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/redundant_run_skip_handler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/redundant_run_skip_handler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/stale_entity_removal_handler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/stale_entity_removal_handler.py`

 * *Files 6% similar despite different names*

```diff
@@ -26,15 +26,14 @@
     StatefulIngestionReport,
     StatefulIngestionSourceBase,
 )
 from datahub.ingestion.source.state.use_case_handler import (
     StatefulIngestionUsecaseHandlerBase,
 )
 from datahub.metadata.schema_classes import StatusClass
-from datahub.utilities.lossy_collections import LossyList
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 
 class StatefulStaleMetadataRemovalConfig(StatefulIngestionConfig):
     """
     Base specialized config for Stateful Ingestion with stale metadata removal capability.
@@ -51,15 +50,15 @@
         ge=0.0,
         hidden_from_docs=True,
     )
 
 
 @dataclass
 class StaleEntityRemovalSourceReport(StatefulIngestionReport):
-    soft_deleted_stale_entities: LossyList[str] = field(default_factory=LossyList)
+    soft_deleted_stale_entities: List[str] = field(default_factory=list)
 
     def report_stale_entity_soft_deleted(self, urn: str) -> None:
         self.soft_deleted_stale_entities.append(urn)
 
 
 Derived = TypeVar("Derived", bound=CheckpointStateBase)
 
@@ -173,44 +172,34 @@
             else False
         )
         self._job_id = self._init_job_id()
         self._urns_to_skip: Set[str] = set()
         self.source.register_stateful_ingestion_usecase_handler(self)
 
     @classmethod
-    def compute_job_id(
-        cls, platform: Optional[str], unique_id: Optional[str] = None
-    ) -> JobId:
+    def compute_job_id(cls, platform: Optional[str]) -> JobId:
         # Handle backward-compatibility for existing sources.
         backward_comp_platform_to_job_name: Dict[str, str] = {
             "bigquery": "ingest_from_bigquery_source",
             "dbt": "dbt_stateful_ingestion",
             "glue": "glue_stateful_ingestion",
             "kafka": "ingest_from_kafka_source",
             "pulsar": "ingest_from_pulsar_source",
             "snowflake": "common_ingest_from_sql_source",
         }
         if platform in backward_comp_platform_to_job_name:
             return JobId(backward_comp_platform_to_job_name[platform])
 
         # Default name for everything else
         job_name_suffix = "stale_entity_removal"
-        # Used with set_job_id when creating multiple checkpoints in one recipe source
-        # Because job_id is used as dictionary key when committing checkpoint, we have to set a new job_id
-        # Refer to https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/source/state/stateful_ingestion_base.py#L390
-        unique_suffix = f"_{unique_id}" if unique_id else ""
-        return JobId(
-            f"{platform}_{job_name_suffix}{unique_suffix}"
-            if platform
-            else job_name_suffix
-        )
+        return JobId(f"{platform}_{job_name_suffix}" if platform else job_name_suffix)
 
-    def _init_job_id(self, unique_id: Optional[str] = None) -> JobId:
+    def _init_job_id(self) -> JobId:
         platform: Optional[str] = getattr(self.source, "platform", "default")
-        return self.compute_job_id(platform, unique_id)
+        return self.compute_job_id(platform)
 
     def _ignore_old_state(self) -> bool:
         if (
             self.stateful_ingestion_config is not None
             and self.stateful_ingestion_config.ignore_old_state
         ):
             return True
@@ -224,17 +213,14 @@
             return True
         return False
 
     @property
     def job_id(self) -> JobId:
         return self._job_id
 
-    def set_job_id(self, unique_id):
-        self._job_id = self._init_job_id(unique_id)
-
     def is_checkpointing_enabled(self) -> bool:
         return self.checkpointing_enabled
 
     def create_checkpoint(self) -> Optional[Checkpoint]:
         if self.is_checkpointing_enabled() and not self._ignore_new_state():
             assert self.stateful_ingestion_config is not None
             assert self.pipeline_name is not None
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/stateful_ingestion_base.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/stateful_ingestion_base.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state/use_case_handler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/use_case_handler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/state_provider/datahub_ingestion_checkpointing_provider.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state_provider/datahub_ingestion_checkpointing_provider.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/superset.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/superset.py`

 * *Files 12% similar despite different names*

```diff
@@ -4,57 +4,41 @@
 from typing import Dict, Iterable, Optional
 
 import dateutil.parser as dp
 import requests
 from pydantic.class_validators import root_validator, validator
 from pydantic.fields import Field
 
+from datahub.configuration.common import ConfigModel
 from datahub.emitter.mce_builder import DEFAULT_ENV
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
-    SourceCapability,
     SupportStatus,
-    capability,
     config_class,
     platform_name,
     support_status,
 )
-from datahub.ingestion.api.source import Source
+from datahub.ingestion.api.source import Source, SourceReport
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.sql import sql_common
-from datahub.ingestion.source.state.entity_removal_state import GenericCheckpointState
-from datahub.ingestion.source.state.stale_entity_removal_handler import (
-    StaleEntityRemovalHandler,
-    StaleEntityRemovalSourceReport,
-    StatefulStaleMetadataRemovalConfig,
-)
-from datahub.ingestion.source.state.stateful_ingestion_base import (
-    StatefulIngestionConfigBase,
-    StatefulIngestionSourceBase,
-)
 from datahub.metadata.com.linkedin.pegasus2avro.common import (
     AuditStamp,
     ChangeAuditStamps,
-    Status,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.metadata.snapshot import (
     ChartSnapshot,
     DashboardSnapshot,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.mxe import MetadataChangeEvent
 from datahub.metadata.schema_classes import (
     ChartInfoClass,
     ChartTypeClass,
     DashboardInfoClass,
 )
 from datahub.utilities import config_clean
-from datahub.utilities.source_helpers import (
-    auto_stale_entity_removal,
-    auto_status_aspect,
-)
 
 logger = logging.getLogger(__name__)
 
 PAGE_SIZE = 25
 
 
 chart_type_from_viz_type = {
@@ -70,32 +54,26 @@
     "dual_line": ChartTypeClass.LINE,
     "line_multi": ChartTypeClass.LINE,
     "treemap": ChartTypeClass.AREA,
     "box_plot": ChartTypeClass.BAR,
 }
 
 
-class SupersetConfig(StatefulIngestionConfigBase):
+class SupersetConfig(ConfigModel):
     # See the Superset /security/login endpoint for details
     # https://superset.apache.org/docs/rest-api
     connect_uri: str = Field(
         default="http://localhost:8088", description="Superset host URL."
     )
     display_uri: Optional[str] = Field(
         default=None,
         description="optional URL to use in links (if `connect_uri` is only for ingestion)",
     )
     username: Optional[str] = Field(default=None, description="Superset username.")
     password: Optional[str] = Field(default=None, description="Superset password.")
-
-    # Configuration for stateful ingestion
-    stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = Field(
-        default=None, description="Superset Stateful Ingestion Config."
-    )
-
     provider: str = Field(default="db", description="Superset provider.")
     options: Dict = Field(default={}, description="")
     env: str = Field(
         default=DEFAULT_ENV,
         description="Environment to use in namespace when constructing URNs",
     )
     database_alias: Dict[str, str] = Field(
@@ -137,37 +115,33 @@
     comparator = filter_obj.get("comparator")
     return f"{clause} {column} {operator} {comparator}"
 
 
 @platform_name("Superset")
 @config_class(SupersetConfig)
 @support_status(SupportStatus.CERTIFIED)
-@capability(
-    SourceCapability.DELETION_DETECTION, "Optionally enabled via stateful_ingestion"
-)
-class SupersetSource(StatefulIngestionSourceBase):
+class SupersetSource(Source):
     """
     This plugin extracts the following:
     - Charts, dashboards, and associated metadata
 
     See documentation for superset's /security/login at https://superset.apache.org/docs/rest-api for more details on superset's login api.
     """
 
     config: SupersetConfig
-    report: StaleEntityRemovalSourceReport
+    report: SourceReport
     platform = "superset"
-    stale_entity_removal_handler: StaleEntityRemovalHandler
 
     def __hash__(self):
         return id(self)
 
     def __init__(self, ctx: PipelineContext, config: SupersetConfig):
-        super().__init__(config, ctx)
+        super().__init__(ctx)
         self.config = config
-        self.report = StaleEntityRemovalSourceReport()
+        self.report = SourceReport()
 
         login_response = requests.post(
             f"{self.config.connect_uri}/api/v1/security/login",
             json={
                 "username": self.config.username,
                 "password": self.config.password,
                 "refresh": True,
@@ -189,23 +163,14 @@
 
         # Test the connection
         test_response = self.session.get(f"{self.config.connect_uri}/api/v1/dashboard/")
         if test_response.status_code == 200:
             pass
             # TODO(Gabe): how should we message about this error?
 
-        # Create and register the stateful ingestion use-case handlers.
-        self.stale_entity_removal_handler = StaleEntityRemovalHandler(
-            source=self,
-            config=self.config,
-            state_type_class=GenericCheckpointState,
-            pipeline_name=self.ctx.pipeline_name,
-            run_id=self.ctx.run_id,
-        )
-
     @classmethod
     def create(cls, config_dict: dict, ctx: PipelineContext) -> Source:
         config = SupersetConfig.parse_obj(config_dict)
         return cls(ctx, config)
 
     @lru_cache(maxsize=None)
     def get_platform_from_database_id(self, database_id):
@@ -240,15 +205,15 @@
             return dataset_urn
         return None
 
     def construct_dashboard_from_api_data(self, dashboard_data):
         dashboard_urn = f"urn:li:dashboard:({self.platform},{dashboard_data['id']})"
         dashboard_snapshot = DashboardSnapshot(
             urn=dashboard_urn,
-            aspects=[Status(removed=False)],
+            aspects=[],
         )
 
         modified_actor = f"urn:li:corpuser:{(dashboard_data.get('changed_by') or {}).get('username', 'unknown')}"
         modified_ts = int(
             dp.parse(dashboard_data.get("changed_on_utc", "now")).timestamp() * 1000
         )
         title = dashboard_data.get("dashboard_title", "")
@@ -315,15 +280,15 @@
 
                 yield wu
 
     def construct_chart_from_chart_data(self, chart_data):
         chart_urn = f"urn:li:chart:({self.platform},{chart_data['id']})"
         chart_snapshot = ChartSnapshot(
             urn=chart_urn,
-            aspects=[Status(removed=False)],
+            aspects=[],
         )
 
         modified_actor = f"urn:li:corpuser:{(chart_data.get('changed_by') or {}).get('username', 'unknown')}"
         modified_ts = int(
             dp.parse(chart_data.get("changed_on_utc", "now")).timestamp() * 1000
         )
         title = chart_data.get("slice_name", "")
@@ -413,19 +378,13 @@
 
                 mce = MetadataChangeEvent(proposedSnapshot=chart_snapshot)
                 wu = MetadataWorkUnit(id=chart_snapshot.urn, mce=mce)
                 self.report.report_workunit(wu)
 
                 yield wu
 
-    def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
+    def get_workunits(self) -> Iterable[MetadataWorkUnit]:
         yield from self.emit_dashboard_mces()
         yield from self.emit_chart_mces()
 
-    def get_workunits(self) -> Iterable[MetadataWorkUnit]:
-        return auto_stale_entity_removal(
-            self.stale_entity_removal_handler,
-            auto_status_aspect(self.get_workunits_internal()),
-        )
-
-    def get_report(self) -> StaleEntityRemovalSourceReport:
+    def get_report(self) -> SourceReport:
         return self.report
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/tableau.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/tableau.py`

 * *Files 1% similar despite different names*

```diff
@@ -395,15 +395,14 @@
     config: TableauConfig
     report: StaleEntityRemovalSourceReport
     platform = "tableau"
     server: Optional[Server]
     upstream_tables: Dict[str, Tuple[Any, Optional[str], bool]] = {}
     tableau_stat_registry: Dict[str, UsageStat] = {}
     tableau_project_registry: Dict[str, TableauProject] = {}
-    workbook_project_map: Dict[str, str] = {}
     datasource_project_map: Dict[str, str] = {}
 
     def __hash__(self):
         return id(self)
 
     def __init__(
         self,
@@ -414,15 +413,14 @@
 
         self.config = config
         self.report = StaleEntityRemovalSourceReport()
         self.server = None
         self.upstream_tables = {}
         self.tableau_stat_registry = {}
         self.tableau_project_registry = {}
-        self.workbook_project_map = {}
         self.datasource_project_map = {}
 
         # This list keeps track of sheets in workbooks so that we retrieve those
         # when emitting sheets.
         self.sheet_ids: List[str] = []
         # This list keeps track of dashboards in workbooks so that we retrieve those
         # when emitting dashboards.
@@ -572,47 +570,30 @@
                 logger.debug(
                     f"project id ({ds.project_id}) of datasource {ds.name} is not present in project "
                     f"registry"
                 )
                 continue
             self.datasource_project_map[ds.id] = ds.project_id
 
-    def _init_workbook_registry(self) -> None:
-        if self.server is None:
-            return
-
-        for wb in TSC.Pager(self.server.workbooks):
-            if wb.project_id not in self.tableau_project_registry:
-                logger.debug(
-                    f"project id ({wb.project_id}) of workbook {wb.name} is not present in project "
-                    f"registry"
-                )
-                continue
-            self.workbook_project_map[wb.id] = wb.project_id
-
     def _populate_projects_registry(self):
         if self.server is None:
             return
 
         logger.info("Initializing site project registry")
 
         all_project_map: Dict[str, TableauProject] = self._get_all_project()
 
         self._init_tableau_project_registry(all_project_map)
         self._init_datasource_registry()
-        self._init_workbook_registry()
 
         logger.debug(f"All site projects {all_project_map}")
         logger.debug(f"Projects selected for ingestion {self.tableau_project_registry}")
         logger.debug(
             f"Tableau data-sources {self.datasource_project_map}",
         )
-        logger.debug(
-            f"Tableau workbooks {self.workbook_project_map}",
-        )
 
     def _authenticate(self):
         try:
             self.server = self.config.make_tableau_client()
             logger.info("Authenticated to Tableau server")
         # Note that we're not catching ConfigurationError, since we want that to throw.
         except ValueError as e:
@@ -737,24 +718,26 @@
                 projects,
                 page_size_override=self.config.workbook_page_size,
             ):
                 # This check is needed as we are using projectNameWithin which return project as per project name so if
                 # user want to ingest only nested project C from A->B->C then tableau might return more than one Project
                 # if multiple project has name C. Ideal solution is to use projectLuidWithin to avoid duplicate project,
                 # however Tableau supports projectLuidWithin in Tableau Cloud June 2022 / Server 2022.3 and later.
-                project_luid: Optional[str] = self._get_workbook_project_luid(workbook)
-                if project_luid not in self.tableau_project_registry.keys():
+                if (
+                    workbook.get(tableau_constant.PROJECT_LUID)
+                    not in self.tableau_project_registry.keys()
+                ):
                     wrk_name: Optional[str] = workbook.get(tableau_constant.NAME)
                     wrk_id: Optional[str] = workbook.get(tableau_constant.ID)
                     prj_name: Optional[str] = workbook.get(
                         tableau_constant.PROJECT_NAME
                     )
-
+                    prj_id: Optional[str] = workbook.get(tableau_constant.PROJECT_LUID)
                     logger.debug(
-                        f"Skipping workbook {wrk_name}({wrk_id}) as it is project {prj_name}({project_luid}) not "
+                        f"Skipping workbook {wrk_name}({wrk_id}) as it is project {prj_name}({prj_id}) not "
                         f"present in project registry"
                     )
                     continue
 
                 yield from self.emit_workbook_as_container(workbook)
 
                 for sheet in workbook.get(tableau_constant.SHEETS, []):
@@ -1265,31 +1248,22 @@
 
         logger.debug(
             f"published datasource {ds.get(tableau_constant.NAME)} project_luid not found"
         )
 
         return None
 
-    def _get_workbook_project_luid(self, wb):
-        if wb.get(tableau_constant.LUID) and self.workbook_project_map.get(
-            wb[tableau_constant.LUID]
-        ):
-            return self.workbook_project_map[wb[tableau_constant.LUID]]
-
-        logger.debug(f"workbook {wb.get(tableau_constant.NAME)} project_luid not found")
-
-        return None
-
     def _get_embedded_datasource_project_luid(self, ds):
-        if ds.get(tableau_constant.WORKBOOK):
-            project_luid: Optional[str] = self._get_workbook_project_luid(
-                ds.get(tableau_constant.WORKBOOK)
-            )
-            if project_luid and project_luid in self.tableau_project_registry:
-                return project_luid
+        if (
+            ds.get(tableau_constant.WORKBOOK)
+            and ds.get(tableau_constant.WORKBOOK).get(tableau_constant.PROJECT_LUID)
+            and ds[tableau_constant.WORKBOOK][tableau_constant.PROJECT_LUID]
+            in self.tableau_project_registry
+        ):
+            return ds[tableau_constant.WORKBOOK][tableau_constant.PROJECT_LUID]
 
         logger.debug(
             f"embedded datasource {ds.get(tableau_constant.NAME)} project_luid not found"
         )
 
         return None
 
@@ -1367,44 +1341,35 @@
         database = csql.get(tableau_constant.DATABASE) or {}
         if (
             csql.get(tableau_constant.IS_UNSUPPORTED_CUSTOM_SQL, False)
             and tableau_constant.NAME in database
             and tableau_constant.CONNECTION_TYPE in database
         ):
             upstream_tables = []
-            query = csql.get(tableau_constant.QUERY)
-            parser = LineageRunner(query)
+            parser = LineageRunner(csql.get(tableau_constant.QUERY))
 
-            try:
-                for table in parser.source_tables:
-                    split_table = str(table).split(".")
-                    if len(split_table) == 2:
-                        datset = make_table_urn(
-                            env=self.config.env,
-                            upstream_db=database.get(tableau_constant.NAME),
-                            connection_type=database.get(
-                                tableau_constant.CONNECTION_TYPE, ""
-                            ),
-                            schema=split_table[0],
-                            full_name=split_table[1],
-                            platform_instance_map=self.config.platform_instance_map,
-                            lineage_overrides=self.config.lineage_overrides,
-                        )
-                        upstream_tables.append(
-                            UpstreamClass(
-                                type=DatasetLineageType.TRANSFORMED, dataset=datset
-                            )
+            for table in parser.source_tables:
+                split_table = str(table).split(".")
+                if len(split_table) == 2:
+                    datset = make_table_urn(
+                        env=self.config.env,
+                        upstream_db=database.get(tableau_constant.NAME),
+                        connection_type=database.get(
+                            tableau_constant.CONNECTION_TYPE, ""
+                        ),
+                        schema=split_table[0],
+                        full_name=split_table[1],
+                        platform_instance_map=self.config.platform_instance_map,
+                        lineage_overrides=self.config.lineage_overrides,
+                    )
+                    upstream_tables.append(
+                        UpstreamClass(
+                            type=DatasetLineageType.TRANSFORMED, dataset=datset
                         )
-            except Exception as e:
-                self.report.report_warning(
-                    key="csql-lineage",
-                    reason=f"Unable to retrieve lineage from query. "
-                    f"Query: {query} "
-                    f"Reason: {str(e)} ",
-                )
+                    )
             upstream_lineage = UpstreamLineage(upstreams=upstream_tables)
             yield self.get_metadata_change_proposal(
                 csql_urn,
                 aspect_name=tableau_constant.UPSTREAM_LINEAGE,
                 aspect=upstream_lineage,
             )
 
@@ -1825,25 +1790,23 @@
 
         if self.config.extract_usage_stats:
             wu = self._get_chart_stat_wu(sheet, sheet_urn)
             if wu is not None:
                 self.report.report_workunit(wu)
                 yield wu
 
-        project_luid: Optional[str] = self._get_workbook_project_luid(workbook)
-
         if (
             workbook is not None
-            and project_luid
-            and project_luid in self.tableau_project_registry
+            and workbook.get(tableau_constant.PROJECT_LUID)
+            and workbook[tableau_constant.PROJECT_LUID] in self.tableau_project_registry
             and workbook.get(tableau_constant.NAME)
         ):
             browse_paths = BrowsePathsClass(
                 paths=[
-                    f"/{self.platform}/{self._project_luid_to_browse_path_name(project_luid)}"
+                    f"/{self.platform}/{self._project_luid_to_browse_path_name(workbook[tableau_constant.PROJECT_LUID])}"
                     f"/{workbook[tableau_constant.NAME].replace('/', REPLACE_SLASH_CHAR)}"
                 ]
             )
             chart_snapshot.aspects.append(browse_paths)
         else:
             logger.warning(
                 f"Could not set browse path for workbook {sheet[tableau_constant.ID]}. Please check permissions."
@@ -1961,17 +1924,20 @@
         tag_list = workbook.get(tableau_constant.TAGS, [])
         tag_list_str = (
             [t.get(tableau_constant.NAME, "") for t in tag_list if t is not None]
             if (tag_list and self.config.ingest_tags)
             else None
         )
         parent_key = None
-        project_luid: Optional[str] = self._get_workbook_project_luid(workbook)
-        if project_luid and project_luid in self.tableau_project_registry.keys():
-            parent_key = self.gen_project_key(project_luid)
+        if (
+            workbook.get(tableau_constant.PROJECT_LUID)
+            and workbook[tableau_constant.PROJECT_LUID]
+            in self.tableau_project_registry.keys()
+        ):
+            parent_key = self.gen_project_key(workbook[tableau_constant.PROJECT_LUID])
         else:
             workbook_id: Optional[str] = workbook.get(tableau_constant.ID)
             workbook_name: Optional[str] = workbook.get(tableau_constant.NAME)
             logger.warning(
                 f"Could not load project hierarchy for workbook {workbook_name}({workbook_id}). Please check permissions."
             )
 
@@ -2140,24 +2106,23 @@
         if self.config.extract_usage_stats:
             # dashboard_snapshot doesn't support the stat aspect as list element and hence need to emit MetadataWorkUnit
             wu = self._get_dashboard_stat_wu(dashboard, dashboard_urn)
             if wu is not None:
                 self.report.report_workunit(wu)
                 yield wu
 
-        project_luid: Optional[str] = self._get_workbook_project_luid(workbook)
         if (
             workbook is not None
-            and project_luid
-            and project_luid in self.tableau_project_registry
+            and workbook.get(tableau_constant.PROJECT_LUID)
+            and workbook[tableau_constant.PROJECT_LUID] in self.tableau_project_registry
             and workbook.get(tableau_constant.NAME)
         ):
             browse_paths = BrowsePathsClass(
                 paths=[
-                    f"/{self.platform}/{self._project_luid_to_browse_path_name(project_luid)}"
+                    f"/{self.platform}/{self._project_luid_to_browse_path_name(workbook[tableau_constant.PROJECT_LUID])}"
                     f"/{workbook[tableau_constant.NAME].replace('/', REPLACE_SLASH_CHAR)}"
                 ]
             )
             dashboard_snapshot.aspects.append(browse_paths)
         elif (
             workbook is not None
             and workbook.get(tableau_constant.PROJECT_NAME)
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/tableau_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/tableau_common.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,14 +42,15 @@
 workbook_graphql_query = """
     {
       id
       name
       luid
       uri
       projectName
+      projectLuid
       owner {
         username
       }
       description
       uri
       createdAt
       updatedAt
@@ -84,15 +85,15 @@
         name
         path
     }
     workbook {
         id
         name
         projectName
-        luid
+        projectLuid
         owner {
           username
         }
     }
     datasourceFields {
         __typename
         id
@@ -162,15 +163,15 @@
     tags {
         name
     }
     workbook {
         id
         name
         projectName
-        luid
+        projectLuid
         owner {
           username
         }
     }
 }
 """
 
@@ -246,15 +247,15 @@
         id
         name
     }
     workbook {
         id
         name
         projectName
-        luid
+        projectLuid
         owner {
           username
         }
     }
 }
 """
 
@@ -290,15 +291,15 @@
               luid
             }
             ... on EmbeddedDatasource {
               workbook {
                 id
                 name
                 projectName
-                luid
+                projectLuid
               }
             }
           }
         }
       }
       tables {
         id
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/tableau_constant.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/tableau_constant.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/pulsar.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,159 +1,137 @@
-import os
-from datetime import datetime, timedelta, timezone
-from typing import Any, Dict, Optional
-
-import pydantic
-from pydantic import Field
-
-from datahub.configuration.common import AllowDenyPattern, ConfigModel
-from datahub.configuration.source_common import DatasetSourceConfigMixin
-from datahub.configuration.validate_field_rename import pydantic_renamed_field
+import re
+from typing import Dict, List, Optional, Union
+from urllib.parse import urlparse
+
+from pydantic import Field, validator
+
+from datahub.configuration.common import AllowDenyPattern, ConfigurationError
+from datahub.configuration.source_common import (
+    EnvConfigMixin,
+    PlatformInstanceConfigMixin,
+)
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StatefulStaleMetadataRemovalConfig,
 )
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulIngestionConfigBase,
-    StatefulProfilingConfigMixin,
 )
-from datahub.ingestion.source.usage.usage_common import BaseUsageConfig
-
-
-class UnityCatalogProfilerConfig(ConfigModel):
-    # TODO: Reduce duplicate code with DataLakeProfilerConfig, GEProfilingConfig, SQLAlchemyConfig
-    enabled: bool = Field(
-        default=False, description="Whether profiling should be done."
-    )
-
-    warehouse_id: Optional[str] = Field(
-        default=None, description="SQL Warehouse id, for running profiling queries."
-    )
-
-    profile_table_level_only: bool = Field(
-        default=False,
-        description="Whether to perform profiling at table-level only or include column-level profiling as well.",
-    )
-
-    pattern: AllowDenyPattern = Field(
-        default=AllowDenyPattern.allow_all(),
-        description=(
-            "Regex patterns to filter tables for profiling during ingestion. "
-            "Specify regex to match the `catalog.schema.table` format. "
-            "Note that only tables allowed by the `table_pattern` will be considered."
-        ),
-    )
+from datahub.utilities import config_clean
 
-    call_analyze: bool = Field(
-        default=True,
-        description=(
-            "Whether to call ANALYZE TABLE as part of profile ingestion."
-            "If false, will ingest the results of the most recent ANALYZE TABLE call, if any."
-        ),
-    )
 
-    max_wait_secs: int = Field(
-        default=int(timedelta(hours=1).total_seconds()),
-        description="Maximum time to wait for an ANALYZE TABLE query to complete.",
-    )
+def _is_valid_hostname(hostname: str) -> bool:
+    """
+    Loosely ascii hostname validation. A hostname is considered valid when the total length does not exceed 253
+    characters, contains valid characters and are max 63 octets per label.
+    """
+    if len(hostname) > 253:
+        return False
+    # Hostnames ending on a dot are valid, if present strip exactly one
+    if hostname[-1] == ".":
+        hostname = hostname[:-1]
+    allowed = re.compile(r"(?!-)[A-Z\d-]{1,63}(?<!-)$", re.IGNORECASE)
+    return all(allowed.match(x) for x in hostname.split("."))
 
-    max_workers: int = Field(
-        default=5 * (os.cpu_count() or 4),
-        description="Number of worker threads to use for profiling. Set to 1 to disable.",
-    )
 
-    @pydantic.root_validator
-    def warehouse_id_required_for_profiling(
-        cls, values: Dict[str, Any]
-    ) -> Dict[str, Any]:
-        if values.get("enabled") and not values.get("warehouse_id"):
-            raise ValueError("warehouse_id must be set when profiling is enabled.")
-        return values
-
-    @property
-    def include_columns(self):
-        return not self.profile_table_level_only
-
-
-class UnityCatalogSourceConfig(
-    StatefulIngestionConfigBase,
-    BaseUsageConfig,
-    DatasetSourceConfigMixin,
-    StatefulProfilingConfigMixin,
+class PulsarSourceConfig(
+    StatefulIngestionConfigBase, PlatformInstanceConfigMixin, EnvConfigMixin
 ):
-    token: str = pydantic.Field(description="Databricks personal access token")
-    workspace_url: str = pydantic.Field(
-        description="Databricks workspace url. e.g. https://my-workspace.cloud.databricks.com"
+    web_service_url: str = Field(
+        default="http://localhost:8080", description="The web URL for the cluster."
     )
-    workspace_name: Optional[str] = pydantic.Field(
+    timeout: int = Field(
+        default=5,
+        description="Timout setting, how long to wait for the Pulsar rest api to send data before giving up",
+    )
+    # Mandatory for oauth authentication
+    issuer_url: Optional[str] = Field(
         default=None,
-        description="Name of the workspace. Default to deployment name present in workspace_url",
+        description="The complete URL for a Custom Authorization Server. Mandatory for OAuth based authentication.",
     )
-
-    metastore_id_pattern: AllowDenyPattern = Field(
-        default=AllowDenyPattern.allow_all(),
-        description="Regex patterns for metastore id to filter in ingestion.",
+    client_id: Optional[str] = Field(
+        default=None, description="The application's client ID"
     )
-
-    catalog_pattern: AllowDenyPattern = Field(
-        default=AllowDenyPattern.allow_all(),
-        description="Regex patterns for catalogs to filter in ingestion. Specify regex to match the full `metastore.catalog` name.",
+    client_secret: Optional[str] = Field(
+        default=None, description="The application's client secret"
     )
-
-    schema_pattern: AllowDenyPattern = Field(
-        default=AllowDenyPattern.allow_all(),
-        description="Regex patterns for schemas to filter in ingestion. Specify regex to the full `metastore.catalog.schema` name. e.g. to match all tables in schema analytics, use the regex `^mymetastore\\.mycatalog\\.analytics$`.",
+    # Mandatory for token authentication
+    token: Optional[str] = Field(
+        default=None,
+        description="The access token for the application. Mandatory for token based authentication.",
     )
-
-    table_pattern: AllowDenyPattern = Field(
-        default=AllowDenyPattern.allow_all(),
-        description="Regex patterns for tables to filter in ingestion. Specify regex to match the entire table name in `catalog.schema.table` format. e.g. to match all tables starting with customer in Customer catalog and public schema, use the regex `Customer\\.public\\.customer.*`.",
+    verify_ssl: Union[bool, str] = Field(
+        default=True,
+        description="Either a boolean, in which case it controls whether we verify the server's TLS certificate, or a string, in which case it must be a path to a CA bundle to use.",
     )
-    domain: Dict[str, AllowDenyPattern] = Field(
-        default=dict(),
-        description='Attach domains to catalogs, schemas or tables during ingestion using regex patterns. Domain key can be a guid like *urn:li:domain:ec428203-ce86-4db3-985d-5a8ee6df32ba* or a string like "Marketing".) If you provide strings, then datahub will attempt to resolve this name to a guid, and will error out if this fails. There can be multiple domain keys specified.',
+    # By default, allow all topics and deny the pulsar system topics
+    tenant_patterns: AllowDenyPattern = Field(
+        default=AllowDenyPattern(allow=[".*"], deny=["pulsar"]),
+        description="List of regex patterns for tenants to include/exclude from ingestion. By default all tenants are allowed.",
+    )
+    namespace_patterns: AllowDenyPattern = Field(
+        default=AllowDenyPattern(allow=[".*"], deny=["public/functions"]),
+        description="List of regex patterns for namespaces to include/exclude from ingestion. By default the functions namespace is denied.",
+    )
+    topic_patterns: AllowDenyPattern = Field(
+        default=AllowDenyPattern(allow=[".*"], deny=["/__.*$"]),
+        description="List of regex patterns for topics to include/exclude from ingestion. By default the Pulsar system topics are denied.",
     )
-
-    include_table_lineage: Optional[bool] = pydantic.Field(
+    exclude_individual_partitions: bool = Field(
         default=True,
-        description="Option to enable/disable lineage generation.",
+        description="Extract each individual partitioned topic. e.g. when turned off a topic with 100 partitions will result in 100 Datasets.",
     )
 
-    include_ownership: bool = pydantic.Field(
-        default=False,
-        description="Option to enable/disable ownership generation for metastores, catalogs, schemas, and tables.",
+    tenants: List[str] = Field(
+        default=[],
+        description="Listing all tenants requires superUser role, alternative you can set a list of tenants you want to scrape using the tenant admin role",
     )
 
-    _rename_table_ownership = pydantic_renamed_field(
-        "include_table_ownership", "include_ownership"
+    domain: Dict[str, AllowDenyPattern] = Field(
+        default_factory=dict, description="Domain patterns"
     )
 
-    include_column_lineage: Optional[bool] = pydantic.Field(
-        default=True,
-        description="Option to enable/disable lineage generation. Currently we have to call a rest call per column to get column level lineage due to the Databrick api which can slow down ingestion. ",
+    stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = Field(
+        default=None, description="see Stateful Ingestion"
     )
 
-    include_usage_statistics: bool = Field(
-        default=True,
-        description="Generate usage statistics.",
+    oid_config: dict = Field(
+        default_factory=dict, description="Placeholder for OpenId discovery document"
     )
 
-    profiling: UnityCatalogProfilerConfig = Field(
-        default=UnityCatalogProfilerConfig(), description="Data profiling configuration"
-    )
+    @validator("token")
+    def ensure_only_issuer_or_token(
+        cls, token: Optional[str], values: Dict[str, Optional[str]]
+    ) -> Optional[str]:
+        if token is not None and values.get("issuer_url") is not None:
+            raise ConfigurationError(
+                "Expected only one authentication method, either issuer_url or token."
+            )
+        return token
 
-    stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = pydantic.Field(
-        default=None, description="Unity Catalog Stateful Ingestion Config."
-    )
+    @validator("client_secret", always=True)
+    def ensure_client_id_and_secret_for_issuer_url(
+        cls, client_secret: Optional[str], values: Dict[str, Optional[str]]
+    ) -> Optional[str]:
+        if values.get("issuer_url") is not None and (
+            client_secret is None or values.get("client_id") is None
+        ):
+            raise ConfigurationError(
+                "Missing configuration: client_id and client_secret are mandatory when issuer_url is set."
+            )
+        return client_secret
 
-    @pydantic.validator("start_time")
-    def within_thirty_days(cls, v: datetime) -> datetime:
-        if (datetime.now(timezone.utc) - v).days > 30:
-            raise ValueError("Query history is only maintained for 30 days.")
-        return v
+    @validator("web_service_url")
+    def web_service_url_scheme_host_port(cls, val: str) -> str:
+        # Tokenize the web url
+        url = urlparse(val)
+
+        if url.scheme not in ["http", "https"]:
+            raise ConfigurationError(
+                f"Scheme should be http or https, found {url.scheme}"
+            )
 
-    @pydantic.validator("workspace_url")
-    def workspace_url_should_start_with_http_scheme(cls, workspace_url: str) -> str:
-        if not workspace_url.lower().startswith(("http://", "https://")):
-            raise ValueError(
-                "Workspace URL must start with http scheme. e.g. https://my-workspace.cloud.databricks.com"
+        if not _is_valid_hostname(url.hostname.__str__()):
+            raise ConfigurationError(
+                f"Not a valid hostname, hostname contains invalid characters, found {url.hostname}"
             )
-        return workspace_url
+
+        return config_clean.remove_trailing_slashes(val)
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/proxy.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/proxy.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,81 +1,137 @@
 """
 Manage the communication with DataBricks Server and provide equivalent dataclasses for dependent modules
 """
-import dataclasses
+import datetime
 import logging
-from datetime import datetime
+from dataclasses import dataclass, field
 from typing import Any, Dict, Iterable, List, Optional
 
-from databricks.sdk import WorkspaceClient
-from databricks.sdk.service.sql import (
-    QueryFilter,
-    QueryInfo,
-    QueryStatementType,
-    QueryStatus,
-)
 from databricks_cli.sdk.api_client import ApiClient
 from databricks_cli.unity_catalog.api import UnityCatalogApi
 
-from datahub.ingestion.source.unity.proxy_profiling import (
-    UnityCatalogProxyProfilingMixin,
-)
-from datahub.ingestion.source.unity.proxy_types import (
-    ALLOWED_STATEMENT_TYPES,
-    DATA_TYPE_REGISTRY,
-    Catalog,
-    Column,
-    Metastore,
-    Query,
-    Schema,
-    ServicePrincipal,
-    Table,
-    TableReference,
-)
 from datahub.ingestion.source.unity.report import UnityCatalogReport
-from datahub.metadata.schema_classes import SchemaFieldDataTypeClass
+from datahub.metadata.schema_classes import (
+    ArrayTypeClass,
+    BooleanTypeClass,
+    BytesTypeClass,
+    DateTypeClass,
+    MapTypeClass,
+    NullTypeClass,
+    NumberTypeClass,
+    RecordTypeClass,
+    SchemaFieldDataTypeClass,
+    StringTypeClass,
+    TimeTypeClass,
+)
 
 logger: logging.Logger = logging.getLogger(__name__)
 
+# Supported types are available at
+# https://api-docs.databricks.com/rest/latest/unity-catalog-api-specification-2-1.html?_ga=2.151019001.1795147704.1666247755-2119235717.1666247755
+
+DATA_TYPE_REGISTRY: dict = {
+    "BOOLEAN": BooleanTypeClass,
+    "BYTE": BytesTypeClass,
+    "DATE": DateTypeClass,
+    "SHORT": NumberTypeClass,
+    "INT": NumberTypeClass,
+    "LONG": NumberTypeClass,
+    "FLOAT": NumberTypeClass,
+    "DOUBLE": NumberTypeClass,
+    "TIMESTAMP": TimeTypeClass,
+    "STRING": StringTypeClass,
+    "BINARY": BytesTypeClass,
+    "DECIMAL": NumberTypeClass,
+    "INTERVAL": TimeTypeClass,
+    "ARRAY": ArrayTypeClass,
+    "STRUCT": RecordTypeClass,
+    "MAP": MapTypeClass,
+    "CHAR": StringTypeClass,
+    "NULL": NullTypeClass,
+}
+
+
+@dataclass
+class CommonProperty:
+    id: str
+    name: str
+    type: str
+    comment: Optional[str]
+
+
+@dataclass
+class Metastore(CommonProperty):
+    metastore_id: str
+
+
+@dataclass
+class Catalog(CommonProperty):
+    metastore: Metastore
+
+
+@dataclass
+class Schema(CommonProperty):
+    catalog: Catalog
+
+
+@dataclass
+class Column(CommonProperty):
+    type_text: str
+    type_name: SchemaFieldDataTypeClass
+    type_precision: int
+    type_scale: int
+    position: int
+    nullable: bool
+    comment: Optional[str]
+
+
+@dataclass
+class ColumnLineage:
+    source: str
+    destination: str
+
+
+@dataclass
+class Table(CommonProperty):
+    schema: Schema
+    columns: List[Column]
+    storage_location: Optional[str]
+    data_source_format: Optional[str]
+    comment: Optional[str]
+    table_type: str
+    owner: str
+    generation: int
+    created_at: datetime.datetime
+    created_by: str
+    updated_at: Optional[datetime.datetime]
+    updated_by: Optional[str]
+    table_id: str
+    view_definition: Optional[str]
+    properties: Dict[str, str]
+    upstreams: Dict[str, Dict[str, List[str]]] = field(default_factory=dict)
 
-@dataclasses.dataclass
-class QueryFilterWithStatementTypes(QueryFilter):
-    statement_types: List[QueryStatementType] = dataclasses.field(default_factory=list)
-
-    def as_dict(self) -> dict:
-        return {**super().as_dict(), "statement_types": self.statement_types}
-
-    @classmethod
-    def from_dict(cls, d: Dict[str, Any]) -> "QueryFilterWithStatementTypes":
-        v = super().from_dict(d)
-        v.statement_types = d["statement_types"]
-        return v
+    # lineage: Optional[Lineage]
 
 
-class UnityCatalogApiProxy(UnityCatalogProxyProfilingMixin):
-    _workspace_client: WorkspaceClient
+class UnityCatalogApiProxy:
     _unity_catalog_api: UnityCatalogApi
     _workspace_url: str
     report: UnityCatalogReport
-    warehouse_id: str
 
     def __init__(
-        self,
-        workspace_url: str,
-        personal_access_token: str,
-        warehouse_id: Optional[str],
-        report: UnityCatalogReport,
+        self, workspace_url: str, personal_access_token: str, report: UnityCatalogReport
     ):
-        self._workspace_client = WorkspaceClient(
-            host=workspace_url, token=personal_access_token
-        )
         self._unity_catalog_api = UnityCatalogApi(
-            ApiClient(host=workspace_url, token=personal_access_token)
+            ApiClient(
+                host=workspace_url,
+                token=personal_access_token,
+            )
         )
-        self.warehouse_id = warehouse_id or ""
+        self._workspace_url = workspace_url
         self.report = report
 
     def check_connectivity(self) -> bool:
         self._unity_catalog_api.list_metastores()
         return True
 
     def assigned_metastore(self) -> Optional[Metastore]:
@@ -126,96 +182,20 @@
         response: dict = self._unity_catalog_api.list_tables(
             catalog_name=schema.catalog.name,
             schema_name=schema.name,
             name_pattern=None,
         )
 
         if response.get("tables") is None:
-            logger.info(
-                f"Tables not found for schema {schema.catalog.name}.{schema.name}"
-            )
+            logger.info(f"Tables not found for schema {schema.name}")
             return []
 
         for table in response["tables"]:
             yield self._create_table(schema=schema, obj=table)
 
-    def service_principals(self) -> Iterable[ServicePrincipal]:
-        # TODO: Replace with self._workspace_client.service_principals.list() when it supports pagination
-        start_index = 1  # Unfortunately 1-indexed
-        items_per_page = 0
-        total_results = float("inf")
-        while start_index + items_per_page <= total_results:
-            response: dict = self._unity_catalog_api.client.client.perform_query(
-                "GET", "/account/scim/v2/ServicePrincipals"
-            )
-            start_index = response["startIndex"]
-            items_per_page = response["itemsPerPage"]
-            total_results = response["totalResults"]
-            for principal in response["Resources"]:
-                yield self._create_service_principal(principal)
-
-    def query_history(
-        self,
-        start_time: datetime,
-        end_time: datetime,
-    ) -> Iterable[Query]:
-        """Returns all queries that were run between start_time and end_time with relevant statement_type.
-
-        Raises:
-            DatabricksError: If the query history API returns an error.
-        """
-        filter_by = QueryFilterWithStatementTypes.from_dict(
-            {
-                "query_start_time_range": {
-                    "start_time_ms": start_time.timestamp() * 1000,
-                    "end_time_ms": end_time.timestamp() * 1000,
-                },
-                "statuses": [QueryStatus.FINISHED.value],
-                "statement_types": [typ.value for typ in ALLOWED_STATEMENT_TYPES],
-            }
-        )
-        for query_info in self._query_history(filter_by=filter_by):
-            try:
-                yield self._create_query(query_info)
-            except Exception as e:
-                logger.warning(f"Error parsing query: {e}")
-                self.report.report_warning("query-parse", str(e))
-
-    def _query_history(
-        self,
-        filter_by: QueryFilterWithStatementTypes,
-        max_results: int = 1000,
-        include_metrics: bool = False,
-    ) -> Iterable[QueryInfo]:
-        """Manual implementation of the query_history.list() endpoint.
-
-        Needed because:
-        - WorkspaceClient incorrectly passes params as query params, not body
-        - It does not paginate correctly -- needs to remove filter_by argument
-        Remove if these issues are fixed.
-        """
-        method = "GET"
-        path = "/api/2.0/sql/history/queries"
-        body: Dict[str, Any] = {
-            "include_metrics": include_metrics,
-            "max_results": max_results,  # Max batch size
-        }
-
-        response: dict = self._workspace_client.api_client.do(
-            method, path, body={**body, "filter_by": filter_by.as_dict()}
-        )
-        while True:
-            if "res" not in response or not response["res"]:
-                return
-            for v in response["res"]:
-                yield QueryInfo.from_dict(v)
-            response = self._workspace_client.api_client.do(
-                method, path, body={**body, "page_token": response["next_page_token"]}
-            )
-
     def list_lineages_by_table(self, table_name=None, headers=None):
         """
         List table lineage by table name
         """
         _data = {}
         if table_name is not None:
             _data["table_name"] = table_name
@@ -250,20 +230,15 @@
     def table_lineage(self, table: Table) -> None:
         # Lineage endpoint doesn't exists on 2.1 version
         try:
             response: dict = self.list_lineages_by_table(
                 table_name=f"{table.schema.catalog.name}.{table.schema.name}.{table.name}"
             )
             table.upstreams = {
-                TableReference(
-                    table.schema.catalog.metastore.id,
-                    item["catalog_name"],
-                    item["schema_name"],
-                    item["name"],
-                ): {}
+                f"{item['catalog_name']}.{item['schema_name']}.{item['name']}": {}
                 for item in response.get("upstream_tables", [])
             }
         except Exception as e:
             logger.error(f"Error getting lineage: {e}")
 
     def get_column_lineage(self, table: Table) -> None:
         try:
@@ -273,23 +248,25 @@
             if table_lineage_response:
                 for column in table.columns:
                     response: dict = self.list_lineages_by_column(
                         table_name=f"{table.schema.catalog.name}.{table.schema.name}.{table.name}",
                         column_name=column.name,
                     )
                     for item in response.get("upstream_cols", []):
-                        table_ref = TableReference(
-                            table.schema.catalog.metastore.id,
-                            item["catalog_name"],
-                            item["schema_name"],
-                            item["table_name"],
-                        )
-                        table.upstreams.setdefault(table_ref, {}).setdefault(
-                            column.name, []
-                        ).append(item["name"])
+                        table_name = f"{item['catalog_name']}.{item['schema_name']}.{item['table_name']}"
+                        col_name = item["name"]
+                        if not table.upstreams.get(table_name):
+                            table.upstreams[table_name] = {column.name: [col_name]}
+                        else:
+                            if column.name in table.upstreams[table_name]:
+                                table.upstreams[table_name][column.name].append(
+                                    col_name
+                                )
+                            else:
+                                table.upstreams[table_name][column.name] = [col_name]
 
         except Exception as e:
             logger.error(f"Error getting lineage: {e}")
 
     @staticmethod
     def _escape_sequence(value: str) -> str:
         return value.replace(" ", "_")
@@ -298,41 +275,38 @@
     def _create_metastore(obj: Any) -> Metastore:
         return Metastore(
             name=obj["name"],
             id=UnityCatalogApiProxy._escape_sequence(obj["name"]),
             metastore_id=obj["metastore_id"],
             type="Metastore",
             comment=obj.get("comment"),
-            owner=obj.get("owner"),
         )
 
     def _create_catalog(self, metastore: Metastore, obj: Any) -> Catalog:
         return Catalog(
             name=obj["name"],
             id="{}.{}".format(
                 metastore.id,
                 self._escape_sequence(obj["name"]),
             ),
             metastore=metastore,
             type="Catalog",
             comment=obj.get("comment"),
-            owner=obj.get("owner"),
         )
 
     def _create_schema(self, catalog: Catalog, obj: Any) -> Schema:
         return Schema(
             name=obj["name"],
             id="{}.{}".format(
                 catalog.id,
                 self._escape_sequence(obj["name"]),
             ),
             catalog=catalog,
             type="Schema",
             comment=obj.get("comment"),
-            owner=obj.get("owner"),
         )
 
     def _create_column(self, table_id: str, obj: Any) -> Column:
         return Column(
             name=obj["name"],
             id="{}.{}".format(table_id, self._escape_sequence(obj["name"])),
             type_text=obj["type_text"],
@@ -358,41 +332,18 @@
             data_source_format=obj.get("data_source_format"),
             columns=[self._create_column(table_id, column) for column in obj["columns"]]
             if obj.get("columns") is not None
             else [],
             type="view" if str(obj["table_type"]).lower() == "view" else "table",
             view_definition=obj.get("view_definition", None),
             properties=obj.get("properties", {}),
-            owner=obj.get("owner"),
+            owner=obj["owner"],
             generation=obj["generation"],
-            created_at=datetime.utcfromtimestamp(obj["created_at"] / 1000),
+            created_at=datetime.datetime.utcfromtimestamp(obj["created_at"] / 1000),
             created_by=obj["created_by"],
-            updated_at=datetime.utcfromtimestamp(obj["updated_at"] / 1000)
+            updated_at=datetime.datetime.utcfromtimestamp(obj["updated_at"] / 1000)
             if "updated_at" in obj
             else None,
             updated_by=obj.get("updated_by", None),
             table_id=obj["table_id"],
             comment=obj.get("comment"),
         )
-
-    def _create_service_principal(self, obj: dict) -> ServicePrincipal:
-        display_name = obj["displayName"]
-        return ServicePrincipal(
-            id="{}.{}".format(obj["id"], self._escape_sequence(display_name)),
-            display_name=display_name,
-            application_id=obj["applicationId"],
-            active=obj.get("active"),
-        )
-
-    @staticmethod
-    def _create_query(info: QueryInfo) -> Query:
-        return Query(
-            query_id=info.query_id,
-            query_text=info.query_text,
-            statement_type=info.statement_type,
-            start_time=datetime.utcfromtimestamp(info.query_start_time_ms / 1000),
-            end_time=datetime.utcfromtimestamp(info.query_end_time_ms / 1000),
-            user_id=info.user_id,
-            user_name=info.user_name,
-            executed_as_user_id=info.executed_as_user_id,
-            executed_as_user_name=info.executed_as_user_name,
-        )
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/unity/source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/source.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,12 +1,10 @@
 import logging
 import re
-import time
-from datetime import timedelta
-from typing import Dict, Iterable, List, Optional, Set
+from typing import Dict, Iterable, List, Optional
 
 from datahub.emitter.mce_builder import (
     make_data_platform_urn,
     make_dataset_urn_with_platform_instance,
     make_domain_urn,
     make_schema_field_urn,
     make_user_urn,
@@ -42,47 +40,34 @@
 from datahub.ingestion.source.state.entity_removal_state import GenericCheckpointState
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StaleEntityRemovalHandler,
 )
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulIngestionSourceBase,
 )
+from datahub.ingestion.source.unity import proxy
 from datahub.ingestion.source.unity.config import UnityCatalogSourceConfig
-from datahub.ingestion.source.unity.profiler import UnityCatalogProfiler
-from datahub.ingestion.source.unity.proxy import UnityCatalogApiProxy
-from datahub.ingestion.source.unity.proxy_types import (
-    Catalog,
-    Column,
-    Metastore,
-    Schema,
-    ServicePrincipal,
-    Table,
-    TableReference,
-)
+from datahub.ingestion.source.unity.proxy import Catalog, Metastore, Schema
 from datahub.ingestion.source.unity.report import UnityCatalogReport
-from datahub.ingestion.source.unity.usage import UnityCatalogUsageExtractor
 from datahub.metadata.com.linkedin.pegasus2avro.dataset import (
     FineGrainedLineage,
     FineGrainedLineageUpstreamType,
     ViewProperties,
 )
 from datahub.metadata.schema_classes import (
     DatasetLineageTypeClass,
     DatasetPropertiesClass,
     DomainsClass,
     MySqlDDLClass,
-    OperationClass,
-    OperationTypeClass,
     OwnerClass,
     OwnershipClass,
     OwnershipTypeClass,
     SchemaFieldClass,
     SchemaMetadataClass,
     SubTypesClass,
-    TimeStampClass,
     UpstreamClass,
     UpstreamLineageClass,
 )
 from datahub.utilities.hive_schema_to_avro import get_schema_fields_for_hive_column
 from datahub.utilities.registries.domain_registry import DomainRegistry
 from datahub.utilities.source_helpers import (
     auto_stale_entity_removal,
@@ -95,19 +80,20 @@
 
 @platform_name("Databricks")
 @config_class(UnityCatalogSourceConfig)
 @capability(SourceCapability.SCHEMA_METADATA, "Enabled by default")
 @capability(SourceCapability.DESCRIPTIONS, "Enabled by default")
 @capability(SourceCapability.LINEAGE_COARSE, "Enabled by default")
 @capability(SourceCapability.LINEAGE_FINE, "Enabled by default")
-@capability(SourceCapability.USAGE_STATS, "Enabled by default")
 @capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
 @capability(SourceCapability.DOMAINS, "Supported via the `domain` config field")
 @capability(SourceCapability.CONTAINERS, "Enabled by default")
-@capability(SourceCapability.OWNERSHIP, "Supported via the `include_ownership` config")
+@capability(
+    SourceCapability.OWNERSHIP, "Supported via the `include_table_ownership` config"
+)
 @capability(
     SourceCapability.DELETION_DETECTION,
     "Optionally enabled via `stateful_ingestion.remove_stale_metadata`",
     supported=True,
 )
 @support_status(SupportStatus.INCUBATING)
 class UnityCatalogSource(StatefulIngestionSourceBase, TestableSource):
@@ -115,31 +101,28 @@
     This plugin extracts the following metadata from Databricks Unity Catalog:
     - metastores
     - schemas
     - tables and column lineage
     """
 
     config: UnityCatalogSourceConfig
-    unity_catalog_api_proxy: UnityCatalogApiProxy
+    unity_catalog_api_proxy: proxy.UnityCatalogApiProxy
     platform: str = "databricks"
     platform_instance_name: str
 
     def get_report(self) -> UnityCatalogReport:
         return self.report
 
     def __init__(self, ctx: PipelineContext, config: UnityCatalogSourceConfig):
         super(UnityCatalogSource, self).__init__(config, ctx)
 
         self.config = config
         self.report: UnityCatalogReport = UnityCatalogReport()
-        self.unity_catalog_api_proxy = UnityCatalogApiProxy(
-            config.workspace_url,
-            config.token,
-            config.profiling.warehouse_id,
-            report=self.report,
+        self.unity_catalog_api_proxy = proxy.UnityCatalogApiProxy(
+            config.workspace_url, config.token, report=self.report
         )
 
         # Determine the platform_instance_name
         self.platform_instance_name = (
             config.workspace_name
             if config.workspace_name is not None
             else config.workspace_url.split("//")[1].split(".")[0]
@@ -155,55 +138,28 @@
         )
 
         if self.config.domain:
             self.domain_registry = DomainRegistry(
                 cached_domains=[k for k in self.config.domain], graph=self.ctx.graph
             )
 
-        # Global map of service principal application id -> ServicePrincipal
-        self.service_principals: Dict[str, ServicePrincipal] = {}
-        # Global set of table refs
-        self.table_refs: Set[TableReference] = set()
-        self.view_refs: Set[TableReference] = set()
-
     @staticmethod
     def test_connection(config_dict: dict) -> TestConnectionReport:
         test_report = TestConnectionReport()
-        test_report.capability_report = {}
         try:
             config = UnityCatalogSourceConfig.parse_obj_allow_extras(config_dict)
             report = UnityCatalogReport()
-            unity_proxy = UnityCatalogApiProxy(
-                config.workspace_url,
-                config.token,
-                config.profiling.warehouse_id,
-                report=report,
+            unity_proxy = proxy.UnityCatalogApiProxy(
+                config.workspace_url, config.token, report=report
             )
             if unity_proxy.check_connectivity():
                 test_report.basic_connectivity = CapabilityReport(capable=True)
             else:
                 test_report.basic_connectivity = CapabilityReport(capable=False)
 
-            # TODO: Refactor into separate file / method
-            if config.profiling.enabled and not config.profiling.warehouse_id:
-                test_report.capability_report[
-                    SourceCapability.DATA_PROFILING
-                ] = CapabilityReport(
-                    capable=False, failure_reason="Warehouse ID not provided"
-                )
-            elif config.profiling.enabled:
-                try:
-                    unity_proxy.check_profiling_connectivity()
-                    test_report.capability_report[
-                        SourceCapability.DATA_PROFILING
-                    ] = CapabilityReport(capable=True)
-                except Exception as e:
-                    test_report.capability_report[
-                        SourceCapability.DATA_PROFILING
-                    ] = CapabilityReport(capable=False, failure_reason=str(e))
         except Exception as e:
             test_report.basic_connectivity = CapabilityReport(
                 capable=False, failure_reason=f"{e}"
             )
         return test_report
 
     @classmethod
@@ -217,59 +173,16 @@
             auto_workunit_reporter(
                 self.report,
                 auto_status_aspect(self.get_workunits_internal()),
             ),
         )
 
     def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
-        wait_on_warehouse = None
-        if self.config.profiling.enabled:
-            # Can take several minutes, so start now and wait later
-            wait_on_warehouse = self.unity_catalog_api_proxy.start_warehouse()
-            if wait_on_warehouse is None:
-                self.report.report_failure(
-                    "initialization",
-                    f"SQL warehouse {self.config.profiling.warehouse_id} not found",
-                )
-                return
-
-        self.build_service_principal_map()
         yield from self.process_metastores()
 
-        if self.config.include_usage_statistics:
-            usage_extractor = UnityCatalogUsageExtractor(
-                config=self.config,
-                report=self.report,
-                proxy=self.unity_catalog_api_proxy,
-                table_urn_builder=self.gen_dataset_urn,
-                user_urn_builder=self.gen_user_urn,
-            )
-            yield from usage_extractor.run(self.table_refs | self.view_refs)
-
-        if self.config.profiling.enabled:
-            assert wait_on_warehouse
-            timeout = timedelta(seconds=self.config.profiling.max_wait_secs)
-            wait_on_warehouse.result(timeout)
-            profiling_extractor = UnityCatalogProfiler(
-                self.config.profiling,
-                self.report,
-                self.unity_catalog_api_proxy,
-                self.gen_dataset_urn,
-            )
-            yield from profiling_extractor.get_workunits(self.table_refs)
-
-    def build_service_principal_map(self) -> None:
-        try:
-            for sp in self.unity_catalog_api_proxy.service_principals():
-                self.service_principals[sp.application_id] = sp
-        except Exception as e:
-            self.report.report_warning(
-                "service-principals", f"Unable to fetch service principals: {e}"
-            )
-
     def process_metastores(self) -> Iterable[MetadataWorkUnit]:
         metastores: Dict[str, Metastore] = {}
         assigned_metastore = self.unity_catalog_api_proxy.assigned_metastore()
         if assigned_metastore:
             metastores[assigned_metastore.metastore_id] = assigned_metastore
         for metastore in self.unity_catalog_api_proxy.metastores():
             metastores[metastore.metastore_id] = metastore
@@ -281,112 +194,125 @@
 
             logger.info(f"Started to process metastore: {metastore.metastore_id}")
             yield from self.gen_metastore_containers(metastore)
             yield from self.process_catalogs(metastore)
 
             self.report.metastores.processed(metastore.metastore_id)
 
-    def process_catalogs(self, metastore: Metastore) -> Iterable[MetadataWorkUnit]:
+    def process_catalogs(
+        self, metastore: proxy.Metastore
+    ) -> Iterable[MetadataWorkUnit]:
         for catalog in self.unity_catalog_api_proxy.catalogs(metastore=metastore):
-            if not self.config.catalog_pattern.allowed(catalog.id):
+            if not self.config.catalog_pattern.allowed(catalog.name):
                 self.report.catalogs.dropped(catalog.id)
                 continue
 
             yield from self.gen_catalog_containers(catalog)
             yield from self.process_schemas(catalog)
 
             self.report.catalogs.processed(catalog.id)
 
-    def process_schemas(self, catalog: Catalog) -> Iterable[MetadataWorkUnit]:
+    def process_schemas(self, catalog: proxy.Catalog) -> Iterable[MetadataWorkUnit]:
         for schema in self.unity_catalog_api_proxy.schemas(catalog=catalog):
-            if not self.config.schema_pattern.allowed(schema.id):
+            if not self.config.schema_pattern.allowed(schema.name):
                 self.report.schemas.dropped(schema.id)
                 continue
 
             yield from self.gen_schema_containers(schema)
             yield from self.process_tables(schema)
 
             self.report.schemas.processed(schema.id)
 
-    def process_tables(self, schema: Schema) -> Iterable[MetadataWorkUnit]:
+    def process_tables(self, schema: proxy.Schema) -> Iterable[MetadataWorkUnit]:
         for table in self.unity_catalog_api_proxy.tables(schema=schema):
-            if not self.config.table_pattern.allowed(table.ref.qualified_table_name):
+            filter_table_name = (
+                f"{table.schema.catalog.name}.{table.schema.name}.{table.name}"
+            )
+
+            if not self.config.table_pattern.allowed(filter_table_name):
                 self.report.tables.dropped(table.id, type=table.type)
                 continue
 
-            if table.type.lower() == "view":
-                self.view_refs.add(table.ref)
-            else:
-                self.table_refs.add(table.ref)
             yield from self.process_table(table, schema)
+
             self.report.tables.processed(table.id, type=table.type)
 
-    def process_table(self, table: Table, schema: Schema) -> Iterable[MetadataWorkUnit]:
-        dataset_urn = self.gen_dataset_urn(table.ref)
+    def process_table(
+        self, table: proxy.Table, schema: proxy.Schema
+    ) -> Iterable[MetadataWorkUnit]:
+        dataset_urn: str = make_dataset_urn_with_platform_instance(
+            platform=self.platform,
+            platform_instance=self.platform_instance_name,
+            name=table.id,
+        )
         yield from self.add_table_to_dataset_container(dataset_urn, schema)
 
         table_props = self._create_table_property_aspect(table)
 
         view_props = None
         if table.view_definition:
             view_props = self._create_view_property_aspect(table)
 
         sub_type = self._create_table_sub_type_aspect(table)
         schema_metadata = self._create_schema_metadata_aspect(table)
-        operation = self._create_table_operation_aspect(table)
-        domain = self._get_domain_aspect(dataset_name=table.ref.qualified_table_name)
+
+        domain = self._get_domain_aspect(
+            dataset_name=str(
+                f"{table.schema.catalog.name}.{table.schema.name}.{table.name}"
+            )
+        )
+
         ownership = self._create_table_ownership_aspect(table)
 
-        lineage: Optional[UpstreamLineageClass] = None
         if self.config.include_column_lineage:
             self.unity_catalog_api_proxy.get_column_lineage(table)
             lineage = self._generate_column_lineage_aspect(dataset_urn, table)
-        elif self.config.include_table_lineage:
+        else:
             self.unity_catalog_api_proxy.table_lineage(table)
             lineage = self._generate_lineage_aspect(dataset_urn, table)
 
         yield from [
             mcp.as_workunit()
             for mcp in MetadataChangeProposalWrapper.construct_many(
                 entityUrn=dataset_urn,
                 aspects=[
                     table_props,
                     view_props,
                     sub_type,
                     schema_metadata,
-                    operation,
                     domain,
                     ownership,
                     lineage,
                 ],
             )
         ]
 
     def _generate_column_lineage_aspect(
-        self, dataset_urn: str, table: Table
+        self, dataset_urn: str, table: proxy.Table
     ) -> Optional[UpstreamLineageClass]:
         upstreams: List[UpstreamClass] = []
         finegrained_lineages: List[FineGrainedLineage] = []
-        for upstream_ref, downstream_to_upstream_cols in sorted(
-            table.upstreams.items()
-        ):
-            upstream_urn = self.gen_dataset_urn(upstream_ref)
+        for upstream in sorted(table.upstreams.keys()):
+            upstream_urn = make_dataset_urn_with_platform_instance(
+                self.platform,
+                f"{table.schema.catalog.metastore.id}.{upstream}",
+                self.platform_instance_name,
+            )
 
-            finegrained_lineages.extend(
-                FineGrainedLineage(
+            for col in sorted(table.upstreams[upstream].keys()):
+                fl = FineGrainedLineage(
                     upstreamType=FineGrainedLineageUpstreamType.FIELD_SET,
                     upstreams=[
                         make_schema_field_urn(upstream_urn, upstream_col)
-                        for upstream_col in sorted(u_cols)
+                        for upstream_col in sorted(table.upstreams[upstream][col])
                     ],
                     downstreamType=FineGrainedLineageUpstreamType.FIELD_SET,
-                    downstreams=[make_schema_field_urn(dataset_urn, d_col)],
+                    downstreams=[make_schema_field_urn(dataset_urn, col)],
                 )
-                for d_col, u_cols in sorted(downstream_to_upstream_cols.items())
-            )
+                finegrained_lineages.append(fl)
 
             upstream_table = UpstreamClass(
                 upstream_urn,
                 DatasetLineageTypeClass.TRANSFORMED,
             )
             upstreams.append(upstream_table)
 
@@ -394,15 +320,15 @@
             return UpstreamLineageClass(
                 upstreams=upstreams, fineGrainedLineages=finegrained_lineages
             )
         else:
             return None
 
     def _generate_lineage_aspect(
-        self, dataset_urn: str, table: Table
+        self, dataset_urn: str, table: proxy.Table
     ) -> Optional[UpstreamLineageClass]:
         upstreams: List[UpstreamClass] = []
         for upstream in sorted(table.upstreams.keys()):
             upstream_urn = make_dataset_urn_with_platform_instance(
                 self.platform,
                 f"{table.schema.catalog.metastore.id}.{upstream}",
                 self.platform_instance_name,
@@ -421,73 +347,56 @@
 
     def _get_domain_aspect(self, dataset_name: str) -> Optional[DomainsClass]:
         domain_urn = self._gen_domain_urn(dataset_name)
         if not domain_urn:
             return None
         return DomainsClass(domains=[domain_urn])
 
-    def get_owner_urn(self, user: Optional[str]) -> Optional[str]:
-        if self.config.include_ownership and user is not None:
-            return self.gen_user_urn(user)
-        return None
-
-    def gen_user_urn(self, user: str) -> str:
-        if user in self.service_principals:
-            user = self.service_principals[user].display_name
-        return make_user_urn(user)
-
-    def gen_dataset_urn(self, table_ref: TableReference) -> str:
-        return make_dataset_urn_with_platform_instance(
-            platform=self.platform,
-            platform_instance=self.platform_instance_name,
-            name=str(table_ref),
-        )
-
     def gen_schema_containers(self, schema: Schema) -> Iterable[MetadataWorkUnit]:
         domain_urn = self._gen_domain_urn(f"{schema.catalog.name}.{schema.name}")
 
         schema_container_key = self.gen_schema_key(schema)
         yield from gen_containers(
             container_key=schema_container_key,
             name=schema.name,
             sub_types=[DatasetContainerSubTypes.SCHEMA],
             parent_container_key=self.gen_catalog_key(catalog=schema.catalog),
             domain_urn=domain_urn,
             description=schema.comment,
-            owner_urn=self.get_owner_urn(schema.owner),
         )
 
     def gen_metastore_containers(
         self, metastore: Metastore
     ) -> Iterable[MetadataWorkUnit]:
         domain_urn = self._gen_domain_urn(metastore.name)
 
         metastore_container_key = self.gen_metastore_key(metastore)
+
         yield from gen_containers(
             container_key=metastore_container_key,
             name=metastore.name,
             sub_types=[DatasetContainerSubTypes.DATABRICKS_METASTORE],
             domain_urn=domain_urn,
             description=metastore.comment,
-            owner_urn=self.get_owner_urn(metastore.owner),
         )
 
     def gen_catalog_containers(self, catalog: Catalog) -> Iterable[MetadataWorkUnit]:
         domain_urn = self._gen_domain_urn(catalog.name)
 
         metastore_container_key = self.gen_metastore_key(catalog.metastore)
+
         catalog_container_key = self.gen_catalog_key(catalog)
+
         yield from gen_containers(
             container_key=catalog_container_key,
             name=catalog.name,
             sub_types=[DatasetContainerSubTypes.PRESTO_CATALOG],
             domain_urn=domain_urn,
             parent_container_key=metastore_container_key,
             description=catalog.comment,
-            owner_urn=self.get_owner_urn(catalog.owner),
         )
 
     def gen_schema_key(self, schema: Schema) -> PlatformKey:
         return UnitySchemaKey(
             unity_schema=schema.name,
             platform=self.platform,
             instance=self.config.platform_instance,
@@ -526,109 +435,71 @@
     ) -> Iterable[MetadataWorkUnit]:
         schema_container_key = self.gen_schema_key(schema)
         yield from add_dataset_to_container(
             container_key=schema_container_key,
             dataset_urn=dataset_urn,
         )
 
-    def _create_table_property_aspect(self, table: Table) -> DatasetPropertiesClass:
+    def _create_table_property_aspect(
+        self, table: proxy.Table
+    ) -> DatasetPropertiesClass:
         custom_properties: dict = {}
         if table.storage_location is not None:
             custom_properties["storage_location"] = table.storage_location
         if table.data_source_format is not None:
             custom_properties["data_source_format"] = table.data_source_format
 
         custom_properties["generation"] = str(table.generation)
         custom_properties["table_type"] = table.table_type
 
         custom_properties["created_by"] = table.created_by
         custom_properties["created_at"] = str(table.created_at)
         if table.properties:
-            custom_properties.update({k: str(v) for k, v in table.properties.items()})
+            custom_properties["properties"] = str(table.properties)
         custom_properties["table_id"] = table.table_id
         custom_properties["owner"] = table.owner
         custom_properties["updated_by"] = table.updated_by
         custom_properties["updated_at"] = str(table.updated_at)
 
-        created = TimeStampClass(
-            int(table.created_at.timestamp() * 1000), make_user_urn(table.created_by)
-        )
-        last_modified = created
-        if table.updated_at and table.updated_by is not None:
-            last_modified = TimeStampClass(
-                int(table.updated_at.timestamp() * 1000),
-                make_user_urn(table.updated_by),
-            )
-
         return DatasetPropertiesClass(
             name=table.name,
             description=table.comment,
             customProperties=custom_properties,
-            created=created,
-            lastModified=last_modified,
-        )
-
-    def _create_table_operation_aspect(self, table: Table) -> OperationClass:
-        """Produce an operation aspect for a table.
-
-        If a last updated time is present, we produce an update operation.
-        Otherwise, we produce a create operation. We do this in addition to
-        setting the last updated time in the dataset properties aspect, as
-        the UI is currently missing the ability to display the last updated
-        from the properties aspect.
-        """
-
-        reported_time = int(time.time() * 1000)
-
-        operation = OperationClass(
-            timestampMillis=reported_time,
-            lastUpdatedTimestamp=int(table.created_at.timestamp() * 1000),
-            actor=make_user_urn(table.created_by),
-            operationType=OperationTypeClass.CREATE,
         )
 
-        if table.updated_at and table.updated_by is not None:
-            operation = OperationClass(
-                timestampMillis=reported_time,
-                lastUpdatedTimestamp=int(table.updated_at.timestamp() * 1000),
-                actor=make_user_urn(table.updated_by),
-                operationType=OperationTypeClass.UPDATE,
-            )
-
-        return operation
-
-    def _create_table_ownership_aspect(self, table: Table) -> Optional[OwnershipClass]:
-        owner_urn = self.get_owner_urn(table.owner)
-        if owner_urn is not None:
+    def _create_table_ownership_aspect(
+        self, table: proxy.Table
+    ) -> Optional[OwnershipClass]:
+        if self.config.include_table_ownership and table.owner:
             return OwnershipClass(
                 owners=[
                     OwnerClass(
-                        owner=owner_urn,
+                        owner=make_user_urn(table.owner),
                         type=OwnershipTypeClass.DATAOWNER,
                     )
                 ]
             )
         return None
 
-    def _create_table_sub_type_aspect(self, table: Table) -> SubTypesClass:
+    def _create_table_sub_type_aspect(self, table: proxy.Table) -> SubTypesClass:
         return SubTypesClass(
             typeNames=[
                 DatasetSubTypes.VIEW
                 if table.table_type.lower() == "view"
                 else DatasetSubTypes.TABLE
             ]
         )
 
-    def _create_view_property_aspect(self, table: Table) -> ViewProperties:
+    def _create_view_property_aspect(self, table: proxy.Table) -> ViewProperties:
         assert table.view_definition
         return ViewProperties(
             materialized=False, viewLanguage="SQL", viewLogic=table.view_definition
         )
 
-    def _create_schema_metadata_aspect(self, table: Table) -> SchemaMetadataClass:
+    def _create_schema_metadata_aspect(self, table: proxy.Table) -> SchemaMetadataClass:
         schema_fields: List[SchemaFieldClass] = []
 
         for column in table.columns:
             schema_fields.extend(self._create_schema_field(column))
 
         return SchemaMetadataClass(
             schemaName=table.id,
@@ -636,15 +507,15 @@
             fields=schema_fields,
             hash="",
             version=0,
             platformSchema=MySqlDDLClass(tableSchema=""),
         )
 
     @staticmethod
-    def _create_schema_field(column: Column) -> List[SchemaFieldClass]:
+    def _create_schema_field(column: proxy.Column) -> List[SchemaFieldClass]:
         _COMPLEX_TYPE = re.compile("^(struct|array)")
 
         if _COMPLEX_TYPE.match(column.type_text.lower()):
             return get_schema_fields_for_hive_column(
                 column.name, column.type_text.lower(), description=column.comment
             )
         else:
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/usage/clickhouse_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/clickhouse_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/usage/redshift_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/redshift_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source/usage/starburst_trino_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/starburst_trino_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/bigquery.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/bigquery.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/csv_enricher.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/csv_enricher.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/sql/snowflake.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/sql/snowflake.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/usage/bigquery_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/bigquery_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_config/usage/snowflake_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/snowflake_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/pulsar.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/pulsar.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/sql/bigquery.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/bigquery.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/sql/snowflake.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/snowflake.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/usage/bigquery_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/bigquery_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/source_report/usage/snowflake_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/snowflake_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_browse_path.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_browse_path.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_ownership.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_ownership.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_properties.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_properties.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_schema_tags.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_schema_tags.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_schema_terms.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_schema_terms.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_tags.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_tags.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/add_dataset_terms.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_terms.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/base_transformer.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/base_transformer.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/dataset_domain.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/dataset_domain.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/dataset_transformer.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/dataset_transformer.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/mark_dataset_status.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/mark_dataset_status.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/ingestion/transformer/remove_dataset_ownership.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/remove_dataset_ownership.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/integrations/great_expectations/action.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/great_expectations/action.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/lite/duckdb_lite.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/lite/duckdb_lite.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/lite/lite_local.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_local.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/lite/lite_server.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_server.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/lite/lite_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/chart/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/chart/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/common/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/common/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,17 +3,15 @@
 # This file is autogenerated by /metadata-ingestion/scripts/avro_codegen.py
 # Do not modify manually!
 
 # pylint: skip-file
 # fmt: off
 from .....schema_classes import AccessLevelClass
 from .....schema_classes import AuditStampClass
-from .....schema_classes import BrowsePathEntryClass
 from .....schema_classes import BrowsePathsClass
-from .....schema_classes import BrowsePathsV2Class
 from .....schema_classes import ChangeAuditStampsClass
 from .....schema_classes import CostClass
 from .....schema_classes import CostCostClass
 from .....schema_classes import CostCostDiscriminatorClass
 from .....schema_classes import CostTypeClass
 from .....schema_classes import DataPlatformInstanceClass
 from .....schema_classes import DeprecationClass
@@ -47,17 +45,15 @@
 from .....schema_classes import TimeStampClass
 from .....schema_classes import VersionTagClass
 from .....schema_classes import WindowDurationClass
 
 
 AccessLevel = AccessLevelClass
 AuditStamp = AuditStampClass
-BrowsePathEntry = BrowsePathEntryClass
 BrowsePaths = BrowsePathsClass
-BrowsePathsV2 = BrowsePathsV2Class
 ChangeAuditStamps = ChangeAuditStampsClass
 Cost = CostClass
 CostCost = CostCostClass
 CostCostDiscriminator = CostCostDiscriminatorClass
 CostType = CostTypeClass
 DataPlatformInstance = DataPlatformInstanceClass
 Deprecation = DeprecationClass
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/execution/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/execution/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/identity/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/identity/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/policy/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/policy/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/query/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/query/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/retention/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/retention/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/schema/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/schema/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/test/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/test/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/com/linkedin/pegasus2avro/usage/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/usage/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schema.avsc` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schema.avsc`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.46480000971124785%*

 * *Differences: {'1': "{'Aspect': {'name': 'editableDataJobProperties'}, 'name': 'EditableDataJobProperties', "*

 * *      "'namespace': 'com.linkedin.pegasus2avro.datajob', 'fields': {2: {'type': {insert: [(1, "*

 * *      "'com.linkedin.pegasus2avro.common.AuditStamp')], delete: [1]}, 'name': 'deleted', 'doc': "*

 * *      "'An AuditStamp corresponding to the deletion of this resource/association/sub-resource. "*

 * *      'Logically, deleted MUST have a later timestamp than creation. It may or may not have the '*

 * *      'same time as lastMod []*

```diff
@@ -1,904 +1,690 @@
 [
     "null",
     {
         "Aspect": {
-            "name": "globalSettingsInfo"
-        },
-        "doc": "DataHub Global platform settings. Careful - these should not be modified by the outside world!",
-        "fields": [
-            {
-                "default": null,
-                "doc": "Settings related to the Views Feature",
-                "name": "views",
-                "type": [
-                    "null",
-                    {
-                        "doc": "Settings for DataHub Views feature.",
-                        "fields": [
-                            {
-                                "Urn": "Urn",
-                                "default": null,
-                                "doc": "The default View for the instance, or organization.",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "defaultView",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            }
-                        ],
-                        "name": "GlobalViewsSettings",
-                        "namespace": "com.linkedin.pegasus2avro.settings.global",
-                        "type": "record"
-                    }
-                ]
-            }
-        ],
-        "name": "GlobalSettingsInfo",
-        "namespace": "com.linkedin.pegasus2avro.settings.global",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataHubAccessTokenInfo"
+            "name": "editableDataJobProperties"
         },
-        "doc": "Information about a DataHub Access Token",
+        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
         "fields": [
             {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "User defined name for the access token if defined.",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "URN"
-                },
-                "Urn": "Urn",
-                "doc": "Urn of the actor to which this access token belongs to.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "actorUrn",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "URN"
-                },
-                "Urn": "Urn",
-                "doc": "Urn of the actor which created this access token.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
                 },
-                "name": "ownerUrn",
-                "type": "string"
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                "name": "created",
+                "type": {
+                    "doc": "Data captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into a particular lifecycle stage, and who acted to move it into that specific lifecycle stage.",
+                    "fields": [
+                        {
+                            "doc": "When did the resource/association/sub-resource move into the specific lifecycle stage represented by this AuditEvent.",
+                            "name": "time",
+                            "type": "long"
+                        },
+                        {
+                            "Urn": "Urn",
+                            "doc": "The entity (e.g. a member URN) which will be credited for moving the resource/association/sub-resource into the specific lifecycle stage. It is also the one used to authorize the change.",
+                            "java": {
+                                "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                            },
+                            "name": "actor",
+                            "type": "string"
+                        },
+                        {
+                            "Urn": "Urn",
+                            "default": null,
+                            "doc": "The entity (e.g. a service URN) which performs the change on behalf of the Actor and must be authorized to act as the Actor.",
+                            "java": {
+                                "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                            },
+                            "name": "impersonator",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": "Additional context around how DataHub was informed of the particular change. For example: was the change created by an automated process, or manually.",
+                            "name": "message",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        }
+                    ],
+                    "name": "AuditStamp",
+                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "type": "record"
+                }
             },
             {
-                "Searchable": {
-                    "fieldType": "COUNT",
-                    "queryByDefault": false
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
                 },
-                "doc": "When the token was created.",
-                "name": "createdAt",
-                "type": "long"
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             },
             {
-                "Searchable": {
-                    "fieldType": "COUNT",
-                    "queryByDefault": false
-                },
                 "default": null,
-                "doc": "When the token expires.",
-                "name": "expiresAt",
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
                 "type": [
                     "null",
-                    "long"
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
                 ]
             },
             {
+                "Searchable": {
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
+                },
                 "default": null,
-                "doc": "Description of the token if defined.",
+                "doc": "Edited documentation of the data job ",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "DataHubAccessTokenInfo",
-        "namespace": "com.linkedin.pegasus2avro.access.token",
+        "name": "EditableDataJobProperties",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "mlHyperParam"
+            "name": "dataFlowInfo"
         },
-        "doc": "Properties associated with an ML Hyper Param",
+        "doc": "Information about a Data processing flow",
         "fields": [
             {
-                "doc": "Name of the MLHyperParam",
-                "name": "name",
-                "type": "string"
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
             },
             {
                 "default": null,
-                "doc": "Documentation of the MLHyperParam",
-                "name": "description",
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "default": null,
-                "doc": "The value of the MLHyperParam",
-                "name": "value",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Flow name",
+                "name": "name",
+                "type": "string"
             },
             {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
                 "default": null,
-                "doc": "Date when the MLHyperParam was developed",
-                "name": "createdAt",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            }
-        ],
-        "name": "MLHyperParam",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelEvaluationData"
-        },
-        "doc": "All referenced datasets would ideally point to any set of documents that provide visibility into the source and composition of the dataset.",
-        "fields": [
-            {
-                "doc": "Details on the dataset(s) used for the quantitative analyses in the MLModel",
-                "name": "evaluationData",
-                "type": {
-                    "items": {
-                        "doc": "BaseData record",
-                        "fields": [
-                            {
-                                "Urn": "DatasetUrn",
-                                "doc": "What dataset were used in the MLModel?",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
-                                },
-                                "name": "dataset",
-                                "type": "string"
-                            },
-                            {
-                                "default": null,
-                                "doc": "Why was this dataset chosen?",
-                                "name": "motivation",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "How was the data preprocessed (e.g., tokenization of sentences, cropping of images, any filtering such as dropping images without faces)?",
-                                "name": "preProcessing",
-                                "type": [
-                                    "null",
-                                    {
-                                        "items": "string",
-                                        "type": "array"
-                                    }
-                                ]
-                            }
-                        ],
-                        "name": "BaseData",
-                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "EvaluationData",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlPrimaryKeyProperties"
-        },
-        "doc": "Properties associated with a MLPrimaryKey",
-        "fields": [
-            {
-                "default": null,
-                "doc": "Documentation of the MLPrimaryKey",
+                "doc": "Flow description",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL",
+                    "queryByDefault": false
+                },
                 "default": null,
-                "doc": "Data Type of the MLPrimaryKey",
-                "name": "dataType",
+                "doc": "Optional project/namespace associated with the flow",
+                "name": "project",
                 "type": [
                     "null",
-                    {
-                        "doc": "MLFeature Data Type",
-                        "name": "MLFeatureDataType",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "symbolDocs": {
-                            "AUDIO": "Audio Data",
-                            "BINARY": "Binary data is discrete data that can be in only one of two categories - either yes or no, 1 or 0, off or on, etc",
-                            "BYTE": "Bytes data are binary-encoded values that can represent complex objects.",
-                            "CONTINUOUS": "Continuous data are made of uncountable values, often the result of a measurement such as height, weight, age etc.",
-                            "COUNT": "Count data is discrete whole number data - no negative numbers here.\nCount data often has many small values, such as zero and one.",
-                            "IMAGE": "Image Data",
-                            "INTERVAL": "Interval data has equal spaces between the numbers and does not represent a temporal pattern.\nExamples include percentages, temperatures, and income.",
-                            "MAP": "Mapping Data Type ex: dict, map",
-                            "NOMINAL": "Nominal data is made of discrete values with no numerical relationship between the different categories - mean and median are meaningless.\nAnimal species is one example. For example, pig is not higher than bird and lower than fish.",
-                            "ORDINAL": "Ordinal data are discrete integers that can be ranked or sorted.\nFor example, the distance between first and second may not be the same as the distance between second and third.",
-                            "SEQUENCE": "Sequence Data Type ex: list, tuple, range",
-                            "SET": "Set Data Type ex: set, frozenset",
-                            "TEXT": "Text Data",
-                            "TIME": "Time data is a cyclical, repeating continuous form of data.\nThe relevant time features can be any period- daily, weekly, monthly, annual, etc.",
-                            "UNKNOWN": "Unknown data are data that we don't know the type for.",
-                            "USELESS": "Useless data is unique, discrete data with no potential relationship with the outcome variable.\nA useless feature has high cardinality. An example would be bank account numbers that were generated randomly.",
-                            "VIDEO": "Video Data"
-                        },
-                        "symbols": [
-                            "USELESS",
-                            "NOMINAL",
-                            "ORDINAL",
-                            "BINARY",
-                            "COUNT",
-                            "TIME",
-                            "INTERVAL",
-                            "IMAGE",
-                            "VIDEO",
-                            "AUDIO",
-                            "TEXT",
-                            "MAP",
-                            "SEQUENCE",
-                            "SET",
-                            "CONTINUOUS",
-                            "BYTE",
-                            "UNKNOWN"
-                        ],
-                        "type": "enum"
-                    }
+                    "string"
                 ]
             },
             {
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "createdAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
                 "default": null,
-                "doc": "Version of the MLPrimaryKey",
-                "name": "version",
+                "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
+                "name": "created",
                 "type": [
                     "null",
                     {
-                        "doc": "A resource-defined string representing the resource state for the purpose of concurrency control",
+                        "doc": "A standard event timestamp",
                         "fields": [
                             {
+                                "doc": "When did the event occur",
+                                "name": "time",
+                                "type": "long"
+                            },
+                            {
+                                "Urn": "Urn",
                                 "default": null,
-                                "name": "versionTag",
+                                "doc": "Optional: The actor urn involved in the event.",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "actor",
                                 "type": [
                                     "null",
                                     "string"
                                 ]
                             }
                         ],
-                        "name": "VersionTag",
+                        "name": "TimeStamp",
                         "namespace": "com.linkedin.pegasus2avro.common",
                         "type": "record"
                     }
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "DerivedFrom"
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "lastModifiedAt",
+                        "fieldType": "DATETIME"
                     }
                 },
-                "Urn": "Urn",
-                "doc": "Source of the MLPrimaryKey",
-                "name": "sources",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                },
-                "urn_is_array": true
-            }
-        ],
-        "name": "MLPrimaryKeyProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlMetric"
-        },
-        "doc": "Properties associated with an ML Metric",
-        "fields": [
-            {
-                "doc": "Name of the mlMetric",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "default": null,
-                "doc": "Documentation of the mlMetric",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
                 "default": null,
-                "doc": "The value of the mlMetric",
-                "name": "value",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Date when the mlMetric was developed",
-                "name": "createdAt",
+                "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
+                "name": "lastModified",
                 "type": [
                     "null",
-                    "long"
+                    "com.linkedin.pegasus2avro.common.TimeStamp"
                 ]
             }
         ],
-        "name": "MLMetric",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "name": "DataFlowInfo",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableMlFeatureProperties"
+            "name": "editableDataFlowProperties"
         },
-        "doc": "Properties associated with a MLFeature editable from the UI",
+        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
         "fields": [
             {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
                 },
-                "default": null,
-                "doc": "Documentation of the MLFeature",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "EditableMLFeatureProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelMetrics"
-        },
-        "doc": "Metrics to be featured for the MLModel.",
-        "fields": [
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
             {
-                "default": null,
-                "doc": "Measures of MLModel performance",
-                "name": "performanceMeasures",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             },
             {
                 "default": null,
-                "doc": "Decision Thresholds used (if any)?",
-                "name": "decisionThreshold",
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
                 ]
-            }
-        ],
-        "name": "Metrics",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "editableMlFeatureTableProperties"
-        },
-        "doc": "Properties associated with a MLFeatureTable editable from the ui",
-        "fields": [
+            },
             {
                 "Searchable": {
                     "fieldName": "editedDescription",
                     "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "Documentation of the MLFeatureTable",
+                "doc": "Edited documentation of the data flow",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "EditableMLFeatureTableProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "name": "EditableDataFlowProperties",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "sourceCode"
+            "name": "dataJobInputOutput"
         },
-        "doc": "Source Code",
+        "doc": "Information about the inputs and outputs of a Data processing job",
         "fields": [
             {
-                "doc": "Source Code along with types",
-                "name": "sourceCode",
-                "type": {
-                    "items": {
-                        "doc": "Source Code Url Entity",
-                        "fields": [
-                            {
-                                "doc": "Source Code Url Types",
-                                "name": "type",
-                                "type": {
-                                    "name": "SourceCodeUrlType",
-                                    "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                                    "symbols": [
-                                        "ML_MODEL_SOURCE_CODE",
-                                        "TRAINING_PIPELINE_SOURCE_CODE",
-                                        "EVALUATION_PIPELINE_SOURCE_CODE"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "doc": "Source Code Url",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                                },
-                                "name": "sourceCodeUrl",
-                                "type": "string"
-                            }
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
                         ],
-                        "name": "SourceCodeUrl",
-                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "SourceCode",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "intendedUse"
-        },
-        "doc": "Intended Use for the ML Model",
-        "fields": [
-            {
-                "default": null,
-                "doc": "Primary Use cases for the MLModel.",
-                "name": "primaryUses",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Primary Intended Users - For example, was the MLModel developed for entertainment purposes, for hobbyists, or enterprise solutions?",
-                "name": "primaryUsers",
-                "type": [
-                    "null",
-                    {
-                        "items": {
-                            "name": "IntendedUserType",
-                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                            "symbols": [
-                                "ENTERPRISE",
-                                "HOBBY",
-                                "ENTERTAINMENT"
-                            ],
-                            "type": "enum"
-                        },
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Highlight technology that the MLModel might easily be confused with, or related contexts that users could try to apply the MLModel to.",
-                "name": "outOfScopeUses",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
+                        "isLineage": true,
+                        "name": "Consumes"
                     }
-                ]
-            }
-        ],
-        "name": "IntendedUse",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "editableMlPrimaryKeyProperties"
-        },
-        "doc": "Properties associated with a MLPrimaryKey editable from the UI",
-        "fields": [
-            {
-                "default": null,
-                "doc": "Documentation of the MLPrimaryKey",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "EditableMLPrimaryKeyProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelProperties"
-        },
-        "doc": "Properties associated with a ML Model",
-        "fields": [
-            {
+                },
                 "Searchable": {
                     "/*": {
-                        "queryByDefault": true
+                        "fieldName": "inputs",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numInputDatasets",
+                        "queryByDefault": false
                     }
                 },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
+                "Urn": "DatasetUrn",
+                "deprecated": true,
+                "doc": "Input datasets consumed by the data job during processing\nDeprecated! Use inputDatasetEdges instead.",
+                "name": "inputDatasets",
                 "type": {
-                    "type": "map",
-                    "values": "string"
-                }
-            },
-            {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
-                "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                    "items": "string",
+                    "type": "array"
                 },
-                "name": "externalUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "urn_is_array": true
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
+                "Relationship": {
+                    "/*/destinationUrn": {
+                        "createdActor": "inputDatasetEdges/*/created/actor",
+                        "createdOn": "inputDatasetEdges/*/created/time",
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "name": "Consumes",
+                        "properties": "inputDatasetEdges/*/properties",
+                        "updatedActor": "inputDatasetEdges/*/lastModified/actor",
+                        "updatedOn": "inputDatasetEdges/*/lastModified/time"
+                    }
                 },
-                "default": null,
-                "doc": "Documentation of the MLModel",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Date when the MLModel was developed",
-                "name": "date",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Version of the MLModel",
-                "name": "version",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.VersionTag"
-                ]
-            },
-            {
                 "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "default": null,
-                "doc": "Type of Algorithm or MLModel such as whether it is a Naive Bayes classifier, Convolutional Neural Network, etc",
-                "name": "type",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Hyper Parameters of the MLModel\n\nNOTE: these are deprecated in favor of hyperParams",
-                "name": "hyperParameters",
-                "type": [
-                    "null",
-                    {
-                        "type": "map",
-                        "values": [
-                            "string",
-                            "int",
-                            "float",
-                            "double",
-                            "boolean"
-                        ]
+                    "/*/destinationUrn": {
+                        "fieldName": "inputDatasetEdges",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numInputDatasets",
+                        "queryByDefault": false
                     }
-                ]
-            },
-            {
+                },
                 "default": null,
-                "doc": "Hyperparameters of the MLModel",
-                "name": "hyperParams",
+                "doc": "Input datasets consumed by the data job during processing",
+                "name": "inputDatasetEdges",
                 "type": [
                     "null",
                     {
-                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLHyperParam",
+                        "items": {
+                            "doc": "Information about a relatonship edge.",
+                            "fields": [
+                                {
+                                    "Urn": "Urn",
+                                    "doc": "Urn of the source of this relationship edge.",
+                                    "java": {
+                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                    },
+                                    "name": "sourceUrn",
+                                    "type": "string"
+                                },
+                                {
+                                    "Urn": "Urn",
+                                    "doc": "Urn of the destination of this relationship edge.",
+                                    "java": {
+                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                    },
+                                    "name": "destinationUrn",
+                                    "type": "string"
+                                },
+                                {
+                                    "doc": "Audit stamp containing who created this relationship edge and when",
+                                    "name": "created",
+                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                                },
+                                {
+                                    "doc": "Audit stamp containing who last modified this relationship edge and when",
+                                    "name": "lastModified",
+                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                                },
+                                {
+                                    "default": null,
+                                    "doc": "A generic properties bag that allows us to store specific information on this graph edge.",
+                                    "name": "properties",
+                                    "type": [
+                                        "null",
+                                        {
+                                            "type": "map",
+                                            "values": "string"
+                                        }
+                                    ]
+                                }
+                            ],
+                            "name": "Edge",
+                            "namespace": "com.linkedin.pegasus2avro.common",
+                            "type": "record"
+                        },
                         "type": "array"
                     }
                 ]
             },
             {
-                "default": null,
-                "doc": "Metrics of the MLModel used in training",
-                "name": "trainingMetrics",
-                "type": [
-                    "null",
-                    {
-                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLMetric",
-                        "type": "array"
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "isUpstream": false,
+                        "name": "Produces"
                     }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Metrics of the MLModel used in production",
-                "name": "onlineMetrics",
-                "type": [
-                    "null",
-                    {
-                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLMetric",
-                        "type": "array"
+                },
+                "Searchable": {
+                    "/*": {
+                        "fieldName": "outputs",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numOutputDatasets",
+                        "queryByDefault": false
                     }
-                ]
+                },
+                "Urn": "DatasetUrn",
+                "deprecated": true,
+                "doc": "Output datasets produced by the data job during processing\nDeprecated! Use outputDatasetEdges instead.",
+                "name": "outputDatasets",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             },
             {
                 "Relationship": {
-                    "/*": {
+                    "/*/destinationUrn": {
+                        "createdActor": "outputDatasetEdges/*/created/actor",
+                        "createdOn": "outputDatasetEdges/*/created/time",
                         "entityTypes": [
-                            "mlFeature"
+                            "dataset"
                         ],
                         "isLineage": true,
-                        "name": "Consumes"
+                        "isUpstream": false,
+                        "name": "Produces",
+                        "properties": "outputDatasetEdges/*/properties",
+                        "updatedActor": "outputDatasetEdges/*/lastModified/actor",
+                        "updatedOn": "outputDatasetEdges/*/lastModified/time"
+                    }
+                },
+                "Searchable": {
+                    "/*/destinationUrn": {
+                        "fieldName": "outputDatasetEdges",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numOutputDatasets",
+                        "queryByDefault": false
                     }
                 },
-                "Urn": "MLFeatureUrn",
                 "default": null,
-                "doc": "List of features used for MLModel training",
-                "name": "mlFeatures",
+                "doc": "Output datasets produced by the data job during processing",
+                "name": "outputDatasetEdges",
                 "type": [
                     "null",
                     {
-                        "items": "string",
+                        "items": "com.linkedin.pegasus2avro.common.Edge",
                         "type": "array"
                     }
-                ],
-                "urn_is_array": true
-            },
-            {
-                "default": [],
-                "doc": "Tags for the MLModel",
-                "name": "tags",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
+                ]
             },
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
-                            "mlModelDeployment"
+                            "dataJob"
                         ],
-                        "name": "DeployedTo"
+                        "isLineage": true,
+                        "name": "DownstreamOf"
                     }
                 },
-                "Urn": "Urn",
+                "Urn": "DataJobUrn",
                 "default": null,
-                "doc": "Deployments for the MLModel",
-                "name": "deployments",
+                "deprecated": true,
+                "doc": "Input datajobs that this data job depends on\nDeprecated! Use inputDatajobEdges instead.",
+                "name": "inputDatajobs",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
                 ],
                 "urn_is_array": true
             },
             {
                 "Relationship": {
-                    "/*": {
+                    "/*/destinationUrn": {
+                        "createdActor": "inputDatajobEdges/*/created/actor",
+                        "createdOn": "inputDatajobEdges/*/created/time",
                         "entityTypes": [
                             "dataJob"
                         ],
                         "isLineage": true,
-                        "name": "TrainedBy"
+                        "name": "DownstreamOf",
+                        "properties": "inputDatajobEdges/*/properties",
+                        "updatedActor": "inputDatajobEdges/*/lastModified/actor",
+                        "updatedOn": "inputDatajobEdges/*/lastModified/time"
                     }
                 },
-                "Urn": "Urn",
                 "default": null,
-                "doc": "List of jobs (if any) used to train the model",
-                "name": "trainingJobs",
+                "doc": "Input datajobs that this data job depends on",
+                "name": "inputDatajobEdges",
                 "type": [
                     "null",
                     {
-                        "items": "string",
+                        "items": "com.linkedin.pegasus2avro.common.Edge",
                         "type": "array"
                     }
-                ],
-                "urn_is_array": true
+                ]
             },
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
-                            "dataJob"
+                            "schemaField"
                         ],
-                        "isLineage": true,
-                        "isUpstream": false,
-                        "name": "UsedBy"
+                        "name": "Consumes"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "fieldName": "inputFields",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numInputFields",
+                        "queryByDefault": false
                     }
                 },
                 "Urn": "Urn",
                 "default": null,
-                "doc": "List of jobs (if any) that use the model",
-                "name": "downstreamJobs",
+                "doc": "Fields of the input datasets used by this job",
+                "name": "inputDatasetFields",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
                 ],
                 "urn_is_array": true
             },
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
-                            "mlModelGroup"
+                            "schemaField"
                         ],
-                        "isLineage": true,
-                        "isUpstream": false,
-                        "name": "MemberOf"
+                        "name": "Produces"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "fieldName": "outputFields",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numOutputFields",
+                        "queryByDefault": false
                     }
                 },
                 "Urn": "Urn",
                 "default": null,
-                "doc": "Groups the model belongs to",
-                "name": "groups",
+                "doc": "Fields of the output datasets this job writes to",
+                "name": "outputDatasetFields",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
                 ],
                 "urn_is_array": true
-            }
-        ],
-        "name": "MLModelProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelTrainingData"
-        },
-        "doc": "Ideally, the MLModel card would contain as much information about the training data as the evaluation data. However, there might be cases where it is not feasible to provide this level of detailed information about the training data. For example, the data may be proprietary, or require a non-disclosure agreement. In these cases, we advocate for basic details about the distributions over groups in the data, as well as any other details that could inform stakeholders on the kinds of biases the model may have encoded.",
-        "fields": [
-            {
-                "doc": "Details on the dataset(s) used for training the MLModel",
-                "name": "trainingData",
-                "type": {
-                    "items": "com.linkedin.pegasus2avro.ml.metadata.BaseData",
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "TrainingData",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelQuantitativeAnalyses"
-        },
-        "doc": "Quantitative analyses should be disaggregated, that is, broken down by the chosen factors. Quantitative analyses should provide the results of evaluating the MLModel according to the chosen metrics, providing confidence interval values when possible.",
-        "fields": [
-            {
-                "default": null,
-                "doc": "Link to a dashboard with results showing how the MLModel performed with respect to each factor",
-                "name": "unitaryResults",
-                "type": [
-                    "null",
-                    "string"
-                ]
             },
             {
                 "default": null,
-                "doc": "Link to a dashboard with results showing how the MLModel performed with respect to the intersection of evaluated factors?",
-                "name": "intersectionalResults",
+                "doc": "Fine-grained column-level lineages",
+                "name": "fineGrainedLineages",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": {
+                            "doc": "A fine-grained lineage from upstream fields/datasets to downstream field(s)",
+                            "fields": [
+                                {
+                                    "doc": "The type of upstream entity",
+                                    "name": "upstreamType",
+                                    "type": {
+                                        "doc": "The type of upstream entity in a fine-grained lineage",
+                                        "name": "FineGrainedLineageUpstreamType",
+                                        "namespace": "com.linkedin.pegasus2avro.dataset",
+                                        "symbolDocs": {
+                                            "DATASET": " Indicates that this lineage is originating from upstream dataset(s)",
+                                            "FIELD_SET": " Indicates that this lineage is originating from upstream field(s)",
+                                            "NONE": " Indicates that there is no upstream lineage i.e. the downstream field is not a derived field"
+                                        },
+                                        "symbols": [
+                                            "FIELD_SET",
+                                            "DATASET",
+                                            "NONE"
+                                        ],
+                                        "type": "enum"
+                                    }
+                                },
+                                {
+                                    "Urn": "Urn",
+                                    "default": null,
+                                    "doc": "Upstream entities in the lineage",
+                                    "name": "upstreams",
+                                    "type": [
+                                        "null",
+                                        {
+                                            "items": "string",
+                                            "type": "array"
+                                        }
+                                    ],
+                                    "urn_is_array": true
+                                },
+                                {
+                                    "doc": "The type of downstream field(s)",
+                                    "name": "downstreamType",
+                                    "type": {
+                                        "doc": "The type of downstream field(s) in a fine-grained lineage",
+                                        "name": "FineGrainedLineageDownstreamType",
+                                        "namespace": "com.linkedin.pegasus2avro.dataset",
+                                        "symbolDocs": {
+                                            "FIELD": " Indicates that the lineage is for a single, specific, downstream field",
+                                            "FIELD_SET": " Indicates that the lineage is for a set of downstream fields"
+                                        },
+                                        "symbols": [
+                                            "FIELD",
+                                            "FIELD_SET"
+                                        ],
+                                        "type": "enum"
+                                    }
+                                },
+                                {
+                                    "Urn": "Urn",
+                                    "default": null,
+                                    "doc": "Downstream fields in the lineage",
+                                    "name": "downstreams",
+                                    "type": [
+                                        "null",
+                                        {
+                                            "items": "string",
+                                            "type": "array"
+                                        }
+                                    ],
+                                    "urn_is_array": true
+                                },
+                                {
+                                    "default": null,
+                                    "doc": "The transform operation applied to the upstream entities to produce the downstream field(s)",
+                                    "name": "transformOperation",
+                                    "type": [
+                                        "null",
+                                        "string"
+                                    ]
+                                },
+                                {
+                                    "default": 1.0,
+                                    "doc": "The confidence in this lineage between 0 (low confidence) and 1 (high confidence)",
+                                    "name": "confidenceScore",
+                                    "type": "float"
+                                }
+                            ],
+                            "name": "FineGrainedLineage",
+                            "namespace": "com.linkedin.pegasus2avro.dataset",
+                            "type": "record"
+                        },
+                        "type": "array"
+                    }
                 ]
             }
         ],
-        "name": "QuantitativeAnalyses",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "name": "DataJobInputOutput",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "mlModelDeploymentProperties"
+            "name": "dataJobInfo"
         },
-        "doc": "Properties associated with an ML Model Deployment",
+        "doc": "Information about a Data processing job",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -907,1879 +693,2178 @@
                 "name": "customProperties",
                 "type": {
                     "type": "map",
                     "values": "string"
                 }
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
                 "default": null,
                 "doc": "URL where the reference exist",
                 "java": {
                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                 },
                 "name": "externalUrl",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Job name",
+                "name": "name",
+                "type": "string"
+            },
+            {
+                "Searchable": {
                     "fieldType": "TEXT",
                     "hasValuesFieldName": "hasDescription"
                 },
                 "default": null,
-                "doc": "Documentation of the MLModelDeployment",
+                "doc": "Job description",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "doc": "Datajob type\n*NOTE**: AzkabanJobType is deprecated. Please use strings instead.",
+                "name": "type",
+                "type": [
+                    {
+                        "doc": "The various types of support azkaban jobs",
+                        "name": "AzkabanJobType",
+                        "namespace": "com.linkedin.pegasus2avro.datajob.azkaban",
+                        "symbolDocs": {
+                            "COMMAND": "The command job type is one of the basic built-in types. It runs multiple UNIX commands using java processbuilder.\nUpon execution, Azkaban spawns off a process to run the command.",
+                            "GLUE": "Glue type is for running AWS Glue job transforms.",
+                            "HADOOP_JAVA": "Runs a java program with ability to access Hadoop cluster.\nhttps://azkaban.readthedocs.io/en/latest/jobTypes.html#java-job-type",
+                            "HADOOP_SHELL": "In large part, this is the same Command type. The difference is its ability to talk to a Hadoop cluster\nsecurely, via Hadoop tokens.",
+                            "HIVE": "Hive type is for running Hive jobs.",
+                            "PIG": "Pig type is for running Pig jobs.",
+                            "SQL": "SQL is for running Presto, mysql queries etc"
+                        },
+                        "symbols": [
+                            "COMMAND",
+                            "HADOOP_JAVA",
+                            "HADOOP_SHELL",
+                            "HIVE",
+                            "PIG",
+                            "SQL",
+                            "GLUE"
+                        ],
+                        "type": "enum"
+                    },
+                    "string"
+                ]
+            },
+            {
+                "Urn": "DataFlowUrn",
+                "default": null,
+                "doc": "DataFlow urn that this job is part of",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.DataFlowUrn"
+                },
+                "name": "flowUrn",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "createdAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
                 "default": null,
-                "doc": "Date when the MLModelDeployment was developed",
-                "name": "createdAt",
+                "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
+                "name": "created",
                 "type": [
                     "null",
-                    "long"
+                    "com.linkedin.pegasus2avro.common.TimeStamp"
                 ]
             },
             {
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "lastModifiedAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
                 "default": null,
-                "doc": "Version of the MLModelDeployment",
-                "name": "version",
+                "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
+                "name": "lastModified",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.VersionTag"
+                    "com.linkedin.pegasus2avro.common.TimeStamp"
                 ]
             },
             {
                 "default": null,
-                "doc": "Status of the deployment",
+                "deprecated": "Use Data Process Instance model, instead",
+                "doc": "Status of the job - Deprecated for Data Process Instance model.",
                 "name": "status",
                 "type": [
                     "null",
                     {
-                        "doc": "Model endpoint statuses",
-                        "name": "DeploymentStatus",
-                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                        "doc": "Job statuses",
+                        "name": "JobStatus",
+                        "namespace": "com.linkedin.pegasus2avro.datajob",
                         "symbolDocs": {
-                            "CREATING": "Deployments being created.",
-                            "DELETING": "Deployments being deleted.",
-                            "FAILED": "Deployments with an error state.",
-                            "IN_SERVICE": "Deployments that are active.",
-                            "OUT_OF_SERVICE": "Deployments out of service.",
-                            "ROLLING_BACK": "Deployments being reverted to a previous version.",
-                            "UNKNOWN": "Deployments with unknown/unmappable state.",
-                            "UPDATING": "Deployments being updated."
+                            "COMPLETED": "Jobs with successful completion.",
+                            "FAILED": "Jobs that have failed.",
+                            "IN_PROGRESS": "Jobs currently running.",
+                            "SKIPPED": "Jobs that have been skipped.",
+                            "STARTING": "Jobs being initialized.",
+                            "STOPPED": "Jobs that have stopped.",
+                            "STOPPING": "Jobs being stopped.",
+                            "UNKNOWN": "Jobs with unknown status (either unmappable or unavailable)"
                         },
                         "symbols": [
-                            "OUT_OF_SERVICE",
-                            "CREATING",
-                            "UPDATING",
-                            "ROLLING_BACK",
-                            "IN_SERVICE",
-                            "DELETING",
+                            "STARTING",
+                            "IN_PROGRESS",
+                            "STOPPING",
+                            "STOPPED",
+                            "COMPLETED",
                             "FAILED",
-                            "UNKNOWN"
+                            "UNKNOWN",
+                            "SKIPPED"
                         ],
                         "type": "enum"
                     }
                 ]
             }
         ],
-        "name": "MLModelDeploymentProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "name": "DataJobInfo",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "mlModelEthicalConsiderations"
+            "name": "versionInfo"
         },
-        "doc": "This section is intended to demonstrate the ethical considerations that went into MLModel development, surfacing ethical challenges and solutions to stakeholders.",
+        "doc": "Information about a Data processing job",
         "fields": [
             {
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
+            },
+            {
                 "default": null,
-                "doc": "Does the MLModel use any sensitive data (e.g., protected classes)?",
-                "name": "data",
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
+                    "string"
                 ]
             },
             {
+                "doc": "The version which can indentify a job version like a commit hash or md5 hash",
+                "name": "version",
+                "type": "string"
+            },
+            {
+                "doc": "The type of the version like git hash or md5 hash",
+                "name": "versionType",
+                "type": "string"
+            }
+        ],
+        "name": "VersionInfo",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "datahubIngestionRunSummary",
+            "type": "timeseries"
+        },
+        "doc": "Summary of a datahub ingestion run for a given platform.",
+        "fields": [
+            {
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
+            },
+            {
                 "default": null,
-                "doc": " Is the MLModel intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?",
-                "name": "humanLife",
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
                 "type": [
                     "null",
                     {
-                        "items": "string",
-                        "type": "array"
+                        "doc": "Defines the size of a time window.",
+                        "fields": [
+                            {
+                                "doc": "Interval unit such as minute/hour/day etc.",
+                                "name": "unit",
+                                "type": {
+                                    "name": "CalendarInterval",
+                                    "namespace": "com.linkedin.pegasus2avro.timeseries",
+                                    "symbols": [
+                                        "SECOND",
+                                        "MINUTE",
+                                        "HOUR",
+                                        "DAY",
+                                        "WEEK",
+                                        "MONTH",
+                                        "QUARTER",
+                                        "YEAR"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "default": 1,
+                                "doc": "How many units. Defaults to 1.",
+                                "name": "multiple",
+                                "type": "int"
+                            }
+                        ],
+                        "name": "TimeWindowSize",
+                        "namespace": "com.linkedin.pegasus2avro.timeseries",
+                        "type": "record"
                     }
                 ]
             },
             {
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
+                },
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    {
+                        "doc": "Defines how the data is partitioned",
+                        "fields": [
+                            {
+                                "default": "PARTITION",
+                                "name": "type",
+                                "type": {
+                                    "name": "PartitionType",
+                                    "namespace": "com.linkedin.pegasus2avro.timeseries",
+                                    "symbols": [
+                                        "FULL_TABLE",
+                                        "QUERY",
+                                        "PARTITION"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "TimeseriesField": {},
+                                "doc": "String representation of the partition",
+                                "name": "partition",
+                                "type": "string"
+                            },
+                            {
+                                "default": null,
+                                "doc": "Time window of the partition if applicable",
+                                "name": "timePartition",
+                                "type": [
+                                    "null",
+                                    {
+                                        "fields": [
+                                            {
+                                                "doc": "Start time as epoch at UTC.",
+                                                "name": "startTimeMillis",
+                                                "type": "long"
+                                            },
+                                            {
+                                                "doc": "The length of the window.",
+                                                "name": "length",
+                                                "type": "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                                            }
+                                        ],
+                                        "name": "TimeWindow",
+                                        "namespace": "com.linkedin.pegasus2avro.timeseries",
+                                        "type": "record"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "PartitionSpec",
+                        "namespace": "com.linkedin.pegasus2avro.timeseries",
+                        "type": "record"
+                    },
+                    "null"
+                ]
+            },
+            {
                 "default": null,
-                "doc": "What risk mitigation strategies were used during MLModel development?",
-                "name": "mitigations",
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
+                    "string"
                 ]
             },
             {
+                "TimeseriesField": {},
+                "doc": "The name of the pipeline that ran ingestion, a stable unique user provided identifier.\n e.g. my_snowflake1-to-datahub.",
+                "name": "pipelineName",
+                "type": "string"
+            },
+            {
+                "TimeseriesField": {},
+                "doc": "The id of the instance against which the ingestion pipeline ran.\ne.g.: Bigquery project ids, MySQL hostnames etc.",
+                "name": "platformInstanceId",
+                "type": "string"
+            },
+            {
+                "TimeseriesField": {},
+                "doc": "The runId for this pipeline instance.",
+                "name": "runId",
+                "type": "string"
+            },
+            {
+                "TimeseriesField": {},
+                "doc": "Run Status - Succeeded/Skipped/Failed etc.",
+                "name": "runStatus",
+                "type": "com.linkedin.pegasus2avro.datajob.JobStatus"
+            },
+            {
                 "default": null,
-                "doc": "What risks may be present in MLModel usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown.",
-                "name": "risksAndHarms",
+                "doc": "The number of workunits written to sink.",
+                "name": "numWorkUnitsCommitted",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
+                    "long"
                 ]
             },
             {
                 "default": null,
-                "doc": "Are there any known MLModel use cases that are especially fraught? This may connect directly to the intended use section",
-                "name": "useCases",
+                "doc": "The number of workunits that are produced.",
+                "name": "numWorkUnitsCreated",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
+                    "long"
                 ]
-            }
-        ],
-        "name": "EthicalConsiderations",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelFactorPrompts"
-        },
-        "doc": "Prompts which affect the performance of the MLModel",
-        "fields": [
+            },
             {
                 "default": null,
-                "doc": "What are foreseeable salient factors for which MLModel performance may vary, and how were these determined?",
-                "name": "relevantFactors",
+                "doc": "The number of events produced (MCE + MCP).",
+                "name": "numEvents",
                 "type": [
                     "null",
-                    {
-                        "items": {
-                            "doc": "Factors affecting the performance of the MLModel.",
-                            "fields": [
-                                {
-                                    "default": null,
-                                    "doc": "Groups refers to distinct categories with similar characteristics that are present in the evaluation data instances.\nFor human-centric machine learning MLModels, groups are people who share one or multiple characteristics.",
-                                    "name": "groups",
-                                    "type": [
-                                        "null",
-                                        {
-                                            "items": "string",
-                                            "type": "array"
-                                        }
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "doc": "The performance of a MLModel can vary depending on what instruments were used to capture the input to the MLModel.\nFor example, a face detection model may perform differently depending on the camera's hardware and software,\nincluding lens, image stabilization, high dynamic range techniques, and background blurring for portrait mode.",
-                                    "name": "instrumentation",
-                                    "type": [
-                                        "null",
-                                        {
-                                            "items": "string",
-                                            "type": "array"
-                                        }
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "doc": "A further factor affecting MLModel performance is the environment in which it is deployed.",
-                                    "name": "environment",
-                                    "type": [
-                                        "null",
-                                        {
-                                            "items": "string",
-                                            "type": "array"
-                                        }
-                                    ]
-                                }
-                            ],
-                            "name": "MLModelFactors",
-                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                            "type": "record"
-                        },
-                        "type": "array"
-                    }
+                    "long"
                 ]
             },
             {
                 "default": null,
-                "doc": "Which factors are being reported, and why were these chosen?",
-                "name": "evaluationFactors",
+                "doc": "The total number of entities produced (unique entity urns).",
+                "name": "numEntities",
                 "type": [
                     "null",
-                    {
-                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLModelFactors",
-                        "type": "array"
-                    }
+                    "long"
                 ]
-            }
-        ],
-        "name": "MLModelFactorPrompts",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelGroupProperties"
-        },
-        "doc": "Properties associated with an ML Model Group",
-        "fields": [
+            },
             {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
+                "default": null,
+                "doc": "The total number of aspects produced across all entities.",
+                "name": "numAspects",
+                "type": [
+                    "null",
+                    "long"
+                ]
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
-                },
                 "default": null,
-                "doc": "Documentation of the MLModelGroup",
-                "name": "description",
+                "doc": "Total number of source API calls.",
+                "name": "numSourceAPICalls",
                 "type": [
                     "null",
-                    "string"
+                    "long"
                 ]
             },
             {
                 "default": null,
-                "doc": "Date when the MLModelGroup was developed",
-                "name": "createdAt",
+                "doc": "Total latency across all source API calls.",
+                "name": "totalLatencySourceAPICalls",
                 "type": [
                     "null",
                     "long"
                 ]
             },
             {
                 "default": null,
-                "doc": "Version of the MLModelGroup",
-                "name": "version",
+                "doc": "Total number of sink API calls.",
+                "name": "numSinkAPICalls",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.VersionTag"
+                    "long"
                 ]
-            }
-        ],
-        "name": "MLModelGroupProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelCaveatsAndRecommendations"
-        },
-        "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?",
-        "fields": [
+            },
             {
                 "default": null,
-                "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?",
-                "name": "caveats",
+                "doc": "Total latency across all sink API calls.",
+                "name": "totalLatencySinkAPICalls",
                 "type": [
                     "null",
-                    {
-                        "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?",
-                        "fields": [
-                            {
-                                "default": null,
-                                "doc": "Did the results suggest any further testing?",
-                                "name": "needsFurtherTesting",
-                                "type": [
-                                    "null",
-                                    "boolean"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Caveat Description\nFor ex: Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders.",
-                                "name": "caveatDescription",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Relevant groups that were not represented in the evaluation dataset?",
-                                "name": "groupsNotRepresented",
-                                "type": [
-                                    "null",
-                                    {
-                                        "items": "string",
-                                        "type": "array"
-                                    }
-                                ]
-                            }
-                        ],
-                        "name": "CaveatDetails",
-                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                        "type": "record"
-                    }
+                    "long"
                 ]
             },
             {
                 "default": null,
-                "doc": "Recommendations on where this MLModel should be used.",
-                "name": "recommendations",
+                "doc": "Number of warnings generated.",
+                "name": "numWarnings",
                 "type": [
                     "null",
-                    "string"
+                    "long"
                 ]
             },
             {
                 "default": null,
-                "doc": "Ideal characteristics of an evaluation dataset for this MLModel",
-                "name": "idealDatasetCharacteristics",
+                "doc": "Number of errors generated.",
+                "name": "numErrors",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
+                    "long"
                 ]
-            }
-        ],
-        "name": "CaveatsAndRecommendations",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "editableMlModelProperties"
-        },
-        "doc": "Properties associated with a ML Model editable from the UI",
-        "fields": [
+            },
             {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
                 "default": null,
-                "doc": "Documentation of the ml model",
-                "name": "description",
+                "doc": "Number of entities skipped.",
+                "name": "numEntitiesSkipped",
                 "type": [
                     "null",
-                    "string"
+                    "long"
                 ]
-            }
-        ],
-        "name": "EditableMLModelProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "editableMlModelGroupProperties"
-        },
-        "doc": "Properties associated with an ML Model Group editable from the UI",
-        "fields": [
+            },
             {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
                 "default": null,
-                "doc": "Documentation of the ml model group",
-                "name": "description",
+                "doc": "The non-sensitive key-value pairs of the yaml config used as json string.",
+                "name": "config",
                 "type": [
                     "null",
                     "string"
                 ]
-            }
-        ],
-        "name": "EditableMLModelGroupProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlFeatureProperties"
-        },
-        "doc": "Properties associated with a MLFeature",
-        "fields": [
+            },
             {
-                "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
-                },
                 "default": null,
-                "doc": "Documentation of the MLFeature",
-                "name": "description",
+                "doc": "Custom value.",
+                "name": "custom_summary",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "TimeseriesField": {},
                 "default": null,
-                "doc": "Data Type of the MLFeature",
-                "name": "dataType",
+                "doc": "The software version of this ingestion.",
+                "name": "softwareVersion",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.MLFeatureDataType"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Version of the MLFeature",
-                "name": "version",
+                "doc": "The hostname the ingestion pipeline ran on.",
+                "name": "systemHostName",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.VersionTag"
+                    "string"
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "DerivedFrom"
-                    }
-                },
-                "Urn": "Urn",
+                "TimeseriesField": {},
                 "default": null,
-                "doc": "Source of the MLFeature",
-                "name": "sources",
+                "doc": "The os the ingestion pipeline ran on.",
+                "name": "operatingSystemName",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
-            }
-        ],
-        "name": "MLFeatureProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlFeatureTableProperties"
-        },
-        "doc": "Properties associated with a MLFeatureTable",
-        "fields": [
-            {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
+                    "string"
+                ]
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
-                },
                 "default": null,
-                "doc": "Documentation of the MLFeatureTable",
-                "name": "description",
+                "doc": "The number of processors on the host the ingestion pipeline ran on.",
+                "name": "numProcessors",
                 "type": [
                     "null",
-                    "string"
+                    "int"
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "mlFeature"
-                        ],
-                        "name": "Contains"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "features",
-                        "fieldType": "URN"
-                    }
-                },
-                "Urn": "Urn",
                 "default": null,
-                "doc": "List of features contained in the feature table",
-                "name": "mlFeatures",
+                "doc": "The total amount of memory on the host the ingestion pipeline ran on.",
+                "name": "totalMemory",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
+                    "long"
+                ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "mlPrimaryKey"
-                        ],
-                        "name": "KeyedBy"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "primaryKeys",
-                        "fieldType": "URN"
-                    }
-                },
-                "Urn": "Urn",
                 "default": null,
-                "doc": "List of primary keys in the feature table (if multiple, assumed to act as a composite key)",
-                "name": "mlPrimaryKeys",
+                "doc": "The available memory on the host the ingestion pipeline ran on.",
+                "name": "availableMemory",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
+                    "long"
+                ]
             }
         ],
-        "name": "MLFeatureTableProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "name": "DatahubIngestionRunSummary",
+        "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataHubViewInfo"
+            "name": "datahubIngestionCheckpoint",
+            "type": "timeseries"
         },
-        "doc": "Information about a DataHub View. -- TODO: Understand whether an entity type filter is required.",
+        "doc": "Checkpoint of a datahub ingestion run for a given job.",
         "fields": [
             {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
+            },
+            {
+                "default": null,
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                ]
+            },
+            {
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
                 },
-                "doc": "The name of the View",
-                "name": "name",
-                "type": "string"
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
+                    "null"
+                ]
             },
             {
                 "default": null,
-                "doc": "Description of the view",
-                "name": "description",
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "Searchable": {},
-                "doc": "The type of View",
-                "name": "type",
-                "type": {
-                    "name": "DataHubViewType",
-                    "namespace": "com.linkedin.pegasus2avro.view",
-                    "symbolDocs": {
-                        "GLOBAL": "A global view, which all users can see and use.",
-                        "PERSONAL": "A view private for a specific person."
-                    },
-                    "symbols": [
-                        "PERSONAL",
-                        "GLOBAL"
-                    ],
-                    "type": "enum"
-                }
+                "TimeseriesField": {},
+                "doc": "The name of the pipeline that ran ingestion, a stable unique user provided identifier.\n e.g. my_snowflake1-to-datahub.",
+                "name": "pipelineName",
+                "type": "string"
             },
             {
-                "doc": "The view itself",
-                "name": "definition",
+                "TimeseriesField": {},
+                "doc": "The id of the instance against which the ingestion pipeline ran.\ne.g.: Bigquery project ids, MySQL hostnames etc.",
+                "name": "platformInstanceId",
+                "type": "string"
+            },
+            {
+                "doc": "Json-encoded string representation of the non-secret members of the config .",
+                "name": "config",
+                "type": "string"
+            },
+            {
+                "doc": "Opaque blob of the state representation.",
+                "name": "state",
                 "type": {
-                    "doc": "A View definition.",
+                    "doc": "The checkpoint state object of a datahub ingestion run for a given job.",
                     "fields": [
                         {
-                            "doc": "The Entity Types in the scope of the View.",
-                            "name": "entityTypes",
-                            "type": {
-                                "items": "string",
-                                "type": "array"
-                            }
+                            "doc": "The version of the state format.",
+                            "name": "formatVersion",
+                            "type": "string"
                         },
                         {
-                            "doc": "The filter criteria, which represents the view itself",
-                            "name": "filter",
-                            "type": {
-                                "doc": "The filter for finding a record or a collection of records",
-                                "fields": [
+                            "doc": "The serialization/deserialization protocol.",
+                            "name": "serde",
+                            "type": "string"
+                        },
+                        {
+                            "default": null,
+                            "doc": "Opaque blob of the state representation.",
+                            "name": "payload",
+                            "type": [
+                                "null",
+                                "bytes"
+                            ]
+                        }
+                    ],
+                    "name": "IngestionCheckpointState",
+                    "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
+                    "type": "record"
+                }
+            },
+            {
+                "TimeseriesField": {},
+                "doc": "The run identifier of this job.",
+                "name": "runId",
+                "type": "string"
+            }
+        ],
+        "name": "DatahubIngestionCheckpoint",
+        "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "embed"
+        },
+        "doc": "Information regarding rendering an embed for an asset.",
+        "fields": [
+            {
+                "default": null,
+                "doc": "An embed URL to be rendered inside of an iframe.",
+                "name": "renderUrl",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "Embed",
+        "namespace": "com.linkedin.pegasus2avro.common",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "inputFields"
+        },
+        "doc": "Information about the fields a chart or dashboard references",
+        "fields": [
+            {
+                "doc": "List of fields being referenced",
+                "name": "fields",
+                "type": {
+                    "items": {
+                        "doc": "Information about a field a chart or dashboard references",
+                        "fields": [
+                            {
+                                "Relationship": {
+                                    "entityTypes": [
+                                        "schemaField"
+                                    ],
+                                    "name": "consumesField"
+                                },
+                                "Urn": "Urn",
+                                "doc": "Urn of the schema being referenced for lineage purposes",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "schemaFieldUrn",
+                                "type": "string"
+                            },
+                            {
+                                "default": null,
+                                "doc": "Copied version of the referenced schema field object for indexing purposes",
+                                "name": "schemaField",
+                                "type": [
+                                    "null",
                                     {
-                                        "default": null,
-                                        "doc": "A list of disjunctive criterion for the filter. (or operation to combine filters)",
-                                        "name": "or",
-                                        "type": [
-                                            "null",
+                                        "doc": "SchemaField to describe metadata related to dataset schema.",
+                                        "fields": [
+                                            {
+                                                "Searchable": {
+                                                    "boostScore": 5.0,
+                                                    "fieldName": "fieldPaths",
+                                                    "fieldType": "TEXT"
+                                                },
+                                                "doc": "Flattened name of the field. Field is computed from jsonPath field.",
+                                                "name": "fieldPath",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "Deprecated": true,
+                                                "default": null,
+                                                "doc": "Flattened name of a field in JSON Path notation.",
+                                                "name": "jsonPath",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "default": false,
+                                                "doc": "Indicates if this field is optional or nullable",
+                                                "name": "nullable",
+                                                "type": "boolean"
+                                            },
+                                            {
+                                                "Searchable": {
+                                                    "boostScore": 0.1,
+                                                    "fieldName": "fieldDescriptions",
+                                                    "fieldType": "TEXT"
+                                                },
+                                                "default": null,
+                                                "doc": "Description",
+                                                "name": "description",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "Searchable": {
+                                                    "boostScore": 0.2,
+                                                    "fieldName": "fieldLabels",
+                                                    "fieldType": "TEXT"
+                                                },
+                                                "default": null,
+                                                "doc": "Label of the field. Provides a more human-readable name for the field than field path. Some sources will\nprovide this metadata but not all sources have the concept of a label. If just one string is associated with\na field in a source, that is most likely a description.",
+                                                "name": "label",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "An AuditStamp corresponding to the creation of this schema field.",
+                                                "name": "created",
+                                                "type": [
+                                                    "null",
+                                                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                                                ]
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "An AuditStamp corresponding to the last modification of this schema field.",
+                                                "name": "lastModified",
+                                                "type": [
+                                                    "null",
+                                                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                                                ]
+                                            },
                                             {
-                                                "items": {
-                                                    "doc": "A list of criterion and'd together.",
+                                                "doc": "Platform independent field type of the field.",
+                                                "name": "type",
+                                                "type": {
+                                                    "doc": "Schema field data types",
                                                     "fields": [
                                                         {
-                                                            "doc": "A list of and criteria the filter applies to the query",
-                                                            "name": "and",
-                                                            "type": {
-                                                                "items": {
-                                                                    "doc": "A criterion for matching a field with given value",
+                                                            "doc": "Data platform specific types",
+                                                            "name": "type",
+                                                            "type": [
+                                                                {
+                                                                    "doc": "Boolean field type.",
+                                                                    "fields": [],
+                                                                    "name": "BooleanType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Fixed field type.",
+                                                                    "fields": [],
+                                                                    "name": "FixedType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "String field type.",
+                                                                    "fields": [],
+                                                                    "name": "StringType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Bytes field type.",
+                                                                    "fields": [],
+                                                                    "name": "BytesType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Number data type: long, integer, short, etc..",
+                                                                    "fields": [],
+                                                                    "name": "NumberType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Date field type.",
+                                                                    "fields": [],
+                                                                    "name": "DateType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Time field type. This should also be used for datetimes.",
+                                                                    "fields": [],
+                                                                    "name": "TimeType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Enum field type.",
+                                                                    "fields": [],
+                                                                    "name": "EnumType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Null field type.",
+                                                                    "fields": [],
+                                                                    "name": "NullType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Map field type.",
                                                                     "fields": [
                                                                         {
-                                                                            "doc": "The name of the field that the criterion refers to",
-                                                                            "name": "field",
-                                                                            "type": "string"
+                                                                            "default": null,
+                                                                            "doc": "Key type in a map",
+                                                                            "name": "keyType",
+                                                                            "type": [
+                                                                                "null",
+                                                                                "string"
+                                                                            ]
                                                                         },
                                                                         {
-                                                                            "doc": "The value of the intended field",
-                                                                            "name": "value",
-                                                                            "type": "string"
-                                                                        },
-                                                                        {
-                                                                            "default": [],
-                                                                            "doc": "Values. one of which the intended field should match\nNote, if values is set, the above \"value\" field will be ignored",
-                                                                            "name": "values",
-                                                                            "type": {
-                                                                                "items": "string",
-                                                                                "type": "array"
-                                                                            }
-                                                                        },
+                                                                            "default": null,
+                                                                            "doc": "Type of the value in a map",
+                                                                            "name": "valueType",
+                                                                            "type": [
+                                                                                "null",
+                                                                                "string"
+                                                                            ]
+                                                                        }
+                                                                    ],
+                                                                    "name": "MapType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Array field type.",
+                                                                    "fields": [
                                                                         {
-                                                                            "default": "EQUAL",
-                                                                            "doc": "The condition for the criterion, e.g. EQUAL, START_WITH",
-                                                                            "name": "condition",
-                                                                            "type": {
-                                                                                "doc": "The matching condition in a filter criterion",
-                                                                                "name": "Condition",
-                                                                                "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
-                                                                                "symbolDocs": {
-                                                                                    "CONTAIN": "Represent the relation: String field contains value, e.g. name contains Profile",
-                                                                                    "END_WITH": "Represent the relation: String field ends with value, e.g. name ends with Event",
-                                                                                    "EQUAL": "Represent the relation: field = value, e.g. platform = hdfs",
-                                                                                    "EXISTS": "Represents the relation: field exists and is non-empty, e.g. owners is not null and != [] (empty)",
-                                                                                    "GREATER_THAN": "Represent the relation greater than, e.g. ownerCount > 5",
-                                                                                    "GREATER_THAN_OR_EQUAL_TO": "Represent the relation greater than or equal to, e.g. ownerCount >= 5",
-                                                                                    "IN": "Represent the relation: String field is one of the array values to, e.g. name in [\"Profile\", \"Event\"]",
-                                                                                    "IS_NULL": "Represent the relation: field is null, e.g. platform is null",
-                                                                                    "LESS_THAN": "Represent the relation less than, e.g. ownerCount < 3",
-                                                                                    "LESS_THAN_OR_EQUAL_TO": "Represent the relation less than or equal to, e.g. ownerCount <= 3",
-                                                                                    "START_WITH": "Represent the relation: String field starts with value, e.g. name starts with PageView"
-                                                                                },
-                                                                                "symbols": [
-                                                                                    "CONTAIN",
-                                                                                    "END_WITH",
-                                                                                    "EQUAL",
-                                                                                    "IS_NULL",
-                                                                                    "EXISTS",
-                                                                                    "GREATER_THAN",
-                                                                                    "GREATER_THAN_OR_EQUAL_TO",
-                                                                                    "IN",
-                                                                                    "LESS_THAN",
-                                                                                    "LESS_THAN_OR_EQUAL_TO",
-                                                                                    "START_WITH"
-                                                                                ],
-                                                                                "type": "enum"
-                                                                            }
-                                                                        },
+                                                                            "default": null,
+                                                                            "doc": "List of types this array holds.",
+                                                                            "name": "nestedType",
+                                                                            "type": [
+                                                                                "null",
+                                                                                {
+                                                                                    "items": "string",
+                                                                                    "type": "array"
+                                                                                }
+                                                                            ]
+                                                                        }
+                                                                    ],
+                                                                    "name": "ArrayType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Union field type.",
+                                                                    "fields": [
                                                                         {
-                                                                            "default": false,
-                                                                            "doc": "Whether the condition should be negated",
-                                                                            "name": "negated",
-                                                                            "type": "boolean"
+                                                                            "default": null,
+                                                                            "doc": "List of types in union type.",
+                                                                            "name": "nestedTypes",
+                                                                            "type": [
+                                                                                "null",
+                                                                                {
+                                                                                    "items": "string",
+                                                                                    "type": "array"
+                                                                                }
+                                                                            ]
                                                                         }
                                                                     ],
-                                                                    "name": "Criterion",
-                                                                    "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
+                                                                    "name": "UnionType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                     "type": "record"
                                                                 },
-                                                                "type": "array"
-                                                            }
+                                                                {
+                                                                    "doc": "Record field type.",
+                                                                    "fields": [],
+                                                                    "name": "RecordType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                }
+                                                            ]
                                                         }
                                                     ],
-                                                    "name": "ConjunctiveCriterion",
-                                                    "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
+                                                    "name": "SchemaFieldDataType",
+                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                     "type": "record"
+                                                }
+                                            },
+                                            {
+                                                "doc": "The native type of the field in the dataset's platform as declared by platform schema.",
+                                                "name": "nativeDataType",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "default": false,
+                                                "doc": "There are use cases when a field in type B references type A. A field in A references field of type B. In such cases, we will mark the first field as recursive.",
+                                                "name": "recursive",
+                                                "type": "boolean"
+                                            },
+                                            {
+                                                "Relationship": {
+                                                    "/tags/*/tag": {
+                                                        "entityTypes": [
+                                                            "tag"
+                                                        ],
+                                                        "name": "SchemaFieldTaggedWith"
+                                                    }
                                                 },
-                                                "type": "array"
-                                            }
-                                        ]
-                                    },
-                                    {
-                                        "default": null,
-                                        "doc": "Deprecated! A list of conjunctive criterion for the filter. If \"or\" field is provided, then this field is ignored.",
-                                        "name": "criteria",
-                                        "type": [
-                                            "null",
+                                                "Searchable": {
+                                                    "/tags/*/tag": {
+                                                        "boostScore": 0.5,
+                                                        "fieldName": "fieldTags",
+                                                        "fieldType": "URN"
+                                                    }
+                                                },
+                                                "default": null,
+                                                "doc": "Tags associated with the field",
+                                                "name": "globalTags",
+                                                "type": [
+                                                    "null",
+                                                    {
+                                                        "Aspect": {
+                                                            "name": "globalTags"
+                                                        },
+                                                        "doc": "Tag aspect used for applying tags to an entity",
+                                                        "fields": [
+                                                            {
+                                                                "Relationship": {
+                                                                    "/*/tag": {
+                                                                        "entityTypes": [
+                                                                            "tag"
+                                                                        ],
+                                                                        "name": "TaggedWith"
+                                                                    }
+                                                                },
+                                                                "Searchable": {
+                                                                    "/*/tag": {
+                                                                        "addToFilters": true,
+                                                                        "boostScore": 0.5,
+                                                                        "fieldName": "tags",
+                                                                        "fieldType": "URN",
+                                                                        "filterNameOverride": "Tag",
+                                                                        "hasValuesFieldName": "hasTags",
+                                                                        "queryByDefault": true
+                                                                    }
+                                                                },
+                                                                "doc": "Tags associated with a given entity",
+                                                                "name": "tags",
+                                                                "type": {
+                                                                    "items": {
+                                                                        "doc": "Properties of an applied tag. For now, just an Urn. In the future we can extend this with other properties, e.g.\npropagation parameters.",
+                                                                        "fields": [
+                                                                            {
+                                                                                "Urn": "TagUrn",
+                                                                                "doc": "Urn of the applied tag",
+                                                                                "java": {
+                                                                                    "class": "com.linkedin.pegasus2avro.common.urn.TagUrn"
+                                                                                },
+                                                                                "name": "tag",
+                                                                                "type": "string"
+                                                                            },
+                                                                            {
+                                                                                "default": null,
+                                                                                "doc": "Additional context about the association",
+                                                                                "name": "context",
+                                                                                "type": [
+                                                                                    "null",
+                                                                                    "string"
+                                                                                ]
+                                                                            }
+                                                                        ],
+                                                                        "name": "TagAssociation",
+                                                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                                                        "type": "record"
+                                                                    },
+                                                                    "type": "array"
+                                                                }
+                                                            }
+                                                        ],
+                                                        "name": "GlobalTags",
+                                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                                        "type": "record"
+                                                    }
+                                                ]
+                                            },
+                                            {
+                                                "Relationship": {
+                                                    "/terms/*/urn": {
+                                                        "entityTypes": [
+                                                            "glossaryTerm"
+                                                        ],
+                                                        "name": "SchemaFieldWithGlossaryTerm"
+                                                    }
+                                                },
+                                                "Searchable": {
+                                                    "/terms/*/urn": {
+                                                        "boostScore": 0.5,
+                                                        "fieldName": "fieldGlossaryTerms",
+                                                        "fieldType": "URN"
+                                                    }
+                                                },
+                                                "default": null,
+                                                "doc": "Glossary terms associated with the field",
+                                                "name": "glossaryTerms",
+                                                "type": [
+                                                    "null",
+                                                    {
+                                                        "Aspect": {
+                                                            "name": "glossaryTerms"
+                                                        },
+                                                        "doc": "Related business terms information",
+                                                        "fields": [
+                                                            {
+                                                                "doc": "The related business terms",
+                                                                "name": "terms",
+                                                                "type": {
+                                                                    "items": {
+                                                                        "doc": "Properties of an applied glossary term.",
+                                                                        "fields": [
+                                                                            {
+                                                                                "Relationship": {
+                                                                                    "entityTypes": [
+                                                                                        "glossaryTerm"
+                                                                                    ],
+                                                                                    "name": "TermedWith"
+                                                                                },
+                                                                                "Searchable": {
+                                                                                    "addToFilters": true,
+                                                                                    "fieldName": "glossaryTerms",
+                                                                                    "fieldType": "URN",
+                                                                                    "filterNameOverride": "Glossary Term",
+                                                                                    "hasValuesFieldName": "hasGlossaryTerms"
+                                                                                },
+                                                                                "Urn": "GlossaryTermUrn",
+                                                                                "doc": "Urn of the applied glossary term",
+                                                                                "java": {
+                                                                                    "class": "com.linkedin.pegasus2avro.common.urn.GlossaryTermUrn"
+                                                                                },
+                                                                                "name": "urn",
+                                                                                "type": "string"
+                                                                            },
+                                                                            {
+                                                                                "default": null,
+                                                                                "doc": "Additional context about the association",
+                                                                                "name": "context",
+                                                                                "type": [
+                                                                                    "null",
+                                                                                    "string"
+                                                                                ]
+                                                                            }
+                                                                        ],
+                                                                        "name": "GlossaryTermAssociation",
+                                                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                                                        "type": "record"
+                                                                    },
+                                                                    "type": "array"
+                                                                }
+                                                            },
+                                                            {
+                                                                "doc": "Audit stamp containing who reported the related business term",
+                                                                "name": "auditStamp",
+                                                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                                                            }
+                                                        ],
+                                                        "name": "GlossaryTerms",
+                                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                                        "type": "record"
+                                                    }
+                                                ]
+                                            },
+                                            {
+                                                "default": false,
+                                                "doc": "For schema fields that are part of complex keys, set this field to true\nWe do this to easily distinguish between value and key fields",
+                                                "name": "isPartOfKey",
+                                                "type": "boolean"
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "For Datasets which are partitioned, this determines the partitioning key.",
+                                                "name": "isPartitioningKey",
+                                                "type": [
+                                                    "null",
+                                                    "boolean"
+                                                ]
+                                            },
                                             {
-                                                "items": "com.linkedin.pegasus2avro.metadata.query.filter.Criterion",
-                                                "type": "array"
+                                                "default": null,
+                                                "doc": "For schema fields that have other properties that are not modeled explicitly,\nuse this field to serialize those properties into a JSON string",
+                                                "name": "jsonProps",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
                                             }
-                                        ]
+                                        ],
+                                        "name": "SchemaField",
+                                        "namespace": "com.linkedin.pegasus2avro.schema",
+                                        "type": "record"
                                     }
-                                ],
-                                "name": "Filter",
-                                "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
-                                "type": "record"
+                                ]
                             }
-                        }
+                        ],
+                        "name": "InputField",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
+            }
+        ],
+        "name": "InputFields",
+        "namespace": "com.linkedin.pegasus2avro.common",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "cost"
+        },
+        "fields": [
+            {
+                "name": "costType",
+                "type": {
+                    "doc": "Type of Cost Code",
+                    "name": "CostType",
+                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "symbolDocs": {
+                        "ORG_COST_TYPE": "Org Cost Type to which the Cost of this entity should be attributed to"
+                    },
+                    "symbols": [
+                        "ORG_COST_TYPE"
                     ],
-                    "name": "DataHubViewDefinition",
-                    "namespace": "com.linkedin.pegasus2avro.view",
-                    "type": "record"
+                    "type": "enum"
                 }
             },
             {
-                "Searchable": {
-                    "/actor": {
-                        "fieldName": "createdBy",
-                        "fieldType": "URN"
-                    },
-                    "/time": {
-                        "fieldName": "createdAt",
-                        "fieldType": "DATETIME"
-                    }
-                },
-                "doc": "Audit stamp capturing the time and actor who created the View.",
-                "name": "created",
+                "name": "cost",
                 "type": {
-                    "doc": "Data captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into a particular lifecycle stage, and who acted to move it into that specific lifecycle stage.",
                     "fields": [
                         {
-                            "doc": "When did the resource/association/sub-resource move into the specific lifecycle stage represented by this AuditEvent.",
-                            "name": "time",
-                            "type": "long"
-                        },
-                        {
-                            "Urn": "Urn",
-                            "doc": "The entity (e.g. a member URN) which will be credited for moving the resource/association/sub-resource into the specific lifecycle stage. It is also the one used to authorize the change.",
-                            "java": {
-                                "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                            },
-                            "name": "actor",
-                            "type": "string"
-                        },
-                        {
-                            "Urn": "Urn",
                             "default": null,
-                            "doc": "The entity (e.g. a service URN) which performs the change on behalf of the Actor and must be authorized to act as the Actor.",
-                            "java": {
-                                "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                            },
-                            "name": "impersonator",
+                            "name": "costId",
                             "type": [
                                 "null",
-                                "string"
+                                "double"
                             ]
                         },
                         {
                             "default": null,
-                            "doc": "Additional context around how DataHub was informed of the particular change. For example: was the change created by an automated process, or manually.",
-                            "name": "message",
+                            "name": "costCode",
                             "type": [
                                 "null",
                                 "string"
                             ]
+                        },
+                        {
+                            "doc": "Contains the name of the field that has its value set.",
+                            "name": "fieldDiscriminator",
+                            "type": {
+                                "name": "CostCostDiscriminator",
+                                "namespace": "com.linkedin.pegasus2avro.common",
+                                "symbols": [
+                                    "costId",
+                                    "costCode"
+                                ],
+                                "type": "enum"
+                            }
                         }
                     ],
-                    "name": "AuditStamp",
+                    "name": "CostCost",
                     "namespace": "com.linkedin.pegasus2avro.common",
                     "type": "record"
                 }
-            },
+            }
+        ],
+        "name": "Cost",
+        "namespace": "com.linkedin.pegasus2avro.common",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "subTypes"
+        },
+        "doc": "Sub Types. Use this aspect to specialize a generic Entity\ne.g. Making a Dataset also be a View or also be a LookerExplore",
+        "fields": [
             {
                 "Searchable": {
-                    "/time": {
-                        "fieldName": "lastModifiedAt",
-                        "fieldType": "DATETIME"
+                    "/*": {
+                        "addToFilters": true,
+                        "fieldType": "KEYWORD",
+                        "filterNameOverride": "Sub Type",
+                        "queryByDefault": true
                     }
                 },
-                "doc": "Audit stamp capturing the time and actor who last modified the View.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "doc": "The names of the specific types.",
+                "name": "typeNames",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                }
             }
         ],
-        "name": "DataHubViewInfo",
-        "namespace": "com.linkedin.pegasus2avro.view",
+        "name": "SubTypes",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.common.GlobalTags",
     {
         "Aspect": {
-            "name": "dataProcessInstanceOutput"
+            "name": "siblings"
         },
-        "doc": "Information about the outputs of a Data process",
+        "doc": "Siblings information of an entity.",
         "fields": [
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
                             "dataset"
                         ],
-                        "name": "Produces"
+                        "name": "SiblingOf"
                     }
                 },
                 "Searchable": {
                     "/*": {
-                        "addToFilters": true,
-                        "fieldName": "outputs",
+                        "fieldName": "siblings",
                         "fieldType": "URN",
-                        "numValuesFieldName": "numOutputs",
                         "queryByDefault": false
                     }
                 },
                 "Urn": "Urn",
-                "doc": "Output datasets to be produced",
-                "name": "outputs",
+                "doc": "List of sibling entities",
+                "name": "siblings",
                 "type": {
                     "items": "string",
                     "type": "array"
                 },
                 "urn_is_array": true
+            },
+            {
+                "doc": "If this is the leader entity of the set of siblings",
+                "name": "primary",
+                "type": "boolean"
             }
         ],
-        "name": "DataProcessInstanceOutput",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+        "name": "Siblings",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataProcessInstanceRunEvent",
+            "name": "operation",
             "type": "timeseries"
         },
-        "doc": "An event representing the current status of data process run.\nDataProcessRunEvent should be used for reporting the status of a dataProcess' run.",
+        "doc": "Operational info for an entity.",
         "fields": [
             {
                 "doc": "The event timestamp field as epoch at UTC in milli seconds.",
                 "name": "timestampMillis",
                 "type": "long"
             },
             {
                 "default": null,
                 "doc": "Granularity of the event if applicable",
                 "name": "eventGranularity",
                 "type": [
                     "null",
-                    {
-                        "doc": "Defines the size of a time window.",
-                        "fields": [
-                            {
-                                "doc": "Interval unit such as minute/hour/day etc.",
-                                "name": "unit",
-                                "type": {
-                                    "name": "CalendarInterval",
-                                    "namespace": "com.linkedin.pegasus2avro.timeseries",
-                                    "symbols": [
-                                        "SECOND",
-                                        "MINUTE",
-                                        "HOUR",
-                                        "DAY",
-                                        "WEEK",
-                                        "MONTH",
-                                        "QUARTER",
-                                        "YEAR"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "default": 1,
-                                "doc": "How many units. Defaults to 1.",
-                                "name": "multiple",
-                                "type": "int"
-                            }
-                        ],
-                        "name": "TimeWindowSize",
-                        "namespace": "com.linkedin.pegasus2avro.timeseries",
-                        "type": "record"
-                    }
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
                 ]
             },
             {
                 "default": {
                     "partition": "FULL_TABLE_SNAPSHOT",
                     "timePartition": null,
                     "type": "FULL_TABLE"
                 },
                 "doc": "The optional partition specification.",
                 "name": "partitionSpec",
                 "type": [
-                    {
-                        "doc": "Defines how the data is partitioned",
-                        "fields": [
-                            {
-                                "default": "PARTITION",
-                                "name": "type",
-                                "type": {
-                                    "name": "PartitionType",
-                                    "namespace": "com.linkedin.pegasus2avro.timeseries",
-                                    "symbols": [
-                                        "FULL_TABLE",
-                                        "QUERY",
-                                        "PARTITION"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "TimeseriesField": {},
-                                "doc": "String representation of the partition",
-                                "name": "partition",
-                                "type": "string"
-                            },
-                            {
-                                "default": null,
-                                "doc": "Time window of the partition if applicable",
-                                "name": "timePartition",
-                                "type": [
-                                    "null",
-                                    {
-                                        "fields": [
-                                            {
-                                                "doc": "Start time as epoch at UTC.",
-                                                "name": "startTimeMillis",
-                                                "type": "long"
-                                            },
-                                            {
-                                                "doc": "The length of the window.",
-                                                "name": "length",
-                                                "type": "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
-                                            }
-                                        ],
-                                        "name": "TimeWindow",
-                                        "namespace": "com.linkedin.pegasus2avro.timeseries",
-                                        "type": "record"
-                                    }
-                                ]
-                            }
-                        ],
-                        "name": "PartitionSpec",
-                        "namespace": "com.linkedin.pegasus2avro.timeseries",
-                        "type": "record"
-                    },
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
                     "null"
                 ]
             },
             {
                 "default": null,
                 "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
                 "name": "messageId",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
+                "TimeseriesField": {},
+                "Urn": "Urn",
                 "default": null,
-                "doc": "URL where the reference exist",
+                "doc": "Actor who issued this operation.",
                 "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
-                "name": "externalUrl",
+                "name": "actor",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "TimeseriesField": {},
-                "name": "status",
+                "doc": "Operation type of change.",
+                "name": "operationType",
                 "type": {
-                    "name": "DataProcessRunStatus",
-                    "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                    "doc": "Enum to define the operation type when an entity changes.",
+                    "name": "OperationType",
+                    "namespace": "com.linkedin.pegasus2avro.common",
                     "symbolDocs": {
-                        "STARTED": "The status where the Data processing run is in."
+                        "ALTER": "Asset was altered",
+                        "CREATE": "Asset was created",
+                        "CUSTOM": "Custom asset operation",
+                        "DELETE": "Rows were deleted",
+                        "DROP": "Asset was dropped",
+                        "INSERT": "Rows were inserted",
+                        "UPDATE": "Rows were updated"
                     },
                     "symbols": [
-                        "STARTED",
-                        "COMPLETE"
+                        "INSERT",
+                        "UPDATE",
+                        "DELETE",
+                        "CREATE",
+                        "ALTER",
+                        "DROP",
+                        "CUSTOM",
+                        "UNKNOWN"
                     ],
                     "type": "enum"
                 }
             },
             {
+                "TimeseriesField": {},
                 "default": null,
-                "doc": "Return the try number that this Instance Run is in",
-                "name": "attempt",
+                "doc": "A custom type of operation. Required if operationType is CUSTOM.",
+                "name": "customOperationType",
                 "type": [
                     "null",
-                    "int"
+                    "string"
                 ]
             },
             {
                 "TimeseriesField": {},
                 "default": null,
-                "doc": "The final result of the Data Processing run.",
-                "name": "result",
+                "doc": "How many rows were affected by this operation.",
+                "name": "numAffectedRows",
                 "type": [
                     "null",
-                    {
-                        "fields": [
-                            {
-                                "doc": " The final result, e.g. SUCCESS, FAILURE, SKIPPED, or UP_FOR_RETRY.",
-                                "name": "type",
-                                "type": {
-                                    "name": "RunResultType",
-                                    "namespace": "com.linkedin.pegasus2avro.dataprocess",
-                                    "symbolDocs": {
-                                        "FAILURE": " The Run Failed",
-                                        "SKIPPED": " The Run Skipped",
-                                        "SUCCESS": " The Run Succeeded",
-                                        "UP_FOR_RETRY": " The Run Failed and will Retry"
-                                    },
-                                    "symbols": [
-                                        "SUCCESS",
-                                        "FAILURE",
-                                        "SKIPPED",
-                                        "UP_FOR_RETRY"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "doc": "It identifies the system where the native result comes from like Airflow, Azkaban, etc..",
-                                "name": "nativeResultType",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "DataProcessInstanceRunResult",
-                        "namespace": "com.linkedin.pegasus2avro.dataprocess",
-                        "type": "record"
-                    }
+                    "long"
                 ]
-            }
-        ],
-        "name": "DataProcessInstanceRunEvent",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataProcessInstanceRelationships"
-        },
-        "doc": "Information about Data process relationships",
-        "fields": [
+            },
             {
-                "Relationship": {
-                    "entityTypes": [
-                        "dataJob",
-                        "dataFlow"
-                    ],
-                    "name": "InstanceOf"
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "parentTemplate",
-                        "fieldType": "URN",
-                        "queryByDefault": false
-                    }
+                "TimeseriesFieldCollection": {
+                    "key": "datasetName"
                 },
                 "Urn": "Urn",
                 "default": null,
-                "doc": "The parent entity whose run instance it is",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "parentTemplate",
+                "doc": "Which other datasets were affected by this operation.",
+                "name": "affectedDatasets",
                 "type": [
                     "null",
-                    "string"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
-                "Relationship": {
-                    "entityTypes": [
-                        "dataProcessInstance"
-                    ],
-                    "name": "ChildOf"
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "parentInstance",
-                        "fieldType": "URN",
-                        "queryByDefault": false
-                    }
-                },
-                "Urn": "Urn",
+                "TimeseriesField": {},
                 "default": null,
-                "doc": "The parent DataProcessInstance where it belongs to.\nIf it is a Airflow Task then it should belong to an Airflow Dag run as well\nwhich will be another DataProcessInstance",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "parentInstance",
+                "doc": "Source Type",
+                "name": "sourceType",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "doc": "The source of an operation",
+                        "name": "OperationSourceType",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "symbolDocs": {
+                            "DATA_PLATFORM": "Rows were updated",
+                            "DATA_PROCESS": "Provided by a Data Process"
+                        },
+                        "symbols": [
+                            "DATA_PROCESS",
+                            "DATA_PLATFORM"
+                        ],
+                        "type": "enum"
+                    }
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataProcessInstance"
-                        ],
-                        "name": "UpstreamOf"
+                "default": null,
+                "doc": "Custom properties",
+                "name": "customProperties",
+                "type": [
+                    "null",
+                    {
+                        "type": "map",
+                        "values": "string"
                     }
+                ]
+            },
+            {
+                "Searchable": {
+                    "fieldName": "lastOperationTime",
+                    "fieldType": "DATETIME"
                 },
+                "TimeseriesField": {},
+                "doc": "The time at which the operation occurred. Would be better named 'operationTime'",
+                "name": "lastUpdatedTimestamp",
+                "type": "long"
+            }
+        ],
+        "name": "Operation",
+        "namespace": "com.linkedin.pegasus2avro.common",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "browsePaths"
+        },
+        "doc": "Shared aspect containing Browse Paths to be indexed for an entity.",
+        "fields": [
+            {
                 "Searchable": {
                     "/*": {
-                        "fieldName": "upstream",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numUpstreams",
-                        "queryByDefault": false
+                        "fieldName": "browsePaths",
+                        "fieldType": "BROWSE_PATH"
                     }
                 },
-                "Urn": "Urn",
-                "doc": "Input DataProcessInstance which triggered this dataprocess instance",
-                "name": "upstreamInstances",
+                "doc": "A list of valid browse paths for the entity.\n\nBrowse paths are expected to be forward slash-separated strings. For example: 'prod/snowflake/datasetName'",
+                "name": "paths",
                 "type": {
                     "items": "string",
                     "type": "array"
-                },
-                "urn_is_array": true
+                }
             }
         ],
-        "name": "DataProcessInstanceRelationships",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+        "name": "BrowsePaths",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.common.GlossaryTerms",
     {
         "Aspect": {
-            "name": "dataProcessInstanceProperties"
+            "name": "origin"
         },
-        "doc": "The inputs and outputs of this data process",
+        "doc": "Carries information about where an entity originated from.",
         "fields": [
             {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
+                "doc": "Where an entity originated from. Either NATIVE or EXTERNAL.",
+                "name": "type",
                 "type": {
-                    "type": "map",
-                    "values": "string"
+                    "doc": "Enum to define where an entity originated from.",
+                    "name": "OriginType",
+                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "symbolDocs": {
+                        "EXTERNAL": "The entity is external to DataHub.",
+                        "NATIVE": "The entity is native to DataHub."
+                    },
+                    "symbols": [
+                        "NATIVE",
+                        "EXTERNAL"
+                    ],
+                    "type": "enum"
                 }
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
                 "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
+                "doc": "Only populated if type is EXTERNAL. The externalType of the entity, such as the name of the identity provider.",
+                "name": "externalType",
                 "type": [
                     "null",
                     "string"
                 ]
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Process name",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldType": "KEYWORD",
-                    "filterNameOverride": "Process Type"
-                },
-                "default": null,
-                "doc": "Process type",
-                "name": "type",
-                "type": [
-                    "null",
-                    {
-                        "name": "DataProcessType",
-                        "namespace": "com.linkedin.pegasus2avro.dataprocess",
-                        "symbols": [
-                            "BATCH_SCHEDULED",
-                            "BATCH_AD_HOC",
-                            "STREAMING"
-                        ],
-                        "type": "enum"
-                    }
-                ]
-            },
-            {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "created",
-                        "fieldType": "COUNT",
-                        "queryByDefault": false
-                    }
-                },
-                "doc": "Audit stamp containing who reported the lineage and when",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             }
         ],
-        "name": "DataProcessInstanceProperties",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+        "name": "Origin",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataProcessInfo"
+            "name": "dataPlatformInstance"
         },
-        "doc": "The inputs and outputs of this data process",
+        "doc": "The specific instance of the data platform that this entity belongs to",
         "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "Consumes"
-                    }
-                },
                 "Searchable": {
-                    "/*": {
-                        "fieldName": "inputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numInputDatasets",
-                        "queryByDefault": false
-                    }
+                    "addToFilters": true,
+                    "fieldType": "URN",
+                    "filterNameOverride": "Platform"
                 },
-                "Urn": "DatasetUrn",
-                "default": null,
-                "doc": "the inputs of the data process",
-                "name": "inputs",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
+                "Urn": "Urn",
+                "doc": "Data Platform",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "platform",
+                "type": "string"
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "Consumes"
-                    }
-                },
                 "Searchable": {
-                    "/*": {
-                        "fieldName": "outputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numOutputDatasets",
-                        "queryByDefault": false
-                    }
+                    "addToFilters": true,
+                    "fieldName": "platformInstance",
+                    "fieldType": "URN",
+                    "filterNameOverride": "Platform Instance"
                 },
-                "Urn": "DatasetUrn",
+                "Urn": "Urn",
                 "default": null,
-                "doc": "the outputs of the data process",
-                "name": "outputs",
+                "doc": "Instance of the data platform (e.g. db instance)",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "instance",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
+                    "string"
+                ]
             }
         ],
-        "name": "DataProcessInfo",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+        "name": "DataPlatformInstance",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataProcessInstanceInput"
+            "name": "institutionalMemory"
         },
-        "doc": "Information about the inputs datasets of a Data process",
+        "doc": "Institutional memory of an entity. This is a way to link to relevant documentation and provide description of the documentation. Institutional or tribal knowledge is very important for users to leverage the entity.",
         "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "name": "Consumes"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "addToFilters": true,
-                        "fieldName": "inputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numInputs",
-                        "queryByDefault": false
-                    }
-                },
-                "Urn": "Urn",
-                "doc": "Input datasets to be consumed",
-                "name": "inputs",
+                "doc": "List of records that represent institutional memory of an entity. Each record consists of a link, description, creator and timestamps associated with that record.",
+                "name": "elements",
                 "type": {
-                    "items": "string",
+                    "items": {
+                        "doc": "Metadata corresponding to a record of institutional memory.",
+                        "fields": [
+                            {
+                                "doc": "Link to an engineering design document or a wiki page.",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                                },
+                                "name": "url",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "Description of the link.",
+                                "name": "description",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "Audit stamp associated with creation of this record",
+                                "name": "createStamp",
+                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                            }
+                        ],
+                        "name": "InstitutionalMemoryMetadata",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "type": "record"
+                    },
                     "type": "array"
-                },
-                "urn_is_array": true
+                }
             }
         ],
-        "name": "DataProcessInstanceInput",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+        "name": "InstitutionalMemory",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "chartInfo"
+            "name": "deprecation"
         },
-        "doc": "Information about a chart",
+        "doc": "Deprecation status of an entity",
         "fields": [
             {
                 "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
+                    "fieldType": "BOOLEAN",
+                    "weightsPerFieldValue": {
+                        "true": 0.5
                     }
                 },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
+                "doc": "Whether the entity is deprecated.",
+                "name": "deprecated",
+                "type": "boolean"
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
                 "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
+                "doc": "The time user plan to decommission this entity.",
+                "name": "decommissionTime",
                 "type": [
                     "null",
-                    "string"
+                    "long"
                 ]
             },
             {
-                "Searchable": {
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Title of the chart",
-                "name": "title",
+                "doc": "Additional information about the entity deprecation plan, such as the wiki, doc, RB.",
+                "name": "note",
                 "type": "string"
             },
             {
-                "Searchable": {},
-                "doc": "Detailed description about the chart",
-                "name": "description",
+                "Urn": "Urn",
+                "doc": "The user URN which will be credited for modifying this deprecation content.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "actor",
                 "type": "string"
-            },
+            }
+        ],
+        "name": "Deprecation",
+        "namespace": "com.linkedin.pegasus2avro.common",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "ownership"
+        },
+        "doc": "Ownership information of an entity.",
+        "fields": [
             {
-                "doc": "Captures information about who created/last modified/deleted this chart and when",
-                "name": "lastModified",
+                "doc": "List of owners of the entity.",
+                "name": "owners",
                 "type": {
-                    "doc": "Data captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into various lifecycle stages, and who acted to move it into those lifecycle stages. The recommended best practice is to include this record in your record schema, and annotate its fields as @readOnly in your resource. See https://github.com/linkedin/rest.li/wiki/Validation-in-Rest.li#restli-validation-annotations",
-                    "fields": [
-                        {
-                            "default": {
-                                "actor": "urn:li:corpuser:unknown",
-                                "impersonator": null,
-                                "message": null,
-                                "time": 0
+                    "items": {
+                        "doc": "Ownership information",
+                        "fields": [
+                            {
+                                "Relationship": {
+                                    "entityTypes": [
+                                        "corpuser",
+                                        "corpGroup"
+                                    ],
+                                    "name": "OwnedBy"
+                                },
+                                "Searchable": {
+                                    "addToFilters": true,
+                                    "fieldName": "owners",
+                                    "fieldType": "URN",
+                                    "filterNameOverride": "Owned By",
+                                    "hasValuesFieldName": "hasOwners",
+                                    "queryByDefault": false
+                                },
+                                "Urn": "Urn",
+                                "doc": "Owner URN, e.g. urn:li:corpuser:ldap, urn:li:corpGroup:group_name, and urn:li:multiProduct:mp_name\n(Caveat: only corpuser is currently supported in the frontend.)",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "owner",
+                                "type": "string"
                             },
-                            "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                            "name": "created",
-                            "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                        },
-                        {
-                            "default": {
-                                "actor": "urn:li:corpuser:unknown",
-                                "impersonator": null,
-                                "message": null,
-                                "time": 0
+                            {
+                                "doc": "The type of the ownership",
+                                "name": "type",
+                                "type": {
+                                    "deprecatedSymbols": {
+                                        "CONSUMER": true,
+                                        "DATAOWNER": true,
+                                        "DELEGATE": true,
+                                        "DEVELOPER": true,
+                                        "PRODUCER": true,
+                                        "STAKEHOLDER": true
+                                    },
+                                    "doc": "Asset owner types",
+                                    "name": "OwnershipType",
+                                    "namespace": "com.linkedin.pegasus2avro.common",
+                                    "symbolDocs": {
+                                        "BUSINESS_OWNER": "A person or group who is responsible for logical, or business related, aspects of the asset.",
+                                        "CONSUMER": "A person, group, or service that consumes the data\nDeprecated! Use TECHNICAL_OWNER or BUSINESS_OWNER instead.",
+                                        "DATAOWNER": "A person or group that is owning the data\nDeprecated! Use TECHNICAL_OWNER instead.",
+                                        "DATA_STEWARD": "A steward, expert, or delegate responsible for the asset.",
+                                        "DELEGATE": "A person or a group that overseas the operation, e.g. a DBA or SRE.\nDeprecated! Use TECHNICAL_OWNER instead.",
+                                        "DEVELOPER": "A person or group that is in charge of developing the code\nDeprecated! Use TECHNICAL_OWNER instead.",
+                                        "NONE": "No specific type associated to the owner.",
+                                        "PRODUCER": "A person, group, or service that produces/generates the data\nDeprecated! Use TECHNICAL_OWNER instead.",
+                                        "STAKEHOLDER": "A person or a group that has direct business interest\nDeprecated! Use TECHNICAL_OWNER, BUSINESS_OWNER, or STEWARD instead.",
+                                        "TECHNICAL_OWNER": "person or group who is responsible for technical aspects of the asset."
+                                    },
+                                    "symbols": [
+                                        "TECHNICAL_OWNER",
+                                        "BUSINESS_OWNER",
+                                        "DATA_STEWARD",
+                                        "NONE",
+                                        "DEVELOPER",
+                                        "DATAOWNER",
+                                        "DELEGATE",
+                                        "PRODUCER",
+                                        "CONSUMER",
+                                        "STAKEHOLDER"
+                                    ],
+                                    "type": "enum"
+                                }
                             },
-                            "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                            "name": "lastModified",
-                            "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                        },
-                        {
-                            "default": null,
-                            "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                            "name": "deleted",
-                            "type": [
-                                "null",
-                                "com.linkedin.pegasus2avro.common.AuditStamp"
-                            ]
-                        }
-                    ],
-                    "name": "ChangeAuditStamps",
-                    "namespace": "com.linkedin.pegasus2avro.common",
-                    "type": "record"
+                            {
+                                "default": null,
+                                "doc": "Source information for the ownership",
+                                "name": "source",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "Source/provider of the ownership information",
+                                        "fields": [
+                                            {
+                                                "doc": "The type of the source",
+                                                "name": "type",
+                                                "type": {
+                                                    "name": "OwnershipSourceType",
+                                                    "namespace": "com.linkedin.pegasus2avro.common",
+                                                    "symbolDocs": {
+                                                        "AUDIT": "Auditing system or audit logs",
+                                                        "DATABASE": "Database, e.g. GRANTS table",
+                                                        "FILE_SYSTEM": "File system, e.g. file/directory owner",
+                                                        "ISSUE_TRACKING_SYSTEM": "Issue tracking system, e.g. Jira",
+                                                        "MANUAL": "Manually provided by a user",
+                                                        "OTHER": "Other sources",
+                                                        "SERVICE": "Other ownership-like service, e.g. Nuage, ACL service etc",
+                                                        "SOURCE_CONTROL": "SCM system, e.g. GIT, SVN"
+                                                    },
+                                                    "symbols": [
+                                                        "AUDIT",
+                                                        "DATABASE",
+                                                        "FILE_SYSTEM",
+                                                        "ISSUE_TRACKING_SYSTEM",
+                                                        "MANUAL",
+                                                        "SERVICE",
+                                                        "SOURCE_CONTROL",
+                                                        "OTHER"
+                                                    ],
+                                                    "type": "enum"
+                                                }
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "A reference URL for the source",
+                                                "name": "url",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            }
+                                        ],
+                                        "name": "OwnershipSource",
+                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                        "type": "record"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "Owner",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "type": "record"
+                    },
+                    "type": "array"
                 }
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
-                "default": null,
-                "doc": "URL for the chart. This could be used as an external link on DataHub to allow users access/view the chart",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
                 },
-                "name": "chartUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
+                "doc": "Audit stamp containing who last modified the record and when. A value of 0 in the time field indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            }
+        ],
+        "name": "Ownership",
+        "namespace": "com.linkedin.pegasus2avro.common",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "status"
+        },
+        "doc": "The lifecycle status metadata of an entity, e.g. dataset, metric, feature, etc.\nThis aspect is used to represent soft deletes conventionally.",
+        "fields": [
             {
-                "Relationship": {
-                    "/*/string": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "Consumes"
-                    }
+                "Searchable": {
+                    "fieldType": "BOOLEAN"
                 },
-                "default": null,
-                "deprecated": true,
-                "doc": "Data sources for the chart\nDeprecated! Use inputEdges instead.",
-                "name": "inputs",
-                "type": [
-                    "null",
-                    {
-                        "items": [
-                            "string"
-                        ],
-                        "type": "array"
-                    }
-                ]
-            },
+                "default": false,
+                "doc": "Whether the entity has been removed (soft-deleted).",
+                "name": "removed",
+                "type": "boolean"
+            }
+        ],
+        "name": "Status",
+        "namespace": "com.linkedin.pegasus2avro.common",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "sourceCode"
+        },
+        "doc": "Source Code",
+        "fields": [
             {
-                "Relationship": {
-                    "/*/destinationUrn": {
-                        "createdActor": "inputEdges/*/created/actor",
-                        "createdOn": "inputEdges/*/created/time",
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "Consumes",
-                        "properties": "inputEdges/*/properties",
-                        "updatedActor": "inputEdges/*/lastModified/actor",
-                        "updatedOn": "inputEdges/*/lastModified/time"
-                    }
-                },
-                "default": null,
-                "doc": "Data sources for the chart",
-                "name": "inputEdges",
-                "type": [
-                    "null",
-                    {
-                        "items": {
-                            "doc": "Information about a relatonship edge.",
-                            "fields": [
-                                {
-                                    "Urn": "Urn",
-                                    "doc": "Urn of the source of this relationship edge.",
-                                    "java": {
-                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                    },
-                                    "name": "sourceUrn",
-                                    "type": "string"
-                                },
-                                {
-                                    "Urn": "Urn",
-                                    "doc": "Urn of the destination of this relationship edge.",
-                                    "java": {
-                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                    },
-                                    "name": "destinationUrn",
-                                    "type": "string"
-                                },
-                                {
-                                    "doc": "Audit stamp containing who created this relationship edge and when",
-                                    "name": "created",
-                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                                },
-                                {
-                                    "doc": "Audit stamp containing who last modified this relationship edge and when",
-                                    "name": "lastModified",
-                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                                },
-                                {
-                                    "default": null,
-                                    "doc": "A generic properties bag that allows us to store specific information on this graph edge.",
-                                    "name": "properties",
-                                    "type": [
-                                        "null",
-                                        {
-                                            "type": "map",
-                                            "values": "string"
-                                        }
-                                    ]
+                "doc": "Source Code along with types",
+                "name": "sourceCode",
+                "type": {
+                    "items": {
+                        "doc": "Source Code Url Entity",
+                        "fields": [
+                            {
+                                "doc": "Source Code Url Types",
+                                "name": "type",
+                                "type": {
+                                    "name": "SourceCodeUrlType",
+                                    "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                                    "symbols": [
+                                        "ML_MODEL_SOURCE_CODE",
+                                        "TRAINING_PIPELINE_SOURCE_CODE",
+                                        "EVALUATION_PIPELINE_SOURCE_CODE"
+                                    ],
+                                    "type": "enum"
                                 }
-                            ],
-                            "name": "Edge",
-                            "namespace": "com.linkedin.pegasus2avro.common",
-                            "type": "record"
-                        },
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldType": "KEYWORD",
-                    "filterNameOverride": "Chart Type"
-                },
-                "default": null,
-                "doc": "Type of the chart",
-                "name": "type",
-                "type": [
-                    "null",
-                    {
-                        "doc": "The various types of charts",
-                        "name": "ChartType",
-                        "namespace": "com.linkedin.pegasus2avro.chart",
-                        "symbolDocs": {
-                            "BAR": "Chart showing a Bar chart",
-                            "PIE": "Chart showing a Pie chart",
-                            "SCATTER": "Chart showing a Scatter plot",
-                            "TABLE": "Chart showing a table",
-                            "TEXT": "Chart showing Markdown formatted text"
-                        },
-                        "symbols": [
-                            "BAR",
-                            "PIE",
-                            "SCATTER",
-                            "TABLE",
-                            "TEXT",
-                            "LINE",
-                            "AREA",
-                            "HISTOGRAM",
-                            "BOX_PLOT",
-                            "WORD_CLOUD",
-                            "COHORT"
+                            },
+                            {
+                                "doc": "Source Code Url",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                                },
+                                "name": "sourceCodeUrl",
+                                "type": "string"
+                            }
                         ],
-                        "type": "enum"
-                    }
-                ]
-            },
+                        "name": "SourceCodeUrl",
+                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
+            }
+        ],
+        "name": "SourceCode",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "editableMlModelProperties"
+        },
+        "doc": "Properties associated with a ML Model editable from the UI",
+        "fields": [
             {
                 "Searchable": {
-                    "addToFilters": true,
-                    "fieldType": "KEYWORD",
-                    "filterNameOverride": "Access Level"
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "Access level for the chart",
-                "name": "access",
-                "type": [
-                    "null",
-                    {
-                        "doc": "The various access levels",
-                        "name": "AccessLevel",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "symbolDocs": {
-                            "PRIVATE": "Private availability to certain set of users",
-                            "PUBLIC": "Publicly available access level"
-                        },
-                        "symbols": [
-                            "PUBLIC",
-                            "PRIVATE"
-                        ],
-                        "type": "enum"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The time when this chart last refreshed",
-                "name": "lastRefreshed",
+                "doc": "Documentation of the ml model",
+                "name": "description",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             }
         ],
-        "name": "ChartInfo",
-        "namespace": "com.linkedin.pegasus2avro.chart",
+        "name": "EditableMLModelProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableChartProperties"
+            "name": "editableMlPrimaryKeyProperties"
         },
-        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
+        "doc": "Properties associated with a MLPrimaryKey editable from the UI",
         "fields": [
             {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
                 "default": null,
-                "doc": "Edited documentation of the chart ",
+                "doc": "Documentation of the MLPrimaryKey",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "EditableChartProperties",
-        "namespace": "com.linkedin.pegasus2avro.chart",
+        "name": "EditableMLPrimaryKeyProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "chartQuery"
+            "name": "mlModelEvaluationData"
         },
-        "doc": "Information for chart query which is used for getting data of the chart",
+        "doc": "All referenced datasets would ideally point to any set of documents that provide visibility into the source and composition of the dataset.",
         "fields": [
             {
-                "doc": "Raw query to build a chart from input datasets",
-                "name": "rawQuery",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldName": "queryType",
-                    "fieldType": "KEYWORD",
-                    "filterNameOverride": "Query Type"
-                },
-                "doc": "Chart query type",
-                "name": "type",
+                "doc": "Details on the dataset(s) used for the quantitative analyses in the MLModel",
+                "name": "evaluationData",
                 "type": {
-                    "name": "ChartQueryType",
-                    "namespace": "com.linkedin.pegasus2avro.chart",
-                    "symbolDocs": {
-                        "LOOKML": "LookML queries",
-                        "SQL": "SQL type queries"
+                    "items": {
+                        "doc": "BaseData record",
+                        "fields": [
+                            {
+                                "Urn": "DatasetUrn",
+                                "doc": "What dataset were used in the MLModel?",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
+                                },
+                                "name": "dataset",
+                                "type": "string"
+                            },
+                            {
+                                "default": null,
+                                "doc": "Why was this dataset chosen?",
+                                "name": "motivation",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "How was the data preprocessed (e.g., tokenization of sentences, cropping of images, any filtering such as dropping images without faces)?",
+                                "name": "preProcessing",
+                                "type": [
+                                    "null",
+                                    {
+                                        "items": "string",
+                                        "type": "array"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "BaseData",
+                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                        "type": "record"
                     },
-                    "symbols": [
-                        "LOOKML",
-                        "SQL"
-                    ],
-                    "type": "enum"
+                    "type": "array"
                 }
             }
         ],
-        "name": "ChartQuery",
-        "namespace": "com.linkedin.pegasus2avro.chart",
+        "name": "EvaluationData",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "chartUsageStatistics",
-            "type": "timeseries"
+            "name": "editableMlFeatureTableProperties"
         },
-        "doc": "Experimental (Subject to breaking change) -- Stats corresponding to chart's usage.\n\nIf this aspect represents the latest snapshot of the statistics about a Chart, the eventGranularity field should be null.\nIf this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly.",
+        "doc": "Properties associated with a MLFeatureTable editable from the ui",
         "fields": [
             {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
-            },
+                "Searchable": {
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
+                },
+                "default": null,
+                "doc": "Documentation of the MLFeatureTable",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "EditableMLFeatureTableProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "mlPrimaryKeyProperties"
+        },
+        "doc": "Properties associated with a MLPrimaryKey",
+        "fields": [
             {
                 "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
+                "doc": "Documentation of the MLPrimaryKey",
+                "name": "description",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                    "string"
                 ]
             },
             {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
-                },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
+                "default": null,
+                "doc": "Data Type of the MLPrimaryKey",
+                "name": "dataType",
                 "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
+                    "null",
+                    {
+                        "doc": "MLFeature Data Type",
+                        "name": "MLFeatureDataType",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "symbolDocs": {
+                            "AUDIO": "Audio Data",
+                            "BINARY": "Binary data is discrete data that can be in only one of two categories - either yes or no, 1 or 0, off or on, etc",
+                            "BYTE": "Bytes data are binary-encoded values that can represent complex objects.",
+                            "CONTINUOUS": "Continuous data are made of uncountable values, often the result of a measurement such as height, weight, age etc.",
+                            "COUNT": "Count data is discrete whole number data - no negative numbers here.\nCount data often has many small values, such as zero and one.",
+                            "IMAGE": "Image Data",
+                            "INTERVAL": "Interval data has equal spaces between the numbers and does not represent a temporal pattern.\nExamples include percentages, temperatures, and income.",
+                            "MAP": "Mapping Data Type ex: dict, map",
+                            "NOMINAL": "Nominal data is made of discrete values with no numerical relationship between the different categories - mean and median are meaningless.\nAnimal species is one example. For example, pig is not higher than bird and lower than fish.",
+                            "ORDINAL": "Ordinal data are discrete integers that can be ranked or sorted.\nFor example, the distance between first and second may not be the same as the distance between second and third.",
+                            "SEQUENCE": "Sequence Data Type ex: list, tuple, range",
+                            "SET": "Set Data Type ex: set, frozenset",
+                            "TEXT": "Text Data",
+                            "TIME": "Time data is a cyclical, repeating continuous form of data.\nThe relevant time features can be any period- daily, weekly, monthly, annual, etc.",
+                            "UNKNOWN": "Unknown data are data that we don't know the type for.",
+                            "USELESS": "Useless data is unique, discrete data with no potential relationship with the outcome variable.\nA useless feature has high cardinality. An example would be bank account numbers that were generated randomly.",
+                            "VIDEO": "Video Data"
+                        },
+                        "symbols": [
+                            "USELESS",
+                            "NOMINAL",
+                            "ORDINAL",
+                            "BINARY",
+                            "COUNT",
+                            "TIME",
+                            "INTERVAL",
+                            "IMAGE",
+                            "VIDEO",
+                            "AUDIO",
+                            "TEXT",
+                            "MAP",
+                            "SEQUENCE",
+                            "SET",
+                            "CONTINUOUS",
+                            "BYTE",
+                            "UNKNOWN"
+                        ],
+                        "type": "enum"
+                    }
                 ]
             },
             {
                 "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
+                "doc": "Version of the MLPrimaryKey",
+                "name": "version",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "doc": "A resource-defined string representing the resource state for the purpose of concurrency control",
+                        "fields": [
+                            {
+                                "default": null,
+                                "name": "versionTag",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            }
+                        ],
+                        "name": "VersionTag",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "type": "record"
+                    }
                 ]
             },
             {
-                "TimeseriesField": {},
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "name": "DerivedFrom"
+                    }
+                },
+                "Urn": "Urn",
+                "doc": "Source of the MLPrimaryKey",
+                "name": "sources",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            }
+        ],
+        "name": "MLPrimaryKeyProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "mlHyperParam"
+        },
+        "doc": "Properties associated with an ML Hyper Param",
+        "fields": [
+            {
+                "doc": "Name of the MLHyperParam",
+                "name": "name",
+                "type": "string"
+            },
+            {
                 "default": null,
-                "doc": "The total number of times chart has been viewed",
-                "name": "viewsCount",
+                "doc": "Documentation of the MLHyperParam",
+                "name": "description",
                 "type": [
                     "null",
-                    "int"
+                    "string"
                 ]
             },
             {
-                "TimeseriesField": {},
                 "default": null,
-                "doc": "Unique user count",
-                "name": "uniqueUserCount",
+                "doc": "The value of the MLHyperParam",
+                "name": "value",
                 "type": [
                     "null",
-                    "int"
+                    "string"
                 ]
             },
             {
-                "TimeseriesFieldCollection": {
-                    "key": "user"
-                },
                 "default": null,
-                "doc": "Users within this bucket, with frequency counts",
-                "name": "userCounts",
+                "doc": "Date when the MLHyperParam was developed",
+                "name": "createdAt",
                 "type": [
                     "null",
-                    {
-                        "items": {
-                            "doc": "Records a single user's usage counts for a given resource",
-                            "fields": [
-                                {
-                                    "Urn": "Urn",
-                                    "doc": "The unique id of the user.",
-                                    "java": {
-                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                    },
-                                    "name": "user",
-                                    "type": "string"
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "default": null,
-                                    "doc": "The number of times the user has viewed the chart",
-                                    "name": "viewsCount",
-                                    "type": [
-                                        "null",
-                                        "int"
-                                    ]
-                                }
-                            ],
-                            "name": "ChartUserUsageCounts",
-                            "namespace": "com.linkedin.pegasus2avro.chart",
-                            "type": "record"
-                        },
-                        "type": "array"
-                    }
+                    "long"
                 ]
             }
         ],
-        "name": "ChartUsageStatistics",
-        "namespace": "com.linkedin.pegasus2avro.chart",
+        "name": "MLHyperParam",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "notebookInfo"
+            "name": "mlModelDeploymentProperties"
         },
-        "doc": "Information about a Notebook\nNote: This is IN BETA version",
+        "doc": "Properties associated with an ML Model Deployment",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -2788,577 +2873,546 @@
                 "name": "customProperties",
                 "type": {
                     "type": "map",
                     "values": "string"
                 }
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
                 "default": null,
                 "doc": "URL where the reference exist",
                 "java": {
                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                 },
                 "name": "externalUrl",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Title of the Notebook",
-                "name": "title",
-                "type": "string"
-            },
-            {
-                "Searchable": {
                     "fieldType": "TEXT",
                     "hasValuesFieldName": "hasDescription"
                 },
                 "default": null,
-                "doc": "Detailed description about the Notebook",
+                "doc": "Documentation of the MLModelDeployment",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "doc": "Captures information about who created/last modified/deleted this Notebook and when",
-                "name": "changeAuditStamps",
-                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+                "default": null,
+                "doc": "Date when the MLModelDeployment was developed",
+                "name": "createdAt",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Version of the MLModelDeployment",
+                "name": "version",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.VersionTag"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Status of the deployment",
+                "name": "status",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Model endpoint statuses",
+                        "name": "DeploymentStatus",
+                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                        "symbolDocs": {
+                            "CREATING": "Deployments being created.",
+                            "DELETING": "Deployments being deleted.",
+                            "FAILED": "Deployments with an error state.",
+                            "IN_SERVICE": "Deployments that are active.",
+                            "OUT_OF_SERVICE": "Deployments out of service.",
+                            "ROLLING_BACK": "Deployments being reverted to a previous version.",
+                            "UNKNOWN": "Deployments with unknown/unmappable state.",
+                            "UPDATING": "Deployments being updated."
+                        },
+                        "symbols": [
+                            "OUT_OF_SERVICE",
+                            "CREATING",
+                            "UPDATING",
+                            "ROLLING_BACK",
+                            "IN_SERVICE",
+                            "DELETING",
+                            "FAILED",
+                            "UNKNOWN"
+                        ],
+                        "type": "enum"
+                    }
+                ]
             }
         ],
-        "name": "NotebookInfo",
-        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "name": "MLModelDeploymentProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "notebookContent"
+            "name": "mlModelCaveatsAndRecommendations"
         },
-        "doc": "Content in a Notebook\nNote: This is IN BETA version",
+        "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?",
         "fields": [
             {
-                "default": [],
-                "doc": "The content of a Notebook which is composed by a list of NotebookCell",
-                "name": "cells",
-                "type": {
-                    "items": {
-                        "doc": "A record of all supported cells for a Notebook. Only one type of cell will be non-null.",
+                "default": null,
+                "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?",
+                "name": "caveats",
+                "type": [
+                    "null",
+                    {
+                        "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?",
                         "fields": [
                             {
                                 "default": null,
-                                "doc": "The text cell content. The will be non-null only when all other cell field is null.",
-                                "name": "textCell",
+                                "doc": "Did the results suggest any further testing?",
+                                "name": "needsFurtherTesting",
                                 "type": [
                                     "null",
-                                    {
-                                        "doc": "Text cell in a Notebook, which will present content in text format",
-                                        "fields": [
-                                            {
-                                                "default": null,
-                                                "doc": "Title of the cell",
-                                                "name": "cellTitle",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            },
-                                            {
-                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
-                                                "name": "cellId",
-                                                "type": "string"
-                                            },
-                                            {
-                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
-                                                "name": "changeAuditStamps",
-                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
-                                            },
-                                            {
-                                                "doc": "The actual text in a TextCell in a Notebook",
-                                                "name": "text",
-                                                "type": "string"
-                                            }
-                                        ],
-                                        "name": "TextCell",
-                                        "namespace": "com.linkedin.pegasus2avro.notebook",
-                                        "type": "record"
-                                    }
+                                    "boolean"
                                 ]
                             },
                             {
                                 "default": null,
-                                "doc": "The query cell content. The will be non-null only when all other cell field is null.",
-                                "name": "queryCell",
+                                "doc": "Caveat Description\nFor ex: Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders.",
+                                "name": "caveatDescription",
                                 "type": [
                                     "null",
-                                    {
-                                        "doc": "Query cell in a Notebook, which will present content in query format",
-                                        "fields": [
-                                            {
-                                                "default": null,
-                                                "doc": "Title of the cell",
-                                                "name": "cellTitle",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            },
-                                            {
-                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
-                                                "name": "cellId",
-                                                "type": "string"
-                                            },
-                                            {
-                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
-                                                "name": "changeAuditStamps",
-                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
-                                            },
-                                            {
-                                                "doc": "Raw query to explain some specific logic in a Notebook",
-                                                "name": "rawQuery",
-                                                "type": "string"
-                                            },
-                                            {
-                                                "default": null,
-                                                "doc": "Captures information about who last executed this query cell and when",
-                                                "name": "lastExecuted",
-                                                "type": [
-                                                    "null",
-                                                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                                                ]
-                                            }
-                                        ],
-                                        "name": "QueryCell",
-                                        "namespace": "com.linkedin.pegasus2avro.notebook",
-                                        "type": "record"
-                                    }
+                                    "string"
                                 ]
                             },
                             {
                                 "default": null,
-                                "doc": "The chart cell content. The will be non-null only when all other cell field is null.",
-                                "name": "chartCell",
+                                "doc": "Relevant groups that were not represented in the evaluation dataset?",
+                                "name": "groupsNotRepresented",
                                 "type": [
                                     "null",
                                     {
-                                        "doc": "Chart cell in a notebook, which will present content in chart format",
-                                        "fields": [
-                                            {
-                                                "default": null,
-                                                "doc": "Title of the cell",
-                                                "name": "cellTitle",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            },
-                                            {
-                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
-                                                "name": "cellId",
-                                                "type": "string"
-                                            },
-                                            {
-                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
-                                                "name": "changeAuditStamps",
-                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
-                                            }
-                                        ],
-                                        "name": "ChartCell",
-                                        "namespace": "com.linkedin.pegasus2avro.notebook",
-                                        "type": "record"
+                                        "items": "string",
+                                        "type": "array"
                                     }
                                 ]
-                            },
-                            {
-                                "doc": "The type of this Notebook cell",
-                                "name": "type",
-                                "type": {
-                                    "doc": "Type of Notebook Cell",
-                                    "name": "NotebookCellType",
-                                    "namespace": "com.linkedin.pegasus2avro.notebook",
-                                    "symbolDocs": {
-                                        "CHART_CELL": "CHART Notebook cell type. The cell content is chart only.",
-                                        "QUERY_CELL": "QUERY Notebook cell type. The cell context is query only.",
-                                        "TEXT_CELL": "TEXT Notebook cell type. The cell context is text only."
-                                    },
-                                    "symbols": [
-                                        "TEXT_CELL",
-                                        "QUERY_CELL",
-                                        "CHART_CELL"
-                                    ],
-                                    "type": "enum"
-                                }
                             }
                         ],
-                        "name": "NotebookCell",
-                        "namespace": "com.linkedin.pegasus2avro.notebook",
+                        "name": "CaveatDetails",
+                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                         "type": "record"
-                    },
-                    "type": "array"
-                }
+                    }
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Recommendations on where this MLModel should be used.",
+                "name": "recommendations",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Ideal characteristics of an evaluation dataset for this MLModel",
+                "name": "idealDatasetCharacteristics",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ]
             }
         ],
-        "name": "NotebookContent",
-        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "name": "CaveatsAndRecommendations",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableNotebookProperties"
+            "name": "mlMetric"
         },
-        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines\nNote: This is IN BETA version",
+        "doc": "Properties associated with an ML Metric",
         "fields": [
             {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "doc": "Name of the mlMetric",
+                "name": "name",
+                "type": "string"
             },
             {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "default": null,
+                "doc": "Documentation of the mlMetric",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
             },
             {
                 "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
+                "doc": "The value of the mlMetric",
+                "name": "value",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                    "string"
                 ]
             },
             {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
                 "default": null,
-                "doc": "Edited documentation of the Notebook",
-                "name": "description",
+                "doc": "Date when the mlMetric was developed",
+                "name": "createdAt",
                 "type": [
                     "null",
-                    "string"
+                    "long"
                 ]
             }
         ],
-        "name": "EditableNotebookProperties",
-        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "name": "MLMetric",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "roleMembership"
+            "name": "mlModelTrainingData"
         },
-        "doc": "Carries information about which roles a user is assigned to.",
+        "doc": "Ideally, the MLModel card would contain as much information about the training data as the evaluation data. However, there might be cases where it is not feasible to provide this level of detailed information about the training data. For example, the data may be proprietary, or require a non-disclosure agreement. In these cases, we advocate for basic details about the distributions over groups in the data, as well as any other details that could inform stakeholders on the kinds of biases the model may have encoded.",
         "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataHubRole"
-                        ],
-                        "name": "IsMemberOfRole"
-                    }
-                },
-                "Urn": "Urn",
-                "name": "roles",
+                "doc": "Details on the dataset(s) used for training the MLModel",
+                "name": "trainingData",
                 "type": {
-                    "items": "string",
+                    "items": "com.linkedin.pegasus2avro.ml.metadata.BaseData",
                     "type": "array"
-                },
-                "urn_is_array": true
+                }
             }
         ],
-        "name": "RoleMembership",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "TrainingData",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "inviteToken"
+            "name": "mlModelGroupProperties"
         },
-        "doc": "Aspect used to store invite tokens.",
+        "doc": "Properties associated with an ML Model Group",
         "fields": [
             {
-                "doc": "The encrypted invite token.",
-                "name": "token",
-                "type": "string"
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
             },
             {
                 "Searchable": {
-                    "fieldName": "role",
-                    "fieldType": "KEYWORD",
-                    "hasValuesFieldName": "hasRole"
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
                 },
-                "Urn": "Urn",
                 "default": null,
-                "doc": "The role that this invite token may be associated with",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "role",
+                "doc": "Documentation of the MLModelGroup",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
+            },
+            {
+                "default": null,
+                "doc": "Date when the MLModelGroup was developed",
+                "name": "createdAt",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Version of the MLModelGroup",
+                "name": "version",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.VersionTag"
+                ]
             }
         ],
-        "name": "InviteToken",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "MLModelGroupProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "groupMembership"
+            "name": "mlModelMetrics"
         },
-        "doc": "Carries information about the CorpGroups a user is in.",
+        "doc": "Metrics to be featured for the MLModel.",
         "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "corpGroup"
-                        ],
-                        "name": "IsMemberOfGroup"
+                "default": null,
+                "doc": "Measures of MLModel performance",
+                "name": "performanceMeasures",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
                     }
-                },
-                "Urn": "Urn",
-                "name": "groups",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                },
-                "urn_is_array": true
-            }
-        ],
-        "name": "GroupMembership",
-        "namespace": "com.linkedin.pegasus2avro.identity",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "nativeGroupMembership"
-        },
-        "doc": "Carries information about the native CorpGroups a user is in.",
-        "fields": [
+                ]
+            },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "corpGroup"
-                        ],
-                        "name": "IsMemberOfNativeGroup"
+                "default": null,
+                "doc": "Decision Thresholds used (if any)?",
+                "name": "decisionThreshold",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
                     }
-                },
-                "Urn": "Urn",
-                "name": "nativeGroups",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                },
-                "urn_is_array": true
+                ]
             }
         ],
-        "name": "NativeGroupMembership",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "Metrics",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "corpGroupEditableInfo"
+            "name": "mlModelFactorPrompts"
         },
-        "doc": "Group information that can be edited from UI",
+        "doc": "Prompts which affect the performance of the MLModel",
         "fields": [
             {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
-                "default": null,
-                "doc": "A description of the group",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": "https://raw.githubusercontent.com/datahub-project/datahub/master/datahub-web-react/src/images/default_avatar.png",
-                "doc": "A URL which points to a picture which user wants to set as the photo for the group",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "pictureLink",
-                "type": "string"
-            },
-            {
                 "default": null,
-                "doc": "Slack channel for the group",
-                "name": "slack",
+                "doc": "What are foreseeable salient factors for which MLModel performance may vary, and how were these determined?",
+                "name": "relevantFactors",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": {
+                            "doc": "Factors affecting the performance of the MLModel.",
+                            "fields": [
+                                {
+                                    "default": null,
+                                    "doc": "Groups refers to distinct categories with similar characteristics that are present in the evaluation data instances.\nFor human-centric machine learning MLModels, groups are people who share one or multiple characteristics.",
+                                    "name": "groups",
+                                    "type": [
+                                        "null",
+                                        {
+                                            "items": "string",
+                                            "type": "array"
+                                        }
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "doc": "The performance of a MLModel can vary depending on what instruments were used to capture the input to the MLModel.\nFor example, a face detection model may perform differently depending on the camera's hardware and software,\nincluding lens, image stabilization, high dynamic range techniques, and background blurring for portrait mode.",
+                                    "name": "instrumentation",
+                                    "type": [
+                                        "null",
+                                        {
+                                            "items": "string",
+                                            "type": "array"
+                                        }
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "doc": "A further factor affecting MLModel performance is the environment in which it is deployed.",
+                                    "name": "environment",
+                                    "type": [
+                                        "null",
+                                        {
+                                            "items": "string",
+                                            "type": "array"
+                                        }
+                                    ]
+                                }
+                            ],
+                            "name": "MLModelFactors",
+                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                            "type": "record"
+                        },
+                        "type": "array"
+                    }
                 ]
             },
             {
                 "default": null,
-                "doc": "Email address to contact the group",
-                "name": "email",
+                "doc": "Which factors are being reported, and why were these chosen?",
+                "name": "evaluationFactors",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLModelFactors",
+                        "type": "array"
+                    }
                 ]
             }
         ],
-        "name": "CorpGroupEditableInfo",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "MLModelFactorPrompts",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "EntityUrns": [
-                "com.linkedin.pegasus2avro.common.CorpuserUrn"
-            ],
-            "name": "corpUserEditableInfo"
+            "name": "mlModelEthicalConsiderations"
         },
-        "doc": "Linkedin corp user information that can be edited from UI",
+        "doc": "This section is intended to demonstrate the ethical considerations that went into MLModel development, surfacing ethical challenges and solutions to stakeholders.",
         "fields": [
             {
                 "default": null,
-                "doc": "About me section of the user",
-                "name": "aboutMe",
+                "doc": "Does the MLModel use any sensitive data (e.g., protected classes)?",
+                "name": "data",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
                 ]
             },
             {
-                "Searchable": {
-                    "/*": {
-                        "fieldType": "TEXT"
+                "default": null,
+                "doc": " Is the MLModel intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?",
+                "name": "humanLife",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
                     }
-                },
-                "default": [],
-                "doc": "Teams that the user belongs to e.g. Metadata",
-                "name": "teams",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
+                ]
             },
             {
-                "Searchable": {
-                    "/*": {
-                        "fieldType": "TEXT"
+                "default": null,
+                "doc": "What risk mitigation strategies were used during MLModel development?",
+                "name": "mitigations",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
                     }
-                },
-                "default": [],
-                "doc": "Skills that the user possesses e.g. Machine Learning",
-                "name": "skills",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
+                ]
             },
             {
-                "default": "https://raw.githubusercontent.com/datahub-project/datahub/master/datahub-web-react/src/images/default_avatar.png",
-                "doc": "A URL which points to a picture which user wants to set as a profile photo",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "pictureLink",
-                "type": "string"
+                "default": null,
+                "doc": "What risks may be present in MLModel usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown.",
+                "name": "risksAndHarms",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ]
             },
             {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": true
-                },
                 "default": null,
-                "doc": "DataHub-native display name",
-                "name": "displayName",
+                "doc": "Are there any known MLModel use cases that are especially fraught? This may connect directly to the intended use section",
+                "name": "useCases",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
                 ]
-            },
+            }
+        ],
+        "name": "EthicalConsiderations",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "mlFeatureProperties"
+        },
+        "doc": "Properties associated with a MLFeature",
+        "fields": [
             {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
                 "default": null,
-                "doc": "DataHub-native Title, e.g. 'Software Engineer'",
-                "name": "title",
+                "doc": "Documentation of the MLFeature",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Slack handle for the user",
-                "name": "slack",
+                "doc": "Data Type of the MLFeature",
+                "name": "dataType",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.MLFeatureDataType"
                 ]
             },
             {
                 "default": null,
-                "doc": "Phone number to contact the user",
-                "name": "phone",
+                "doc": "Version of the MLFeature",
+                "name": "version",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.VersionTag"
                 ]
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "name": "DerivedFrom"
+                    }
+                },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "Email address to contact the user",
-                "name": "email",
+                "doc": "Source of the MLFeature",
+                "name": "sources",
                 "type": [
                     "null",
-                    "string"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             }
         ],
-        "name": "CorpUserEditableInfo",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "MLFeatureProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "EntityUrns": [
-                "com.linkedin.pegasus2avro.common.CorpuserUrn"
-            ],
-            "name": "corpUserInfo"
+            "name": "mlModelProperties"
         },
-        "doc": "Linkedin corp user information",
+        "doc": "Properties associated with a ML Model",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -3367,417 +3421,306 @@
                 "name": "customProperties",
                 "type": {
                     "type": "map",
                     "values": "string"
                 }
             },
             {
-                "Searchable": {
-                    "fieldType": "BOOLEAN",
-                    "weightsPerFieldValue": {
-                        "true": 2.0
-                    }
-                },
-                "doc": "Deprecated! Use CorpUserStatus instead. Whether the corpUser is active, ref: https://iwww.corp.linkedin.com/wiki/cf/display/GTSD/Accessing+Active+Directory+via+LDAP+tools",
-                "name": "active",
-                "type": "boolean"
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": true
-                },
-                "default": null,
-                "doc": "displayName of this user ,  e.g.  Hang Zhang(DataHQ)",
-                "name": "displayName",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldType": "KEYWORD",
-                    "queryByDefault": true
-                },
                 "default": null,
-                "doc": "email address of this user",
-                "name": "email",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldType": "KEYWORD",
-                    "queryByDefault": true
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                 },
-                "default": null,
-                "doc": "title of this user",
-                "name": "title",
+                "name": "externalUrl",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "Relationship": {
-                    "entityTypes": [
-                        "corpuser"
-                    ],
-                    "name": "ReportsTo"
-                },
                 "Searchable": {
-                    "fieldName": "managerLdap",
-                    "fieldType": "URN",
-                    "queryByDefault": true
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
                 },
-                "Urn": "CorpuserUrn",
                 "default": null,
-                "doc": "direct manager of this user",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.CorpuserUrn"
-                },
-                "name": "managerUrn",
+                "doc": "Documentation of the MLModel",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "department id this user belong to",
-                "name": "departmentId",
+                "doc": "Date when the MLModel was developed",
+                "name": "date",
                 "type": [
                     "null",
                     "long"
                 ]
             },
             {
                 "default": null,
-                "doc": "department name this user belong to",
-                "name": "departmentName",
+                "doc": "Version of the MLModel",
+                "name": "version",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.VersionTag"
                 ]
             },
             {
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
                 "default": null,
-                "doc": "first name of this user",
-                "name": "firstName",
+                "doc": "Type of Algorithm or MLModel such as whether it is a Naive Bayes classifier, Convolutional Neural Network, etc",
+                "name": "type",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "last name of this user",
-                "name": "lastName",
+                "doc": "Hyper Parameters of the MLModel\n\nNOTE: these are deprecated in favor of hyperParams",
+                "name": "hyperParameters",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "type": "map",
+                        "values": [
+                            "string",
+                            "int",
+                            "float",
+                            "double",
+                            "boolean"
+                        ]
+                    }
                 ]
             },
             {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": true
-                },
                 "default": null,
-                "doc": "Common name of this user, format is firstName + lastName (split by a whitespace)",
-                "name": "fullName",
+                "doc": "Hyperparameters of the MLModel",
+                "name": "hyperParams",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLHyperParam",
+                        "type": "array"
+                    }
                 ]
             },
             {
                 "default": null,
-                "doc": "two uppercase letters country code. e.g.  US",
-                "name": "countryCode",
+                "doc": "Metrics of the MLModel used in training",
+                "name": "trainingMetrics",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLMetric",
+                        "type": "array"
+                    }
                 ]
-            }
-        ],
-        "name": "CorpUserInfo",
-        "namespace": "com.linkedin.pegasus2avro.identity",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "EntityUrns": [
-                "com.linkedin.pegasus2avro.common.CorpuserUrn"
-            ],
-            "name": "corpUserCredentials"
-        },
-        "doc": "Corp user credentials",
-        "fields": [
-            {
-                "doc": "Salt used to hash password",
-                "name": "salt",
-                "type": "string"
-            },
-            {
-                "doc": "Hashed password generated by concatenating salt and password, then hashing",
-                "name": "hashedPassword",
-                "type": "string"
             },
             {
                 "default": null,
-                "doc": "Optional token needed to reset a user's password. Can only be set by the admin.",
-                "name": "passwordResetToken",
+                "doc": "Metrics of the MLModel used in production",
+                "name": "onlineMetrics",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLMetric",
+                        "type": "array"
+                    }
                 ]
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "mlFeature"
+                        ],
+                        "isLineage": true,
+                        "name": "Consumes"
+                    }
+                },
+                "Urn": "MLFeatureUrn",
                 "default": null,
-                "doc": "When the password reset token expires.",
-                "name": "passwordResetTokenExpirationTimeMillis",
+                "doc": "List of features used for MLModel training",
+                "name": "mlFeatures",
                 "type": [
                     "null",
-                    "long"
-                ]
-            }
-        ],
-        "name": "CorpUserCredentials",
-        "namespace": "com.linkedin.pegasus2avro.identity",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "corpUserSettings"
-        },
-        "doc": "Settings that a user can customize through the datahub ui",
-        "fields": [
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
+            },
             {
-                "doc": "Settings for a user around the appearance of their DataHub U",
-                "name": "appearance",
+                "default": [],
+                "doc": "Tags for the MLModel",
+                "name": "tags",
                 "type": {
-                    "doc": "Settings for a user around the appearance of their DataHub UI",
-                    "fields": [
-                        {
-                            "default": null,
-                            "doc": "Flag whether the user should see a homepage with only datasets, charts and dashboards. Intended for users\nwho have less operational use cases for the datahub tool.",
-                            "name": "showSimplifiedHomepage",
-                            "type": [
-                                "null",
-                                "boolean"
-                            ]
-                        }
-                    ],
-                    "name": "CorpUserAppearanceSettings",
-                    "namespace": "com.linkedin.pegasus2avro.identity",
-                    "type": "record"
+                    "items": "string",
+                    "type": "array"
                 }
             },
             {
-                "default": null,
-                "doc": "User preferences for the Views feature.",
-                "name": "views",
-                "type": [
-                    "null",
-                    {
-                        "doc": "Settings related to the 'Views' feature.",
-                        "fields": [
-                            {
-                                "Urn": "Urn",
-                                "default": null,
-                                "doc": "The default View which is selected for the user.\nIf none is chosen, then this value will be left blank.",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "defaultView",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            }
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "mlModelDeployment"
                         ],
-                        "name": "CorpUserViewsSettings",
-                        "namespace": "com.linkedin.pegasus2avro.identity",
-                        "type": "record"
+                        "name": "DeployedTo"
                     }
-                ]
-            }
-        ],
-        "name": "CorpUserSettings",
-        "namespace": "com.linkedin.pegasus2avro.identity",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "corpUserStatus"
-        },
-        "doc": "The status of the user, e.g. provisioned, active, suspended, etc.",
-        "fields": [
-            {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
-                "doc": "Status of the user, e.g. PROVISIONED / ACTIVE / SUSPENDED",
-                "name": "status",
-                "type": "string"
-            },
-            {
-                "doc": "Audit stamp containing who last modified the status and when.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            }
-        ],
-        "name": "CorpUserStatus",
-        "namespace": "com.linkedin.pegasus2avro.identity",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "EntityUrns": [
-                "com.linkedin.pegasus2avro.common.CorpGroupUrn"
-            ],
-            "name": "corpGroupInfo"
-        },
-        "doc": "Information about a Corp Group ingested from a third party source",
-        "fields": [
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": true
                 },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "The name of the group.",
-                "name": "displayName",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "email of this group",
-                "name": "email",
+                "doc": "Deployments for the MLModel",
+                "name": "deployments",
                 "type": [
                     "null",
-                    "string"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
-                            "corpuser"
+                            "dataJob"
                         ],
-                        "name": "OwnedBy"
+                        "isLineage": true,
+                        "name": "TrainedBy"
                     }
                 },
-                "Urn": "CorpuserUrn",
-                "deprecated": true,
-                "doc": "owners of this group\nDeprecated! Replaced by Ownership aspect.",
-                "name": "admins",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                },
+                "Urn": "Urn",
+                "default": null,
+                "doc": "List of jobs (if any) used to train the model",
+                "name": "trainingJobs",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
                 "urn_is_array": true
             },
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
-                            "corpuser"
+                            "dataJob"
                         ],
-                        "name": "IsPartOf"
+                        "isLineage": true,
+                        "isUpstream": false,
+                        "name": "UsedBy"
                     }
                 },
-                "Urn": "CorpuserUrn",
-                "deprecated": true,
-                "doc": "List of ldap urn in this group.\nDeprecated! Replaced by GroupMembership aspect.",
-                "name": "members",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                },
+                "Urn": "Urn",
+                "default": null,
+                "doc": "List of jobs (if any) that use the model",
+                "name": "downstreamJobs",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
                 "urn_is_array": true
             },
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
-                            "corpGroup"
+                            "mlModelGroup"
                         ],
-                        "name": "IsPartOf"
+                        "isLineage": true,
+                        "isUpstream": false,
+                        "name": "MemberOf"
                     }
                 },
-                "Urn": "CorpGroupUrn",
-                "deprecated": true,
-                "doc": "List of groups in this group.\nDeprecated! This field is unused.",
+                "Urn": "Urn",
+                "default": null,
+                "doc": "Groups the model belongs to",
                 "name": "groups",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                },
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
                 "urn_is_array": true
-            },
+            }
+        ],
+        "name": "MLModelProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "editableMlFeatureProperties"
+        },
+        "doc": "Properties associated with a MLFeature editable from the UI",
+        "fields": [
             {
                 "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "A description of the group.",
+                "doc": "Documentation of the MLFeature",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
-            },
-            {
-                "default": null,
-                "doc": "Slack channel for the group",
-                "name": "slack",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
+            }
+        ],
+        "name": "EditableMLFeatureProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "editableMlModelGroupProperties"
+        },
+        "doc": "Properties associated with an ML Model Group editable from the UI",
+        "fields": [
             {
                 "Searchable": {
-                    "/time": {
-                        "fieldName": "createdTime",
-                        "fieldType": "DATETIME"
-                    }
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "Created Audit stamp",
-                "name": "created",
+                "doc": "Documentation of the ml model group",
+                "name": "description",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                    "string"
                 ]
             }
         ],
-        "name": "CorpGroupInfo",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "EditableMLModelGroupProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataPlatformInstanceProperties"
+            "name": "mlFeatureTableProperties"
         },
-        "doc": "Properties associated with a Data Platform Instance",
+        "doc": "Properties associated with a MLFeatureTable",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -3787,622 +3730,452 @@
                 "type": {
                     "type": "map",
                     "values": "string"
                 }
             },
             {
                 "Searchable": {
-                    "fieldType": "KEYWORD"
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
                 },
                 "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
+                "doc": "Documentation of the MLFeatureTable",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "mlFeature"
+                        ],
+                        "name": "Contains"
+                    }
+                },
                 "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
+                    "/*": {
+                        "fieldName": "features",
+                        "fieldType": "URN"
+                    }
                 },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "Display name of the Data Platform Instance",
-                "name": "name",
+                "doc": "List of features contained in the feature table",
+                "name": "mlFeatures",
                 "type": [
                     "null",
-                    "string"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "mlPrimaryKey"
+                        ],
+                        "name": "KeyedBy"
+                    }
+                },
                 "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
+                    "/*": {
+                        "fieldName": "primaryKeys",
+                        "fieldType": "URN"
+                    }
                 },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "Documentation of the Data Platform Instance",
-                "name": "description",
+                "doc": "List of primary keys in the feature table (if multiple, assumed to act as a composite key)",
+                "name": "mlPrimaryKeys",
                 "type": [
                     "null",
-                    "string"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             }
         ],
-        "name": "DataPlatformInstanceProperties",
-        "namespace": "com.linkedin.pegasus2avro.dataplatforminstance",
+        "name": "MLFeatureTableProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "assertionRunEvent",
-            "type": "timeseries"
+            "name": "mlModelQuantitativeAnalyses"
         },
-        "doc": "An event representing the current status of evaluating an assertion on a batch.\nAssertionRunEvent should be used for reporting the status of a run as an assertion evaluation progresses.",
+        "doc": "Quantitative analyses should be disaggregated, that is, broken down by the chosen factors. Quantitative analyses should provide the results of evaluating the MLModel according to the chosen metrics, providing confidence interval values when possible.",
         "fields": [
             {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
-            },
-            {
                 "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
+                "doc": "Link to a dashboard with results showing how the MLModel performed with respect to each factor",
+                "name": "unitaryResults",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
-                ]
-            },
-            {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
-                },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
-                "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
+                "doc": "Link to a dashboard with results showing how the MLModel performed with respect to the intersection of evaluated factors?",
+                "name": "intersectionalResults",
                 "type": [
                     "null",
                     "string"
                 ]
-            },
-            {
-                "doc": " Native (platform-specific) identifier for this run",
-                "name": "runId",
-                "type": "string"
-            },
-            {
-                "TimeseriesField": {},
-                "Urn": "Urn",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "assertionUrn",
-                "type": "string"
-            },
-            {
-                "TimeseriesField": {},
-                "Urn": "Urn",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "asserteeUrn",
-                "type": "string"
-            },
+            }
+        ],
+        "name": "QuantitativeAnalyses",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "intendedUse"
+        },
+        "doc": "Intended Use for the ML Model",
+        "fields": [
             {
                 "default": null,
-                "doc": "Specification of the batch which this run is evaluating",
-                "name": "batchSpec",
+                "doc": "Primary Use cases for the MLModel.",
+                "name": "primaryUses",
                 "type": [
                     "null",
                     {
-                        "doc": "A batch on which certain operations, e.g. data quality evaluation, is done.",
-                        "fields": [
-                            {
-                                "Searchable": {
-                                    "/*": {
-                                        "queryByDefault": true
-                                    }
-                                },
-                                "default": {},
-                                "doc": "Custom property bag.",
-                                "name": "customProperties",
-                                "type": {
-                                    "type": "map",
-                                    "values": "string"
-                                }
-                            },
-                            {
-                                "default": null,
-                                "doc": "The native identifier as specified by the system operating on the batch.",
-                                "name": "nativeBatchId",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "A query that identifies a batch of data",
-                                "name": "query",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Any limit to the number of rows in the batch, if applied",
-                                "name": "limit",
-                                "type": [
-                                    "null",
-                                    "int"
-                                ]
-                            }
-                        ],
-                        "name": "BatchSpec",
-                        "namespace": "com.linkedin.pegasus2avro.assertion",
-                        "type": "record"
+                        "items": "string",
+                        "type": "array"
                     }
                 ]
             },
             {
-                "TimeseriesField": {},
-                "doc": "The status of the assertion run as per this timeseries event.",
-                "name": "status",
-                "type": {
-                    "name": "AssertionRunStatus",
-                    "namespace": "com.linkedin.pegasus2avro.assertion",
-                    "symbolDocs": {
-                        "COMPLETE": "The Assertion Run has completed"
-                    },
-                    "symbols": [
-                        "COMPLETE"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
                 "default": null,
-                "doc": "Results of assertion, present if the status is COMPLETE",
-                "name": "result",
+                "doc": "Primary Intended Users - For example, was the MLModel developed for entertainment purposes, for hobbyists, or enterprise solutions?",
+                "name": "primaryUsers",
                 "type": [
                     "null",
                     {
-                        "doc": "The result of running an assertion",
-                        "fields": [
-                            {
-                                "TimeseriesField": {},
-                                "doc": " The final result, e.g. either SUCCESS or FAILURE.",
-                                "name": "type",
-                                "type": {
-                                    "name": "AssertionResultType",
-                                    "namespace": "com.linkedin.pegasus2avro.assertion",
-                                    "symbolDocs": {
-                                        "FAILURE": " The Assertion Failed",
-                                        "SUCCESS": " The Assertion Succeeded"
-                                    },
-                                    "symbols": [
-                                        "SUCCESS",
-                                        "FAILURE"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "default": null,
-                                "doc": "Number of rows for evaluated batch",
-                                "name": "rowCount",
-                                "type": [
-                                    "null",
-                                    "long"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Number of rows with missing value for evaluated batch",
-                                "name": "missingCount",
-                                "type": [
-                                    "null",
-                                    "long"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Number of rows with unexpected value for evaluated batch",
-                                "name": "unexpectedCount",
-                                "type": [
-                                    "null",
-                                    "long"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Observed aggregate value for evaluated batch",
-                                "name": "actualAggValue",
-                                "type": [
-                                    "null",
-                                    "float"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Other results of evaluation",
-                                "name": "nativeResults",
-                                "type": [
-                                    "null",
-                                    {
-                                        "type": "map",
-                                        "values": "string"
-                                    }
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "URL where full results are available",
-                                "name": "externalUrl",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            }
-                        ],
-                        "name": "AssertionResult",
-                        "namespace": "com.linkedin.pegasus2avro.assertion",
-                        "type": "record"
+                        "items": {
+                            "name": "IntendedUserType",
+                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                            "symbols": [
+                                "ENTERPRISE",
+                                "HOBBY",
+                                "ENTERTAINMENT"
+                            ],
+                            "type": "enum"
+                        },
+                        "type": "array"
                     }
                 ]
             },
             {
                 "default": null,
-                "doc": "Runtime parameters of evaluation",
-                "name": "runtimeContext",
+                "doc": "Highlight technology that the MLModel might easily be confused with, or related contexts that users could try to apply the MLModel to.",
+                "name": "outOfScopeUses",
                 "type": [
                     "null",
                     {
-                        "type": "map",
-                        "values": "string"
+                        "items": "string",
+                        "type": "array"
                     }
                 ]
             }
         ],
-        "name": "AssertionRunEvent",
-        "namespace": "com.linkedin.pegasus2avro.assertion",
+        "name": "IntendedUse",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "assertionInfo"
+            "name": "dataHubPolicyInfo"
         },
-        "doc": "Information about an assertion",
+        "doc": "Information about a DataHub (UI) access policy.",
         "fields": [
             {
                 "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
+                    "fieldType": "TEXT_PARTIAL"
                 },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
+                "doc": "Display name of the Policy",
+                "name": "displayName",
+                "type": "string"
             },
             {
                 "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
-                "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                    "fieldType": "TEXT"
                 },
-                "name": "externalUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "doc": "Description of the Policy",
+                "name": "description",
+                "type": "string"
             },
             {
-                "doc": "Type of assertion. Assertion types can evolve to span Datasets, Flows (Pipelines), Models, Features etc.",
+                "doc": "The type of policy",
                 "name": "type",
-                "type": {
-                    "name": "AssertionType",
-                    "namespace": "com.linkedin.pegasus2avro.assertion",
-                    "symbols": [
-                        "DATASET"
-                    ],
-                    "type": "enum"
-                }
+                "type": "string"
+            },
+            {
+                "doc": "The state of policy, ACTIVE or INACTIVE",
+                "name": "state",
+                "type": "string"
             },
             {
                 "default": null,
-                "doc": "Dataset Assertion information when type is DATASET",
-                "name": "datasetAssertion",
+                "doc": "The resource that the policy applies to. Not required for some 'Platform' privileges.",
+                "name": "resources",
                 "type": [
                     "null",
                     {
-                        "doc": "Attributes that are applicable to single-Dataset Assertions",
+                        "doc": "Information used to filter DataHub resource.",
                         "fields": [
                             {
-                                "Relationship": {
-                                    "entityTypes": [
-                                        "dataset"
-                                    ],
-                                    "name": "Asserts"
-                                },
-                                "Urn": "Urn",
-                                "doc": "The dataset targeted by this assertion.",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "dataset",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "Scope of the Assertion. What part of the dataset does this assertion apply to?",
-                                "name": "scope",
-                                "type": {
-                                    "name": "DatasetAssertionScope",
-                                    "namespace": "com.linkedin.pegasus2avro.assertion",
-                                    "symbolDocs": {
-                                        "DATASET_COLUMN": "This assertion applies to dataset columns",
-                                        "DATASET_ROWS": "This assertion applies to entire rows of the dataset",
-                                        "DATASET_SCHEMA": "This assertion applies to the schema of the dataset",
-                                        "UNKNOWN": "The scope of the assertion is unknown"
-                                    },
-                                    "symbols": [
-                                        "DATASET_COLUMN",
-                                        "DATASET_ROWS",
-                                        "DATASET_SCHEMA",
-                                        "UNKNOWN"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "Relationship": {
-                                    "/*": {
-                                        "entityTypes": [
-                                            "schemaField"
-                                        ],
-                                        "name": "Asserts"
-                                    }
-                                },
-                                "Urn": "Urn",
                                 "default": null,
-                                "doc": "One or more dataset schema fields that are targeted by this assertion",
-                                "name": "fields",
+                                "deprecated": true,
+                                "doc": "The type of resource that the policy applies to. This will most often be a data asset entity name, for\nexample 'dataset'. It is not strictly required because in the future we will want to support filtering a resource\nby domain, as well.",
+                                "name": "type",
                                 "type": [
                                     "null",
-                                    {
-                                        "items": "string",
-                                        "type": "array"
-                                    }
-                                ],
-                                "urn_is_array": true
+                                    "string"
+                                ]
                             },
                             {
                                 "default": null,
-                                "doc": "Standardized assertion operator",
-                                "name": "aggregation",
+                                "deprecated": true,
+                                "doc": "A specific set of resources to apply the policy to, e.g. asset urns",
+                                "name": "resources",
                                 "type": [
                                     "null",
                                     {
-                                        "doc": "The function that is applied to the aggregation input (schema, rows, column values) before evaluating an operator.",
-                                        "name": "AssertionStdAggregation",
-                                        "namespace": "com.linkedin.pegasus2avro.assertion",
-                                        "symbolDocs": {
-                                            "COLUMNS": "Assertion is applied on all columns.",
-                                            "COLUMN_COUNT": "Assertion is applied on number of columns.",
-                                            "IDENTITY": "Assertion is applied on individual column value.",
-                                            "MAX": "Assertion is applied on column std deviation",
-                                            "MEAN": "Assertion is applied on column mean",
-                                            "MEDIAN": "Assertion is applied on column median",
-                                            "MIN": "Assertion is applied on column min",
-                                            "NULL_COUNT": "Assertion is applied on number of null values in column",
-                                            "NULL_PROPORTION": "Assertion is applied on proportion of null values in column",
-                                            "ROW_COUNT": "Assertion is applied on number of rows.",
-                                            "STDDEV": "Assertion is applied on column std deviation",
-                                            "SUM": "Assertion is applied on column sum",
-                                            "UNIQUE_COUNT": "Assertion is applied on number of distinct values in column",
-                                            "UNIQUE_PROPOTION": "Assertion is applied on proportion of distinct values in column",
-                                            "_NATIVE_": "Other"
-                                        },
-                                        "symbols": [
-                                            "ROW_COUNT",
-                                            "COLUMNS",
-                                            "COLUMN_COUNT",
-                                            "IDENTITY",
-                                            "MEAN",
-                                            "MEDIAN",
-                                            "UNIQUE_COUNT",
-                                            "UNIQUE_PROPOTION",
-                                            "NULL_COUNT",
-                                            "NULL_PROPORTION",
-                                            "STDDEV",
-                                            "MIN",
-                                            "MAX",
-                                            "SUM",
-                                            "_NATIVE_"
-                                        ],
-                                        "type": "enum"
+                                        "items": "string",
+                                        "type": "array"
                                     }
                                 ]
                             },
                             {
-                                "doc": "Standardized assertion operator",
-                                "name": "operator",
-                                "type": {
-                                    "doc": "A boolean operator that is applied on the input to an assertion, after an aggregation function has been applied.",
-                                    "name": "AssertionStdOperator",
-                                    "namespace": "com.linkedin.pegasus2avro.assertion",
-                                    "symbolDocs": {
-                                        "BETWEEN": "Value being asserted is between min_value and max_value.  Requires 'minValue' & 'maxValue' parameters.",
-                                        "CONTAIN": "Value being asserted contains value. Requires 'value' parameter.",
-                                        "END_WITH": "Value being asserted ends with value. Requires 'value' parameter.",
-                                        "EQUAL_TO": "Value being asserted is equal to value. Requires 'value' parameter.",
-                                        "GREATER_THAN": "Value being asserted is greater than some value. Requires 'value' parameter.",
-                                        "GREATER_THAN_OR_EQUAL_TO": "Value being asserted is greater than or equal to some value. Requires 'value' parameter.",
-                                        "IN": "Value being asserted is one of the array values. Requires 'value' parameter.",
-                                        "LESS_THAN": "Value being asserted is less than a max value. Requires 'value' parameter.",
-                                        "LESS_THAN_OR_EQUAL_TO": "Value being asserted is less than or equal to some value. Requires 'value' parameter.",
-                                        "NOT_IN": "Value being asserted is not in one of the array values. Requires 'value' parameter.",
-                                        "NOT_NULL": "Value being asserted is not null. Requires no parameters.",
-                                        "REGEX_MATCH": "Value being asserted matches the regex value. Requires 'value' parameter.",
-                                        "START_WITH": "Value being asserted starts with value. Requires 'value' parameter.",
-                                        "_NATIVE_": "Other"
-                                    },
-                                    "symbols": [
-                                        "BETWEEN",
-                                        "LESS_THAN",
-                                        "LESS_THAN_OR_EQUAL_TO",
-                                        "GREATER_THAN",
-                                        "GREATER_THAN_OR_EQUAL_TO",
-                                        "EQUAL_TO",
-                                        "NOT_NULL",
-                                        "CONTAIN",
-                                        "END_WITH",
-                                        "START_WITH",
-                                        "REGEX_MATCH",
-                                        "IN",
-                                        "NOT_IN",
-                                        "_NATIVE_"
-                                    ],
-                                    "type": "enum"
-                                }
+                                "default": false,
+                                "deprecated": true,
+                                "doc": "Whether the policy should be applied to all assets matching the filter.",
+                                "name": "allResources",
+                                "type": "boolean"
                             },
                             {
                                 "default": null,
-                                "doc": "Standard parameters required for the assertion. e.g. min_value, max_value, value, columns",
-                                "name": "parameters",
+                                "doc": "Filter to apply privileges to",
+                                "name": "filter",
                                 "type": [
                                     "null",
                                     {
-                                        "doc": "Parameters for AssertionStdOperators.",
+                                        "doc": "The filter for specifying the resource or actor to apply privileges to",
                                         "fields": [
                                             {
-                                                "default": null,
-                                                "doc": "The value parameter of an assertion",
-                                                "name": "value",
-                                                "type": [
-                                                    "null",
-                                                    {
-                                                        "doc": "Single parameter for AssertionStdOperators.",
+                                                "doc": "A list of criteria to apply conjunctively (so all criteria must pass)",
+                                                "name": "criteria",
+                                                "type": {
+                                                    "items": {
+                                                        "doc": "A criterion for matching a field with given value",
                                                         "fields": [
                                                             {
-                                                                "doc": "The parameter value",
-                                                                "name": "value",
+                                                                "doc": "The name of the field that the criterion refers to",
+                                                                "name": "field",
                                                                 "type": "string"
                                                             },
                                                             {
-                                                                "doc": "The type of the parameter",
-                                                                "name": "type",
+                                                                "doc": "Values. Matches criterion if any one of the values matches condition (OR-relationship)",
+                                                                "name": "values",
                                                                 "type": {
-                                                                    "name": "AssertionStdParameterType",
-                                                                    "namespace": "com.linkedin.pegasus2avro.assertion",
+                                                                    "items": "string",
+                                                                    "type": "array"
+                                                                }
+                                                            },
+                                                            {
+                                                                "default": "EQUALS",
+                                                                "doc": "The condition for the criterion",
+                                                                "name": "condition",
+                                                                "type": {
+                                                                    "doc": "The matching condition in a filter criterion",
+                                                                    "name": "PolicyMatchCondition",
+                                                                    "namespace": "com.linkedin.pegasus2avro.policy",
+                                                                    "symbolDocs": {
+                                                                        "EQUALS": "Whether the field matches the value"
+                                                                    },
                                                                     "symbols": [
-                                                                        "STRING",
-                                                                        "NUMBER",
-                                                                        "LIST",
-                                                                        "SET",
-                                                                        "UNKNOWN"
+                                                                        "EQUALS"
                                                                     ],
                                                                     "type": "enum"
                                                                 }
                                                             }
                                                         ],
-                                                        "name": "AssertionStdParameter",
-                                                        "namespace": "com.linkedin.pegasus2avro.assertion",
+                                                        "name": "PolicyMatchCriterion",
+                                                        "namespace": "com.linkedin.pegasus2avro.policy",
                                                         "type": "record"
-                                                    }
-                                                ]
-                                            },
-                                            {
-                                                "default": null,
-                                                "doc": "The maxValue parameter of an assertion",
-                                                "name": "maxValue",
-                                                "type": [
-                                                    "null",
-                                                    "com.linkedin.pegasus2avro.assertion.AssertionStdParameter"
-                                                ]
-                                            },
-                                            {
-                                                "default": null,
-                                                "doc": "The minValue parameter of an assertion",
-                                                "name": "minValue",
-                                                "type": [
-                                                    "null",
-                                                    "com.linkedin.pegasus2avro.assertion.AssertionStdParameter"
-                                                ]
+                                                    },
+                                                    "type": "array"
+                                                }
                                             }
                                         ],
-                                        "name": "AssertionStdParameters",
-                                        "namespace": "com.linkedin.pegasus2avro.assertion",
+                                        "name": "PolicyMatchFilter",
+                                        "namespace": "com.linkedin.pegasus2avro.policy",
                                         "type": "record"
                                     }
                                 ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Native assertion type",
-                                "name": "nativeType",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Native parameters required for the assertion.",
-                                "name": "nativeParameters",
-                                "type": [
-                                    "null",
-                                    {
-                                        "type": "map",
-                                        "values": "string"
-                                    }
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "name": "logic",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
                             }
                         ],
-                        "name": "DatasetAssertionInfo",
-                        "namespace": "com.linkedin.pegasus2avro.assertion",
+                        "name": "DataHubResourceFilter",
+                        "namespace": "com.linkedin.pegasus2avro.policy",
                         "type": "record"
                     }
                 ]
+            },
+            {
+                "doc": "The privileges that the policy grants.",
+                "name": "privileges",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                }
+            },
+            {
+                "doc": "The actors that the policy applies to.",
+                "name": "actors",
+                "type": {
+                    "doc": "Information used to filter DataHub actors.",
+                    "fields": [
+                        {
+                            "Urn": "Urn",
+                            "default": null,
+                            "doc": "A specific set of users to apply the policy to (disjunctive)",
+                            "name": "users",
+                            "type": [
+                                "null",
+                                {
+                                    "items": "string",
+                                    "type": "array"
+                                }
+                            ],
+                            "urn_is_array": true
+                        },
+                        {
+                            "Urn": "Urn",
+                            "default": null,
+                            "doc": "A specific set of groups to apply the policy to (disjunctive)",
+                            "name": "groups",
+                            "type": [
+                                "null",
+                                {
+                                    "items": "string",
+                                    "type": "array"
+                                }
+                            ],
+                            "urn_is_array": true
+                        },
+                        {
+                            "default": false,
+                            "doc": "Whether the filter should return true for owners of a particular resource.\nOnly applies to policies of type 'Metadata', which have a resource associated with them.",
+                            "name": "resourceOwners",
+                            "type": "boolean"
+                        },
+                        {
+                            "default": false,
+                            "doc": "Whether the filter should apply to all users.",
+                            "name": "allUsers",
+                            "type": "boolean"
+                        },
+                        {
+                            "default": false,
+                            "doc": "Whether the filter should apply to all groups.",
+                            "name": "allGroups",
+                            "type": "boolean"
+                        },
+                        {
+                            "Relationship": {
+                                "/*": {
+                                    "entityTypes": [
+                                        "dataHubRole"
+                                    ],
+                                    "name": "IsAssociatedWithRole"
+                                }
+                            },
+                            "Urn": "Urn",
+                            "default": null,
+                            "doc": "A specific set of roles to apply the policy to (disjunctive).",
+                            "name": "roles",
+                            "type": [
+                                "null",
+                                {
+                                    "items": "string",
+                                    "type": "array"
+                                }
+                            ],
+                            "urn_is_array": true
+                        }
+                    ],
+                    "name": "DataHubActorFilter",
+                    "namespace": "com.linkedin.pegasus2avro.policy",
+                    "type": "record"
+                }
+            },
+            {
+                "default": true,
+                "doc": "Whether the policy should be editable via the UI",
+                "name": "editable",
+                "type": "boolean"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "DATETIME"
+                },
+                "default": null,
+                "doc": "Timestamp when the policy was last updated",
+                "name": "lastUpdatedTimestamp",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            }
+        ],
+        "name": "DataHubPolicyInfo",
+        "namespace": "com.linkedin.pegasus2avro.policy",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataHubRoleInfo"
+        },
+        "doc": "Information about a DataHub Role.",
+        "fields": [
+            {
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Name of the Role",
+                "name": "name",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT"
+                },
+                "doc": "Description of the Role",
+                "name": "description",
+                "type": "string"
+            },
+            {
+                "default": false,
+                "doc": "Whether the role should be editable via the UI",
+                "name": "editable",
+                "type": "boolean"
             }
         ],
-        "name": "AssertionInfo",
-        "namespace": "com.linkedin.pegasus2avro.assertion",
+        "name": "DataHubRoleInfo",
+        "namespace": "com.linkedin.pegasus2avro.policy",
         "type": "record"
     },
     {
         "Aspect": {
             "name": "dataHubStepStateProperties"
         },
         "doc": "The properties associated with a DataHub step state",
@@ -4424,98 +4197,100 @@
         ],
         "name": "DataHubStepStateProperties",
         "namespace": "com.linkedin.pegasus2avro.step",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataHubSecretValue"
+            "name": "dataPlatformInfo"
         },
-        "doc": "The value of a DataHub Secret",
+        "doc": "Information about a data platform",
         "fields": [
             {
                 "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": false,
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "The display name for the secret",
+                "doc": "Name of the data platform",
                 "name": "name",
-                "type": "string"
-            },
-            {
-                "doc": "The AES-encrypted value of the DataHub secret.",
-                "name": "value",
-                "type": "string"
-            },
-            {
-                "default": null,
-                "doc": "Description of the secret",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "createdTime",
-                        "fieldType": "DATETIME"
+                "type": "string",
+                "validate": {
+                    "strlen": {
+                        "max": 15
                     }
-                },
-                "default": null,
-                "doc": "Created Audit stamp",
-                "name": "created",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
-            }
-        ],
-        "name": "DataHubSecretValue",
-        "namespace": "com.linkedin.pegasus2avro.secret",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "tagProperties"
-        },
-        "doc": "Properties associated with a Tag",
-        "fields": [
+                }
+            },
             {
                 "Searchable": {
                     "boostScore": 10.0,
                     "enableAutocomplete": true,
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "Display name of the tag",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "Searchable": {},
                 "default": null,
-                "doc": "Documentation of the tag",
-                "name": "description",
+                "doc": "The name that will be used for displaying a platform type.",
+                "name": "displayName",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "doc": "Platform type this data platform describes",
+                "name": "type",
+                "type": {
+                    "doc": "Platform types available at LinkedIn",
+                    "name": "PlatformType",
+                    "namespace": "com.linkedin.pegasus2avro.dataplatform",
+                    "symbolDocs": {
+                        "FILE_SYSTEM": "Value for a file system, e.g. hdfs",
+                        "KEY_VALUE_STORE": "Value for a key value store, e.g. espresso, voldemort",
+                        "MESSAGE_BROKER": "Value for a message broker, e.g. kafka",
+                        "OBJECT_STORE": "Value for an object store, e.g. ambry",
+                        "OLAP_DATASTORE": "Value for an OLAP datastore, e.g. pinot",
+                        "OTHERS": "Value for other platforms, e.g salesforce, dovetail",
+                        "QUERY_ENGINE": "Value for a query engine, e.g. presto",
+                        "RELATIONAL_DB": "Value for a relational database, e.g. oracle, mysql",
+                        "SEARCH_ENGINE": "Value for a search engine, e.g seas"
+                    },
+                    "symbols": [
+                        "FILE_SYSTEM",
+                        "KEY_VALUE_STORE",
+                        "MESSAGE_BROKER",
+                        "OBJECT_STORE",
+                        "OLAP_DATASTORE",
+                        "OTHERS",
+                        "QUERY_ENGINE",
+                        "RELATIONAL_DB",
+                        "SEARCH_ENGINE"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "doc": "The delimiter in the dataset names on the data platform, e.g. '/' for HDFS and '.' for Oracle",
+                "name": "datasetNameDelimiter",
+                "type": "string"
+            },
+            {
                 "default": null,
-                "doc": "The color associated with the Tag in Hex. For example #FFFFFF.",
-                "name": "colorHex",
+                "doc": "The URL for a logo associated with the platform",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "logoUrl",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "TagProperties",
-        "namespace": "com.linkedin.pegasus2avro.tag",
+        "name": "DataPlatformInfo",
+        "namespace": "com.linkedin.pegasus2avro.dataplatform",
         "type": "record"
     },
     {
         "Aspect": {
             "name": "dataHubExecutionRequestResult"
         },
         "doc": "The result of an execution request",
@@ -4700,216 +4475,112 @@
         ],
         "name": "ExecutionRequestSignal",
         "namespace": "com.linkedin.pegasus2avro.execution",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dashboardInfo"
+            "name": "chartQuery"
         },
-        "doc": "Information about a dashboard",
+        "doc": "Information for chart query which is used for getting data of the chart",
         "fields": [
             {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
-            },
-            {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
-                "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Title of the dashboard",
-                "name": "title",
+                "doc": "Raw query to build a chart from input datasets",
+                "name": "rawQuery",
                 "type": "string"
             },
             {
                 "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
-                },
-                "doc": "Detailed description about the dashboard",
-                "name": "description",
-                "type": "string"
-            },
-            {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "chart"
-                        ],
-                        "isLineage": true,
-                        "name": "Contains"
-                    }
+                    "addToFilters": true,
+                    "fieldName": "queryType",
+                    "fieldType": "KEYWORD",
+                    "filterNameOverride": "Query Type"
                 },
-                "Urn": "ChartUrn",
-                "default": [],
-                "deprecated": true,
-                "doc": "Charts in a dashboard\nDeprecated! Use chartEdges instead.",
-                "name": "charts",
+                "doc": "Chart query type",
+                "name": "type",
                 "type": {
-                    "items": "string",
-                    "type": "array"
-                },
-                "urn_is_array": true
-            },
-            {
-                "Relationship": {
-                    "/*/destinationUrn": {
-                        "createdActor": "chartEdges/*/created/actor",
-                        "createdOn": "chartEdges/*/created/time",
-                        "entityTypes": [
-                            "chart"
-                        ],
-                        "isLineage": true,
-                        "name": "Contains",
-                        "properties": "chartEdges/*/properties",
-                        "updatedActor": "chartEdges/*/lastModified/actor",
-                        "updatedOn": "chartEdges/*/lastModified/time"
-                    }
-                },
-                "default": null,
-                "doc": "Charts in a dashboard",
-                "name": "chartEdges",
-                "type": [
-                    "null",
-                    {
-                        "items": "com.linkedin.pegasus2avro.common.Edge",
-                        "type": "array"
-                    }
-                ]
-            },
+                    "name": "ChartQueryType",
+                    "namespace": "com.linkedin.pegasus2avro.chart",
+                    "symbolDocs": {
+                        "LOOKML": "LookML queries",
+                        "SQL": "SQL type queries"
+                    },
+                    "symbols": [
+                        "LOOKML",
+                        "SQL"
+                    ],
+                    "type": "enum"
+                }
+            }
+        ],
+        "name": "ChartQuery",
+        "namespace": "com.linkedin.pegasus2avro.chart",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "editableChartProperties"
+        },
+        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
+        "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "Consumes"
-                    }
-                },
-                "Urn": "Urn",
-                "default": [],
-                "deprecated": true,
-                "doc": "Datasets consumed by a dashboard\nDeprecated! Use datasetEdges instead.",
-                "name": "datasets",
-                "type": {
-                    "items": "string",
-                    "type": "array"
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
                 },
-                "urn_is_array": true
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             },
             {
-                "Relationship": {
-                    "/*/destinationUrn": {
-                        "createdActor": "datasetEdges/*/created/actor",
-                        "createdOn": "datasetEdges/*/created/time",
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "Consumes",
-                        "properties": "datasetEdges/*/properties",
-                        "updatedActor": "datasetEdges/*/lastModified/actor",
-                        "updatedOn": "datasetEdges/*/lastModified/time"
-                    }
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
                 },
-                "default": null,
-                "doc": "Datasets consumed by a dashboard",
-                "name": "datasetEdges",
-                "type": [
-                    "null",
-                    {
-                        "items": "com.linkedin.pegasus2avro.common.Edge",
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "doc": "Captures information about who created/last modified/deleted this dashboard and when",
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
                 "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
                 "default": null,
-                "doc": "URL for the dashboard. This could be used as an external link on DataHub to allow users access/view the dashboard",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "dashboardUrl",
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
                 ]
             },
             {
                 "Searchable": {
-                    "addToFilters": true,
-                    "fieldType": "KEYWORD",
-                    "filterNameOverride": "Access Level"
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "Access level for the dashboard",
-                "name": "access",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AccessLevel"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The time when this dashboard last refreshed",
-                "name": "lastRefreshed",
+                "doc": "Edited documentation of the chart ",
+                "name": "description",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             }
         ],
-        "name": "DashboardInfo",
-        "namespace": "com.linkedin.pegasus2avro.dashboard",
+        "name": "EditableChartProperties",
+        "namespace": "com.linkedin.pegasus2avro.chart",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dashboardUsageStatistics",
+            "name": "chartUsageStatistics",
             "type": "timeseries"
         },
-        "doc": "Experimental (Subject to breaking change) -- Stats corresponding to dashboard's usage.\n\nIf this aspect represents the latest snapshot of the statistics about a Dashboard, the eventGranularity field should be null. \nIf this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly. ",
+        "doc": "Experimental (Subject to breaking change) -- Stats corresponding to chart's usage.\n\nIf this aspect represents the latest snapshot of the statistics about a Chart, the eventGranularity field should be null.\nIf this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly.",
         "fields": [
             {
                 "doc": "The event timestamp field as epoch at UTC in milli seconds.",
                 "name": "timestampMillis",
                 "type": "long"
             },
             {
@@ -4942,34 +4613,24 @@
                     "null",
                     "string"
                 ]
             },
             {
                 "TimeseriesField": {},
                 "default": null,
-                "doc": "The total number of times dashboard has been viewed",
+                "doc": "The total number of times chart has been viewed",
                 "name": "viewsCount",
                 "type": [
                     "null",
                     "int"
                 ]
             },
             {
                 "TimeseriesField": {},
                 "default": null,
-                "doc": "The total number of dashboard executions (refreshes / syncs) ",
-                "name": "executionsCount",
-                "type": [
-                    "null",
-                    "int"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
                 "doc": "Unique user count",
                 "name": "uniqueUserCount",
                 "type": [
                     "null",
                     "int"
                 ]
             },
@@ -4994,722 +4655,298 @@
                                     },
                                     "name": "user",
                                     "type": "string"
                                 },
                                 {
                                     "TimeseriesField": {},
                                     "default": null,
-                                    "doc": "The number of times the user has viewed the dashboard",
+                                    "doc": "The number of times the user has viewed the chart",
                                     "name": "viewsCount",
                                     "type": [
                                         "null",
                                         "int"
                                     ]
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "default": null,
-                                    "doc": "The number of times the user has executed (refreshed) the dashboard",
-                                    "name": "executionsCount",
-                                    "type": [
-                                        "null",
-                                        "int"
-                                    ]
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "default": null,
-                                    "doc": "Normalized numeric metric representing user's dashboard usage -- the number of times the user executed or viewed the dashboard. ",
-                                    "name": "usageCount",
-                                    "type": [
-                                        "null",
-                                        "int"
-                                    ]
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "default": null,
-                                    "doc": "If user_email is set, we attempt to resolve the user's urn upon ingest",
-                                    "name": "userEmail",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
                                 }
                             ],
-                            "name": "DashboardUserUsageCounts",
-                            "namespace": "com.linkedin.pegasus2avro.dashboard",
+                            "name": "ChartUserUsageCounts",
+                            "namespace": "com.linkedin.pegasus2avro.chart",
                             "type": "record"
                         },
                         "type": "array"
                     }
                 ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "The total number of times that the dashboard has been favorited ",
-                "name": "favoritesCount",
-                "type": [
-                    "null",
-                    "int"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "Last viewed at\n\nThis should not be set in cases where statistics are windowed. ",
-                "name": "lastViewedAt",
-                "type": [
-                    "null",
-                    "long"
-                ]
             }
         ],
-        "name": "DashboardUsageStatistics",
-        "namespace": "com.linkedin.pegasus2avro.dashboard",
+        "name": "ChartUsageStatistics",
+        "namespace": "com.linkedin.pegasus2avro.chart",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableDashboardProperties"
+            "name": "chartInfo"
         },
-        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
+        "doc": "Information about a chart",
         "fields": [
             {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
                 },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
             },
             {
                 "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                 },
-                "default": null,
-                "doc": "Edited documentation of the dashboard",
-                "name": "description",
+                "name": "externalUrl",
                 "type": [
                     "null",
                     "string"
                 ]
-            }
-        ],
-        "name": "EditableDashboardProperties",
-        "namespace": "com.linkedin.pegasus2avro.dashboard",
-        "type": "record"
-    },
-    {
-        "Event": {
-            "name": "entityChangeEvent"
-        },
-        "doc": "Shared fields for all entity change events.",
-        "fields": [
-            {
-                "doc": "The type of the entity affected. Corresponds to the entity registry, e.g. 'dataset', 'chart', 'dashboard', etc.",
-                "name": "entityType",
-                "type": "string"
             },
             {
-                "Urn": "Urn",
-                "doc": "The urn of the entity which was affected.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                "Searchable": {
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
                 },
-                "name": "entityUrn",
-                "type": "string"
-            },
-            {
-                "doc": "The category type (TAG, GLOSSARY_TERM, OWNERSHIP, TECHNICAL_SCHEMA, etc). This is used to determine what the rest of the schema will look like.",
-                "name": "category",
+                "doc": "Title of the chart",
+                "name": "title",
                 "type": "string"
             },
             {
-                "doc": "The operation type. This is used to determine what the rest of the schema will look like.",
-                "name": "operation",
+                "Searchable": {},
+                "doc": "Detailed description about the chart",
+                "name": "description",
                 "type": "string"
             },
             {
-                "default": null,
-                "doc": "The urn of the entity which was affected.",
-                "name": "modifier",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Arbitrary key-value parameters corresponding to the event.",
-                "name": "parameters",
-                "type": [
-                    "null",
-                    {
-                        "doc": "Arbitrary key-value parameters for an Entity Change Event. (any record).",
-                        "fields": [],
-                        "name": "Parameters",
-                        "namespace": "com.linkedin.pegasus2avro.platform.event.v1",
-                        "type": "record"
-                    }
-                ]
-            },
-            {
-                "doc": "Audit stamp of the operation",
-                "name": "auditStamp",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "doc": "The version of the event type, incremented in integers.",
-                "name": "version",
-                "type": "int"
-            }
-        ],
-        "name": "EntityChangeEvent",
-        "namespace": "com.linkedin.pegasus2avro.platform.event.v1",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "editableDataJobProperties"
-        },
-        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
-        "fields": [
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                "doc": "Captures information about who created/last modified/deleted this chart and when",
                 "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "type": {
+                    "doc": "Data captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into various lifecycle stages, and who acted to move it into those lifecycle stages. The recommended best practice is to include this record in your record schema, and annotate its fields as @readOnly in your resource. See https://github.com/linkedin/rest.li/wiki/Validation-in-Rest.li#restli-validation-annotations",
+                    "fields": [
+                        {
+                            "default": {
+                                "actor": "urn:li:corpuser:unknown",
+                                "impersonator": null,
+                                "message": null,
+                                "time": 0
+                            },
+                            "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                            "name": "created",
+                            "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                        },
+                        {
+                            "default": {
+                                "actor": "urn:li:corpuser:unknown",
+                                "impersonator": null,
+                                "message": null,
+                                "time": 0
+                            },
+                            "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                            "name": "lastModified",
+                            "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                        },
+                        {
+                            "default": null,
+                            "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                            "name": "deleted",
+                            "type": [
+                                "null",
+                                "com.linkedin.pegasus2avro.common.AuditStamp"
+                            ]
+                        }
+                    ],
+                    "name": "ChangeAuditStamps",
+                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "type": "record"
+                }
             },
             {
                 "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
+                "doc": "URL for the chart. This could be used as an external link on DataHub to allow users access/view the chart",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                 },
-                "default": null,
-                "doc": "Edited documentation of the data job ",
-                "name": "description",
+                "name": "chartUrl",
                 "type": [
                     "null",
                     "string"
                 ]
-            }
-        ],
-        "name": "EditableDataJobProperties",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataJobInputOutput"
-        },
-        "doc": "Information about the inputs and outputs of a Data processing job",
-        "fields": [
-            {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "Consumes"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "inputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numInputDatasets",
-                        "queryByDefault": false
-                    }
-                },
-                "Urn": "DatasetUrn",
-                "deprecated": true,
-                "doc": "Input datasets consumed by the data job during processing\nDeprecated! Use inputDatasetEdges instead.",
-                "name": "inputDatasets",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                },
-                "urn_is_array": true
             },
             {
                 "Relationship": {
-                    "/*/destinationUrn": {
-                        "createdActor": "inputDatasetEdges/*/created/actor",
-                        "createdOn": "inputDatasetEdges/*/created/time",
+                    "/*/string": {
                         "entityTypes": [
                             "dataset"
                         ],
                         "isLineage": true,
-                        "name": "Consumes",
-                        "properties": "inputDatasetEdges/*/properties",
-                        "updatedActor": "inputDatasetEdges/*/lastModified/actor",
-                        "updatedOn": "inputDatasetEdges/*/lastModified/time"
-                    }
-                },
-                "Searchable": {
-                    "/*/destinationUrn": {
-                        "fieldName": "inputDatasetEdges",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numInputDatasets",
-                        "queryByDefault": false
+                        "name": "Consumes"
                     }
                 },
                 "default": null,
-                "doc": "Input datasets consumed by the data job during processing",
-                "name": "inputDatasetEdges",
+                "deprecated": true,
+                "doc": "Data sources for the chart\nDeprecated! Use inputEdges instead.",
+                "name": "inputs",
                 "type": [
                     "null",
                     {
-                        "items": "com.linkedin.pegasus2avro.common.Edge",
+                        "items": [
+                            "string"
+                        ],
                         "type": "array"
                     }
                 ]
             },
             {
                 "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "isUpstream": false,
-                        "name": "Produces"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "outputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numOutputDatasets",
-                        "queryByDefault": false
-                    }
-                },
-                "Urn": "DatasetUrn",
-                "deprecated": true,
-                "doc": "Output datasets produced by the data job during processing\nDeprecated! Use outputDatasetEdges instead.",
-                "name": "outputDatasets",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                },
-                "urn_is_array": true
-            },
-            {
-                "Relationship": {
                     "/*/destinationUrn": {
-                        "createdActor": "outputDatasetEdges/*/created/actor",
-                        "createdOn": "outputDatasetEdges/*/created/time",
+                        "createdActor": "inputEdges/*/created/actor",
+                        "createdOn": "inputEdges/*/created/time",
                         "entityTypes": [
                             "dataset"
                         ],
                         "isLineage": true,
-                        "isUpstream": false,
-                        "name": "Produces",
-                        "properties": "outputDatasetEdges/*/properties",
-                        "updatedActor": "outputDatasetEdges/*/lastModified/actor",
-                        "updatedOn": "outputDatasetEdges/*/lastModified/time"
-                    }
-                },
-                "Searchable": {
-                    "/*/destinationUrn": {
-                        "fieldName": "outputDatasetEdges",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numOutputDatasets",
-                        "queryByDefault": false
+                        "name": "Consumes",
+                        "properties": "inputEdges/*/properties",
+                        "updatedActor": "inputEdges/*/lastModified/actor",
+                        "updatedOn": "inputEdges/*/lastModified/time"
                     }
                 },
                 "default": null,
-                "doc": "Output datasets produced by the data job during processing",
-                "name": "outputDatasetEdges",
+                "doc": "Data sources for the chart",
+                "name": "inputEdges",
                 "type": [
                     "null",
                     {
                         "items": "com.linkedin.pegasus2avro.common.Edge",
                         "type": "array"
                     }
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataJob"
-                        ],
-                        "isLineage": true,
-                        "name": "DownstreamOf"
-                    }
+                "Searchable": {
+                    "addToFilters": true,
+                    "fieldType": "KEYWORD",
+                    "filterNameOverride": "Chart Type"
                 },
-                "Urn": "DataJobUrn",
                 "default": null,
-                "deprecated": true,
-                "doc": "Input datajobs that this data job depends on\nDeprecated! Use inputDatajobEdges instead.",
-                "name": "inputDatajobs",
+                "doc": "Type of the chart",
+                "name": "type",
                 "type": [
                     "null",
                     {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
-            },
-            {
-                "Relationship": {
-                    "/*/destinationUrn": {
-                        "createdActor": "inputDatajobEdges/*/created/actor",
-                        "createdOn": "inputDatajobEdges/*/created/time",
-                        "entityTypes": [
-                            "dataJob"
+                        "doc": "The various types of charts",
+                        "name": "ChartType",
+                        "namespace": "com.linkedin.pegasus2avro.chart",
+                        "symbolDocs": {
+                            "BAR": "Chart showing a Bar chart",
+                            "PIE": "Chart showing a Pie chart",
+                            "SCATTER": "Chart showing a Scatter plot",
+                            "TABLE": "Chart showing a table",
+                            "TEXT": "Chart showing Markdown formatted text"
+                        },
+                        "symbols": [
+                            "BAR",
+                            "PIE",
+                            "SCATTER",
+                            "TABLE",
+                            "TEXT",
+                            "LINE",
+                            "AREA",
+                            "HISTOGRAM",
+                            "BOX_PLOT",
+                            "WORD_CLOUD",
+                            "COHORT"
                         ],
-                        "isLineage": true,
-                        "name": "DownstreamOf",
-                        "properties": "inputDatajobEdges/*/properties",
-                        "updatedActor": "inputDatajobEdges/*/lastModified/actor",
-                        "updatedOn": "inputDatajobEdges/*/lastModified/time"
-                    }
-                },
-                "default": null,
-                "doc": "Input datajobs that this data job depends on",
-                "name": "inputDatajobEdges",
-                "type": [
-                    "null",
-                    {
-                        "items": "com.linkedin.pegasus2avro.common.Edge",
-                        "type": "array"
+                        "type": "enum"
                     }
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "schemaField"
-                        ],
-                        "name": "Consumes"
-                    }
-                },
                 "Searchable": {
-                    "/*": {
-                        "fieldName": "inputFields",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numInputFields",
-                        "queryByDefault": false
-                    }
+                    "addToFilters": true,
+                    "fieldType": "KEYWORD",
+                    "filterNameOverride": "Access Level"
                 },
-                "Urn": "Urn",
                 "default": null,
-                "doc": "Fields of the input datasets used by this job",
-                "name": "inputDatasetFields",
+                "doc": "Access level for the chart",
+                "name": "access",
                 "type": [
                     "null",
                     {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
-            },
-            {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "schemaField"
+                        "doc": "The various access levels",
+                        "name": "AccessLevel",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "symbolDocs": {
+                            "PRIVATE": "Private availability to certain set of users",
+                            "PUBLIC": "Publicly available access level"
+                        },
+                        "symbols": [
+                            "PUBLIC",
+                            "PRIVATE"
                         ],
-                        "name": "Produces"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "outputFields",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numOutputFields",
-                        "queryByDefault": false
-                    }
-                },
-                "Urn": "Urn",
-                "default": null,
-                "doc": "Fields of the output datasets this job writes to",
-                "name": "outputDatasetFields",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
+                        "type": "enum"
                     }
-                ],
-                "urn_is_array": true
+                ]
             },
             {
                 "default": null,
-                "doc": "Fine-grained column-level lineages\nNot currently supported in the UI\nUse UpstreamLineage aspect for datasets to express Column Level Lineage for the UI",
-                "name": "fineGrainedLineages",
+                "doc": "The time when this chart last refreshed",
+                "name": "lastRefreshed",
                 "type": [
                     "null",
-                    {
-                        "items": {
-                            "doc": "A fine-grained lineage from upstream fields/datasets to downstream field(s)",
-                            "fields": [
-                                {
-                                    "doc": "The type of upstream entity",
-                                    "name": "upstreamType",
-                                    "type": {
-                                        "doc": "The type of upstream entity in a fine-grained lineage",
-                                        "name": "FineGrainedLineageUpstreamType",
-                                        "namespace": "com.linkedin.pegasus2avro.dataset",
-                                        "symbolDocs": {
-                                            "DATASET": " Indicates that this lineage is originating from upstream dataset(s)",
-                                            "FIELD_SET": " Indicates that this lineage is originating from upstream field(s)",
-                                            "NONE": " Indicates that there is no upstream lineage i.e. the downstream field is not a derived field"
-                                        },
-                                        "symbols": [
-                                            "FIELD_SET",
-                                            "DATASET",
-                                            "NONE"
-                                        ],
-                                        "type": "enum"
-                                    }
-                                },
-                                {
-                                    "Urn": "Urn",
-                                    "default": null,
-                                    "doc": "Upstream entities in the lineage",
-                                    "name": "upstreams",
-                                    "type": [
-                                        "null",
-                                        {
-                                            "items": "string",
-                                            "type": "array"
-                                        }
-                                    ],
-                                    "urn_is_array": true
-                                },
-                                {
-                                    "doc": "The type of downstream field(s)",
-                                    "name": "downstreamType",
-                                    "type": {
-                                        "doc": "The type of downstream field(s) in a fine-grained lineage",
-                                        "name": "FineGrainedLineageDownstreamType",
-                                        "namespace": "com.linkedin.pegasus2avro.dataset",
-                                        "symbolDocs": {
-                                            "FIELD": " Indicates that the lineage is for a single, specific, downstream field",
-                                            "FIELD_SET": " Indicates that the lineage is for a set of downstream fields"
-                                        },
-                                        "symbols": [
-                                            "FIELD",
-                                            "FIELD_SET"
-                                        ],
-                                        "type": "enum"
-                                    }
-                                },
-                                {
-                                    "Urn": "Urn",
-                                    "default": null,
-                                    "doc": "Downstream fields in the lineage",
-                                    "name": "downstreams",
-                                    "type": [
-                                        "null",
-                                        {
-                                            "items": "string",
-                                            "type": "array"
-                                        }
-                                    ],
-                                    "urn_is_array": true
-                                },
-                                {
-                                    "default": null,
-                                    "doc": "The transform operation applied to the upstream entities to produce the downstream field(s)",
-                                    "name": "transformOperation",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
-                                },
-                                {
-                                    "default": 1.0,
-                                    "doc": "The confidence in this lineage between 0 (low confidence) and 1 (high confidence)",
-                                    "name": "confidenceScore",
-                                    "type": "float"
-                                }
-                            ],
-                            "name": "FineGrainedLineage",
-                            "namespace": "com.linkedin.pegasus2avro.dataset",
-                            "type": "record"
-                        },
-                        "type": "array"
-                    }
+                    "long"
                 ]
             }
         ],
-        "name": "DataJobInputOutput",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "name": "ChartInfo",
+        "namespace": "com.linkedin.pegasus2avro.chart",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableDataFlowProperties"
+            "name": "editableContainerProperties"
         },
-        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
+        "doc": "Editable information about an Asset Container as defined on the DataHub Platform",
         "fields": [
             {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
-            },
-            {
                 "Searchable": {
                     "fieldName": "editedDescription",
                     "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "Edited documentation of the data flow",
+                "doc": "Description of the Asset Container as its received on the DataHub Platform",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "EditableDataFlowProperties",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "versionInfo"
-        },
-        "doc": "Information about a Data processing job",
-        "fields": [
-            {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
-            },
-            {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
-                "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "doc": "The version which can indentify a job version like a commit hash or md5 hash",
-                "name": "version",
-                "type": "string"
-            },
-            {
-                "doc": "The type of the version like git hash or md5 hash",
-                "name": "versionType",
-                "type": "string"
-            }
-        ],
-        "name": "VersionInfo",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "name": "EditableContainerProperties",
+        "namespace": "com.linkedin.pegasus2avro.container",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataFlowInfo"
+            "name": "containerProperties"
         },
-        "doc": "Information about a Data processing flow",
+        "doc": "Information about a Asset Container as received from a 3rd party source system",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -5718,17 +4955,14 @@
                 "name": "customProperties",
                 "type": {
                     "type": "map",
                     "values": "string"
                 }
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
                 "default": null,
                 "doc": "URL where the reference exist",
                 "java": {
                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                 },
                 "name": "externalUrl",
@@ -5739,39 +4973,40 @@
             },
             {
                 "Searchable": {
                     "boostScore": 10.0,
                     "enableAutocomplete": true,
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "Flow name",
+                "doc": "Display name of the Asset Container",
                 "name": "name",
                 "type": "string"
             },
             {
                 "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
                 },
                 "default": null,
-                "doc": "Flow description",
-                "name": "description",
+                "doc": "Fully-qualified name of the Container",
+                "name": "qualifiedName",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": false
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
                 },
                 "default": null,
-                "doc": "Optional project/namespace associated with the flow",
-                "name": "project",
+                "doc": "Description of the Asset Container as it exists inside a source system",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
@@ -5781,40 +5016,15 @@
                     }
                 },
                 "default": null,
                 "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
                 "name": "created",
                 "type": [
                     "null",
-                    {
-                        "doc": "A standard event timestamp",
-                        "fields": [
-                            {
-                                "doc": "When did the event occur",
-                                "name": "time",
-                                "type": "long"
-                            },
-                            {
-                                "Urn": "Urn",
-                                "default": null,
-                                "doc": "Optional: The actor urn involved in the event.",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "actor",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            }
-                        ],
-                        "name": "TimeStamp",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "type": "record"
-                    }
+                    "com.linkedin.pegasus2avro.common.TimeStamp"
                 ]
             },
             {
                 "Searchable": {
                     "/time": {
                         "fieldName": "lastModifiedAt",
                         "fieldType": "DATETIME"
@@ -5825,604 +5035,664 @@
                 "name": "lastModified",
                 "type": [
                     "null",
                     "com.linkedin.pegasus2avro.common.TimeStamp"
                 ]
             }
         ],
-        "name": "DataFlowInfo",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "name": "ContainerProperties",
+        "namespace": "com.linkedin.pegasus2avro.container",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataJobInfo"
+            "name": "container"
         },
-        "doc": "Information about a Data processing job",
+        "doc": "Link from an asset to its parent container",
         "fields": [
             {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
+                "Relationship": {
+                    "entityTypes": [
+                        "container"
+                    ],
+                    "name": "IsPartOf"
                 },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
-            },
-            {
                 "Searchable": {
-                    "fieldType": "KEYWORD"
+                    "addToFilters": true,
+                    "fieldName": "container",
+                    "fieldType": "URN",
+                    "filterNameOverride": "Container",
+                    "hasValuesFieldName": "hasContainer"
                 },
-                "default": null,
-                "doc": "URL where the reference exist",
+                "Urn": "Urn",
+                "doc": "The parent container of an asset",
                 "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
-                "name": "externalUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
+                "name": "container",
+                "type": "string"
+            }
+        ],
+        "name": "Container",
+        "namespace": "com.linkedin.pegasus2avro.container",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "glossaryNodeInfo"
+        },
+        "doc": "Properties associated with a GlossaryNode",
+        "fields": [
             {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Job name",
-                "name": "name",
+                "Searchable": {},
+                "doc": "Definition of business node",
+                "name": "definition",
                 "type": "string"
             },
             {
+                "Relationship": {
+                    "entityTypes": [
+                        "glossaryNode"
+                    ],
+                    "name": "IsPartOf"
+                },
                 "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
+                    "fieldName": "parentNode",
+                    "fieldType": "URN",
+                    "hasValuesFieldName": "hasParentNode"
                 },
+                "Urn": "GlossaryNodeUrn",
                 "default": null,
-                "doc": "Job description",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "doc": "Datajob type\n*NOTE**: AzkabanJobType is deprecated. Please use strings instead.",
-                "name": "type",
-                "type": [
-                    {
-                        "doc": "The various types of support azkaban jobs",
-                        "name": "AzkabanJobType",
-                        "namespace": "com.linkedin.pegasus2avro.datajob.azkaban",
-                        "symbolDocs": {
-                            "COMMAND": "The command job type is one of the basic built-in types. It runs multiple UNIX commands using java processbuilder.\nUpon execution, Azkaban spawns off a process to run the command.",
-                            "GLUE": "Glue type is for running AWS Glue job transforms.",
-                            "HADOOP_JAVA": "Runs a java program with ability to access Hadoop cluster.\nhttps://azkaban.readthedocs.io/en/latest/jobTypes.html#java-job-type",
-                            "HADOOP_SHELL": "In large part, this is the same Command type. The difference is its ability to talk to a Hadoop cluster\nsecurely, via Hadoop tokens.",
-                            "HIVE": "Hive type is for running Hive jobs.",
-                            "PIG": "Pig type is for running Pig jobs.",
-                            "SQL": "SQL is for running Presto, mysql queries etc"
-                        },
-                        "symbols": [
-                            "COMMAND",
-                            "HADOOP_JAVA",
-                            "HADOOP_SHELL",
-                            "HIVE",
-                            "PIG",
-                            "SQL",
-                            "GLUE"
-                        ],
-                        "type": "enum"
-                    },
-                    "string"
-                ]
-            },
-            {
-                "Urn": "DataFlowUrn",
-                "default": null,
-                "doc": "DataFlow urn that this job is part of",
+                "doc": "Parent node of the glossary term",
                 "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.DataFlowUrn"
+                    "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
                 },
-                "name": "flowUrn",
+                "name": "parentNode",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
-                    "/time": {
-                        "fieldName": "createdAt",
-                        "fieldType": "DATETIME"
-                    }
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldName": "displayName",
+                    "fieldType": "TEXT_PARTIAL"
                 },
                 "default": null,
-                "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
-                "name": "created",
+                "doc": "Display name of the node",
+                "name": "name",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.TimeStamp"
+                    "string"
                 ]
             },
             {
                 "Searchable": {
-                    "/time": {
-                        "fieldName": "lastModifiedAt",
-                        "fieldType": "DATETIME"
-                    }
+                    "fieldType": "TEXT_PARTIAL"
                 },
                 "default": null,
-                "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
-                "name": "lastModified",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.TimeStamp"
-                ]
-            },
-            {
-                "default": null,
-                "deprecated": "Use Data Process Instance model, instead",
-                "doc": "Status of the job - Deprecated for Data Process Instance model.",
-                "name": "status",
+                "doc": "Optional id for the GlossaryNode",
+                "name": "id",
                 "type": [
                     "null",
-                    {
-                        "doc": "Job statuses",
-                        "name": "JobStatus",
-                        "namespace": "com.linkedin.pegasus2avro.datajob",
-                        "symbolDocs": {
-                            "COMPLETED": "Jobs with successful completion.",
-                            "FAILED": "Jobs that have failed.",
-                            "IN_PROGRESS": "Jobs currently running.",
-                            "SKIPPED": "Jobs that have been skipped.",
-                            "STARTING": "Jobs being initialized.",
-                            "STOPPED": "Jobs that have stopped.",
-                            "STOPPING": "Jobs being stopped.",
-                            "UNKNOWN": "Jobs with unknown status (either unmappable or unavailable)"
-                        },
-                        "symbols": [
-                            "STARTING",
-                            "IN_PROGRESS",
-                            "STOPPING",
-                            "STOPPED",
-                            "COMPLETED",
-                            "FAILED",
-                            "UNKNOWN",
-                            "SKIPPED"
-                        ],
-                        "type": "enum"
-                    }
+                    "string"
                 ]
             }
         ],
-        "name": "DataJobInfo",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "name": "GlossaryNodeInfo",
+        "namespace": "com.linkedin.pegasus2avro.glossary",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "datahubIngestionRunSummary",
-            "type": "timeseries"
+            "name": "glossaryTermInfo"
         },
-        "doc": "Summary of a datahub ingestion run for a given platform.",
+        "doc": "Properties associated with a GlossaryTerm",
         "fields": [
             {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
             },
             {
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
                 "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
+                "doc": "Optional id for the term",
+                "name": "id",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                    "string"
                 ]
             },
             {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
-                "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
-                ]
-            },
-            {
                 "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
+                "doc": "Display name of the term",
+                "name": "name",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "TimeseriesField": {},
-                "doc": "The name of the pipeline that ran ingestion, a stable unique user provided identifier.\n e.g. my_snowflake1-to-datahub.",
-                "name": "pipelineName",
-                "type": "string"
-            },
-            {
-                "TimeseriesField": {},
-                "doc": "The id of the instance against which the ingestion pipeline ran.\ne.g.: Bigquery project ids, MySQL hostnames etc.",
-                "name": "platformInstanceId",
-                "type": "string"
-            },
-            {
-                "TimeseriesField": {},
-                "doc": "The runId for this pipeline instance.",
-                "name": "runId",
+                "Searchable": {},
+                "doc": "Definition of business term.",
+                "name": "definition",
                 "type": "string"
             },
             {
-                "TimeseriesField": {},
-                "doc": "Run Status - Succeeded/Skipped/Failed etc.",
-                "name": "runStatus",
-                "type": "com.linkedin.pegasus2avro.datajob.JobStatus"
-            },
-            {
-                "default": null,
-                "doc": "The number of workunits written to sink.",
-                "name": "numWorkUnitsCommitted",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The number of workunits that are produced.",
-                "name": "numWorkUnitsCreated",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            },
-            {
+                "Relationship": {
+                    "entityTypes": [
+                        "glossaryNode"
+                    ],
+                    "name": "IsPartOf"
+                },
+                "Searchable": {
+                    "fieldName": "parentNode",
+                    "fieldType": "URN",
+                    "hasValuesFieldName": "hasParentNode"
+                },
+                "Urn": "GlossaryNodeUrn",
                 "default": null,
-                "doc": "The number of events produced (MCE + MCP).",
-                "name": "numEvents",
+                "doc": "Parent node of the glossary term",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
+                },
+                "name": "parentNode",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
-                "default": null,
-                "doc": "The total number of entities produced (unique entity urns).",
-                "name": "numEntities",
-                "type": [
-                    "null",
-                    "long"
-                ]
+                "Searchable": {
+                    "fieldType": "KEYWORD"
+                },
+                "doc": "Source of the Business Term (INTERNAL or EXTERNAL) with default value as INTERNAL",
+                "name": "termSource",
+                "type": "string"
             },
             {
+                "Searchable": {
+                    "fieldType": "KEYWORD"
+                },
                 "default": null,
-                "doc": "The total number of aspects produced across all entities.",
-                "name": "numAspects",
+                "doc": "External Reference to the business-term",
+                "name": "sourceRef",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Total number of source API calls.",
-                "name": "numSourceAPICalls",
+                "doc": "The abstracted URL such as https://spec.edmcouncil.org/fibo/ontology/FBC/FinancialInstruments/FinancialInstruments/CashInstrument.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "sourceUrl",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Total latency across all source API calls.",
-                "name": "totalLatencySourceAPICalls",
+                "deprecated": true,
+                "doc": "Schema definition of the glossary term",
+                "name": "rawSchema",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
-            },
+            }
+        ],
+        "name": "GlossaryTermInfo",
+        "namespace": "com.linkedin.pegasus2avro.glossary",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "glossaryRelatedTerms"
+        },
+        "doc": "Has A / Is A lineage information about a glossary Term reporting the lineage",
+        "fields": [
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "glossaryTerm"
+                        ],
+                        "name": "IsA"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "boostScore": 2.0,
+                        "fieldName": "isRelatedTerms",
+                        "fieldType": "URN"
+                    }
+                },
+                "Urn": "GlossaryTermUrn",
                 "default": null,
-                "doc": "Total number of sink API calls.",
-                "name": "numSinkAPICalls",
+                "doc": "The relationship Is A with glossary term",
+                "name": "isRelatedTerms",
                 "type": [
                     "null",
-                    "long"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "glossaryTerm"
+                        ],
+                        "name": "HasA"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "boostScore": 2.0,
+                        "fieldName": "hasRelatedTerms",
+                        "fieldType": "URN"
+                    }
+                },
+                "Urn": "GlossaryTermUrn",
                 "default": null,
-                "doc": "Total latency across all sink API calls.",
-                "name": "totalLatencySinkAPICalls",
+                "doc": "The relationship Has A with glossary term",
+                "name": "hasRelatedTerms",
                 "type": [
                     "null",
-                    "long"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "glossaryTerm"
+                        ],
+                        "name": "HasValue"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "fieldName": "values",
+                        "fieldType": "URN"
+                    }
+                },
+                "Urn": "GlossaryTermUrn",
                 "default": null,
-                "doc": "Number of warnings generated.",
-                "name": "numWarnings",
+                "doc": "The relationship Has Value with glossary term.\nThese are fixed value a term has. For example a ColorEnum where RED, GREEN and YELLOW are fixed values.",
+                "name": "values",
                 "type": [
                     "null",
-                    "long"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "glossaryTerm"
+                        ],
+                        "name": "IsRelatedTo"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "fieldName": "relatedTerms",
+                        "fieldType": "URN"
+                    }
+                },
+                "Urn": "GlossaryTermUrn",
                 "default": null,
-                "doc": "Number of errors generated.",
-                "name": "numErrors",
+                "doc": "The relationship isRelatedTo with glossary term",
+                "name": "relatedTerms",
                 "type": [
                     "null",
-                    "long"
-                ]
-            },
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
+            }
+        ],
+        "name": "GlossaryRelatedTerms",
+        "namespace": "com.linkedin.pegasus2avro.glossary",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "tagProperties"
+        },
+        "doc": "Properties associated with a Tag",
+        "fields": [
             {
-                "default": null,
-                "doc": "Number of entities skipped.",
-                "name": "numEntitiesSkipped",
-                "type": [
-                    "null",
-                    "long"
-                ]
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Display name of the tag",
+                "name": "name",
+                "type": "string"
             },
             {
+                "Searchable": {},
                 "default": null,
-                "doc": "The non-sensitive key-value pairs of the yaml config used as json string.",
-                "name": "config",
+                "doc": "Documentation of the tag",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Custom value.",
-                "name": "custom_summary",
+                "doc": "The color associated with the Tag in Hex. For example #FFFFFF.",
+                "name": "colorHex",
                 "type": [
                     "null",
                     "string"
                 ]
+            }
+        ],
+        "name": "TagProperties",
+        "namespace": "com.linkedin.pegasus2avro.tag",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "querySubjects"
+        },
+        "doc": "Information about the subjects of a particular Query, i.e. the assets\nbeing queried.",
+        "fields": [
+            {
+                "doc": "One or more subjects of the query.\n\nIn single-asset queries (e.g. table select), this will contain the Table reference\nand optionally schema field references.\n\nIn multi-asset queries (e.g. table joins), this may contain multiple Table references\nand optionally schema field references.",
+                "name": "subjects",
+                "type": {
+                    "items": {
+                        "doc": "A single subject of a particular query.\nIn the future, we may evolve this model to include richer details\nabout the Query Subject in relation to the query.",
+                        "fields": [
+                            {
+                                "Relationship": {
+                                    "entityTypes": [
+                                        "dataset",
+                                        "schemaField"
+                                    ],
+                                    "name": "IsAssociatedWith"
+                                },
+                                "Searchable": {
+                                    "fieldName": "entities",
+                                    "fieldType": "URN"
+                                },
+                                "Urn": "Urn",
+                                "doc": "An entity which is the subject of a query.",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "entity",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "QuerySubject",
+                        "namespace": "com.linkedin.pegasus2avro.query",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
+            }
+        ],
+        "name": "QuerySubjects",
+        "namespace": "com.linkedin.pegasus2avro.query",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "queryProperties"
+        },
+        "doc": "Information about a Query against one or more data assets (e.g. Tables or Views).",
+        "fields": [
+            {
+                "doc": "The Query Statement.",
+                "name": "statement",
+                "type": {
+                    "doc": "A query statement against one or more data assets.",
+                    "fields": [
+                        {
+                            "doc": "The query text",
+                            "name": "value",
+                            "type": "string"
+                        },
+                        {
+                            "default": "SQL",
+                            "doc": "The language of the Query, e.g. SQL.",
+                            "name": "language",
+                            "type": {
+                                "name": "QueryLanguage",
+                                "namespace": "com.linkedin.pegasus2avro.query",
+                                "symbolDocs": {
+                                    "SQL": "A SQL Query"
+                                },
+                                "symbols": [
+                                    "SQL"
+                                ],
+                                "type": "enum"
+                            }
+                        }
+                    ],
+                    "name": "QueryStatement",
+                    "namespace": "com.linkedin.pegasus2avro.query",
+                    "type": "record"
+                }
             },
             {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "The software version of this ingestion.",
-                "name": "softwareVersion",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "Searchable": {},
+                "doc": "The source of the Query",
+                "name": "source",
+                "type": {
+                    "name": "QuerySource",
+                    "namespace": "com.linkedin.pegasus2avro.query",
+                    "symbolDocs": {
+                        "MANUAL": "The query was entered manually by a user (via the UI)."
+                    },
+                    "symbols": [
+                        "MANUAL"
+                    ],
+                    "type": "enum"
+                }
             },
             {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
                 "default": null,
-                "doc": "The hostname the ingestion pipeline ran on.",
-                "name": "systemHostName",
+                "doc": "Optional display name to identify the query.",
+                "name": "name",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "TimeseriesField": {},
                 "default": null,
-                "doc": "The os the ingestion pipeline ran on.",
-                "name": "operatingSystemName",
+                "doc": "The Query description.",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "default": null,
-                "doc": "The number of processors on the host the ingestion pipeline ran on.",
-                "name": "numProcessors",
-                "type": [
-                    "null",
-                    "int"
-                ]
+                "Searchable": {
+                    "/actor": {
+                        "fieldName": "createdBy",
+                        "fieldType": "URN"
+                    },
+                    "/time": {
+                        "fieldName": "createdAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "doc": "Audit stamp capturing the time and actor who created the Query.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             },
             {
-                "default": null,
-                "doc": "The total amount of memory on the host the ingestion pipeline ran on.",
-                "name": "totalMemory",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            },
+                "Searchable": {
+                    "/actor": {
+                        "fieldName": "lastModifiedBy",
+                        "fieldType": "URN"
+                    },
+                    "/time": {
+                        "fieldName": "lastModifiedAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "doc": "Audit stamp capturing the time and actor who last modified the Query.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            }
+        ],
+        "name": "QueryProperties",
+        "namespace": "com.linkedin.pegasus2avro.query",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "telemetryClientId"
+        },
+        "doc": "A simple wrapper around a String to persist the client ID for telemetry in DataHub's backend DB",
+        "fields": [
             {
-                "default": null,
-                "doc": "The available memory on the host the ingestion pipeline ran on.",
-                "name": "availableMemory",
-                "type": [
-                    "null",
-                    "long"
-                ]
+                "doc": "A string representing the telemetry client ID",
+                "name": "clientId",
+                "type": "string"
             }
         ],
-        "name": "DatahubIngestionRunSummary",
-        "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
+        "name": "TelemetryClientId",
+        "namespace": "com.linkedin.pegasus2avro.telemetry",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "datahubIngestionCheckpoint",
-            "type": "timeseries"
+            "name": "domainProperties"
         },
-        "doc": "Checkpoint of a datahub ingestion run for a given job.",
+        "doc": "Information about a Domain",
         "fields": [
             {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Display name of the Domain",
+                "name": "name",
+                "type": "string"
             },
             {
                 "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
+                "doc": "Description of the Domain",
+                "name": "description",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                    "string"
                 ]
             },
             {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "createdTime",
+                        "fieldType": "DATETIME"
+                    }
                 },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
-                "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
-                ]
-            },
-            {
                 "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
+                "doc": "Created Audit stamp",
+                "name": "created",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
                 ]
-            },
-            {
-                "TimeseriesField": {},
-                "doc": "The name of the pipeline that ran ingestion, a stable unique user provided identifier.\n e.g. my_snowflake1-to-datahub.",
-                "name": "pipelineName",
-                "type": "string"
-            },
-            {
-                "TimeseriesField": {},
-                "doc": "The id of the instance against which the ingestion pipeline ran.\ne.g.: Bigquery project ids, MySQL hostnames etc.",
-                "name": "platformInstanceId",
-                "type": "string"
-            },
-            {
-                "doc": "Json-encoded string representation of the non-secret members of the config .",
-                "name": "config",
-                "type": "string"
-            },
-            {
-                "doc": "Opaque blob of the state representation.",
-                "name": "state",
-                "type": {
-                    "doc": "The checkpoint state object of a datahub ingestion run for a given job.",
-                    "fields": [
-                        {
-                            "doc": "The version of the state format.",
-                            "name": "formatVersion",
-                            "type": "string"
-                        },
-                        {
-                            "doc": "The serialization/deserialization protocol.",
-                            "name": "serde",
-                            "type": "string"
-                        },
-                        {
-                            "default": null,
-                            "doc": "Opaque blob of the state representation.",
-                            "name": "payload",
-                            "type": [
-                                "null",
-                                "bytes"
-                            ]
-                        }
-                    ],
-                    "name": "IngestionCheckpointState",
-                    "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
-                    "type": "record"
-                }
-            },
-            {
-                "TimeseriesField": {},
-                "doc": "The run identifier of this job.",
-                "name": "runId",
-                "type": "string"
             }
         ],
-        "name": "DatahubIngestionCheckpoint",
-        "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
+        "name": "DomainProperties",
+        "namespace": "com.linkedin.pegasus2avro.domain",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataHubRetentionConfig"
+            "name": "domains"
         },
+        "doc": "Links from an Asset to its Domains",
         "fields": [
             {
-                "name": "retention",
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "domain"
+                        ],
+                        "name": "AssociatedWith"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "addToFilters": true,
+                        "fieldName": "domains",
+                        "fieldType": "URN",
+                        "filterNameOverride": "Domain",
+                        "hasValuesFieldName": "hasDomain"
+                    }
+                },
+                "Urn": "Urn",
+                "doc": "The Domains attached to an Asset",
+                "name": "domains",
                 "type": {
-                    "doc": "Base class that encapsulates different retention policies.\nOnly one of the fields should be set",
-                    "fields": [
-                        {
-                            "default": null,
-                            "name": "version",
-                            "type": [
-                                "null",
-                                {
-                                    "doc": "Keep max N latest records",
-                                    "fields": [
-                                        {
-                                            "name": "maxVersions",
-                                            "type": "int"
-                                        }
-                                    ],
-                                    "name": "VersionBasedRetention",
-                                    "namespace": "com.linkedin.pegasus2avro.retention",
-                                    "type": "record"
-                                }
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "name": "time",
-                            "type": [
-                                "null",
-                                {
-                                    "doc": "Keep records that are less than X seconds old",
-                                    "fields": [
-                                        {
-                                            "name": "maxAgeInSeconds",
-                                            "type": "int"
-                                        }
-                                    ],
-                                    "name": "TimeBasedRetention",
-                                    "namespace": "com.linkedin.pegasus2avro.retention",
-                                    "type": "record"
-                                }
-                            ]
-                        }
-                    ],
-                    "name": "Retention",
-                    "namespace": "com.linkedin.pegasus2avro.retention",
-                    "type": "record"
-                }
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             }
         ],
-        "name": "DataHubRetentionConfig",
-        "namespace": "com.linkedin.pegasus2avro.retention",
+        "name": "Domains",
+        "namespace": "com.linkedin.pegasus2avro.domain",
         "type": "record"
     },
     {
         "Aspect": {
             "name": "editableSchemaMetadata"
         },
         "doc": "EditableSchemaMetadata stores editable changes made to schema metadata. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines.",
@@ -6501,77 +5771,15 @@
                                     }
                                 },
                                 "default": null,
                                 "doc": "Tags associated with the field",
                                 "name": "globalTags",
                                 "type": [
                                     "null",
-                                    {
-                                        "Aspect": {
-                                            "name": "globalTags"
-                                        },
-                                        "doc": "Tag aspect used for applying tags to an entity",
-                                        "fields": [
-                                            {
-                                                "Relationship": {
-                                                    "/*/tag": {
-                                                        "entityTypes": [
-                                                            "tag"
-                                                        ],
-                                                        "name": "TaggedWith"
-                                                    }
-                                                },
-                                                "Searchable": {
-                                                    "/*/tag": {
-                                                        "addToFilters": true,
-                                                        "boostScore": 0.5,
-                                                        "fieldName": "tags",
-                                                        "fieldType": "URN",
-                                                        "filterNameOverride": "Tag",
-                                                        "hasValuesFieldName": "hasTags",
-                                                        "queryByDefault": true
-                                                    }
-                                                },
-                                                "doc": "Tags associated with a given entity",
-                                                "name": "tags",
-                                                "type": {
-                                                    "items": {
-                                                        "doc": "Properties of an applied tag. For now, just an Urn. In the future we can extend this with other properties, e.g.\npropagation parameters.",
-                                                        "fields": [
-                                                            {
-                                                                "Urn": "TagUrn",
-                                                                "doc": "Urn of the applied tag",
-                                                                "java": {
-                                                                    "class": "com.linkedin.pegasus2avro.common.urn.TagUrn"
-                                                                },
-                                                                "name": "tag",
-                                                                "type": "string"
-                                                            },
-                                                            {
-                                                                "default": null,
-                                                                "doc": "Additional context about the association",
-                                                                "name": "context",
-                                                                "type": [
-                                                                    "null",
-                                                                    "string"
-                                                                ]
-                                                            }
-                                                        ],
-                                                        "name": "TagAssociation",
-                                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                                        "type": "record"
-                                                    },
-                                                    "type": "array"
-                                                }
-                                            }
-                                        ],
-                                        "name": "GlobalTags",
-                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                        "type": "record"
-                                    }
+                                    "com.linkedin.pegasus2avro.common.GlobalTags"
                                 ]
                             },
                             {
                                 "Relationship": {
                                     "/terms/*/urn": {
                                         "entityTypes": [
                                             "glossaryTerm"
@@ -6587,76 +5795,15 @@
                                     }
                                 },
                                 "default": null,
                                 "doc": "Glossary terms associated with the field",
                                 "name": "glossaryTerms",
                                 "type": [
                                     "null",
-                                    {
-                                        "Aspect": {
-                                            "name": "glossaryTerms"
-                                        },
-                                        "doc": "Related business terms information",
-                                        "fields": [
-                                            {
-                                                "doc": "The related business terms",
-                                                "name": "terms",
-                                                "type": {
-                                                    "items": {
-                                                        "doc": "Properties of an applied glossary term.",
-                                                        "fields": [
-                                                            {
-                                                                "Relationship": {
-                                                                    "entityTypes": [
-                                                                        "glossaryTerm"
-                                                                    ],
-                                                                    "name": "TermedWith"
-                                                                },
-                                                                "Searchable": {
-                                                                    "addToFilters": true,
-                                                                    "fieldName": "glossaryTerms",
-                                                                    "fieldType": "URN",
-                                                                    "filterNameOverride": "Glossary Term",
-                                                                    "hasValuesFieldName": "hasGlossaryTerms"
-                                                                },
-                                                                "Urn": "GlossaryTermUrn",
-                                                                "doc": "Urn of the applied glossary term",
-                                                                "java": {
-                                                                    "class": "com.linkedin.pegasus2avro.common.urn.GlossaryTermUrn"
-                                                                },
-                                                                "name": "urn",
-                                                                "type": "string"
-                                                            },
-                                                            {
-                                                                "default": null,
-                                                                "doc": "Additional context about the association",
-                                                                "name": "context",
-                                                                "type": [
-                                                                    "null",
-                                                                    "string"
-                                                                ]
-                                                            }
-                                                        ],
-                                                        "name": "GlossaryTermAssociation",
-                                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                                        "type": "record"
-                                                    },
-                                                    "type": "array"
-                                                }
-                                            },
-                                            {
-                                                "doc": "Audit stamp containing who reported the related business term",
-                                                "name": "auditStamp",
-                                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                                            }
-                                        ],
-                                        "name": "GlossaryTerms",
-                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                        "type": "record"
-                                    }
+                                    "com.linkedin.pegasus2avro.common.GlossaryTerms"
                                 ]
                             }
                         ],
                         "name": "EditableSchemaFieldInfo",
                         "namespace": "com.linkedin.pegasus2avro.schema",
                         "type": "record"
                     },
@@ -6906,332 +6053,15 @@
                     }
                 ]
             },
             {
                 "doc": "Client provided a list of fields from document schema.",
                 "name": "fields",
                 "type": {
-                    "items": {
-                        "doc": "SchemaField to describe metadata related to dataset schema.",
-                        "fields": [
-                            {
-                                "Searchable": {
-                                    "boostScore": 5.0,
-                                    "fieldName": "fieldPaths",
-                                    "fieldType": "TEXT",
-                                    "queryByDefault": "true"
-                                },
-                                "doc": "Flattened name of the field. Field is computed from jsonPath field.",
-                                "name": "fieldPath",
-                                "type": "string"
-                            },
-                            {
-                                "Deprecated": true,
-                                "default": null,
-                                "doc": "Flattened name of a field in JSON Path notation.",
-                                "name": "jsonPath",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": false,
-                                "doc": "Indicates if this field is optional or nullable",
-                                "name": "nullable",
-                                "type": "boolean"
-                            },
-                            {
-                                "Searchable": {
-                                    "boostScore": 0.1,
-                                    "fieldName": "fieldDescriptions",
-                                    "fieldType": "TEXT"
-                                },
-                                "default": null,
-                                "doc": "Description",
-                                "name": "description",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "Searchable": {
-                                    "boostScore": 0.2,
-                                    "fieldName": "fieldLabels",
-                                    "fieldType": "TEXT"
-                                },
-                                "default": null,
-                                "doc": "Label of the field. Provides a more human-readable name for the field than field path. Some sources will\nprovide this metadata but not all sources have the concept of a label. If just one string is associated with\na field in a source, that is most likely a description.",
-                                "name": "label",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "An AuditStamp corresponding to the creation of this schema field.",
-                                "name": "created",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "An AuditStamp corresponding to the last modification of this schema field.",
-                                "name": "lastModified",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                                ]
-                            },
-                            {
-                                "doc": "Platform independent field type of the field.",
-                                "name": "type",
-                                "type": {
-                                    "doc": "Schema field data types",
-                                    "fields": [
-                                        {
-                                            "doc": "Data platform specific types",
-                                            "name": "type",
-                                            "type": [
-                                                {
-                                                    "doc": "Boolean field type.",
-                                                    "fields": [],
-                                                    "name": "BooleanType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Fixed field type.",
-                                                    "fields": [],
-                                                    "name": "FixedType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "String field type.",
-                                                    "fields": [],
-                                                    "name": "StringType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Bytes field type.",
-                                                    "fields": [],
-                                                    "name": "BytesType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Number data type: long, integer, short, etc..",
-                                                    "fields": [],
-                                                    "name": "NumberType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Date field type.",
-                                                    "fields": [],
-                                                    "name": "DateType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Time field type. This should also be used for datetimes.",
-                                                    "fields": [],
-                                                    "name": "TimeType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Enum field type.",
-                                                    "fields": [],
-                                                    "name": "EnumType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Null field type.",
-                                                    "fields": [],
-                                                    "name": "NullType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Map field type.",
-                                                    "fields": [
-                                                        {
-                                                            "default": null,
-                                                            "doc": "Key type in a map",
-                                                            "name": "keyType",
-                                                            "type": [
-                                                                "null",
-                                                                "string"
-                                                            ]
-                                                        },
-                                                        {
-                                                            "default": null,
-                                                            "doc": "Type of the value in a map",
-                                                            "name": "valueType",
-                                                            "type": [
-                                                                "null",
-                                                                "string"
-                                                            ]
-                                                        }
-                                                    ],
-                                                    "name": "MapType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Array field type.",
-                                                    "fields": [
-                                                        {
-                                                            "default": null,
-                                                            "doc": "List of types this array holds.",
-                                                            "name": "nestedType",
-                                                            "type": [
-                                                                "null",
-                                                                {
-                                                                    "items": "string",
-                                                                    "type": "array"
-                                                                }
-                                                            ]
-                                                        }
-                                                    ],
-                                                    "name": "ArrayType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Union field type.",
-                                                    "fields": [
-                                                        {
-                                                            "default": null,
-                                                            "doc": "List of types in union type.",
-                                                            "name": "nestedTypes",
-                                                            "type": [
-                                                                "null",
-                                                                {
-                                                                    "items": "string",
-                                                                    "type": "array"
-                                                                }
-                                                            ]
-                                                        }
-                                                    ],
-                                                    "name": "UnionType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Record field type.",
-                                                    "fields": [],
-                                                    "name": "RecordType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                }
-                                            ]
-                                        }
-                                    ],
-                                    "name": "SchemaFieldDataType",
-                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                    "type": "record"
-                                }
-                            },
-                            {
-                                "doc": "The native type of the field in the dataset's platform as declared by platform schema.",
-                                "name": "nativeDataType",
-                                "type": "string"
-                            },
-                            {
-                                "default": false,
-                                "doc": "There are use cases when a field in type B references type A. A field in A references field of type B. In such cases, we will mark the first field as recursive.",
-                                "name": "recursive",
-                                "type": "boolean"
-                            },
-                            {
-                                "Relationship": {
-                                    "/tags/*/tag": {
-                                        "entityTypes": [
-                                            "tag"
-                                        ],
-                                        "name": "SchemaFieldTaggedWith"
-                                    }
-                                },
-                                "Searchable": {
-                                    "/tags/*/tag": {
-                                        "boostScore": 0.5,
-                                        "fieldName": "fieldTags",
-                                        "fieldType": "URN"
-                                    }
-                                },
-                                "default": null,
-                                "doc": "Tags associated with the field",
-                                "name": "globalTags",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.common.GlobalTags"
-                                ]
-                            },
-                            {
-                                "Relationship": {
-                                    "/terms/*/urn": {
-                                        "entityTypes": [
-                                            "glossaryTerm"
-                                        ],
-                                        "name": "SchemaFieldWithGlossaryTerm"
-                                    }
-                                },
-                                "Searchable": {
-                                    "/terms/*/urn": {
-                                        "boostScore": 0.5,
-                                        "fieldName": "fieldGlossaryTerms",
-                                        "fieldType": "URN"
-                                    }
-                                },
-                                "default": null,
-                                "doc": "Glossary terms associated with the field",
-                                "name": "glossaryTerms",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.common.GlossaryTerms"
-                                ]
-                            },
-                            {
-                                "default": false,
-                                "doc": "For schema fields that are part of complex keys, set this field to true\nWe do this to easily distinguish between value and key fields",
-                                "name": "isPartOfKey",
-                                "type": "boolean"
-                            },
-                            {
-                                "default": null,
-                                "doc": "For Datasets which are partitioned, this determines the partitioning key.",
-                                "name": "isPartitioningKey",
-                                "type": [
-                                    "null",
-                                    "boolean"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "For schema fields that have other properties that are not modeled explicitly,\nuse this field to serialize those properties into a JSON string",
-                                "name": "jsonProps",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            }
-                        ],
-                        "name": "SchemaField",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
+                    "items": "com.linkedin.pegasus2avro.schema.SchemaField",
                     "type": "array"
                 }
             },
             {
                 "default": null,
                 "doc": "Client provided list of fields that define primary keys to access record. Field order defines hierarchical espresso keys. Empty lists indicates absence of primary key access patter. Value is a SchemaField@fieldPath.",
                 "name": "primaryKeys",
@@ -7382,120 +6212,14 @@
         ],
         "name": "SchemaMetadata",
         "namespace": "com.linkedin.pegasus2avro.schema",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataPlatformInfo"
-        },
-        "doc": "Information about a data platform",
-        "fields": [
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": false,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Name of the data platform",
-                "name": "name",
-                "type": "string",
-                "validate": {
-                    "strlen": {
-                        "max": 15
-                    }
-                }
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "default": null,
-                "doc": "The name that will be used for displaying a platform type.",
-                "name": "displayName",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "doc": "Platform type this data platform describes",
-                "name": "type",
-                "type": {
-                    "doc": "Platform types available at LinkedIn",
-                    "name": "PlatformType",
-                    "namespace": "com.linkedin.pegasus2avro.dataplatform",
-                    "symbolDocs": {
-                        "FILE_SYSTEM": "Value for a file system, e.g. hdfs",
-                        "KEY_VALUE_STORE": "Value for a key value store, e.g. espresso, voldemort",
-                        "MESSAGE_BROKER": "Value for a message broker, e.g. kafka",
-                        "OBJECT_STORE": "Value for an object store, e.g. ambry",
-                        "OLAP_DATASTORE": "Value for an OLAP datastore, e.g. pinot",
-                        "OTHERS": "Value for other platforms, e.g salesforce, dovetail",
-                        "QUERY_ENGINE": "Value for a query engine, e.g. presto",
-                        "RELATIONAL_DB": "Value for a relational database, e.g. oracle, mysql",
-                        "SEARCH_ENGINE": "Value for a search engine, e.g seas"
-                    },
-                    "symbols": [
-                        "FILE_SYSTEM",
-                        "KEY_VALUE_STORE",
-                        "MESSAGE_BROKER",
-                        "OBJECT_STORE",
-                        "OLAP_DATASTORE",
-                        "OTHERS",
-                        "QUERY_ENGINE",
-                        "RELATIONAL_DB",
-                        "SEARCH_ENGINE"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "doc": "The delimiter in the dataset names on the data platform, e.g. '/' for HDFS and '.' for Oracle",
-                "name": "datasetNameDelimiter",
-                "type": "string"
-            },
-            {
-                "default": null,
-                "doc": "The URL for a logo associated with the platform",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "logoUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "DataPlatformInfo",
-        "namespace": "com.linkedin.pegasus2avro.dataplatform",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "telemetryClientId"
-        },
-        "doc": "A simple wrapper around a String to persist the client ID for telemetry in DataHub's backend DB",
-        "fields": [
-            {
-                "doc": "A string representing the telemetry client ID",
-                "name": "clientId",
-                "type": "string"
-            }
-        ],
-        "name": "TelemetryClientId",
-        "namespace": "com.linkedin.pegasus2avro.telemetry",
-        "type": "record"
-    },
-    {
-        "Aspect": {
             "name": "dataHubIngestionSourceInfo"
         },
         "doc": "Info about a DataHub ingestion source",
         "fields": [
             {
                 "Searchable": {
                     "fieldType": "TEXT_PARTIAL"
@@ -7593,707 +6317,43 @@
             }
         ],
         "name": "DataHubIngestionSourceInfo",
         "namespace": "com.linkedin.pegasus2avro.ingestion",
         "type": "record"
     },
     {
-        "doc": "Usage data for a given resource, rolled up into a bucket.",
-        "fields": [
-            {
-                "doc": " Bucket start time in milliseconds ",
-                "name": "bucket",
-                "type": "long"
-            },
-            {
-                "doc": " Bucket duration ",
-                "name": "duration",
-                "type": {
-                    "doc": "Enum to define the length of a bucket when doing aggregations",
-                    "name": "WindowDuration",
-                    "namespace": "com.linkedin.pegasus2avro.common",
-                    "symbols": [
-                        "YEAR",
-                        "MONTH",
-                        "WEEK",
-                        "DAY",
-                        "HOUR"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "Urn": "Urn",
-                "doc": " Resource associated with these usage stats ",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "resource",
-                "type": "string"
-            },
-            {
-                "doc": " Metrics associated with this bucket ",
-                "name": "metrics",
-                "type": {
-                    "doc": "Metrics for usage data for a given resource and bucket. Not all fields\nmake sense for all buckets, so every field is optional.",
-                    "fields": [
-                        {
-                            "default": null,
-                            "doc": " Unique user count ",
-                            "name": "uniqueUserCount",
-                            "type": [
-                                "null",
-                                "int"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": " Users within this bucket, with frequency counts ",
-                            "name": "users",
-                            "type": [
-                                "null",
-                                {
-                                    "items": {
-                                        "doc": " Records a single user's usage counts for a given resource ",
-                                        "fields": [
-                                            {
-                                                "Urn": "Urn",
-                                                "default": null,
-                                                "java": {
-                                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                                },
-                                                "name": "user",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            },
-                                            {
-                                                "name": "count",
-                                                "type": "int"
-                                            },
-                                            {
-                                                "default": null,
-                                                "doc": " If user_email is set, we attempt to resolve the user's urn upon ingest ",
-                                                "name": "userEmail",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            }
-                                        ],
-                                        "name": "UserUsageCounts",
-                                        "namespace": "com.linkedin.pegasus2avro.usage",
-                                        "type": "record"
-                                    },
-                                    "type": "array"
-                                }
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": " Total SQL query count ",
-                            "name": "totalSqlQueries",
-                            "type": [
-                                "null",
-                                "int"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": " Frequent SQL queries; mostly makes sense for datasets in SQL databases ",
-                            "name": "topSqlQueries",
-                            "type": [
-                                "null",
-                                {
-                                    "items": "string",
-                                    "type": "array"
-                                }
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": " Field-level usage stats ",
-                            "name": "fields",
-                            "type": [
-                                "null",
-                                {
-                                    "items": {
-                                        "doc": " Records field-level usage counts for a given resource ",
-                                        "fields": [
-                                            {
-                                                "name": "fieldName",
-                                                "type": "string"
-                                            },
-                                            {
-                                                "name": "count",
-                                                "type": "int"
-                                            }
-                                        ],
-                                        "name": "FieldUsageCounts",
-                                        "namespace": "com.linkedin.pegasus2avro.usage",
-                                        "type": "record"
-                                    },
-                                    "type": "array"
-                                }
-                            ]
-                        }
-                    ],
-                    "name": "UsageAggregationMetrics",
-                    "namespace": "com.linkedin.pegasus2avro.usage",
-                    "type": "record"
-                }
-            }
-        ],
-        "name": "UsageAggregation",
-        "namespace": "com.linkedin.pegasus2avro.usage",
-        "type": "record"
-    },
-    {
         "Aspect": {
-            "name": "glossaryTermInfo"
-        },
-        "doc": "Properties associated with a GlossaryTerm",
-        "fields": [
-            {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
-            },
-            {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "default": null,
-                "doc": "Optional id for the term",
-                "name": "id",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "default": null,
-                "doc": "Display name of the term",
-                "name": "name",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {},
-                "doc": "Definition of business term.",
-                "name": "definition",
-                "type": "string"
-            },
-            {
-                "Relationship": {
-                    "entityTypes": [
-                        "glossaryNode"
-                    ],
-                    "name": "IsPartOf"
-                },
-                "Searchable": {
-                    "fieldName": "parentNode",
-                    "fieldType": "URN",
-                    "hasValuesFieldName": "hasParentNode"
-                },
-                "Urn": "GlossaryNodeUrn",
-                "default": null,
-                "doc": "Parent node of the glossary term",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
-                },
-                "name": "parentNode",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
-                "doc": "Source of the Business Term (INTERNAL or EXTERNAL) with default value as INTERNAL",
-                "name": "termSource",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
-                "default": null,
-                "doc": "External Reference to the business-term",
-                "name": "sourceRef",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The abstracted URL such as https://spec.edmcouncil.org/fibo/ontology/FBC/FinancialInstruments/FinancialInstruments/CashInstrument.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "sourceUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "deprecated": true,
-                "doc": "Schema definition of the glossary term",
-                "name": "rawSchema",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "GlossaryTermInfo",
-        "namespace": "com.linkedin.pegasus2avro.glossary",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "glossaryRelatedTerms"
+            "name": "viewProperties"
         },
-        "doc": "Has A / Is A lineage information about a glossary Term reporting the lineage",
+        "doc": "Details about a View. \ne.g. Gets activated when subTypes is view",
         "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "glossaryTerm"
-                        ],
-                        "name": "IsA"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "boostScore": 2.0,
-                        "fieldName": "isRelatedTerms",
-                        "fieldType": "URN"
-                    }
-                },
-                "Urn": "GlossaryTermUrn",
-                "default": null,
-                "doc": "The relationship Is A with glossary term",
-                "name": "isRelatedTerms",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
-            },
-            {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "glossaryTerm"
-                        ],
-                        "name": "HasA"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "boostScore": 2.0,
-                        "fieldName": "hasRelatedTerms",
-                        "fieldType": "URN"
-                    }
-                },
-                "Urn": "GlossaryTermUrn",
-                "default": null,
-                "doc": "The relationship Has A with glossary term",
-                "name": "hasRelatedTerms",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
-            },
-            {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "glossaryTerm"
-                        ],
-                        "name": "HasValue"
-                    }
-                },
                 "Searchable": {
-                    "/*": {
-                        "fieldName": "values",
-                        "fieldType": "URN"
+                    "fieldType": "BOOLEAN",
+                    "weightsPerFieldValue": {
+                        "true": 0.5
                     }
                 },
-                "Urn": "GlossaryTermUrn",
-                "default": null,
-                "doc": "The relationship Has Value with glossary term.\nThese are fixed value a term has. For example a ColorEnum where RED, GREEN and YELLOW are fixed values.",
-                "name": "values",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
+                "doc": "Whether the view is materialized",
+                "name": "materialized",
+                "type": "boolean"
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "glossaryTerm"
-                        ],
-                        "name": "IsRelatedTo"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "relatedTerms",
-                        "fieldType": "URN"
-                    }
-                },
-                "Urn": "GlossaryTermUrn",
-                "default": null,
-                "doc": "The relationship isRelatedTo with glossary term",
-                "name": "relatedTerms",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
-            }
-        ],
-        "name": "GlossaryRelatedTerms",
-        "namespace": "com.linkedin.pegasus2avro.glossary",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "glossaryNodeInfo"
-        },
-        "doc": "Properties associated with a GlossaryNode",
-        "fields": [
-            {
-                "Searchable": {},
-                "doc": "Definition of business node",
-                "name": "definition",
+                "doc": "The view logic",
+                "name": "viewLogic",
                 "type": "string"
             },
             {
-                "Relationship": {
-                    "entityTypes": [
-                        "glossaryNode"
-                    ],
-                    "name": "IsPartOf"
-                },
-                "Searchable": {
-                    "fieldName": "parentNode",
-                    "fieldType": "URN",
-                    "hasValuesFieldName": "hasParentNode"
-                },
-                "Urn": "GlossaryNodeUrn",
-                "default": null,
-                "doc": "Parent node of the glossary term",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
-                },
-                "name": "parentNode",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldName": "displayName",
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "default": null,
-                "doc": "Display name of the node",
-                "name": "name",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "default": null,
-                "doc": "Optional id for the GlossaryNode",
-                "name": "id",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "GlossaryNodeInfo",
-        "namespace": "com.linkedin.pegasus2avro.glossary",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "domains"
-        },
-        "doc": "Links from an Asset to its Domains",
-        "fields": [
-            {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "domain"
-                        ],
-                        "name": "AssociatedWith"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "addToFilters": true,
-                        "fieldName": "domains",
-                        "fieldType": "URN",
-                        "filterNameOverride": "Domain",
-                        "hasValuesFieldName": "hasDomain"
-                    }
-                },
-                "Urn": "Urn",
-                "doc": "The Domains attached to an Asset",
-                "name": "domains",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                },
-                "urn_is_array": true
-            }
-        ],
-        "name": "Domains",
-        "namespace": "com.linkedin.pegasus2avro.domain",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "domainProperties"
-        },
-        "doc": "Information about a Domain",
-        "fields": [
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Display name of the Domain",
-                "name": "name",
+                "doc": "The view logic language / dialect",
+                "name": "viewLanguage",
                 "type": "string"
-            },
-            {
-                "default": null,
-                "doc": "Description of the Domain",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "createdTime",
-                        "fieldType": "DATETIME"
-                    }
-                },
-                "default": null,
-                "doc": "Created Audit stamp",
-                "name": "created",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
-            }
-        ],
-        "name": "DomainProperties",
-        "namespace": "com.linkedin.pegasus2avro.domain",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "postInfo"
-        },
-        "doc": "Information about a DataHub Post.",
-        "fields": [
-            {
-                "doc": "Type of the Post.",
-                "name": "type",
-                "type": {
-                    "doc": "Enum defining types of Posts.",
-                    "name": "PostType",
-                    "namespace": "com.linkedin.pegasus2avro.post",
-                    "symbolDocs": {
-                        "HOME_PAGE_ANNOUNCEMENT": "The Post is an Home Page announcement."
-                    },
-                    "symbols": [
-                        "HOME_PAGE_ANNOUNCEMENT"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "doc": "Content stored in the post.",
-                "name": "content",
-                "type": {
-                    "doc": "Content stored inside a Post.",
-                    "fields": [
-                        {
-                            "Searchable": {
-                                "fieldType": "TEXT_PARTIAL"
-                            },
-                            "doc": "Title of the post.",
-                            "name": "title",
-                            "type": "string"
-                        },
-                        {
-                            "doc": "Type of content held in the post.",
-                            "name": "type",
-                            "type": {
-                                "doc": "Enum defining the type of content held in a Post.",
-                                "name": "PostContentType",
-                                "namespace": "com.linkedin.pegasus2avro.post",
-                                "symbolDocs": {
-                                    "LINK": "Link content",
-                                    "TEXT": "Text content"
-                                },
-                                "symbols": [
-                                    "TEXT",
-                                    "LINK"
-                                ],
-                                "type": "enum"
-                            }
-                        },
-                        {
-                            "default": null,
-                            "doc": "Optional description of the post.",
-                            "name": "description",
-                            "type": [
-                                "null",
-                                "string"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": "Optional link that the post is associated with.",
-                            "java": {
-                                "class": "com.linkedin.pegasus2avro.common.url.Url",
-                                "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                            },
-                            "name": "link",
-                            "type": [
-                                "null",
-                                "string"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": "Optional media that the post is storing",
-                            "name": "media",
-                            "type": [
-                                "null",
-                                {
-                                    "doc": "Carries information about which roles a user is assigned to.",
-                                    "fields": [
-                                        {
-                                            "doc": "Type of content the Media is storing, e.g. image, video, etc.",
-                                            "name": "type",
-                                            "type": {
-                                                "doc": "Enum defining the type of content a Media object holds.",
-                                                "name": "MediaType",
-                                                "namespace": "com.linkedin.pegasus2avro.common",
-                                                "symbolDocs": {
-                                                    "IMAGE": "The Media holds an image."
-                                                },
-                                                "symbols": [
-                                                    "IMAGE"
-                                                ],
-                                                "type": "enum"
-                                            }
-                                        },
-                                        {
-                                            "doc": "Where the media content is stored.",
-                                            "java": {
-                                                "class": "com.linkedin.pegasus2avro.common.url.Url",
-                                                "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                                            },
-                                            "name": "location",
-                                            "type": "string"
-                                        }
-                                    ],
-                                    "name": "Media",
-                                    "namespace": "com.linkedin.pegasus2avro.common",
-                                    "type": "record"
-                                }
-                            ]
-                        }
-                    ],
-                    "name": "PostContent",
-                    "namespace": "com.linkedin.pegasus2avro.post",
-                    "type": "record"
-                }
-            },
-            {
-                "Searchable": {
-                    "fieldType": "COUNT"
-                },
-                "doc": "The time at which the post was initially created",
-                "name": "created",
-                "type": "long"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "COUNT"
-                },
-                "doc": "The time at which the post was last modified",
-                "name": "lastModified",
-                "type": "long"
             }
         ],
-        "name": "PostInfo",
-        "namespace": "com.linkedin.pegasus2avro.post",
+        "name": "ViewProperties",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
         "type": "record"
     },
     {
         "Aspect": {
             "name": "datasetDeprecation"
         },
         "Deprecated": true,
@@ -8340,64 +6400,95 @@
         ],
         "name": "DatasetDeprecation",
         "namespace": "com.linkedin.pegasus2avro.dataset",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableDatasetProperties"
+            "name": "datasetUpstreamLineage"
         },
-        "doc": "EditableDatasetProperties stores editable changes made to dataset properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
+        "deprecated": "use UpstreamLineage.fineGrainedLineages instead",
+        "doc": "Fine Grained upstream lineage for fields in a dataset",
         "fields": [
             {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
-                "default": null,
-                "doc": "Documentation of the dataset",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "doc": "Upstream to downstream field level lineage mappings",
+                "name": "fieldMappings",
+                "type": {
+                    "items": {
+                        "deprecated": "use FineGrainedLineage instead",
+                        "doc": "Representation of mapping between fields in source dataset to the field in destination dataset",
+                        "fields": [
+                            {
+                                "doc": "Audit stamp containing who reported the field mapping and when",
+                                "name": "created",
+                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                            },
+                            {
+                                "doc": "Transfomration function between the fields involved",
+                                "name": "transformation",
+                                "type": [
+                                    {
+                                        "doc": "Type of the transformation involved in generating destination fields from source fields.",
+                                        "name": "TransformationType",
+                                        "namespace": "com.linkedin.pegasus2avro.common.fieldtransformer",
+                                        "symbolDocs": {
+                                            "BLACKBOX": "Field transformation expressed as unknown black box function.",
+                                            "IDENTITY": "Field transformation expressed as Identity function."
+                                        },
+                                        "symbols": [
+                                            "BLACKBOX",
+                                            "IDENTITY"
+                                        ],
+                                        "type": "enum"
+                                    },
+                                    {
+                                        "doc": "Field transformation expressed in UDF",
+                                        "fields": [
+                                            {
+                                                "doc": "A UDF mentioning how the source fields got transformed to destination field. This is the FQCN(Fully Qualified Class Name) of the udf.",
+                                                "name": "udf",
+                                                "type": "string"
+                                            }
+                                        ],
+                                        "name": "UDFTransformer",
+                                        "namespace": "com.linkedin.pegasus2avro.common.fieldtransformer",
+                                        "type": "record"
+                                    }
+                                ]
+                            },
+                            {
+                                "doc": "Source fields from which the fine grained lineage is derived",
+                                "name": "sourceFields",
+                                "type": {
+                                    "items": [
+                                        "string"
+                                    ],
+                                    "type": "array"
+                                }
+                            },
+                            {
+                                "Urn": "DatasetFieldUrn",
+                                "deprecated": "use SchemaFieldPath and represent as generic Urn instead",
+                                "doc": "Destination field which is derived from source fields",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetFieldUrn"
+                                },
+                                "name": "destinationField",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "DatasetFieldMapping",
+                        "namespace": "com.linkedin.pegasus2avro.dataset",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
             }
         ],
-        "name": "EditableDatasetProperties",
+        "name": "DatasetUpstreamLineage",
         "namespace": "com.linkedin.pegasus2avro.dataset",
         "type": "record"
     },
     {
         "Aspect": {
             "name": "datasetUsageStatistics",
             "type": "timeseries"
@@ -8556,14 +6647,147 @@
         ],
         "name": "DatasetUsageStatistics",
         "namespace": "com.linkedin.pegasus2avro.dataset",
         "type": "record"
     },
     {
         "Aspect": {
+            "name": "datasetProperties"
+        },
+        "doc": "Properties associated with a Dataset",
+        "fields": [
+            {
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
+            },
+            {
+                "default": null,
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "default": null,
+                "doc": "Display name of the Dataset",
+                "name": "name",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "addToFilters": false,
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT"
+                },
+                "default": null,
+                "doc": "Fully-qualified name of the Dataset",
+                "name": "qualifiedName",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
+                "default": null,
+                "doc": "Documentation of the dataset",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "deprecated": "Use ExternalReference.externalUrl field instead.",
+                "doc": "The abstracted URI such as hdfs:///data/tracking/PageViewEvent, file:///dir/file_name. Uri should not include any environment specific properties. Some datasets might not have a standardized uri, which makes this field optional (i.e. kafka topic).",
+                "java": {
+                    "class": "java.net.URI"
+                },
+                "name": "uri",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "createdAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "default": null,
+                "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
+                "name": "created",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.TimeStamp"
+                ]
+            },
+            {
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "lastModifiedAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "default": null,
+                "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
+                "name": "lastModified",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.TimeStamp"
+                ]
+            },
+            {
+                "default": [],
+                "deprecated": "Use GlobalTags aspect instead.",
+                "doc": "[Legacy] Unstructured tags for the dataset. Structured tags can be applied via the `GlobalTags` aspect.\nThis is now deprecated.",
+                "name": "tags",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                }
+            }
+        ],
+        "name": "DatasetProperties",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "type": "record"
+    },
+    {
+        "Aspect": {
             "name": "upstreamLineage"
         },
         "doc": "Upstream lineage of a dataset",
         "fields": [
             {
                 "doc": "List of upstream dataset lineage information",
                 "name": "upstreams",
@@ -8934,32 +7158,776 @@
                     }
                 ]
             },
             {
                 "Searchable": {
                     "fieldType": "COUNT"
                 },
-                "default": null,
-                "doc": "Storage size in bytes",
-                "name": "sizeInBytes",
+                "default": null,
+                "doc": "Storage size in bytes",
+                "name": "sizeInBytes",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            }
+        ],
+        "name": "DatasetProfile",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "editableDatasetProperties"
+        },
+        "doc": "EditableDatasetProperties stores editable changes made to dataset properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
+        "fields": [
+            {
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "default": null,
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                ]
+            },
+            {
+                "Searchable": {
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
+                },
+                "default": null,
+                "doc": "Documentation of the dataset",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "EditableDatasetProperties",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "editableDashboardProperties"
+        },
+        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
+        "fields": [
+            {
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "default": null,
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                ]
+            },
+            {
+                "Searchable": {
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
+                },
+                "default": null,
+                "doc": "Edited documentation of the dashboard",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "EditableDashboardProperties",
+        "namespace": "com.linkedin.pegasus2avro.dashboard",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dashboardUsageStatistics",
+            "type": "timeseries"
+        },
+        "doc": "Experimental (Subject to breaking change) -- Stats corresponding to dashboard's usage.\n\nIf this aspect represents the latest snapshot of the statistics about a Dashboard, the eventGranularity field should be null. \nIf this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly. ",
+        "fields": [
+            {
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
+            },
+            {
+                "default": null,
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                ]
+            },
+            {
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
+                },
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
+                    "null"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "The total number of times dashboard has been viewed",
+                "name": "viewsCount",
+                "type": [
+                    "null",
+                    "int"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "The total number of dashboard executions (refreshes / syncs) ",
+                "name": "executionsCount",
+                "type": [
+                    "null",
+                    "int"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "Unique user count",
+                "name": "uniqueUserCount",
+                "type": [
+                    "null",
+                    "int"
+                ]
+            },
+            {
+                "TimeseriesFieldCollection": {
+                    "key": "user"
+                },
+                "default": null,
+                "doc": "Users within this bucket, with frequency counts",
+                "name": "userCounts",
+                "type": [
+                    "null",
+                    {
+                        "items": {
+                            "doc": "Records a single user's usage counts for a given resource",
+                            "fields": [
+                                {
+                                    "Urn": "Urn",
+                                    "doc": "The unique id of the user.",
+                                    "java": {
+                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                    },
+                                    "name": "user",
+                                    "type": "string"
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "default": null,
+                                    "doc": "The number of times the user has viewed the dashboard",
+                                    "name": "viewsCount",
+                                    "type": [
+                                        "null",
+                                        "int"
+                                    ]
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "default": null,
+                                    "doc": "The number of times the user has executed (refreshed) the dashboard",
+                                    "name": "executionsCount",
+                                    "type": [
+                                        "null",
+                                        "int"
+                                    ]
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "default": null,
+                                    "doc": "Normalized numeric metric representing user's dashboard usage -- the number of times the user executed or viewed the dashboard. ",
+                                    "name": "usageCount",
+                                    "type": [
+                                        "null",
+                                        "int"
+                                    ]
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "default": null,
+                                    "doc": "If user_email is set, we attempt to resolve the user's urn upon ingest",
+                                    "name": "userEmail",
+                                    "type": [
+                                        "null",
+                                        "string"
+                                    ]
+                                }
+                            ],
+                            "name": "DashboardUserUsageCounts",
+                            "namespace": "com.linkedin.pegasus2avro.dashboard",
+                            "type": "record"
+                        },
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "The total number of times that the dashboard has been favorited ",
+                "name": "favoritesCount",
+                "type": [
+                    "null",
+                    "int"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "Last viewed at\n\nThis should not be set in cases where statistics are windowed. ",
+                "name": "lastViewedAt",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            }
+        ],
+        "name": "DashboardUsageStatistics",
+        "namespace": "com.linkedin.pegasus2avro.dashboard",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dashboardInfo"
+        },
+        "doc": "Information about a dashboard",
+        "fields": [
+            {
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
+            },
+            {
+                "default": null,
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Title of the dashboard",
+                "name": "title",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
+                "doc": "Detailed description about the dashboard",
+                "name": "description",
+                "type": "string"
+            },
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "chart"
+                        ],
+                        "isLineage": true,
+                        "name": "Contains"
+                    }
+                },
+                "Urn": "ChartUrn",
+                "default": [],
+                "deprecated": true,
+                "doc": "Charts in a dashboard\nDeprecated! Use chartEdges instead.",
+                "name": "charts",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            },
+            {
+                "Relationship": {
+                    "/*/destinationUrn": {
+                        "createdActor": "chartEdges/*/created/actor",
+                        "createdOn": "chartEdges/*/created/time",
+                        "entityTypes": [
+                            "chart"
+                        ],
+                        "isLineage": true,
+                        "name": "Contains",
+                        "properties": "chartEdges/*/properties",
+                        "updatedActor": "chartEdges/*/lastModified/actor",
+                        "updatedOn": "chartEdges/*/lastModified/time"
+                    }
+                },
+                "default": null,
+                "doc": "Charts in a dashboard",
+                "name": "chartEdges",
+                "type": [
+                    "null",
+                    {
+                        "items": "com.linkedin.pegasus2avro.common.Edge",
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "name": "Consumes"
+                    }
+                },
+                "Urn": "Urn",
+                "default": [],
+                "deprecated": true,
+                "doc": "Datasets consumed by a dashboard\nDeprecated! Use datasetEdges instead.",
+                "name": "datasets",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            },
+            {
+                "Relationship": {
+                    "/*/destinationUrn": {
+                        "createdActor": "datasetEdges/*/created/actor",
+                        "createdOn": "datasetEdges/*/created/time",
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "name": "Consumes",
+                        "properties": "datasetEdges/*/properties",
+                        "updatedActor": "datasetEdges/*/lastModified/actor",
+                        "updatedOn": "datasetEdges/*/lastModified/time"
+                    }
+                },
+                "default": null,
+                "doc": "Datasets consumed by a dashboard",
+                "name": "datasetEdges",
+                "type": [
+                    "null",
+                    {
+                        "items": "com.linkedin.pegasus2avro.common.Edge",
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "doc": "Captures information about who created/last modified/deleted this dashboard and when",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+            },
+            {
+                "default": null,
+                "doc": "URL for the dashboard. This could be used as an external link on DataHub to allow users access/view the dashboard",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "dashboardUrl",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "addToFilters": true,
+                    "fieldType": "KEYWORD",
+                    "filterNameOverride": "Access Level"
+                },
+                "default": null,
+                "doc": "Access level for the dashboard",
+                "name": "access",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.AccessLevel"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The time when this dashboard last refreshed",
+                "name": "lastRefreshed",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            }
+        ],
+        "name": "DashboardInfo",
+        "namespace": "com.linkedin.pegasus2avro.dashboard",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataHubRetentionConfig"
+        },
+        "fields": [
+            {
+                "name": "retention",
+                "type": {
+                    "doc": "Base class that encapsulates different retention policies.\nOnly one of the fields should be set",
+                    "fields": [
+                        {
+                            "default": null,
+                            "name": "version",
+                            "type": [
+                                "null",
+                                {
+                                    "doc": "Keep max N latest records",
+                                    "fields": [
+                                        {
+                                            "name": "maxVersions",
+                                            "type": "int"
+                                        }
+                                    ],
+                                    "name": "VersionBasedRetention",
+                                    "namespace": "com.linkedin.pegasus2avro.retention",
+                                    "type": "record"
+                                }
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "name": "time",
+                            "type": [
+                                "null",
+                                {
+                                    "doc": "Keep records that are less than X seconds old",
+                                    "fields": [
+                                        {
+                                            "name": "maxAgeInSeconds",
+                                            "type": "int"
+                                        }
+                                    ],
+                                    "name": "TimeBasedRetention",
+                                    "namespace": "com.linkedin.pegasus2avro.retention",
+                                    "type": "record"
+                                }
+                            ]
+                        }
+                    ],
+                    "name": "Retention",
+                    "namespace": "com.linkedin.pegasus2avro.retention",
+                    "type": "record"
+                }
+            }
+        ],
+        "name": "DataHubRetentionConfig",
+        "namespace": "com.linkedin.pegasus2avro.retention",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "corpUserStatus"
+        },
+        "doc": "The status of the user, e.g. provisioned, active, suspended, etc.",
+        "fields": [
+            {
+                "Searchable": {
+                    "fieldType": "KEYWORD"
+                },
+                "doc": "Status of the user, e.g. PROVISIONED / ACTIVE / SUSPENDED",
+                "name": "status",
+                "type": "string"
+            },
+            {
+                "doc": "Audit stamp containing who last modified the status and when.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            }
+        ],
+        "name": "CorpUserStatus",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "EntityUrns": [
+                "com.linkedin.pegasus2avro.common.CorpuserUrn"
+            ],
+            "name": "corpUserCredentials"
+        },
+        "doc": "Corp user credentials",
+        "fields": [
+            {
+                "doc": "Salt used to hash password",
+                "name": "salt",
+                "type": "string"
+            },
+            {
+                "doc": "Hashed password generated by concatenating salt and password, then hashing",
+                "name": "hashedPassword",
+                "type": "string"
+            },
+            {
+                "default": null,
+                "doc": "Optional token needed to reset a user's password. Can only be set by the admin.",
+                "name": "passwordResetToken",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "When the password reset token expires.",
+                "name": "passwordResetTokenExpirationTimeMillis",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            }
+        ],
+        "name": "CorpUserCredentials",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "EntityUrns": [
+                "com.linkedin.pegasus2avro.common.CorpuserUrn"
+            ],
+            "name": "corpUserEditableInfo"
+        },
+        "doc": "Linkedin corp user information that can be edited from UI",
+        "fields": [
+            {
+                "default": null,
+                "doc": "About me section of the user",
+                "name": "aboutMe",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "/*": {
+                        "fieldType": "TEXT"
+                    }
+                },
+                "default": [],
+                "doc": "Teams that the user belongs to e.g. Metadata",
+                "name": "teams",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                }
+            },
+            {
+                "Searchable": {
+                    "/*": {
+                        "fieldType": "TEXT"
+                    }
+                },
+                "default": [],
+                "doc": "Skills that the user possesses e.g. Machine Learning",
+                "name": "skills",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                }
+            },
+            {
+                "default": "https://raw.githubusercontent.com/datahub-project/datahub/master/datahub-web-react/src/images/default_avatar.png",
+                "doc": "A URL which points to a picture which user wants to set as a profile photo",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "pictureLink",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "fieldType": "TEXT_PARTIAL",
+                    "queryByDefault": true
+                },
+                "default": null,
+                "doc": "DataHub-native display name",
+                "name": "displayName",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "DataHub-native Title, e.g. 'Software Engineer'",
+                "name": "title",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Slack handle for the user",
+                "name": "slack",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Phone number to contact the user",
+                "name": "phone",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Email address to contact the user",
+                "name": "email",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "CorpUserEditableInfo",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "inviteToken"
+        },
+        "doc": "Aspect used to store invite tokens.",
+        "fields": [
+            {
+                "doc": "The encrypted invite token.",
+                "name": "token",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldName": "role",
+                    "fieldType": "KEYWORD",
+                    "hasValuesFieldName": "hasRole"
+                },
+                "Urn": "Urn",
+                "default": null,
+                "doc": "The role that this invite token may be associated with",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "role",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             }
         ],
-        "name": "DatasetProfile",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "name": "InviteToken",
+        "namespace": "com.linkedin.pegasus2avro.identity",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "datasetProperties"
+            "EntityUrns": [
+                "com.linkedin.pegasus2avro.common.CorpuserUrn"
+            ],
+            "name": "corpUserInfo"
         },
-        "doc": "Properties associated with a Dataset",
+        "doc": "Linkedin corp user information",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -8969,252 +7937,613 @@
                 "type": {
                     "type": "map",
                     "values": "string"
                 }
             },
             {
                 "Searchable": {
-                    "fieldType": "KEYWORD"
+                    "fieldType": "BOOLEAN",
+                    "weightsPerFieldValue": {
+                        "true": 2.0
+                    }
                 },
-                "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                "doc": "Deprecated! Use CorpUserStatus instead. Whether the corpUser is active, ref: https://iwww.corp.linkedin.com/wiki/cf/display/GTSD/Accessing+Active+Directory+via+LDAP+tools",
+                "name": "active",
+                "type": "boolean"
+            },
+            {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL",
+                    "queryByDefault": true
                 },
-                "name": "externalUrl",
+                "default": null,
+                "doc": "displayName of this user ,  e.g.  Hang Zhang(DataHQ)",
+                "name": "displayName",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
+                    "fieldType": "KEYWORD",
+                    "queryByDefault": true
                 },
                 "default": null,
-                "doc": "Display name of the Dataset",
-                "name": "name",
+                "doc": "email address of this user",
+                "name": "email",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
-                    "addToFilters": false,
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT"
+                    "fieldType": "KEYWORD",
+                    "queryByDefault": true
                 },
                 "default": null,
-                "doc": "Fully-qualified name of the Dataset",
-                "name": "qualifiedName",
+                "doc": "title of this user",
+                "name": "title",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "Relationship": {
+                    "entityTypes": [
+                        "corpuser"
+                    ],
+                    "name": "ReportsTo"
+                },
                 "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
+                    "fieldName": "managerLdap",
+                    "fieldType": "URN",
+                    "queryByDefault": true
                 },
+                "Urn": "CorpuserUrn",
                 "default": null,
-                "doc": "Documentation of the dataset",
-                "name": "description",
+                "doc": "direct manager of this user",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.CorpuserUrn"
+                },
+                "name": "managerUrn",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "deprecated": "Use ExternalReference.externalUrl field instead.",
-                "doc": "The abstracted URI such as hdfs:///data/tracking/PageViewEvent, file:///dir/file_name. Uri should not include any environment specific properties. Some datasets might not have a standardized uri, which makes this field optional (i.e. kafka topic).",
-                "java": {
-                    "class": "java.net.URI"
-                },
-                "name": "uri",
+                "doc": "department id this user belong to",
+                "name": "departmentId",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "department name this user belong to",
+                "name": "departmentName",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "createdAt",
-                        "fieldType": "DATETIME"
-                    }
-                },
                 "default": null,
-                "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
-                "name": "created",
+                "doc": "first name of this user",
+                "name": "firstName",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.TimeStamp"
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "last name of this user",
+                "name": "lastName",
+                "type": [
+                    "null",
+                    "string"
                 ]
             },
             {
                 "Searchable": {
-                    "/time": {
-                        "fieldName": "lastModifiedAt",
-                        "fieldType": "DATETIME"
-                    }
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL",
+                    "queryByDefault": true
                 },
                 "default": null,
-                "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
-                "name": "lastModified",
+                "doc": "Common name of this user, format is firstName + lastName (split by a whitespace)",
+                "name": "fullName",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.TimeStamp"
+                    "string"
                 ]
             },
             {
-                "default": [],
-                "deprecated": "Use GlobalTags aspect instead.",
-                "doc": "[Legacy] Unstructured tags for the dataset. Structured tags can be applied via the `GlobalTags` aspect.\nThis is now deprecated.",
-                "name": "tags",
+                "default": null,
+                "doc": "two uppercase letters country code. e.g.  US",
+                "name": "countryCode",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "CorpUserInfo",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "roleMembership"
+        },
+        "doc": "Carries information about which roles a user is assigned to.",
+        "fields": [
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataHubRole"
+                        ],
+                        "name": "IsMemberOfRole"
+                    }
+                },
+                "Urn": "Urn",
+                "name": "roles",
                 "type": {
                     "items": "string",
                     "type": "array"
-                }
+                },
+                "urn_is_array": true
             }
         ],
-        "name": "DatasetProperties",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "name": "RoleMembership",
+        "namespace": "com.linkedin.pegasus2avro.identity",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "datasetUpstreamLineage"
+            "name": "groupMembership"
         },
-        "deprecated": "use UpstreamLineage.fineGrainedLineages instead",
-        "doc": "Fine Grained upstream lineage for fields in a dataset",
+        "doc": "Carries information about the CorpGroups a user is in.",
         "fields": [
             {
-                "doc": "Upstream to downstream field level lineage mappings",
-                "name": "fieldMappings",
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "corpGroup"
+                        ],
+                        "name": "IsMemberOfGroup"
+                    }
+                },
+                "Urn": "Urn",
+                "name": "groups",
                 "type": {
-                    "items": {
-                        "deprecated": "use FineGrainedLineage instead",
-                        "doc": "Representation of mapping between fields in source dataset to the field in destination dataset",
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            }
+        ],
+        "name": "GroupMembership",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "corpUserSettings"
+        },
+        "doc": "Settings that a user can customize through the datahub ui",
+        "fields": [
+            {
+                "doc": "Settings for a user around the appearance of their DataHub U",
+                "name": "appearance",
+                "type": {
+                    "doc": "Settings for a user around the appearance of their DataHub UI",
+                    "fields": [
+                        {
+                            "default": null,
+                            "doc": "Flag whether the user should see a homepage with only datasets, charts and dashboards. Intended for users\nwho have less operational use cases for the datahub tool.",
+                            "name": "showSimplifiedHomepage",
+                            "type": [
+                                "null",
+                                "boolean"
+                            ]
+                        }
+                    ],
+                    "name": "CorpUserAppearanceSettings",
+                    "namespace": "com.linkedin.pegasus2avro.identity",
+                    "type": "record"
+                }
+            },
+            {
+                "default": null,
+                "doc": "User preferences for the Views feature.",
+                "name": "views",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Settings related to the 'Views' feature.",
                         "fields": [
                             {
-                                "doc": "Audit stamp containing who reported the field mapping and when",
-                                "name": "created",
-                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                            },
-                            {
-                                "doc": "Transfomration function between the fields involved",
-                                "name": "transformation",
-                                "type": [
-                                    {
-                                        "doc": "Type of the transformation involved in generating destination fields from source fields.",
-                                        "name": "TransformationType",
-                                        "namespace": "com.linkedin.pegasus2avro.common.fieldtransformer",
-                                        "symbolDocs": {
-                                            "BLACKBOX": "Field transformation expressed as unknown black box function.",
-                                            "IDENTITY": "Field transformation expressed as Identity function."
-                                        },
-                                        "symbols": [
-                                            "BLACKBOX",
-                                            "IDENTITY"
-                                        ],
-                                        "type": "enum"
-                                    },
-                                    {
-                                        "doc": "Field transformation expressed in UDF",
-                                        "fields": [
-                                            {
-                                                "doc": "A UDF mentioning how the source fields got transformed to destination field. This is the FQCN(Fully Qualified Class Name) of the udf.",
-                                                "name": "udf",
-                                                "type": "string"
-                                            }
-                                        ],
-                                        "name": "UDFTransformer",
-                                        "namespace": "com.linkedin.pegasus2avro.common.fieldtransformer",
-                                        "type": "record"
-                                    }
-                                ]
-                            },
-                            {
-                                "doc": "Source fields from which the fine grained lineage is derived",
-                                "name": "sourceFields",
-                                "type": {
-                                    "items": [
-                                        "string"
-                                    ],
-                                    "type": "array"
-                                }
-                            },
-                            {
-                                "Urn": "DatasetFieldUrn",
-                                "deprecated": "use SchemaFieldPath and represent as generic Urn instead",
-                                "doc": "Destination field which is derived from source fields",
+                                "Urn": "Urn",
+                                "default": null,
+                                "doc": "The default View which is selected for the user.\nIf none is chosen, then this value will be left blank.",
                                 "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetFieldUrn"
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
-                                "name": "destinationField",
-                                "type": "string"
+                                "name": "defaultView",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
                             }
                         ],
-                        "name": "DatasetFieldMapping",
-                        "namespace": "com.linkedin.pegasus2avro.dataset",
+                        "name": "CorpUserViewsSettings",
+                        "namespace": "com.linkedin.pegasus2avro.identity",
                         "type": "record"
-                    },
+                    }
+                ]
+            }
+        ],
+        "name": "CorpUserSettings",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "corpGroupEditableInfo"
+        },
+        "doc": "Group information that can be edited from UI",
+        "fields": [
+            {
+                "Searchable": {
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
+                },
+                "default": null,
+                "doc": "A description of the group",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": "https://raw.githubusercontent.com/datahub-project/datahub/master/datahub-web-react/src/images/default_avatar.png",
+                "doc": "A URL which points to a picture which user wants to set as the photo for the group",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "pictureLink",
+                "type": "string"
+            },
+            {
+                "default": null,
+                "doc": "Slack channel for the group",
+                "name": "slack",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Email address to contact the group",
+                "name": "email",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "CorpGroupEditableInfo",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "nativeGroupMembership"
+        },
+        "doc": "Carries information about the native CorpGroups a user is in.",
+        "fields": [
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "corpGroup"
+                        ],
+                        "name": "IsMemberOfNativeGroup"
+                    }
+                },
+                "Urn": "Urn",
+                "name": "nativeGroups",
+                "type": {
+                    "items": "string",
                     "type": "array"
-                }
+                },
+                "urn_is_array": true
             }
         ],
-        "name": "DatasetUpstreamLineage",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "name": "NativeGroupMembership",
+        "namespace": "com.linkedin.pegasus2avro.identity",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "viewProperties"
+            "EntityUrns": [
+                "com.linkedin.pegasus2avro.common.CorpGroupUrn"
+            ],
+            "name": "corpGroupInfo"
         },
-        "doc": "Details about a View. \ne.g. Gets activated when subTypes is view",
+        "doc": "Information about a Corp Group ingested from a third party source",
         "fields": [
             {
                 "Searchable": {
-                    "fieldType": "BOOLEAN",
-                    "weightsPerFieldValue": {
-                        "true": 0.5
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL",
+                    "queryByDefault": true
+                },
+                "default": null,
+                "doc": "The name of the group.",
+                "name": "displayName",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "email of this group",
+                "name": "email",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "corpuser"
+                        ],
+                        "name": "OwnedBy"
                     }
                 },
-                "doc": "Whether the view is materialized",
-                "name": "materialized",
-                "type": "boolean"
+                "Urn": "CorpuserUrn",
+                "deprecated": true,
+                "doc": "owners of this group\nDeprecated! Replaced by Ownership aspect.",
+                "name": "admins",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             },
             {
-                "doc": "The view logic",
-                "name": "viewLogic",
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "corpuser"
+                        ],
+                        "name": "IsPartOf"
+                    }
+                },
+                "Urn": "CorpuserUrn",
+                "deprecated": true,
+                "doc": "List of ldap urn in this group.\nDeprecated! Replaced by GroupMembership aspect.",
+                "name": "members",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            },
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "corpGroup"
+                        ],
+                        "name": "IsPartOf"
+                    }
+                },
+                "Urn": "CorpGroupUrn",
+                "deprecated": true,
+                "doc": "List of groups in this group.\nDeprecated! This field is unused.",
+                "name": "groups",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "default": null,
+                "doc": "A description of the group.",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Slack channel for the group",
+                "name": "slack",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "createdTime",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "default": null,
+                "doc": "Created Audit stamp",
+                "name": "created",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                ]
+            }
+        ],
+        "name": "CorpGroupInfo",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataHubAccessTokenInfo"
+        },
+        "doc": "Information about a DataHub Access Token",
+        "fields": [
+            {
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "User defined name for the access token if defined.",
+                "name": "name",
                 "type": "string"
             },
             {
-                "doc": "The view logic language / dialect",
-                "name": "viewLanguage",
+                "Searchable": {
+                    "fieldType": "URN"
+                },
+                "Urn": "Urn",
+                "doc": "Urn of the actor to which this access token belongs to.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "actorUrn",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "URN"
+                },
+                "Urn": "Urn",
+                "doc": "Urn of the actor which created this access token.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "ownerUrn",
                 "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "COUNT",
+                    "queryByDefault": false
+                },
+                "doc": "When the token was created.",
+                "name": "createdAt",
+                "type": "long"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "COUNT",
+                    "queryByDefault": false
+                },
+                "default": null,
+                "doc": "When the token expires.",
+                "name": "expiresAt",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Description of the token if defined.",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
             }
         ],
-        "name": "ViewProperties",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "name": "DataHubAccessTokenInfo",
+        "namespace": "com.linkedin.pegasus2avro.access.token",
         "type": "record"
     },
     {
-        "doc": "Kafka event for proposing a metadata change for an entity. A corresponding MetadataAuditEvent is emitted when the change is accepted and committed, otherwise a FailedMetadataChangeEvent will be emitted instead.",
+        "doc": "A DataHub Platform Event.",
+        "fields": [
+            {
+                "doc": "Header information stored with the event.",
+                "name": "header",
+                "type": {
+                    "doc": "A header included with each DataHub platform event.",
+                    "fields": [
+                        {
+                            "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                            "name": "timestampMillis",
+                            "type": "long"
+                        }
+                    ],
+                    "name": "PlatformEventHeader",
+                    "namespace": "com.linkedin.pegasus2avro.mxe",
+                    "type": "record"
+                }
+            },
+            {
+                "doc": "The name of the event, e.g. the type of event. For example, 'notificationRequestEvent', 'entityChangeEvent'",
+                "name": "name",
+                "type": "string"
+            },
+            {
+                "doc": "The event payload.",
+                "name": "payload",
+                "type": {
+                    "doc": "Generic payload record structure for serializing a Platform Event.",
+                    "fields": [
+                        {
+                            "doc": "The value of the event, serialized as bytes.",
+                            "name": "value",
+                            "type": "bytes"
+                        },
+                        {
+                            "doc": "The content type, which represents the fashion in which the event was serialized.\nThe only type currently supported is application/json.",
+                            "name": "contentType",
+                            "type": "string"
+                        }
+                    ],
+                    "name": "GenericPayload",
+                    "namespace": "com.linkedin.pegasus2avro.mxe",
+                    "type": "record"
+                }
+            }
+        ],
+        "name": "PlatformEvent",
+        "namespace": "com.linkedin.pegasus2avro.mxe",
+        "type": "record"
+    },
+    {
+        "doc": "Kafka event for capturing update made to an entity's metadata.",
         "fields": [
             {
                 "default": null,
-                "doc": "Kafka audit header. See go/kafkaauditheader for more info.",
+                "doc": "Kafka audit header. Currently remains unused in the open source.",
                 "name": "auditHeader",
                 "type": [
                     "null",
                     {
                         "doc": "This header records information about the context of an event as it is emitted into kafka and is intended to be used by the kafka audit application.  For more information see go/kafkaauditheader",
                         "fields": [
                             {
@@ -9294,14 +8623,210 @@
                         "name": "KafkaAuditHeader",
                         "namespace": "com.linkedin.events",
                         "type": "record"
                     }
                 ]
             },
             {
+                "doc": "Type of the entity being written to",
+                "name": "entityType",
+                "type": "string"
+            },
+            {
+                "Urn": "Urn",
+                "default": null,
+                "doc": "Urn of the entity being written",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "entityUrn",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Key aspect of the entity being written",
+                "name": "entityKeyAspect",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Generic record structure for serializing an Aspect",
+                        "fields": [
+                            {
+                                "doc": "The value of the aspect, serialized as bytes.",
+                                "name": "value",
+                                "type": "bytes"
+                            },
+                            {
+                                "doc": "The content type, which represents the fashion in which the aspect was serialized.\nThe only type currently supported is application/json.",
+                                "name": "contentType",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "GenericAspect",
+                        "namespace": "com.linkedin.pegasus2avro.mxe",
+                        "type": "record"
+                    }
+                ]
+            },
+            {
+                "doc": "Type of change being proposed",
+                "name": "changeType",
+                "type": {
+                    "doc": "Descriptor for a change action",
+                    "name": "ChangeType",
+                    "namespace": "com.linkedin.pegasus2avro.events.metadata",
+                    "symbolDocs": {
+                        "CREATE": "NOT SUPPORTED YET\ninsert if not exists. otherwise fail",
+                        "DELETE": "NOT SUPPORTED YET\ndelete action",
+                        "PATCH": "NOT SUPPORTED YET\npatch the changes instead of full replace",
+                        "RESTATE": "Restate an aspect, eg. in a index refresh.",
+                        "UPDATE": "NOT SUPPORTED YET\nupdate if exists. otherwise fail",
+                        "UPSERT": "insert if not exists. otherwise update"
+                    },
+                    "symbols": [
+                        "UPSERT",
+                        "CREATE",
+                        "UPDATE",
+                        "DELETE",
+                        "PATCH",
+                        "RESTATE"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "default": null,
+                "doc": "Aspect of the entity being written to\nNot filling this out implies that the writer wants to affect the entire entity\nNote: This is only valid for CREATE, UPSERT, and DELETE operations.",
+                "name": "aspectName",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The value of the new aspect.",
+                "name": "aspect",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "A string->string map of custom properties that one might want to attach to an event",
+                "name": "systemMetadata",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Metadata associated with each metadata change that is processed by the system",
+                        "fields": [
+                            {
+                                "default": 0,
+                                "doc": "The timestamp the metadata was observed at",
+                                "name": "lastObserved",
+                                "type": [
+                                    "long",
+                                    "null"
+                                ]
+                            },
+                            {
+                                "default": "no-run-id-provided",
+                                "doc": "The run id that produced the metadata. Populated in case of batch-ingestion.",
+                                "name": "runId",
+                                "type": [
+                                    "string",
+                                    "null"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "The model registry name that was used to process this event",
+                                "name": "registryName",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "The model registry version that was used to process this event",
+                                "name": "registryVersion",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Additional properties",
+                                "name": "properties",
+                                "type": [
+                                    "null",
+                                    {
+                                        "type": "map",
+                                        "values": "string"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "SystemMetadata",
+                        "namespace": "com.linkedin.pegasus2avro.mxe",
+                        "type": "record"
+                    }
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The previous value of the aspect that has changed.",
+                "name": "previousAspectValue",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The previous value of the system metadata field that has changed.",
+                "name": "previousSystemMetadata",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.mxe.SystemMetadata"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "An audit stamp detailing who and when the aspect was changed by. Required for all intents and purposes.",
+                "name": "created",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                ]
+            }
+        ],
+        "name": "MetadataChangeLog",
+        "namespace": "com.linkedin.pegasus2avro.mxe",
+        "type": "record"
+    },
+    {
+        "doc": "Kafka event for proposing a metadata change for an entity. A corresponding MetadataAuditEvent is emitted when the change is accepted and committed, otherwise a FailedMetadataChangeEvent will be emitted instead.",
+        "fields": [
+            {
+                "default": null,
+                "doc": "Kafka audit header. See go/kafkaauditheader for more info.",
+                "name": "auditHeader",
+                "type": [
+                    "null",
+                    "com.linkedin.events.KafkaAuditHeader"
+                ]
+            },
+            {
                 "doc": "Snapshot of the proposed metadata change. Include only the aspects affected by the change in the snapshot.",
                 "name": "proposedSnapshot",
                 "type": [
                     {
                         "Entity": {
                             "keyAspect": "chartKey",
                             "name": "chart"
@@ -9347,307 +8872,21 @@
                                             "name": "ChartKey",
                                             "namespace": "com.linkedin.pegasus2avro.metadata.key",
                                             "type": "record"
                                         },
                                         "com.linkedin.pegasus2avro.chart.ChartInfo",
                                         "com.linkedin.pegasus2avro.chart.ChartQuery",
                                         "com.linkedin.pegasus2avro.chart.EditableChartProperties",
-                                        {
-                                            "Aspect": {
-                                                "name": "ownership"
-                                            },
-                                            "doc": "Ownership information of an entity.",
-                                            "fields": [
-                                                {
-                                                    "doc": "List of owners of the entity.",
-                                                    "name": "owners",
-                                                    "type": {
-                                                        "items": {
-                                                            "doc": "Ownership information",
-                                                            "fields": [
-                                                                {
-                                                                    "Relationship": {
-                                                                        "entityTypes": [
-                                                                            "corpuser",
-                                                                            "corpGroup"
-                                                                        ],
-                                                                        "name": "OwnedBy"
-                                                                    },
-                                                                    "Searchable": {
-                                                                        "addToFilters": true,
-                                                                        "fieldName": "owners",
-                                                                        "fieldType": "URN",
-                                                                        "filterNameOverride": "Owned By",
-                                                                        "hasValuesFieldName": "hasOwners",
-                                                                        "queryByDefault": false
-                                                                    },
-                                                                    "Urn": "Urn",
-                                                                    "doc": "Owner URN, e.g. urn:li:corpuser:ldap, urn:li:corpGroup:group_name, and urn:li:multiProduct:mp_name\n(Caveat: only corpuser is currently supported in the frontend.)",
-                                                                    "java": {
-                                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                                                    },
-                                                                    "name": "owner",
-                                                                    "type": "string"
-                                                                },
-                                                                {
-                                                                    "doc": "The type of the ownership",
-                                                                    "name": "type",
-                                                                    "type": {
-                                                                        "deprecatedSymbols": {
-                                                                            "CONSUMER": true,
-                                                                            "DATAOWNER": true,
-                                                                            "DELEGATE": true,
-                                                                            "DEVELOPER": true,
-                                                                            "PRODUCER": true,
-                                                                            "STAKEHOLDER": true
-                                                                        },
-                                                                        "doc": "Asset owner types",
-                                                                        "name": "OwnershipType",
-                                                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                                                        "symbolDocs": {
-                                                                            "BUSINESS_OWNER": "A person or group who is responsible for logical, or business related, aspects of the asset.",
-                                                                            "CONSUMER": "A person, group, or service that consumes the data\nDeprecated! Use TECHNICAL_OWNER or BUSINESS_OWNER instead.",
-                                                                            "DATAOWNER": "A person or group that is owning the data\nDeprecated! Use TECHNICAL_OWNER instead.",
-                                                                            "DATA_STEWARD": "A steward, expert, or delegate responsible for the asset.",
-                                                                            "DELEGATE": "A person or a group that overseas the operation, e.g. a DBA or SRE.\nDeprecated! Use TECHNICAL_OWNER instead.",
-                                                                            "DEVELOPER": "A person or group that is in charge of developing the code\nDeprecated! Use TECHNICAL_OWNER instead.",
-                                                                            "NONE": "No specific type associated to the owner.",
-                                                                            "PRODUCER": "A person, group, or service that produces/generates the data\nDeprecated! Use TECHNICAL_OWNER instead.",
-                                                                            "STAKEHOLDER": "A person or a group that has direct business interest\nDeprecated! Use TECHNICAL_OWNER, BUSINESS_OWNER, or STEWARD instead.",
-                                                                            "TECHNICAL_OWNER": "person or group who is responsible for technical aspects of the asset."
-                                                                        },
-                                                                        "symbols": [
-                                                                            "TECHNICAL_OWNER",
-                                                                            "BUSINESS_OWNER",
-                                                                            "DATA_STEWARD",
-                                                                            "NONE",
-                                                                            "DEVELOPER",
-                                                                            "DATAOWNER",
-                                                                            "DELEGATE",
-                                                                            "PRODUCER",
-                                                                            "CONSUMER",
-                                                                            "STAKEHOLDER"
-                                                                        ],
-                                                                        "type": "enum"
-                                                                    }
-                                                                },
-                                                                {
-                                                                    "default": null,
-                                                                    "doc": "Source information for the ownership",
-                                                                    "name": "source",
-                                                                    "type": [
-                                                                        "null",
-                                                                        {
-                                                                            "doc": "Source/provider of the ownership information",
-                                                                            "fields": [
-                                                                                {
-                                                                                    "doc": "The type of the source",
-                                                                                    "name": "type",
-                                                                                    "type": {
-                                                                                        "name": "OwnershipSourceType",
-                                                                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                                                                        "symbolDocs": {
-                                                                                            "AUDIT": "Auditing system or audit logs",
-                                                                                            "DATABASE": "Database, e.g. GRANTS table",
-                                                                                            "FILE_SYSTEM": "File system, e.g. file/directory owner",
-                                                                                            "ISSUE_TRACKING_SYSTEM": "Issue tracking system, e.g. Jira",
-                                                                                            "MANUAL": "Manually provided by a user",
-                                                                                            "OTHER": "Other sources",
-                                                                                            "SERVICE": "Other ownership-like service, e.g. Nuage, ACL service etc",
-                                                                                            "SOURCE_CONTROL": "SCM system, e.g. GIT, SVN"
-                                                                                        },
-                                                                                        "symbols": [
-                                                                                            "AUDIT",
-                                                                                            "DATABASE",
-                                                                                            "FILE_SYSTEM",
-                                                                                            "ISSUE_TRACKING_SYSTEM",
-                                                                                            "MANUAL",
-                                                                                            "SERVICE",
-                                                                                            "SOURCE_CONTROL",
-                                                                                            "OTHER"
-                                                                                        ],
-                                                                                        "type": "enum"
-                                                                                    }
-                                                                                },
-                                                                                {
-                                                                                    "default": null,
-                                                                                    "doc": "A reference URL for the source",
-                                                                                    "name": "url",
-                                                                                    "type": [
-                                                                                        "null",
-                                                                                        "string"
-                                                                                    ]
-                                                                                }
-                                                                            ],
-                                                                            "name": "OwnershipSource",
-                                                                            "namespace": "com.linkedin.pegasus2avro.common",
-                                                                            "type": "record"
-                                                                        }
-                                                                    ]
-                                                                }
-                                                            ],
-                                                            "name": "Owner",
-                                                            "namespace": "com.linkedin.pegasus2avro.common",
-                                                            "type": "record"
-                                                        },
-                                                        "type": "array"
-                                                    }
-                                                },
-                                                {
-                                                    "default": {
-                                                        "actor": "urn:li:corpuser:unknown",
-                                                        "impersonator": null,
-                                                        "message": null,
-                                                        "time": 0
-                                                    },
-                                                    "doc": "Audit stamp containing who last modified the record and when. A value of 0 in the time field indicates missing data.",
-                                                    "name": "lastModified",
-                                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                                                }
-                                            ],
-                                            "name": "Ownership",
-                                            "namespace": "com.linkedin.pegasus2avro.common",
-                                            "type": "record"
-                                        },
-                                        {
-                                            "Aspect": {
-                                                "name": "status"
-                                            },
-                                            "doc": "The lifecycle status metadata of an entity, e.g. dataset, metric, feature, etc.\nThis aspect is used to represent soft deletes conventionally.",
-                                            "fields": [
-                                                {
-                                                    "Searchable": {
-                                                        "fieldType": "BOOLEAN"
-                                                    },
-                                                    "default": false,
-                                                    "doc": "Whether the entity has been removed (soft-deleted).",
-                                                    "name": "removed",
-                                                    "type": "boolean"
-                                                }
-                                            ],
-                                            "name": "Status",
-                                            "namespace": "com.linkedin.pegasus2avro.common",
-                                            "type": "record"
-                                        },
+                                        "com.linkedin.pegasus2avro.common.Ownership",
+                                        "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
-                                        {
-                                            "Aspect": {
-                                                "name": "browsePaths"
-                                            },
-                                            "doc": "Shared aspect containing Browse Paths to be indexed for an entity.",
-                                            "fields": [
-                                                {
-                                                    "Searchable": {
-                                                        "/*": {
-                                                            "fieldName": "browsePaths",
-                                                            "fieldType": "BROWSE_PATH"
-                                                        }
-                                                    },
-                                                    "doc": "A list of valid browse paths for the entity.\n\nBrowse paths are expected to be forward slash-separated strings. For example: 'prod/snowflake/datasetName'",
-                                                    "name": "paths",
-                                                    "type": {
-                                                        "items": "string",
-                                                        "type": "array"
-                                                    }
-                                                }
-                                            ],
-                                            "name": "BrowsePaths",
-                                            "namespace": "com.linkedin.pegasus2avro.common",
-                                            "type": "record"
-                                        },
+                                        "com.linkedin.pegasus2avro.common.BrowsePaths",
                                         "com.linkedin.pegasus2avro.common.GlossaryTerms",
-                                        {
-                                            "Aspect": {
-                                                "name": "institutionalMemory"
-                                            },
-                                            "doc": "Institutional memory of an entity. This is a way to link to relevant documentation and provide description of the documentation. Institutional or tribal knowledge is very important for users to leverage the entity.",
-                                            "fields": [
-                                                {
-                                                    "doc": "List of records that represent institutional memory of an entity. Each record consists of a link, description, creator and timestamps associated with that record.",
-                                                    "name": "elements",
-                                                    "type": {
-                                                        "items": {
-                                                            "doc": "Metadata corresponding to a record of institutional memory.",
-                                                            "fields": [
-                                                                {
-                                                                    "doc": "Link to an engineering design document or a wiki page.",
-                                                                    "java": {
-                                                                        "class": "com.linkedin.pegasus2avro.common.url.Url",
-                                                                        "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                                                                    },
-                                                                    "name": "url",
-                                                                    "type": "string"
-                                                                },
-                                                                {
-                                                                    "doc": "Description of the link.",
-                                                                    "name": "description",
-                                                                    "type": "string"
-                                                                },
-                                                                {
-                                                                    "doc": "Audit stamp associated with creation of this record",
-                                                                    "name": "createStamp",
-                                                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                                                                }
-                                                            ],
-                                                            "name": "InstitutionalMemoryMetadata",
-                                                            "namespace": "com.linkedin.pegasus2avro.common",
-                                                            "type": "record"
-                                                        },
-                                                        "type": "array"
-                                                    }
-                                                }
-                                            ],
-                                            "name": "InstitutionalMemory",
-                                            "namespace": "com.linkedin.pegasus2avro.common",
-                                            "type": "record"
-                                        },
-                                        {
-                                            "Aspect": {
-                                                "name": "dataPlatformInstance"
-                                            },
-                                            "doc": "The specific instance of the data platform that this entity belongs to",
-                                            "fields": [
-                                                {
-                                                    "Searchable": {
-                                                        "addToFilters": true,
-                                                        "fieldType": "URN",
-                                                        "filterNameOverride": "Platform"
-                                                    },
-                                                    "Urn": "Urn",
-                                                    "doc": "Data Platform",
-                                                    "java": {
-                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                                    },
-                                                    "name": "platform",
-                                                    "type": "string"
-                                                },
-                                                {
-                                                    "Searchable": {
-                                                        "addToFilters": true,
-                                                        "fieldName": "platformInstance",
-                                                        "fieldType": "URN",
-                                                        "filterNameOverride": "Platform Instance"
-                                                    },
-                                                    "Urn": "Urn",
-                                                    "default": null,
-                                                    "doc": "Instance of the data platform (e.g. db instance)",
-                                                    "java": {
-                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                                    },
-                                                    "name": "instance",
-                                                    "type": [
-                                                        "null",
-                                                        "string"
-                                                    ]
-                                                }
-                                            ],
-                                            "name": "DataPlatformInstance",
-                                            "namespace": "com.linkedin.pegasus2avro.common",
-                                            "type": "record"
-                                        }
+                                        "com.linkedin.pegasus2avro.common.InstitutionalMemory",
+                                        "com.linkedin.pegasus2avro.common.DataPlatformInstance"
                                     ],
                                     "type": "array"
                                 }
                             }
                         ],
                         "name": "ChartSnapshot",
                         "namespace": "com.linkedin.pegasus2avro.metadata.snapshot",
@@ -10164,15 +9403,87 @@
                                                 }
                                             ],
                                             "name": "DataProcessKey",
                                             "namespace": "com.linkedin.pegasus2avro.metadata.key",
                                             "type": "record"
                                         },
                                         "com.linkedin.pegasus2avro.common.Ownership",
-                                        "com.linkedin.pegasus2avro.dataprocess.DataProcessInfo",
+                                        {
+                                            "Aspect": {
+                                                "name": "dataProcessInfo"
+                                            },
+                                            "doc": "The inputs and outputs of this data process",
+                                            "fields": [
+                                                {
+                                                    "Relationship": {
+                                                        "/*": {
+                                                            "entityTypes": [
+                                                                "dataset"
+                                                            ],
+                                                            "isLineage": true,
+                                                            "name": "Consumes"
+                                                        }
+                                                    },
+                                                    "Searchable": {
+                                                        "/*": {
+                                                            "fieldName": "inputs",
+                                                            "fieldType": "URN",
+                                                            "numValuesFieldName": "numInputDatasets",
+                                                            "queryByDefault": false
+                                                        }
+                                                    },
+                                                    "Urn": "DatasetUrn",
+                                                    "default": null,
+                                                    "doc": "the inputs of the data process",
+                                                    "name": "inputs",
+                                                    "type": [
+                                                        "null",
+                                                        {
+                                                            "items": "string",
+                                                            "type": "array"
+                                                        }
+                                                    ],
+                                                    "urn_is_array": true
+                                                },
+                                                {
+                                                    "Relationship": {
+                                                        "/*": {
+                                                            "entityTypes": [
+                                                                "dataset"
+                                                            ],
+                                                            "isLineage": true,
+                                                            "name": "Consumes"
+                                                        }
+                                                    },
+                                                    "Searchable": {
+                                                        "/*": {
+                                                            "fieldName": "outputs",
+                                                            "fieldType": "URN",
+                                                            "numValuesFieldName": "numOutputDatasets",
+                                                            "queryByDefault": false
+                                                        }
+                                                    },
+                                                    "Urn": "DatasetUrn",
+                                                    "default": null,
+                                                    "doc": "the outputs of the data process",
+                                                    "name": "outputs",
+                                                    "type": [
+                                                        "null",
+                                                        {
+                                                            "items": "string",
+                                                            "type": "array"
+                                                        }
+                                                    ],
+                                                    "urn_is_array": true
+                                                }
+                                            ],
+                                            "name": "DataProcessInfo",
+                                            "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.common.Status"
                                     ],
                                     "type": "array"
                                 }
                             }
                         ],
                         "name": "DataProcessSnapshot",
@@ -10297,123 +9608,16 @@
                                         "com.linkedin.pegasus2avro.ml.metadata.TrainingData",
                                         "com.linkedin.pegasus2avro.ml.metadata.QuantitativeAnalyses",
                                         "com.linkedin.pegasus2avro.ml.metadata.EthicalConsiderations",
                                         "com.linkedin.pegasus2avro.ml.metadata.CaveatsAndRecommendations",
                                         "com.linkedin.pegasus2avro.common.InstitutionalMemory",
                                         "com.linkedin.pegasus2avro.ml.metadata.SourceCode",
                                         "com.linkedin.pegasus2avro.common.Status",
-                                        {
-                                            "Aspect": {
-                                                "name": "cost"
-                                            },
-                                            "fields": [
-                                                {
-                                                    "name": "costType",
-                                                    "type": {
-                                                        "doc": "Type of Cost Code",
-                                                        "name": "CostType",
-                                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                                        "symbolDocs": {
-                                                            "ORG_COST_TYPE": "Org Cost Type to which the Cost of this entity should be attributed to"
-                                                        },
-                                                        "symbols": [
-                                                            "ORG_COST_TYPE"
-                                                        ],
-                                                        "type": "enum"
-                                                    }
-                                                },
-                                                {
-                                                    "name": "cost",
-                                                    "type": {
-                                                        "fields": [
-                                                            {
-                                                                "default": null,
-                                                                "name": "costId",
-                                                                "type": [
-                                                                    "null",
-                                                                    "double"
-                                                                ]
-                                                            },
-                                                            {
-                                                                "default": null,
-                                                                "name": "costCode",
-                                                                "type": [
-                                                                    "null",
-                                                                    "string"
-                                                                ]
-                                                            },
-                                                            {
-                                                                "doc": "Contains the name of the field that has its value set.",
-                                                                "name": "fieldDiscriminator",
-                                                                "type": {
-                                                                    "name": "CostCostDiscriminator",
-                                                                    "namespace": "com.linkedin.pegasus2avro.common",
-                                                                    "symbols": [
-                                                                        "costId",
-                                                                        "costCode"
-                                                                    ],
-                                                                    "type": "enum"
-                                                                }
-                                                            }
-                                                        ],
-                                                        "name": "CostCost",
-                                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                                        "type": "record"
-                                                    }
-                                                }
-                                            ],
-                                            "name": "Cost",
-                                            "namespace": "com.linkedin.pegasus2avro.common",
-                                            "type": "record"
-                                        },
-                                        {
-                                            "Aspect": {
-                                                "name": "deprecation"
-                                            },
-                                            "doc": "Deprecation status of an entity",
-                                            "fields": [
-                                                {
-                                                    "Searchable": {
-                                                        "fieldType": "BOOLEAN",
-                                                        "weightsPerFieldValue": {
-                                                            "true": 0.5
-                                                        }
-                                                    },
-                                                    "doc": "Whether the entity is deprecated.",
-                                                    "name": "deprecated",
-                                                    "type": "boolean"
-                                                },
-                                                {
-                                                    "default": null,
-                                                    "doc": "The time user plan to decommission this entity.",
-                                                    "name": "decommissionTime",
-                                                    "type": [
-                                                        "null",
-                                                        "long"
-                                                    ]
-                                                },
-                                                {
-                                                    "doc": "Additional information about the entity deprecation plan, such as the wiki, doc, RB.",
-                                                    "name": "note",
-                                                    "type": "string"
-                                                },
-                                                {
-                                                    "Urn": "Urn",
-                                                    "doc": "The user URN which will be credited for modifying this deprecation content.",
-                                                    "java": {
-                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                                    },
-                                                    "name": "actor",
-                                                    "type": "string"
-                                                }
-                                            ],
-                                            "name": "Deprecation",
-                                            "namespace": "com.linkedin.pegasus2avro.common",
-                                            "type": "record"
-                                        },
+                                        "com.linkedin.pegasus2avro.common.Cost",
+                                        "com.linkedin.pegasus2avro.common.Deprecation",
                                         "com.linkedin.pegasus2avro.common.BrowsePaths",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
                                         "com.linkedin.pegasus2avro.common.DataPlatformInstance"
                                     ],
                                     "type": "array"
                                 }
                             }
@@ -10973,264 +10177,15 @@
                                                     "type": "string"
                                                 }
                                             ],
                                             "name": "DataHubPolicyKey",
                                             "namespace": "com.linkedin.pegasus2avro.metadata.key",
                                             "type": "record"
                                         },
-                                        {
-                                            "Aspect": {
-                                                "name": "dataHubPolicyInfo"
-                                            },
-                                            "doc": "Information about a DataHub (UI) access policy.",
-                                            "fields": [
-                                                {
-                                                    "Searchable": {
-                                                        "fieldType": "TEXT_PARTIAL"
-                                                    },
-                                                    "doc": "Display name of the Policy",
-                                                    "name": "displayName",
-                                                    "type": "string"
-                                                },
-                                                {
-                                                    "Searchable": {
-                                                        "fieldType": "TEXT"
-                                                    },
-                                                    "doc": "Description of the Policy",
-                                                    "name": "description",
-                                                    "type": "string"
-                                                },
-                                                {
-                                                    "doc": "The type of policy",
-                                                    "name": "type",
-                                                    "type": "string"
-                                                },
-                                                {
-                                                    "doc": "The state of policy, ACTIVE or INACTIVE",
-                                                    "name": "state",
-                                                    "type": "string"
-                                                },
-                                                {
-                                                    "default": null,
-                                                    "doc": "The resource that the policy applies to. Not required for some 'Platform' privileges.",
-                                                    "name": "resources",
-                                                    "type": [
-                                                        "null",
-                                                        {
-                                                            "doc": "Information used to filter DataHub resource.",
-                                                            "fields": [
-                                                                {
-                                                                    "default": null,
-                                                                    "deprecated": true,
-                                                                    "doc": "The type of resource that the policy applies to. This will most often be a data asset entity name, for\nexample 'dataset'. It is not strictly required because in the future we will want to support filtering a resource\nby domain, as well.",
-                                                                    "name": "type",
-                                                                    "type": [
-                                                                        "null",
-                                                                        "string"
-                                                                    ]
-                                                                },
-                                                                {
-                                                                    "default": null,
-                                                                    "deprecated": true,
-                                                                    "doc": "A specific set of resources to apply the policy to, e.g. asset urns",
-                                                                    "name": "resources",
-                                                                    "type": [
-                                                                        "null",
-                                                                        {
-                                                                            "items": "string",
-                                                                            "type": "array"
-                                                                        }
-                                                                    ]
-                                                                },
-                                                                {
-                                                                    "default": false,
-                                                                    "deprecated": true,
-                                                                    "doc": "Whether the policy should be applied to all assets matching the filter.",
-                                                                    "name": "allResources",
-                                                                    "type": "boolean"
-                                                                },
-                                                                {
-                                                                    "default": null,
-                                                                    "doc": "Filter to apply privileges to",
-                                                                    "name": "filter",
-                                                                    "type": [
-                                                                        "null",
-                                                                        {
-                                                                            "doc": "The filter for specifying the resource or actor to apply privileges to",
-                                                                            "fields": [
-                                                                                {
-                                                                                    "doc": "A list of criteria to apply conjunctively (so all criteria must pass)",
-                                                                                    "name": "criteria",
-                                                                                    "type": {
-                                                                                        "items": {
-                                                                                            "doc": "A criterion for matching a field with given value",
-                                                                                            "fields": [
-                                                                                                {
-                                                                                                    "doc": "The name of the field that the criterion refers to",
-                                                                                                    "name": "field",
-                                                                                                    "type": "string"
-                                                                                                },
-                                                                                                {
-                                                                                                    "doc": "Values. Matches criterion if any one of the values matches condition (OR-relationship)",
-                                                                                                    "name": "values",
-                                                                                                    "type": {
-                                                                                                        "items": "string",
-                                                                                                        "type": "array"
-                                                                                                    }
-                                                                                                },
-                                                                                                {
-                                                                                                    "default": "EQUALS",
-                                                                                                    "doc": "The condition for the criterion",
-                                                                                                    "name": "condition",
-                                                                                                    "type": {
-                                                                                                        "doc": "The matching condition in a filter criterion",
-                                                                                                        "name": "PolicyMatchCondition",
-                                                                                                        "namespace": "com.linkedin.pegasus2avro.policy",
-                                                                                                        "symbolDocs": {
-                                                                                                            "EQUALS": "Whether the field matches the value"
-                                                                                                        },
-                                                                                                        "symbols": [
-                                                                                                            "EQUALS"
-                                                                                                        ],
-                                                                                                        "type": "enum"
-                                                                                                    }
-                                                                                                }
-                                                                                            ],
-                                                                                            "name": "PolicyMatchCriterion",
-                                                                                            "namespace": "com.linkedin.pegasus2avro.policy",
-                                                                                            "type": "record"
-                                                                                        },
-                                                                                        "type": "array"
-                                                                                    }
-                                                                                }
-                                                                            ],
-                                                                            "name": "PolicyMatchFilter",
-                                                                            "namespace": "com.linkedin.pegasus2avro.policy",
-                                                                            "type": "record"
-                                                                        }
-                                                                    ]
-                                                                }
-                                                            ],
-                                                            "name": "DataHubResourceFilter",
-                                                            "namespace": "com.linkedin.pegasus2avro.policy",
-                                                            "type": "record"
-                                                        }
-                                                    ]
-                                                },
-                                                {
-                                                    "doc": "The privileges that the policy grants.",
-                                                    "name": "privileges",
-                                                    "type": {
-                                                        "items": "string",
-                                                        "type": "array"
-                                                    }
-                                                },
-                                                {
-                                                    "doc": "The actors that the policy applies to.",
-                                                    "name": "actors",
-                                                    "type": {
-                                                        "doc": "Information used to filter DataHub actors.",
-                                                        "fields": [
-                                                            {
-                                                                "Urn": "Urn",
-                                                                "default": null,
-                                                                "doc": "A specific set of users to apply the policy to (disjunctive)",
-                                                                "name": "users",
-                                                                "type": [
-                                                                    "null",
-                                                                    {
-                                                                        "items": "string",
-                                                                        "type": "array"
-                                                                    }
-                                                                ],
-                                                                "urn_is_array": true
-                                                            },
-                                                            {
-                                                                "Urn": "Urn",
-                                                                "default": null,
-                                                                "doc": "A specific set of groups to apply the policy to (disjunctive)",
-                                                                "name": "groups",
-                                                                "type": [
-                                                                    "null",
-                                                                    {
-                                                                        "items": "string",
-                                                                        "type": "array"
-                                                                    }
-                                                                ],
-                                                                "urn_is_array": true
-                                                            },
-                                                            {
-                                                                "default": false,
-                                                                "doc": "Whether the filter should return true for owners of a particular resource.\nOnly applies to policies of type 'Metadata', which have a resource associated with them.",
-                                                                "name": "resourceOwners",
-                                                                "type": "boolean"
-                                                            },
-                                                            {
-                                                                "default": false,
-                                                                "doc": "Whether the filter should apply to all users.",
-                                                                "name": "allUsers",
-                                                                "type": "boolean"
-                                                            },
-                                                            {
-                                                                "default": false,
-                                                                "doc": "Whether the filter should apply to all groups.",
-                                                                "name": "allGroups",
-                                                                "type": "boolean"
-                                                            },
-                                                            {
-                                                                "Relationship": {
-                                                                    "/*": {
-                                                                        "entityTypes": [
-                                                                            "dataHubRole"
-                                                                        ],
-                                                                        "name": "IsAssociatedWithRole"
-                                                                    }
-                                                                },
-                                                                "Urn": "Urn",
-                                                                "default": null,
-                                                                "doc": "A specific set of roles to apply the policy to (disjunctive).",
-                                                                "name": "roles",
-                                                                "type": [
-                                                                    "null",
-                                                                    {
-                                                                        "items": "string",
-                                                                        "type": "array"
-                                                                    }
-                                                                ],
-                                                                "urn_is_array": true
-                                                            }
-                                                        ],
-                                                        "name": "DataHubActorFilter",
-                                                        "namespace": "com.linkedin.pegasus2avro.policy",
-                                                        "type": "record"
-                                                    }
-                                                },
-                                                {
-                                                    "default": true,
-                                                    "doc": "Whether the policy should be editable via the UI",
-                                                    "name": "editable",
-                                                    "type": "boolean"
-                                                },
-                                                {
-                                                    "Searchable": {
-                                                        "fieldType": "DATETIME"
-                                                    },
-                                                    "default": null,
-                                                    "doc": "Timestamp when the policy was last updated",
-                                                    "name": "lastUpdatedTimestamp",
-                                                    "type": [
-                                                        "null",
-                                                        "long"
-                                                    ]
-                                                }
-                                            ],
-                                            "name": "DataHubPolicyInfo",
-                                            "namespace": "com.linkedin.pegasus2avro.policy",
-                                            "type": "record"
-                                        }
+                                        "com.linkedin.pegasus2avro.policy.DataHubPolicyInfo"
                                     ],
                                     "type": "array"
                                 }
                             }
                         ],
                         "name": "DataHubPolicySnapshot",
                         "namespace": "com.linkedin.pegasus2avro.metadata.snapshot",
@@ -11361,70 +10316,15 @@
             },
             {
                 "default": null,
                 "doc": "Metadata around how the snapshot was ingested",
                 "name": "systemMetadata",
                 "type": [
                     "null",
-                    {
-                        "doc": "Metadata associated with each metadata change that is processed by the system",
-                        "fields": [
-                            {
-                                "default": 0,
-                                "doc": "The timestamp the metadata was observed at",
-                                "name": "lastObserved",
-                                "type": [
-                                    "long",
-                                    "null"
-                                ]
-                            },
-                            {
-                                "default": "no-run-id-provided",
-                                "doc": "The run id that produced the metadata. Populated in case of batch-ingestion.",
-                                "name": "runId",
-                                "type": [
-                                    "string",
-                                    "null"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "The model registry name that was used to process this event",
-                                "name": "registryName",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "The model registry version that was used to process this event",
-                                "name": "registryVersion",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Additional properties",
-                                "name": "properties",
-                                "type": [
-                                    "null",
-                                    {
-                                        "type": "map",
-                                        "values": "string"
-                                    }
-                                ]
-                            }
-                        ],
-                        "name": "SystemMetadata",
-                        "namespace": "com.linkedin.pegasus2avro.mxe",
-                        "type": "record"
-                    }
+                    "com.linkedin.pegasus2avro.mxe.SystemMetadata"
                 ]
             }
         ],
         "name": "MetadataChangeEvent",
         "namespace": "com.linkedin.pegasus2avro.mxe",
         "type": "record"
     },
@@ -11460,59 +10360,21 @@
             },
             {
                 "default": null,
                 "doc": "Key aspect of the entity being written",
                 "name": "entityKeyAspect",
                 "type": [
                     "null",
-                    {
-                        "doc": "Generic record structure for serializing an Aspect",
-                        "fields": [
-                            {
-                                "doc": "The value of the aspect, serialized as bytes.",
-                                "name": "value",
-                                "type": "bytes"
-                            },
-                            {
-                                "doc": "The content type, which represents the fashion in which the aspect was serialized.\nThe only type currently supported is application/json.",
-                                "name": "contentType",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "GenericAspect",
-                        "namespace": "com.linkedin.pegasus2avro.mxe",
-                        "type": "record"
-                    }
+                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
                 ]
             },
             {
                 "doc": "Type of change being proposed",
                 "name": "changeType",
-                "type": {
-                    "doc": "Descriptor for a change action",
-                    "name": "ChangeType",
-                    "namespace": "com.linkedin.pegasus2avro.events.metadata",
-                    "symbolDocs": {
-                        "CREATE": "NOT SUPPORTED YET\ninsert if not exists. otherwise fail",
-                        "DELETE": "NOT SUPPORTED YET\ndelete action",
-                        "PATCH": "NOT SUPPORTED YET\npatch the changes instead of full replace",
-                        "RESTATE": "Restate an aspect, eg. in a index refresh.",
-                        "UPDATE": "NOT SUPPORTED YET\nupdate if exists. otherwise fail",
-                        "UPSERT": "insert if not exists. otherwise update"
-                    },
-                    "symbols": [
-                        "UPSERT",
-                        "CREATE",
-                        "UPDATE",
-                        "DELETE",
-                        "PATCH",
-                        "RESTATE"
-                    ],
-                    "type": "enum"
-                }
+                "type": "com.linkedin.pegasus2avro.events.metadata.ChangeType"
             },
             {
                 "default": null,
                 "doc": "Aspect of the entity being written to\nNot filling this out implies that the writer wants to affect the entire entity\nNote: This is only valid for CREATE, UPSERT, and DELETE operations.",
                 "name": "aspectName",
                 "type": [
                     "null",
@@ -11539,230 +10401,396 @@
             }
         ],
         "name": "MetadataChangeProposal",
         "namespace": "com.linkedin.pegasus2avro.mxe",
         "type": "record"
     },
     {
-        "doc": "Kafka event for capturing update made to an entity's metadata.",
+        "Aspect": {
+            "name": "dataProcessInstanceRunEvent",
+            "type": "timeseries"
+        },
+        "doc": "An event representing the current status of data process run.\nDataProcessRunEvent should be used for reporting the status of a dataProcess' run.",
         "fields": [
             {
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
+            },
+            {
                 "default": null,
-                "doc": "Kafka audit header. Currently remains unused in the open source.",
-                "name": "auditHeader",
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
                 "type": [
                     "null",
-                    "com.linkedin.events.KafkaAuditHeader"
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
                 ]
             },
             {
-                "doc": "Type of the entity being written to",
-                "name": "entityType",
-                "type": "string"
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
+                },
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
+                    "null"
+                ]
             },
             {
-                "Urn": "Urn",
                 "default": null,
-                "doc": "Urn of the entity being written",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "entityUrn",
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Key aspect of the entity being written",
-                "name": "entityKeyAspect",
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
+                    "string"
                 ]
             },
             {
-                "doc": "Type of change being proposed",
-                "name": "changeType",
-                "type": "com.linkedin.pegasus2avro.events.metadata.ChangeType"
+                "TimeseriesField": {},
+                "name": "status",
+                "type": {
+                    "name": "DataProcessRunStatus",
+                    "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                    "symbolDocs": {
+                        "STARTED": "The status where the Data processing run is in."
+                    },
+                    "symbols": [
+                        "STARTED",
+                        "COMPLETE"
+                    ],
+                    "type": "enum"
+                }
             },
             {
                 "default": null,
-                "doc": "Aspect of the entity being written to\nNot filling this out implies that the writer wants to affect the entire entity\nNote: This is only valid for CREATE, UPSERT, and DELETE operations.",
-                "name": "aspectName",
+                "doc": "Return the try number that this Instance Run is in",
+                "name": "attempt",
                 "type": [
                     "null",
-                    "string"
+                    "int"
                 ]
             },
             {
+                "TimeseriesField": {},
                 "default": null,
-                "doc": "The value of the new aspect.",
-                "name": "aspect",
+                "doc": "The final result of the Data Processing run.",
+                "name": "result",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
+                    {
+                        "fields": [
+                            {
+                                "doc": " The final result, e.g. SUCCESS, FAILURE, SKIPPED, or UP_FOR_RETRY.",
+                                "name": "type",
+                                "type": {
+                                    "name": "RunResultType",
+                                    "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                                    "symbolDocs": {
+                                        "FAILURE": " The Run Failed",
+                                        "SKIPPED": " The Run Skipped",
+                                        "SUCCESS": " The Run Succeeded",
+                                        "UP_FOR_RETRY": " The Run Failed and will Retry"
+                                    },
+                                    "symbols": [
+                                        "SUCCESS",
+                                        "FAILURE",
+                                        "SKIPPED",
+                                        "UP_FOR_RETRY"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "doc": "It identifies the system where the native result comes from like Airflow, Azkaban, etc..",
+                                "name": "nativeResultType",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "DataProcessInstanceRunResult",
+                        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                        "type": "record"
+                    }
                 ]
+            }
+        ],
+        "name": "DataProcessInstanceRunEvent",
+        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataProcessInstanceInput"
+        },
+        "doc": "Information about the inputs datasets of a Data process",
+        "fields": [
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "name": "Consumes"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "addToFilters": true,
+                        "fieldName": "inputs",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numInputs",
+                        "queryByDefault": false
+                    }
+                },
+                "Urn": "Urn",
+                "doc": "Input datasets to be consumed",
+                "name": "inputs",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            }
+        ],
+        "name": "DataProcessInstanceInput",
+        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataProcessInstanceProperties"
+        },
+        "doc": "The inputs and outputs of this data process",
+        "fields": [
+            {
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
             },
             {
                 "default": null,
-                "doc": "A string->string map of custom properties that one might want to attach to an event",
-                "name": "systemMetadata",
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.mxe.SystemMetadata"
+                    "string"
                 ]
             },
             {
-                "default": null,
-                "doc": "The previous value of the aspect that has changed.",
-                "name": "previousAspectValue",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
-                ]
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Process name",
+                "name": "name",
+                "type": "string"
             },
             {
+                "Searchable": {
+                    "addToFilters": true,
+                    "fieldType": "KEYWORD",
+                    "filterNameOverride": "Process Type"
+                },
                 "default": null,
-                "doc": "The previous value of the system metadata field that has changed.",
-                "name": "previousSystemMetadata",
+                "doc": "Process type",
+                "name": "type",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.mxe.SystemMetadata"
+                    {
+                        "name": "DataProcessType",
+                        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                        "symbols": [
+                            "BATCH_SCHEDULED",
+                            "BATCH_AD_HOC",
+                            "STREAMING"
+                        ],
+                        "type": "enum"
+                    }
                 ]
             },
             {
-                "default": null,
-                "doc": "An audit stamp detailing who and when the aspect was changed by. Required for all intents and purposes.",
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "created",
+                        "fieldType": "COUNT",
+                        "queryByDefault": false
+                    }
+                },
+                "doc": "Audit stamp containing who reported the lineage and when",
                 "name": "created",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             }
         ],
-        "name": "MetadataChangeLog",
-        "namespace": "com.linkedin.pegasus2avro.mxe",
+        "name": "DataProcessInstanceProperties",
+        "namespace": "com.linkedin.pegasus2avro.dataprocess",
         "type": "record"
     },
     {
-        "doc": "A DataHub Platform Event.",
+        "Aspect": {
+            "name": "dataProcessInstanceOutput"
+        },
+        "doc": "Information about the outputs of a Data process",
         "fields": [
             {
-                "doc": "Header information stored with the event.",
-                "name": "header",
-                "type": {
-                    "doc": "A header included with each DataHub platform event.",
-                    "fields": [
-                        {
-                            "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                            "name": "timestampMillis",
-                            "type": "long"
-                        }
-                    ],
-                    "name": "PlatformEventHeader",
-                    "namespace": "com.linkedin.pegasus2avro.mxe",
-                    "type": "record"
-                }
-            },
-            {
-                "doc": "The name of the event, e.g. the type of event. For example, 'notificationRequestEvent', 'entityChangeEvent'",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "doc": "The event payload.",
-                "name": "payload",
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "name": "Produces"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "addToFilters": true,
+                        "fieldName": "outputs",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numOutputs",
+                        "queryByDefault": false
+                    }
+                },
+                "Urn": "Urn",
+                "doc": "Output datasets to be produced",
+                "name": "outputs",
                 "type": {
-                    "doc": "Generic payload record structure for serializing a Platform Event.",
-                    "fields": [
-                        {
-                            "doc": "The value of the event, serialized as bytes.",
-                            "name": "value",
-                            "type": "bytes"
-                        },
-                        {
-                            "doc": "The content type, which represents the fashion in which the event was serialized.\nThe only type currently supported is application/json.",
-                            "name": "contentType",
-                            "type": "string"
-                        }
-                    ],
-                    "name": "GenericPayload",
-                    "namespace": "com.linkedin.pegasus2avro.mxe",
-                    "type": "record"
-                }
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             }
         ],
-        "name": "PlatformEvent",
-        "namespace": "com.linkedin.pegasus2avro.mxe",
+        "name": "DataProcessInstanceOutput",
+        "namespace": "com.linkedin.pegasus2avro.dataprocess",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.dataprocess.DataProcessInfo",
     {
         "Aspect": {
-            "name": "editableContainerProperties"
+            "name": "dataProcessInstanceRelationships"
         },
-        "doc": "Editable information about an Asset Container as defined on the DataHub Platform",
+        "doc": "Information about Data process relationships",
         "fields": [
             {
+                "Relationship": {
+                    "entityTypes": [
+                        "dataJob",
+                        "dataFlow"
+                    ],
+                    "name": "InstanceOf"
+                },
                 "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
+                    "/*": {
+                        "fieldName": "parentTemplate",
+                        "fieldType": "URN",
+                        "queryByDefault": false
+                    }
                 },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "Description of the Asset Container as its received on the DataHub Platform",
-                "name": "description",
+                "doc": "The parent entity whose run instance it is",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "parentTemplate",
                 "type": [
                     "null",
                     "string"
                 ]
-            }
-        ],
-        "name": "EditableContainerProperties",
-        "namespace": "com.linkedin.pegasus2avro.container",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "container"
-        },
-        "doc": "Link from an asset to its parent container",
-        "fields": [
+            },
             {
                 "Relationship": {
                     "entityTypes": [
-                        "container"
+                        "dataProcessInstance"
                     ],
-                    "name": "IsPartOf"
+                    "name": "ChildOf"
                 },
                 "Searchable": {
-                    "addToFilters": true,
-                    "fieldName": "container",
-                    "fieldType": "URN",
-                    "filterNameOverride": "Container",
-                    "hasValuesFieldName": "hasContainer"
+                    "/*": {
+                        "fieldName": "parentInstance",
+                        "fieldType": "URN",
+                        "queryByDefault": false
+                    }
                 },
                 "Urn": "Urn",
-                "doc": "The parent container of an asset",
+                "default": null,
+                "doc": "The parent DataProcessInstance where it belongs to.\nIf it is a Airflow Task then it should belong to an Airflow Dag run as well\nwhich will be another DataProcessInstance",
                 "java": {
                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
-                "name": "container",
-                "type": "string"
+                "name": "parentInstance",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataProcessInstance"
+                        ],
+                        "name": "UpstreamOf"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "fieldName": "upstream",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numUpstreams",
+                        "queryByDefault": false
+                    }
+                },
+                "Urn": "Urn",
+                "doc": "Input DataProcessInstance which triggered this dataprocess instance",
+                "name": "upstreamInstances",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             }
         ],
-        "name": "Container",
-        "namespace": "com.linkedin.pegasus2avro.container",
+        "name": "DataProcessInstanceRelationships",
+        "namespace": "com.linkedin.pegasus2avro.dataprocess",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "containerProperties"
+            "name": "assertionInfo"
         },
-        "doc": "Information about a Asset Container as received from a 3rd party source system",
+        "doc": "Information about an assertion",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -11771,503 +10799,1422 @@
                 "name": "customProperties",
                 "type": {
                     "type": "map",
                     "values": "string"
                 }
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
                 "default": null,
                 "doc": "URL where the reference exist",
                 "java": {
                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                 },
                 "name": "externalUrl",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Display name of the Asset Container",
-                "name": "name",
-                "type": "string"
+                "doc": "Type of assertion. Assertion types can evolve to span Datasets, Flows (Pipelines), Models, Features etc.",
+                "name": "type",
+                "type": {
+                    "name": "AssertionType",
+                    "namespace": "com.linkedin.pegasus2avro.assertion",
+                    "symbols": [
+                        "DATASET"
+                    ],
+                    "type": "enum"
+                }
             },
             {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
                 "default": null,
-                "doc": "Fully-qualified name of the Container",
-                "name": "qualifiedName",
+                "doc": "Dataset Assertion information when type is DATASET",
+                "name": "datasetAssertion",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "doc": "Attributes that are applicable to single-Dataset Assertions",
+                        "fields": [
+                            {
+                                "Relationship": {
+                                    "entityTypes": [
+                                        "dataset"
+                                    ],
+                                    "name": "Asserts"
+                                },
+                                "Urn": "Urn",
+                                "doc": "The dataset targeted by this assertion.",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "dataset",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "Scope of the Assertion. What part of the dataset does this assertion apply to?",
+                                "name": "scope",
+                                "type": {
+                                    "name": "DatasetAssertionScope",
+                                    "namespace": "com.linkedin.pegasus2avro.assertion",
+                                    "symbolDocs": {
+                                        "DATASET_COLUMN": "This assertion applies to dataset columns",
+                                        "DATASET_ROWS": "This assertion applies to entire rows of the dataset",
+                                        "DATASET_SCHEMA": "This assertion applies to the schema of the dataset",
+                                        "UNKNOWN": "The scope of the assertion is unknown"
+                                    },
+                                    "symbols": [
+                                        "DATASET_COLUMN",
+                                        "DATASET_ROWS",
+                                        "DATASET_SCHEMA",
+                                        "UNKNOWN"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "Relationship": {
+                                    "/*": {
+                                        "entityTypes": [
+                                            "schemaField"
+                                        ],
+                                        "name": "Asserts"
+                                    }
+                                },
+                                "Urn": "Urn",
+                                "default": null,
+                                "doc": "One or more dataset schema fields that are targeted by this assertion",
+                                "name": "fields",
+                                "type": [
+                                    "null",
+                                    {
+                                        "items": "string",
+                                        "type": "array"
+                                    }
+                                ],
+                                "urn_is_array": true
+                            },
+                            {
+                                "default": null,
+                                "doc": "Standardized assertion operator",
+                                "name": "aggregation",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "The function that is applied to the aggregation input (schema, rows, column values) before evaluating an operator.",
+                                        "name": "AssertionStdAggregation",
+                                        "namespace": "com.linkedin.pegasus2avro.assertion",
+                                        "symbolDocs": {
+                                            "COLUMNS": "Assertion is applied on all columns.",
+                                            "COLUMN_COUNT": "Assertion is applied on number of columns.",
+                                            "IDENTITY": "Assertion is applied on individual column value.",
+                                            "MAX": "Assertion is applied on column std deviation",
+                                            "MEAN": "Assertion is applied on column mean",
+                                            "MEDIAN": "Assertion is applied on column median",
+                                            "MIN": "Assertion is applied on column min",
+                                            "NULL_COUNT": "Assertion is applied on number of null values in column",
+                                            "NULL_PROPORTION": "Assertion is applied on proportion of null values in column",
+                                            "ROW_COUNT": "Assertion is applied on number of rows.",
+                                            "STDDEV": "Assertion is applied on column std deviation",
+                                            "SUM": "Assertion is applied on column sum",
+                                            "UNIQUE_COUNT": "Assertion is applied on number of distinct values in column",
+                                            "UNIQUE_PROPOTION": "Assertion is applied on proportion of distinct values in column",
+                                            "_NATIVE_": "Other"
+                                        },
+                                        "symbols": [
+                                            "ROW_COUNT",
+                                            "COLUMNS",
+                                            "COLUMN_COUNT",
+                                            "IDENTITY",
+                                            "MEAN",
+                                            "MEDIAN",
+                                            "UNIQUE_COUNT",
+                                            "UNIQUE_PROPOTION",
+                                            "NULL_COUNT",
+                                            "NULL_PROPORTION",
+                                            "STDDEV",
+                                            "MIN",
+                                            "MAX",
+                                            "SUM",
+                                            "_NATIVE_"
+                                        ],
+                                        "type": "enum"
+                                    }
+                                ]
+                            },
+                            {
+                                "doc": "Standardized assertion operator",
+                                "name": "operator",
+                                "type": {
+                                    "doc": "A boolean operator that is applied on the input to an assertion, after an aggregation function has been applied.",
+                                    "name": "AssertionStdOperator",
+                                    "namespace": "com.linkedin.pegasus2avro.assertion",
+                                    "symbolDocs": {
+                                        "BETWEEN": "Value being asserted is between min_value and max_value.  Requires 'minValue' & 'maxValue' parameters.",
+                                        "CONTAIN": "Value being asserted contains value. Requires 'value' parameter.",
+                                        "END_WITH": "Value being asserted ends with value. Requires 'value' parameter.",
+                                        "EQUAL_TO": "Value being asserted is equal to value. Requires 'value' parameter.",
+                                        "GREATER_THAN": "Value being asserted is greater than some value. Requires 'value' parameter.",
+                                        "GREATER_THAN_OR_EQUAL_TO": "Value being asserted is greater than or equal to some value. Requires 'value' parameter.",
+                                        "IN": "Value being asserted is one of the array values. Requires 'value' parameter.",
+                                        "LESS_THAN": "Value being asserted is less than a max value. Requires 'value' parameter.",
+                                        "LESS_THAN_OR_EQUAL_TO": "Value being asserted is less than or equal to some value. Requires 'value' parameter.",
+                                        "NOT_IN": "Value being asserted is not in one of the array values. Requires 'value' parameter.",
+                                        "NOT_NULL": "Value being asserted is not null. Requires no parameters.",
+                                        "REGEX_MATCH": "Value being asserted matches the regex value. Requires 'value' parameter.",
+                                        "START_WITH": "Value being asserted starts with value. Requires 'value' parameter.",
+                                        "_NATIVE_": "Other"
+                                    },
+                                    "symbols": [
+                                        "BETWEEN",
+                                        "LESS_THAN",
+                                        "LESS_THAN_OR_EQUAL_TO",
+                                        "GREATER_THAN",
+                                        "GREATER_THAN_OR_EQUAL_TO",
+                                        "EQUAL_TO",
+                                        "NOT_NULL",
+                                        "CONTAIN",
+                                        "END_WITH",
+                                        "START_WITH",
+                                        "REGEX_MATCH",
+                                        "IN",
+                                        "NOT_IN",
+                                        "_NATIVE_"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "default": null,
+                                "doc": "Standard parameters required for the assertion. e.g. min_value, max_value, value, columns",
+                                "name": "parameters",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "Parameters for AssertionStdOperators.",
+                                        "fields": [
+                                            {
+                                                "default": null,
+                                                "doc": "The value parameter of an assertion",
+                                                "name": "value",
+                                                "type": [
+                                                    "null",
+                                                    {
+                                                        "doc": "Single parameter for AssertionStdOperators.",
+                                                        "fields": [
+                                                            {
+                                                                "doc": "The parameter value",
+                                                                "name": "value",
+                                                                "type": "string"
+                                                            },
+                                                            {
+                                                                "doc": "The type of the parameter",
+                                                                "name": "type",
+                                                                "type": {
+                                                                    "name": "AssertionStdParameterType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.assertion",
+                                                                    "symbols": [
+                                                                        "STRING",
+                                                                        "NUMBER",
+                                                                        "LIST",
+                                                                        "SET",
+                                                                        "UNKNOWN"
+                                                                    ],
+                                                                    "type": "enum"
+                                                                }
+                                                            }
+                                                        ],
+                                                        "name": "AssertionStdParameter",
+                                                        "namespace": "com.linkedin.pegasus2avro.assertion",
+                                                        "type": "record"
+                                                    }
+                                                ]
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "The maxValue parameter of an assertion",
+                                                "name": "maxValue",
+                                                "type": [
+                                                    "null",
+                                                    "com.linkedin.pegasus2avro.assertion.AssertionStdParameter"
+                                                ]
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "The minValue parameter of an assertion",
+                                                "name": "minValue",
+                                                "type": [
+                                                    "null",
+                                                    "com.linkedin.pegasus2avro.assertion.AssertionStdParameter"
+                                                ]
+                                            }
+                                        ],
+                                        "name": "AssertionStdParameters",
+                                        "namespace": "com.linkedin.pegasus2avro.assertion",
+                                        "type": "record"
+                                    }
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Native assertion type",
+                                "name": "nativeType",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Native parameters required for the assertion.",
+                                "name": "nativeParameters",
+                                "type": [
+                                    "null",
+                                    {
+                                        "type": "map",
+                                        "values": "string"
+                                    }
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "name": "logic",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            }
+                        ],
+                        "name": "DatasetAssertionInfo",
+                        "namespace": "com.linkedin.pegasus2avro.assertion",
+                        "type": "record"
+                    }
                 ]
+            }
+        ],
+        "name": "AssertionInfo",
+        "namespace": "com.linkedin.pegasus2avro.assertion",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "assertionRunEvent",
+            "type": "timeseries"
+        },
+        "doc": "An event representing the current status of evaluating an assertion on a batch.\nAssertionRunEvent should be used for reporting the status of a run as an assertion evaluation progresses.",
+        "fields": [
+            {
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
+                "default": null,
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                ]
+            },
+            {
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
                 },
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
+                    "null"
+                ]
+            },
+            {
                 "default": null,
-                "doc": "Description of the Asset Container as it exists inside a source system",
-                "name": "description",
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "createdAt",
-                        "fieldType": "DATETIME"
-                    }
+                "doc": " Native (platform-specific) identifier for this run",
+                "name": "runId",
+                "type": "string"
+            },
+            {
+                "TimeseriesField": {},
+                "Urn": "Urn",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "assertionUrn",
+                "type": "string"
+            },
+            {
+                "TimeseriesField": {},
+                "Urn": "Urn",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
+                "name": "asserteeUrn",
+                "type": "string"
+            },
+            {
                 "default": null,
-                "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
-                "name": "created",
+                "doc": "Specification of the batch which this run is evaluating",
+                "name": "batchSpec",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.TimeStamp"
+                    {
+                        "doc": "A batch on which certain operations, e.g. data quality evaluation, is done.",
+                        "fields": [
+                            {
+                                "Searchable": {
+                                    "/*": {
+                                        "queryByDefault": true
+                                    }
+                                },
+                                "default": {},
+                                "doc": "Custom property bag.",
+                                "name": "customProperties",
+                                "type": {
+                                    "type": "map",
+                                    "values": "string"
+                                }
+                            },
+                            {
+                                "default": null,
+                                "doc": "The native identifier as specified by the system operating on the batch.",
+                                "name": "nativeBatchId",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "A query that identifies a batch of data",
+                                "name": "query",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Any limit to the number of rows in the batch, if applied",
+                                "name": "limit",
+                                "type": [
+                                    "null",
+                                    "int"
+                                ]
+                            }
+                        ],
+                        "name": "BatchSpec",
+                        "namespace": "com.linkedin.pegasus2avro.assertion",
+                        "type": "record"
+                    }
                 ]
             },
             {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "lastModifiedAt",
-                        "fieldType": "DATETIME"
+                "TimeseriesField": {},
+                "doc": "The status of the assertion run as per this timeseries event.",
+                "name": "status",
+                "type": {
+                    "name": "AssertionRunStatus",
+                    "namespace": "com.linkedin.pegasus2avro.assertion",
+                    "symbolDocs": {
+                        "COMPLETE": "The Assertion Run has completed"
+                    },
+                    "symbols": [
+                        "COMPLETE"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "default": null,
+                "doc": "Results of assertion, present if the status is COMPLETE",
+                "name": "result",
+                "type": [
+                    "null",
+                    {
+                        "doc": "The result of running an assertion",
+                        "fields": [
+                            {
+                                "TimeseriesField": {},
+                                "doc": " The final result, e.g. either SUCCESS or FAILURE.",
+                                "name": "type",
+                                "type": {
+                                    "name": "AssertionResultType",
+                                    "namespace": "com.linkedin.pegasus2avro.assertion",
+                                    "symbolDocs": {
+                                        "FAILURE": " The Assertion Failed",
+                                        "SUCCESS": " The Assertion Succeeded"
+                                    },
+                                    "symbols": [
+                                        "SUCCESS",
+                                        "FAILURE"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "default": null,
+                                "doc": "Number of rows for evaluated batch",
+                                "name": "rowCount",
+                                "type": [
+                                    "null",
+                                    "long"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Number of rows with missing value for evaluated batch",
+                                "name": "missingCount",
+                                "type": [
+                                    "null",
+                                    "long"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Number of rows with unexpected value for evaluated batch",
+                                "name": "unexpectedCount",
+                                "type": [
+                                    "null",
+                                    "long"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Observed aggregate value for evaluated batch",
+                                "name": "actualAggValue",
+                                "type": [
+                                    "null",
+                                    "float"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Other results of evaluation",
+                                "name": "nativeResults",
+                                "type": [
+                                    "null",
+                                    {
+                                        "type": "map",
+                                        "values": "string"
+                                    }
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "URL where full results are available",
+                                "name": "externalUrl",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            }
+                        ],
+                        "name": "AssertionResult",
+                        "namespace": "com.linkedin.pegasus2avro.assertion",
+                        "type": "record"
                     }
-                },
+                ]
+            },
+            {
                 "default": null,
-                "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
-                "name": "lastModified",
+                "doc": "Runtime parameters of evaluation",
+                "name": "runtimeContext",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.TimeStamp"
+                    {
+                        "type": "map",
+                        "values": "string"
+                    }
                 ]
             }
         ],
-        "name": "ContainerProperties",
-        "namespace": "com.linkedin.pegasus2avro.container",
+        "name": "AssertionRunEvent",
+        "namespace": "com.linkedin.pegasus2avro.assertion",
         "type": "record"
     },
     {
-        "Aspect": {
-            "name": "testInfo"
+        "Event": {
+            "name": "entityChangeEvent"
         },
-        "doc": "Information about a DataHub Test",
+        "doc": "Shared fields for all entity change events.",
         "fields": [
             {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "The name of the test",
-                "name": "name",
+                "doc": "The type of the entity affected. Corresponds to the entity registry, e.g. 'dataset', 'chart', 'dashboard', etc.",
+                "name": "entityType",
                 "type": "string"
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
+                "Urn": "Urn",
+                "doc": "The urn of the entity which was affected.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
-                "doc": "Category of the test",
+                "name": "entityUrn",
+                "type": "string"
+            },
+            {
+                "doc": "The category type (TAG, GLOSSARY_TERM, OWNERSHIP, TECHNICAL_SCHEMA, etc). This is used to determine what the rest of the schema will look like.",
                 "name": "category",
                 "type": "string"
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT"
-                },
+                "doc": "The operation type. This is used to determine what the rest of the schema will look like.",
+                "name": "operation",
+                "type": "string"
+            },
+            {
                 "default": null,
-                "doc": "Description of the test",
-                "name": "description",
+                "doc": "The urn of the entity which was affected.",
+                "name": "modifier",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "doc": "Configuration for the Test",
-                "name": "definition",
-                "type": {
-                    "fields": [
-                        {
-                            "doc": "The Test Definition Type",
-                            "name": "type",
-                            "type": {
-                                "name": "TestDefinitionType",
-                                "namespace": "com.linkedin.pegasus2avro.test",
-                                "symbolDocs": {
-                                    "JSON": "JSON / YAML test def"
-                                },
-                                "symbols": [
-                                    "JSON"
-                                ],
-                                "type": "enum"
-                            }
-                        },
-                        {
-                            "default": null,
-                            "doc": "JSON format configuration for the test",
-                            "name": "json",
-                            "type": [
-                                "null",
-                                "string"
-                            ]
-                        }
-                    ],
-                    "name": "TestDefinition",
-                    "namespace": "com.linkedin.pegasus2avro.test",
-                    "type": "record"
-                }
+                "default": null,
+                "doc": "Arbitrary key-value parameters corresponding to the event.",
+                "name": "parameters",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Arbitrary key-value parameters for an Entity Change Event. (any record).",
+                        "fields": [],
+                        "name": "Parameters",
+                        "namespace": "com.linkedin.pegasus2avro.platform.event.v1",
+                        "type": "record"
+                    }
+                ]
+            },
+            {
+                "doc": "Audit stamp of the operation",
+                "name": "auditStamp",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "doc": "The version of the event type, incremented in integers.",
+                "name": "version",
+                "type": "int"
             }
         ],
-        "name": "TestInfo",
-        "namespace": "com.linkedin.pegasus2avro.test",
+        "name": "EntityChangeEvent",
+        "namespace": "com.linkedin.pegasus2avro.platform.event.v1",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "testResults"
+            "name": "notebookContent"
         },
-        "doc": "Information about a Test Result",
+        "doc": "Content in a Notebook\nNote: This is IN BETA version",
         "fields": [
             {
-                "Relationship": {
-                    "/*/test": {
-                        "entityTypes": [
-                            "test"
-                        ],
-                        "name": "IsFailing"
-                    }
-                },
-                "Searchable": {
-                    "/*/test": {
-                        "fieldName": "failingTests",
-                        "fieldType": "URN",
-                        "hasValuesFieldName": "hasFailingTests"
-                    }
-                },
-                "doc": "Results that are failing",
-                "name": "failing",
+                "default": [],
+                "doc": "The content of a Notebook which is composed by a list of NotebookCell",
+                "name": "cells",
                 "type": {
                     "items": {
-                        "doc": "Information about a Test Result",
+                        "doc": "A record of all supported cells for a Notebook. Only one type of cell will be non-null.",
                         "fields": [
                             {
-                                "Urn": "Urn",
-                                "doc": "The urn of the test",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "test",
-                                "type": "string"
+                                "default": null,
+                                "doc": "The text cell content. The will be non-null only when all other cell field is null.",
+                                "name": "textCell",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "Text cell in a Notebook, which will present content in text format",
+                                        "fields": [
+                                            {
+                                                "default": null,
+                                                "doc": "Title of the cell",
+                                                "name": "cellTitle",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
+                                                "name": "cellId",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
+                                                "name": "changeAuditStamps",
+                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+                                            },
+                                            {
+                                                "doc": "The actual text in a TextCell in a Notebook",
+                                                "name": "text",
+                                                "type": "string"
+                                            }
+                                        ],
+                                        "name": "TextCell",
+                                        "namespace": "com.linkedin.pegasus2avro.notebook",
+                                        "type": "record"
+                                    }
+                                ]
                             },
                             {
-                                "doc": "The type of the result",
+                                "default": null,
+                                "doc": "The query cell content. The will be non-null only when all other cell field is null.",
+                                "name": "queryCell",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "Query cell in a Notebook, which will present content in query format",
+                                        "fields": [
+                                            {
+                                                "default": null,
+                                                "doc": "Title of the cell",
+                                                "name": "cellTitle",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
+                                                "name": "cellId",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
+                                                "name": "changeAuditStamps",
+                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+                                            },
+                                            {
+                                                "doc": "Raw query to explain some specific logic in a Notebook",
+                                                "name": "rawQuery",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "Captures information about who last executed this query cell and when",
+                                                "name": "lastExecuted",
+                                                "type": [
+                                                    "null",
+                                                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                                                ]
+                                            }
+                                        ],
+                                        "name": "QueryCell",
+                                        "namespace": "com.linkedin.pegasus2avro.notebook",
+                                        "type": "record"
+                                    }
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "The chart cell content. The will be non-null only when all other cell field is null.",
+                                "name": "chartCell",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "Chart cell in a notebook, which will present content in chart format",
+                                        "fields": [
+                                            {
+                                                "default": null,
+                                                "doc": "Title of the cell",
+                                                "name": "cellTitle",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
+                                                "name": "cellId",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
+                                                "name": "changeAuditStamps",
+                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+                                            }
+                                        ],
+                                        "name": "ChartCell",
+                                        "namespace": "com.linkedin.pegasus2avro.notebook",
+                                        "type": "record"
+                                    }
+                                ]
+                            },
+                            {
+                                "doc": "The type of this Notebook cell",
                                 "name": "type",
                                 "type": {
-                                    "name": "TestResultType",
-                                    "namespace": "com.linkedin.pegasus2avro.test",
+                                    "doc": "Type of Notebook Cell",
+                                    "name": "NotebookCellType",
+                                    "namespace": "com.linkedin.pegasus2avro.notebook",
                                     "symbolDocs": {
-                                        "FAILURE": " The Test Failed",
-                                        "SUCCESS": " The Test Succeeded"
+                                        "CHART_CELL": "CHART Notebook cell type. The cell content is chart only.",
+                                        "QUERY_CELL": "QUERY Notebook cell type. The cell context is query only.",
+                                        "TEXT_CELL": "TEXT Notebook cell type. The cell context is text only."
                                     },
                                     "symbols": [
-                                        "SUCCESS",
-                                        "FAILURE"
+                                        "TEXT_CELL",
+                                        "QUERY_CELL",
+                                        "CHART_CELL"
                                     ],
                                     "type": "enum"
                                 }
                             }
                         ],
-                        "name": "TestResult",
-                        "namespace": "com.linkedin.pegasus2avro.test",
+                        "name": "NotebookCell",
+                        "namespace": "com.linkedin.pegasus2avro.notebook",
                         "type": "record"
                     },
                     "type": "array"
                 }
-            },
+            }
+        ],
+        "name": "NotebookContent",
+        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "notebookInfo"
+        },
+        "doc": "Information about a Notebook\nNote: This is IN BETA version",
+        "fields": [
             {
-                "Relationship": {
-                    "/*/test": {
-                        "entityTypes": [
-                            "test"
-                        ],
-                        "name": "IsPassing"
-                    }
-                },
                 "Searchable": {
-                    "/*/test": {
-                        "fieldName": "passingTests",
-                        "fieldType": "URN",
-                        "hasValuesFieldName": "hasPassingTests"
+                    "/*": {
+                        "queryByDefault": true
                     }
                 },
-                "doc": "Results that are passing",
-                "name": "passing",
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
                 "type": {
-                    "items": "com.linkedin.pegasus2avro.test.TestResult",
-                    "type": "array"
+                    "type": "map",
+                    "values": "string"
                 }
+            },
+            {
+                "default": null,
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Title of the Notebook",
+                "name": "title",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
+                "default": null,
+                "doc": "Detailed description about the Notebook",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "doc": "Captures information about who created/last modified/deleted this Notebook and when",
+                "name": "changeAuditStamps",
+                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
             }
         ],
-        "name": "TestResults",
-        "namespace": "com.linkedin.pegasus2avro.test",
+        "name": "NotebookInfo",
+        "namespace": "com.linkedin.pegasus2avro.notebook",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "querySubjects"
+            "name": "editableNotebookProperties"
         },
-        "doc": "Information about the subjects of a particular Query, i.e. the assets\nbeing queried.",
+        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines\nNote: This is IN BETA version",
         "fields": [
             {
-                "doc": "One or more subjects of the query.\n\nIn single-asset queries (e.g. table select), this will contain the Table reference\nand optionally schema field references.\n\nIn multi-asset queries (e.g. table joins), this may contain multiple Table references\nand optionally schema field references.",
-                "name": "subjects",
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "default": null,
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                ]
+            },
+            {
+                "Searchable": {
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
+                },
+                "default": null,
+                "doc": "Edited documentation of the Notebook",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "EditableNotebookProperties",
+        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "type": "record"
+    },
+    {
+        "doc": "Usage data for a given resource, rolled up into a bucket.",
+        "fields": [
+            {
+                "doc": " Bucket start time in milliseconds ",
+                "name": "bucket",
+                "type": "long"
+            },
+            {
+                "doc": " Bucket duration ",
+                "name": "duration",
                 "type": {
-                    "items": {
-                        "doc": "A single subject of a particular query.\nIn the future, we may evolve this model to include richer details\nabout the Query Subject in relation to the query.",
-                        "fields": [
-                            {
-                                "Relationship": {
-                                    "entityTypes": [
-                                        "dataset",
-                                        "schemaField"
-                                    ],
-                                    "name": "IsAssociatedWith"
-                                },
-                                "Searchable": {
-                                    "fieldName": "entities",
-                                    "fieldType": "URN"
-                                },
-                                "Urn": "Urn",
-                                "doc": "An entity which is the subject of a query.",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "entity",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "QuerySubject",
-                        "namespace": "com.linkedin.pegasus2avro.query",
-                        "type": "record"
-                    },
-                    "type": "array"
+                    "doc": "Enum to define the length of a bucket when doing aggregations",
+                    "name": "WindowDuration",
+                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "symbols": [
+                        "YEAR",
+                        "MONTH",
+                        "WEEK",
+                        "DAY",
+                        "HOUR"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "Urn": "Urn",
+                "doc": " Resource associated with these usage stats ",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "resource",
+                "type": "string"
+            },
+            {
+                "doc": " Metrics associated with this bucket ",
+                "name": "metrics",
+                "type": {
+                    "doc": "Metrics for usage data for a given resource and bucket. Not all fields\nmake sense for all buckets, so every field is optional.",
+                    "fields": [
+                        {
+                            "default": null,
+                            "doc": " Unique user count ",
+                            "name": "uniqueUserCount",
+                            "type": [
+                                "null",
+                                "int"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": " Users within this bucket, with frequency counts ",
+                            "name": "users",
+                            "type": [
+                                "null",
+                                {
+                                    "items": {
+                                        "doc": " Records a single user's usage counts for a given resource ",
+                                        "fields": [
+                                            {
+                                                "Urn": "Urn",
+                                                "default": null,
+                                                "java": {
+                                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                                },
+                                                "name": "user",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "name": "count",
+                                                "type": "int"
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": " If user_email is set, we attempt to resolve the user's urn upon ingest ",
+                                                "name": "userEmail",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            }
+                                        ],
+                                        "name": "UserUsageCounts",
+                                        "namespace": "com.linkedin.pegasus2avro.usage",
+                                        "type": "record"
+                                    },
+                                    "type": "array"
+                                }
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": " Total SQL query count ",
+                            "name": "totalSqlQueries",
+                            "type": [
+                                "null",
+                                "int"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": " Frequent SQL queries; mostly makes sense for datasets in SQL databases ",
+                            "name": "topSqlQueries",
+                            "type": [
+                                "null",
+                                {
+                                    "items": "string",
+                                    "type": "array"
+                                }
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": " Field-level usage stats ",
+                            "name": "fields",
+                            "type": [
+                                "null",
+                                {
+                                    "items": {
+                                        "doc": " Records field-level usage counts for a given resource ",
+                                        "fields": [
+                                            {
+                                                "name": "fieldName",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "name": "count",
+                                                "type": "int"
+                                            }
+                                        ],
+                                        "name": "FieldUsageCounts",
+                                        "namespace": "com.linkedin.pegasus2avro.usage",
+                                        "type": "record"
+                                    },
+                                    "type": "array"
+                                }
+                            ]
+                        }
+                    ],
+                    "name": "UsageAggregationMetrics",
+                    "namespace": "com.linkedin.pegasus2avro.usage",
+                    "type": "record"
                 }
             }
         ],
-        "name": "QuerySubjects",
-        "namespace": "com.linkedin.pegasus2avro.query",
+        "name": "UsageAggregation",
+        "namespace": "com.linkedin.pegasus2avro.usage",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "queryProperties"
+            "name": "postInfo"
         },
-        "doc": "Information about a Query against one or more data assets (e.g. Tables or Views).",
+        "doc": "Information about a DataHub Post.",
         "fields": [
             {
-                "doc": "The Query Statement.",
-                "name": "statement",
+                "doc": "Type of the Post.",
+                "name": "type",
                 "type": {
-                    "doc": "A query statement against one or more data assets.",
+                    "doc": "Enum defining types of Posts.",
+                    "name": "PostType",
+                    "namespace": "com.linkedin.pegasus2avro.post",
+                    "symbolDocs": {
+                        "HOME_PAGE_ANNOUNCEMENT": "The Post is an Home Page announcement."
+                    },
+                    "symbols": [
+                        "HOME_PAGE_ANNOUNCEMENT"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "doc": "Content stored in the post.",
+                "name": "content",
+                "type": {
+                    "doc": "Content stored inside a Post.",
                     "fields": [
                         {
-                            "doc": "The query text",
-                            "name": "value",
+                            "Searchable": {
+                                "fieldType": "TEXT_PARTIAL"
+                            },
+                            "doc": "Title of the post.",
+                            "name": "title",
                             "type": "string"
                         },
                         {
-                            "default": "SQL",
-                            "doc": "The language of the Query, e.g. SQL.",
-                            "name": "language",
+                            "doc": "Type of content held in the post.",
+                            "name": "type",
                             "type": {
-                                "name": "QueryLanguage",
-                                "namespace": "com.linkedin.pegasus2avro.query",
+                                "doc": "Enum defining the type of content held in a Post.",
+                                "name": "PostContentType",
+                                "namespace": "com.linkedin.pegasus2avro.post",
                                 "symbolDocs": {
-                                    "SQL": "A SQL Query"
+                                    "LINK": "Link content",
+                                    "TEXT": "Text content"
                                 },
                                 "symbols": [
-                                    "SQL"
+                                    "TEXT",
+                                    "LINK"
                                 ],
                                 "type": "enum"
                             }
+                        },
+                        {
+                            "default": null,
+                            "doc": "Optional description of the post.",
+                            "name": "description",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": "Optional link that the post is associated with.",
+                            "java": {
+                                "class": "com.linkedin.pegasus2avro.common.url.Url",
+                                "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                            },
+                            "name": "link",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": "Optional media that the post is storing",
+                            "name": "media",
+                            "type": [
+                                "null",
+                                {
+                                    "doc": "Carries information about which roles a user is assigned to.",
+                                    "fields": [
+                                        {
+                                            "doc": "Type of content the Media is storing, e.g. image, video, etc.",
+                                            "name": "type",
+                                            "type": {
+                                                "doc": "Enum defining the type of content a Media object holds.",
+                                                "name": "MediaType",
+                                                "namespace": "com.linkedin.pegasus2avro.common",
+                                                "symbolDocs": {
+                                                    "IMAGE": "The Media holds an image."
+                                                },
+                                                "symbols": [
+                                                    "IMAGE"
+                                                ],
+                                                "type": "enum"
+                                            }
+                                        },
+                                        {
+                                            "doc": "Where the media content is stored.",
+                                            "java": {
+                                                "class": "com.linkedin.pegasus2avro.common.url.Url",
+                                                "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                                            },
+                                            "name": "location",
+                                            "type": "string"
+                                        }
+                                    ],
+                                    "name": "Media",
+                                    "namespace": "com.linkedin.pegasus2avro.common",
+                                    "type": "record"
+                                }
+                            ]
                         }
                     ],
-                    "name": "QueryStatement",
-                    "namespace": "com.linkedin.pegasus2avro.query",
+                    "name": "PostContent",
+                    "namespace": "com.linkedin.pegasus2avro.post",
                     "type": "record"
                 }
             },
             {
-                "Searchable": {},
-                "doc": "The source of the Query",
-                "name": "source",
-                "type": {
-                    "name": "QuerySource",
-                    "namespace": "com.linkedin.pegasus2avro.query",
-                    "symbolDocs": {
-                        "MANUAL": "The query was entered manually by a user (via the UI)."
-                    },
-                    "symbols": [
-                        "MANUAL"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "default": null,
-                "doc": "Optional display name to identify the query.",
-                "name": "name",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The Query description.",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
                 "Searchable": {
-                    "/actor": {
-                        "fieldName": "createdBy",
-                        "fieldType": "URN"
-                    },
-                    "/time": {
-                        "fieldName": "createdAt",
-                        "fieldType": "DATETIME"
-                    }
+                    "fieldType": "COUNT"
                 },
-                "doc": "Audit stamp capturing the time and actor who created the Query.",
+                "doc": "The time at which the post was initially created",
                 "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "type": "long"
             },
             {
                 "Searchable": {
-                    "/actor": {
-                        "fieldName": "lastModifiedBy",
-                        "fieldType": "URN"
-                    },
-                    "/time": {
-                        "fieldName": "lastModifiedAt",
-                        "fieldType": "DATETIME"
-                    }
+                    "fieldType": "COUNT"
                 },
-                "doc": "Audit stamp capturing the time and actor who last modified the Query.",
+                "doc": "The time at which the post was last modified",
                 "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "type": "long"
             }
         ],
-        "name": "QueryProperties",
-        "namespace": "com.linkedin.pegasus2avro.query",
+        "name": "PostInfo",
+        "namespace": "com.linkedin.pegasus2avro.post",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.policy.DataHubPolicyInfo",
     {
         "Aspect": {
-            "name": "dataHubRoleInfo"
+            "name": "dataHubUpgradeResult"
         },
-        "doc": "Information about a DataHub Role.",
+        "doc": "Information collected when a DataHubUpgrade successfully finishes",
         "fields": [
             {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Name of the Role",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "TEXT"
-                },
-                "doc": "Description of the Role",
-                "name": "description",
-                "type": "string"
+                "doc": "Timestamp when we started this DataHubUpgrade",
+                "name": "timestampMs",
+                "type": "long"
             },
             {
-                "default": false,
-                "doc": "Whether the role should be editable via the UI",
-                "name": "editable",
-                "type": "boolean"
+                "default": null,
+                "doc": "Result map to place helpful information about this upgrade job",
+                "name": "result",
+                "type": [
+                    "null",
+                    {
+                        "type": "map",
+                        "values": "string"
+                    }
+                ]
             }
         ],
-        "name": "DataHubRoleInfo",
-        "namespace": "com.linkedin.pegasus2avro.policy",
+        "name": "DataHubUpgradeResult",
+        "namespace": "com.linkedin.pegasus2avro.upgrade",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.DashboardKey",
     {
         "Aspect": {
-            "entityAspects": [
-                "globalSettingsInfo"
-            ],
-            "entityCategory": "internal",
-            "entityDoc": "Global settings for an the platform",
-            "keyForEntity": "globalSettings",
-            "name": "globalSettingsKey"
+            "name": "dataHubUpgradeRequest"
         },
-        "doc": "Key for a Global Settings",
+        "doc": "Information collected when kicking off a DataHubUpgrade",
         "fields": [
             {
-                "doc": "Id for the settings. There should be only 1 global settings urn: urn:li:globalSettings:0",
-                "name": "id",
+                "doc": "Timestamp when we started this DataHubUpgrade",
+                "name": "timestampMs",
+                "type": "long"
+            },
+            {
+                "doc": "Version of this upgrade",
+                "name": "version",
                 "type": "string"
             }
         ],
-        "name": "GlobalSettingsKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "DataHubUpgradeRequest",
+        "namespace": "com.linkedin.pegasus2avro.upgrade",
+        "type": "record"
+    },
+    {
+        "doc": "The filter for finding a record or a collection of records",
+        "fields": [
+            {
+                "default": null,
+                "doc": "A list of disjunctive criterion for the filter. (or operation to combine filters)",
+                "name": "or",
+                "type": [
+                    "null",
+                    {
+                        "items": {
+                            "doc": "A list of criterion and'd together.",
+                            "fields": [
+                                {
+                                    "doc": "A list of and criteria the filter applies to the query",
+                                    "name": "and",
+                                    "type": {
+                                        "items": {
+                                            "doc": "A criterion for matching a field with given value",
+                                            "fields": [
+                                                {
+                                                    "doc": "The name of the field that the criterion refers to",
+                                                    "name": "field",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "doc": "The value of the intended field",
+                                                    "name": "value",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "default": [],
+                                                    "doc": "Values. one of which the intended field should match\nNote, if values is set, the above \"value\" field will be ignored",
+                                                    "name": "values",
+                                                    "type": {
+                                                        "items": "string",
+                                                        "type": "array"
+                                                    }
+                                                },
+                                                {
+                                                    "default": "EQUAL",
+                                                    "doc": "The condition for the criterion, e.g. EQUAL, START_WITH",
+                                                    "name": "condition",
+                                                    "type": {
+                                                        "doc": "The matching condition in a filter criterion",
+                                                        "name": "Condition",
+                                                        "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
+                                                        "symbolDocs": {
+                                                            "CONTAIN": "Represent the relation: String field contains value, e.g. name contains Profile",
+                                                            "END_WITH": "Represent the relation: String field ends with value, e.g. name ends with Event",
+                                                            "EQUAL": "Represent the relation: field = value, e.g. platform = hdfs",
+                                                            "GREATER_THAN": "Represent the relation greater than, e.g. ownerCount > 5",
+                                                            "GREATER_THAN_OR_EQUAL_TO": "Represent the relation greater than or equal to, e.g. ownerCount >= 5",
+                                                            "IN": "Represent the relation: String field is one of the array values to, e.g. name in [\"Profile\", \"Event\"]",
+                                                            "IS_NULL": "Represent the relation: field is null, e.g. platform is null",
+                                                            "LESS_THAN": "Represent the relation less than, e.g. ownerCount < 3",
+                                                            "LESS_THAN_OR_EQUAL_TO": "Represent the relation less than or equal to, e.g. ownerCount <= 3",
+                                                            "START_WITH": "Represent the relation: String field starts with value, e.g. name starts with PageView"
+                                                        },
+                                                        "symbols": [
+                                                            "CONTAIN",
+                                                            "END_WITH",
+                                                            "EQUAL",
+                                                            "IS_NULL",
+                                                            "GREATER_THAN",
+                                                            "GREATER_THAN_OR_EQUAL_TO",
+                                                            "IN",
+                                                            "LESS_THAN",
+                                                            "LESS_THAN_OR_EQUAL_TO",
+                                                            "START_WITH"
+                                                        ],
+                                                        "type": "enum"
+                                                    }
+                                                },
+                                                {
+                                                    "default": false,
+                                                    "doc": "Whether the condition should be negated",
+                                                    "name": "negated",
+                                                    "type": "boolean"
+                                                }
+                                            ],
+                                            "name": "Criterion",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
+                                            "type": "record"
+                                        },
+                                        "type": "array"
+                                    }
+                                }
+                            ],
+                            "name": "ConjunctiveCriterion",
+                            "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
+                            "type": "record"
+                        },
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Deprecated! A list of conjunctive criterion for the filter. If \"or\" field is provided, then this field is ignored.",
+                "name": "criteria",
+                "type": [
+                    "null",
+                    {
+                        "items": "com.linkedin.pegasus2avro.metadata.query.filter.Criterion",
+                        "type": "array"
+                    }
+                ]
+            }
+        ],
+        "name": "Filter",
+        "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.metadata.key.MLFeatureTableKey",
     {
         "Aspect": {
             "entityAspects": [
-                "dataHubSecretValue"
+                "containerProperties",
+                "editableContainerProperties",
+                "dataPlatformInstance",
+                "subTypes",
+                "ownership",
+                "container",
+                "globalTags",
+                "glossaryTerms",
+                "institutionalMemory",
+                "browsePaths",
+                "status",
+                "domains"
             ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataHubSecret",
-            "name": "dataHubSecretKey"
+            "entityCategory": "_unset_",
+            "entityDoc": "A container of related data assets.",
+            "keyForEntity": "container",
+            "name": "containerKey"
         },
-        "doc": "Key for a DataHub Secret",
+        "doc": "Key for an Asset Container",
         "fields": [
             {
-                "doc": "A unique id for the Secret",
-                "name": "id",
-                "type": "string"
+                "default": null,
+                "doc": "Unique guid for container",
+                "name": "guid",
+                "type": [
+                    "null",
+                    "string"
+                ]
             }
         ],
-        "name": "DataHubSecretKey",
+        "name": "ContainerKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.MLFeatureTableKey",
+    "com.linkedin.pegasus2avro.metadata.key.DataHubRetentionKey",
+    "com.linkedin.pegasus2avro.metadata.key.DataJobKey",
+    "com.linkedin.pegasus2avro.metadata.key.MLModelDeploymentKey",
+    "com.linkedin.pegasus2avro.metadata.key.SchemaFieldKey",
+    "com.linkedin.pegasus2avro.metadata.key.DataProcessKey",
+    "com.linkedin.pegasus2avro.metadata.key.CorpGroupKey",
+    "com.linkedin.pegasus2avro.metadata.key.MLPrimaryKeyKey",
+    "com.linkedin.pegasus2avro.metadata.key.DataFlowKey",
+    "com.linkedin.pegasus2avro.metadata.key.DashboardKey",
+    "com.linkedin.pegasus2avro.metadata.key.MLModelGroupKey",
     {
         "Aspect": {
             "entityAspects": [
                 "telemetryClientId"
             ],
             "entityCategory": "internal",
             "keyForEntity": "telemetry",
@@ -12281,313 +12228,338 @@
                 "type": "string"
             }
         ],
         "name": "TelemetryKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.metadata.key.GlossaryTermKey",
+    "com.linkedin.pegasus2avro.metadata.key.GlossaryNodeKey",
     {
         "Aspect": {
             "entityAspects": [
-                "dataHubAccessTokenInfo"
+                "dataProcessInstanceInput",
+                "dataProcessInstanceOutput",
+                "dataProcessInstanceProperties",
+                "dataProcessInstanceRelationships",
+                "dataProcessInstanceRunEvent"
             ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataHubAccessToken",
-            "name": "dataHubAccessTokenKey"
+            "entityCategory": "_unset_",
+            "entityDoc": "DataProcessInstance represents an instance of a datajob/jobflow run",
+            "keyForEntity": "dataProcessInstance",
+            "name": "dataProcessInstanceKey"
         },
-        "doc": "Key for a DataHub Access Token",
+        "doc": "Key for an Asset DataProcessInstance",
         "fields": [
             {
-                "doc": "Access token's SHA-256 hashed JWT signature",
+                "doc": "A unique id for the DataProcessInstance . Should be separate from the name used for displaying a DataProcessInstance.",
                 "name": "id",
                 "type": "string"
             }
         ],
-        "name": "DataHubAccessTokenKey",
+        "name": "DataProcessInstanceKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.DataHubPolicyKey",
     {
         "Aspect": {
             "entityAspects": [
-                "testInfo"
+                "dataHubRoleInfo"
             ],
             "entityCategory": "core",
-            "entityDoc": "A DataHub test",
-            "keyForEntity": "test",
-            "name": "testKey"
+            "keyForEntity": "dataHubRole",
+            "name": "dataHubRoleKey"
         },
-        "doc": "Key for a Test",
+        "doc": "Key for a DataHub Role",
         "fields": [
             {
-                "doc": "Unique id for the test",
+                "doc": "A unique id for the DataHub role record. Generated on the server side at role creation time.",
                 "name": "id",
                 "type": "string"
             }
         ],
-        "name": "TestKey",
+        "name": "DataHubRoleKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.metadata.key.MLModelKey",
     {
         "Aspect": {
             "entityAspects": [
-                "dataHubIngestionSourceInfo"
+                "dataHubViewInfo"
             ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataHubIngestionSource",
-            "name": "dataHubIngestionSourceKey"
+            "entityCategory": "core",
+            "keyForEntity": "dataHubView",
+            "name": "dataHubViewKey"
         },
-        "doc": "Key for a DataHub ingestion source",
+        "doc": "Key for a DataHub View",
         "fields": [
             {
-                "doc": "A unique id for the Ingestion Source, either generated or provided",
+                "doc": "A unique id for the View",
                 "name": "id",
                 "type": "string"
             }
         ],
-        "name": "DataHubIngestionSourceKey",
+        "name": "DataHubViewKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
     {
         "Aspect": {
             "entityAspects": [
-                "assertionInfo",
-                "dataPlatformInstance",
-                "assertionRunEvent",
+                "queryProperties",
+                "querySubjects",
                 "status"
             ],
             "entityCategory": "core",
-            "entityDoc": "Assertion represents a data quality rule applied on one or more dataset.",
-            "keyForEntity": "assertion",
-            "name": "assertionKey"
+            "keyForEntity": "query",
+            "name": "queryKey"
         },
-        "doc": "Key for a Assertion",
+        "doc": "Key for a Query",
         "fields": [
             {
-                "doc": "Unique id for the assertion.",
-                "name": "assertionId",
+                "doc": "A unique id for the Query.",
+                "name": "id",
                 "type": "string"
             }
         ],
-        "name": "AssertionKey",
+        "name": "QueryKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.metadata.key.CorpUserKey",
     {
         "Aspect": {
             "entityAspects": [
-                "postInfo"
+                "dataPlatformInstanceProperties",
+                "ownership",
+                "globalTags",
+                "institutionalMemory",
+                "deprecation",
+                "status"
             ],
-            "entityCategory": "core",
-            "keyForEntity": "post",
-            "name": "postKey"
+            "entityCategory": "internal",
+            "keyForEntity": "dataPlatformInstance",
+            "name": "dataPlatformInstanceKey"
         },
-        "doc": "Key for a Post.",
+        "doc": "Key for a Dataset",
         "fields": [
             {
-                "doc": "A unique id for the DataHub Post record. Generated on the server side at Post creation time.",
-                "name": "id",
+                "Urn": "Urn",
+                "doc": "Data platform urn associated with the instance",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "platform",
+                "type": "string"
+            },
+            {
+                "doc": "Unique instance id",
+                "name": "instance",
                 "type": "string"
             }
         ],
-        "name": "PostKey",
+        "name": "DataPlatformInstanceKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.MLFeatureKey",
-    "com.linkedin.pegasus2avro.metadata.key.TagKey",
     {
         "Aspect": {
             "entityAspects": [
-                "domainProperties",
+                "notebookInfo",
+                "notebookContent",
+                "editableNotebookProperties",
+                "ownership",
+                "status",
+                "globalTags",
+                "glossaryTerms",
+                "browsePaths",
                 "institutionalMemory",
-                "ownership"
+                "domains",
+                "subTypes",
+                "dataPlatformInstance"
             ],
             "entityCategory": "_unset_",
-            "entityDoc": "A data domain within an organization.",
-            "keyForEntity": "domain",
-            "name": "domainKey"
+            "entityDoc": "Notebook represents a combination of query, text, chart and etc. This is in BETA version",
+            "keyForEntity": "notebook",
+            "name": "notebookKey"
         },
-        "doc": "Key for an Asset Domain",
+        "doc": "Key for a Notebook",
         "fields": [
             {
-                "doc": "A unique id for the domain. Should be separate from the name used for displaying a Domain.",
-                "name": "id",
+                "doc": "The name of the Notebook tool such as QueryBook, etc.",
+                "name": "notebookTool",
+                "type": "string"
+            },
+            {
+                "doc": "Unique id for the Notebook. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773'",
+                "name": "notebookId",
                 "type": "string"
             }
         ],
-        "name": "DomainKey",
+        "name": "NotebookKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.metadata.key.MLFeatureKey",
+    "com.linkedin.pegasus2avro.metadata.key.ChartKey",
     {
         "Aspect": {
             "entityAspects": [
-                "dataHubViewInfo"
+                "assertionInfo",
+                "dataPlatformInstance",
+                "assertionRunEvent",
+                "status"
             ],
             "entityCategory": "core",
-            "keyForEntity": "dataHubView",
-            "name": "dataHubViewKey"
+            "entityDoc": "Assertion represents a data quality rule applied on one or more dataset.",
+            "keyForEntity": "assertion",
+            "name": "assertionKey"
         },
-        "doc": "Key for a DataHub View",
+        "doc": "Key for a Assertion",
         "fields": [
             {
-                "doc": "A unique id for the View",
+                "doc": "Unique id for the assertion.",
+                "name": "assertionId",
+                "type": "string"
+            }
+        ],
+        "name": "AssertionKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataHubSecretValue"
+            ],
+            "entityCategory": "internal",
+            "keyForEntity": "dataHubSecret",
+            "name": "dataHubSecretKey"
+        },
+        "doc": "Key for a DataHub Secret",
+        "fields": [
+            {
+                "doc": "A unique id for the Secret",
                 "name": "id",
                 "type": "string"
             }
         ],
-        "name": "DataHubViewKey",
+        "name": "DataHubSecretKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.MLPrimaryKeyKey",
+    "com.linkedin.pegasus2avro.metadata.key.DataHubPolicyKey",
     {
         "Aspect": {
             "entityAspects": [
-                "dataHubStepStateProperties"
+                "testInfo"
             ],
             "entityCategory": "core",
-            "keyForEntity": "dataHubStepState",
-            "name": "dataHubStepStateKey"
+            "entityDoc": "A DataHub test",
+            "keyForEntity": "test",
+            "name": "testKey"
         },
-        "doc": "Key for a DataHub Step State",
+        "doc": "Key for a Test",
         "fields": [
             {
-                "doc": "A unique id for the state",
+                "doc": "Unique id for the test",
                 "name": "id",
                 "type": "string"
             }
         ],
-        "name": "DataHubStepStateKey",
+        "name": "TestKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
     {
         "Aspect": {
             "entityAspects": [
-                "dataHubUpgradeRequest",
-                "dataHubUpgradeResult"
+                "dataHubAccessTokenInfo"
             ],
             "entityCategory": "internal",
-            "keyForEntity": "dataHubUpgrade",
-            "name": "dataHubUpgradeKey"
+            "keyForEntity": "dataHubAccessToken",
+            "name": "dataHubAccessTokenKey"
         },
-        "doc": "Key for a DataHubUpgrade",
+        "doc": "Key for a DataHub Access Token",
         "fields": [
             {
+                "doc": "Access token's SHA-256 hashed JWT signature",
                 "name": "id",
                 "type": "string"
             }
         ],
-        "name": "DataHubUpgradeKey",
+        "name": "DataHubAccessTokenKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.GlossaryNodeKey",
-    "com.linkedin.pegasus2avro.metadata.key.MLModelDeploymentKey",
     {
         "Aspect": {
             "entityAspects": [
-                "dataHubRoleInfo"
+                "dataHubUpgradeRequest",
+                "dataHubUpgradeResult"
             ],
-            "entityCategory": "core",
-            "keyForEntity": "dataHubRole",
-            "name": "dataHubRoleKey"
+            "entityCategory": "internal",
+            "keyForEntity": "dataHubUpgrade",
+            "name": "dataHubUpgradeKey"
         },
-        "doc": "Key for a DataHub Role",
+        "doc": "Key for a DataHubUpgrade",
         "fields": [
             {
-                "doc": "A unique id for the DataHub role record. Generated on the server side at role creation time.",
                 "name": "id",
                 "type": "string"
             }
         ],
-        "name": "DataHubRoleKey",
+        "name": "DataHubUpgradeKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
     "com.linkedin.pegasus2avro.metadata.key.DataPlatformKey",
     {
         "Aspect": {
             "entityAspects": [
-                "containerProperties",
-                "editableContainerProperties",
-                "dataPlatformInstance",
-                "subTypes",
-                "ownership",
-                "container",
-                "globalTags",
-                "glossaryTerms",
-                "institutionalMemory",
-                "browsePaths",
-                "status",
-                "domains",
-                "browsePathsV2"
+                "inviteToken"
             ],
-            "entityCategory": "_unset_",
-            "entityDoc": "A container of related data assets.",
-            "keyForEntity": "container",
-            "name": "containerKey"
+            "entityCategory": "core",
+            "keyForEntity": "inviteToken",
+            "name": "inviteTokenKey"
         },
-        "doc": "Key for an Asset Container",
+        "doc": "Key for an InviteToken.",
         "fields": [
             {
-                "default": null,
-                "doc": "Unique guid for container",
-                "name": "guid",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "doc": "A unique id for the invite token.",
+                "name": "id",
+                "type": "string"
             }
         ],
-        "name": "ContainerKey",
+        "name": "InviteTokenKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
     {
         "Aspect": {
             "entityAspects": [
-                "dataPlatformInstanceProperties",
-                "ownership",
-                "globalTags",
-                "institutionalMemory",
-                "deprecation",
-                "status"
+                "postInfo"
             ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataPlatformInstance",
-            "name": "dataPlatformInstanceKey"
+            "entityCategory": "core",
+            "keyForEntity": "post",
+            "name": "postKey"
         },
-        "doc": "Key for a Dataset",
+        "doc": "Key for a Post.",
         "fields": [
             {
-                "Urn": "Urn",
-                "doc": "Data platform urn associated with the instance",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "platform",
-                "type": "string"
-            },
-            {
-                "doc": "Unique instance id",
-                "name": "instance",
+                "doc": "A unique id for the DataHub Post record. Generated on the server side at Post creation time.",
+                "name": "id",
                 "type": "string"
             }
         ],
-        "name": "DataPlatformInstanceKey",
+        "name": "PostKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.DataJobKey",
     {
         "Aspect": {
             "entityAspects": [
                 "dataHubExecutionRequestInput",
                 "dataHubExecutionRequestSignal",
                 "dataHubExecutionRequestResult"
             ],
@@ -12603,587 +12575,512 @@
                 "type": "string"
             }
         ],
         "name": "ExecutionRequestKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.DataProcessKey",
-    "com.linkedin.pegasus2avro.metadata.key.CorpUserKey",
     {
         "Aspect": {
             "entityAspects": [
-                "notebookInfo",
-                "notebookContent",
-                "editableNotebookProperties",
-                "ownership",
-                "status",
-                "globalTags",
-                "glossaryTerms",
-                "browsePaths",
-                "institutionalMemory",
-                "domains",
-                "subTypes",
-                "dataPlatformInstance",
-                "browsePathsV2"
+                "dataHubIngestionSourceInfo"
             ],
-            "entityCategory": "_unset_",
-            "entityDoc": "Notebook represents a combination of query, text, chart and etc. This is in BETA version",
-            "keyForEntity": "notebook",
-            "name": "notebookKey"
+            "entityCategory": "internal",
+            "keyForEntity": "dataHubIngestionSource",
+            "name": "dataHubIngestionSourceKey"
         },
-        "doc": "Key for a Notebook",
+        "doc": "Key for a DataHub ingestion source",
         "fields": [
             {
-                "doc": "The name of the Notebook tool such as QueryBook, etc.",
-                "name": "notebookTool",
-                "type": "string"
-            },
-            {
-                "doc": "Unique id for the Notebook. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773'",
-                "name": "notebookId",
+                "doc": "A unique id for the Ingestion Source, either generated or provided",
+                "name": "id",
                 "type": "string"
             }
         ],
-        "name": "NotebookKey",
+        "name": "DataHubIngestionSourceKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.SchemaFieldKey",
-    "com.linkedin.pegasus2avro.metadata.key.MLModelGroupKey",
     {
         "Aspect": {
             "entityAspects": [
-                "dataProcessInstanceInput",
-                "dataProcessInstanceOutput",
-                "dataProcessInstanceProperties",
-                "dataProcessInstanceRelationships",
-                "dataProcessInstanceRunEvent"
+                "domainProperties",
+                "institutionalMemory",
+                "ownership"
             ],
             "entityCategory": "_unset_",
-            "entityDoc": "DataProcessInstance represents an instance of a datajob/jobflow run",
-            "keyForEntity": "dataProcessInstance",
-            "name": "dataProcessInstanceKey"
+            "entityDoc": "A data domain within an organization.",
+            "keyForEntity": "domain",
+            "name": "domainKey"
         },
-        "doc": "Key for an Asset DataProcessInstance",
+        "doc": "Key for an Asset Domain",
         "fields": [
             {
-                "doc": "A unique id for the DataProcessInstance . Should be separate from the name used for displaying a DataProcessInstance.",
+                "doc": "A unique id for the domain. Should be separate from the name used for displaying a Domain.",
                 "name": "id",
                 "type": "string"
             }
         ],
-        "name": "DataProcessInstanceKey",
+        "name": "DomainKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.metadata.key.DatasetKey",
     {
         "Aspect": {
             "entityAspects": [
-                "queryProperties",
-                "querySubjects",
-                "status"
+                "dataHubStepStateProperties"
             ],
             "entityCategory": "core",
-            "keyForEntity": "query",
-            "name": "queryKey"
+            "keyForEntity": "dataHubStepState",
+            "name": "dataHubStepStateKey"
         },
-        "doc": "Key for a Query",
+        "doc": "Key for a DataHub Step State",
         "fields": [
             {
-                "doc": "A unique id for the Query.",
+                "doc": "A unique id for the state",
                 "name": "id",
                 "type": "string"
             }
         ],
-        "name": "QueryKey",
+        "name": "DataHubStepStateKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.MLModelKey",
-    "com.linkedin.pegasus2avro.metadata.key.GlossaryTermKey",
-    "com.linkedin.pegasus2avro.metadata.key.DatasetKey",
     {
         "Aspect": {
             "entityAspects": [
-                "inviteToken"
+                "globalSettingsInfo"
             ],
-            "entityCategory": "core",
-            "keyForEntity": "inviteToken",
-            "name": "inviteTokenKey"
+            "entityCategory": "internal",
+            "entityDoc": "Global settings for an the platform",
+            "keyForEntity": "globalSettings",
+            "name": "globalSettingsKey"
         },
-        "doc": "Key for an InviteToken.",
+        "doc": "Key for a Global Settings",
         "fields": [
             {
-                "doc": "A unique id for the invite token.",
+                "doc": "Id for the settings. There should be only 1 global settings urn: urn:li:globalSettings:0",
                 "name": "id",
                 "type": "string"
             }
         ],
-        "name": "InviteTokenKey",
+        "name": "GlobalSettingsKey",
         "namespace": "com.linkedin.pegasus2avro.metadata.key",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.metadata.key.CorpGroupKey",
-    "com.linkedin.pegasus2avro.metadata.key.DataHubRetentionKey",
-    "com.linkedin.pegasus2avro.metadata.key.DataFlowKey",
-    "com.linkedin.pegasus2avro.metadata.key.ChartKey",
-    "com.linkedin.pegasus2avro.metadata.query.filter.Filter",
-    "com.linkedin.pegasus2avro.common.GlossaryTerms",
+    "com.linkedin.pegasus2avro.metadata.key.TagKey",
     {
         "Aspect": {
-            "name": "embed"
+            "name": "globalSettingsInfo"
         },
-        "doc": "Information regarding rendering an embed for an asset.",
+        "doc": "DataHub Global platform settings. Careful - these should not be modified by the outside world!",
         "fields": [
             {
                 "default": null,
-                "doc": "An embed URL to be rendered inside of an iframe.",
-                "name": "renderUrl",
+                "doc": "Settings related to the Views Feature",
+                "name": "views",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "doc": "Settings for DataHub Views feature.",
+                        "fields": [
+                            {
+                                "Urn": "Urn",
+                                "default": null,
+                                "doc": "The default View for the instance, or organization.",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "defaultView",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            }
+                        ],
+                        "name": "GlobalViewsSettings",
+                        "namespace": "com.linkedin.pegasus2avro.settings.global",
+                        "type": "record"
+                    }
                 ]
             }
         ],
-        "name": "Embed",
-        "namespace": "com.linkedin.pegasus2avro.common",
+        "name": "GlobalSettingsInfo",
+        "namespace": "com.linkedin.pegasus2avro.settings.global",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.common.DataPlatformInstance",
     {
         "Aspect": {
-            "name": "inputFields"
+            "name": "testResults"
         },
-        "doc": "Information about the fields a chart or dashboard references",
+        "doc": "Information about a Test Result",
         "fields": [
             {
-                "doc": "List of fields being referenced",
-                "name": "fields",
+                "Relationship": {
+                    "/*/test": {
+                        "entityTypes": [
+                            "test"
+                        ],
+                        "name": "IsFailing"
+                    }
+                },
+                "Searchable": {
+                    "/*/test": {
+                        "fieldName": "failingTests",
+                        "fieldType": "URN",
+                        "hasValuesFieldName": "hasFailingTests"
+                    }
+                },
+                "doc": "Results that are failing",
+                "name": "failing",
                 "type": {
                     "items": {
-                        "doc": "Information about a field a chart or dashboard references",
+                        "doc": "Information about a Test Result",
                         "fields": [
                             {
-                                "Relationship": {
-                                    "entityTypes": [
-                                        "schemaField"
-                                    ],
-                                    "name": "consumesField"
-                                },
                                 "Urn": "Urn",
-                                "doc": "Urn of the schema being referenced for lineage purposes",
+                                "doc": "The urn of the test",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
-                                "name": "schemaFieldUrn",
+                                "name": "test",
                                 "type": "string"
                             },
                             {
-                                "default": null,
-                                "doc": "Copied version of the referenced schema field object for indexing purposes",
-                                "name": "schemaField",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.schema.SchemaField"
-                                ]
+                                "doc": "The type of the result",
+                                "name": "type",
+                                "type": {
+                                    "name": "TestResultType",
+                                    "namespace": "com.linkedin.pegasus2avro.test",
+                                    "symbolDocs": {
+                                        "FAILURE": " The Test Failed",
+                                        "SUCCESS": " The Test Succeeded"
+                                    },
+                                    "symbols": [
+                                        "SUCCESS",
+                                        "FAILURE"
+                                    ],
+                                    "type": "enum"
+                                }
                             }
                         ],
-                        "name": "InputField",
-                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "name": "TestResult",
+                        "namespace": "com.linkedin.pegasus2avro.test",
                         "type": "record"
                     },
                     "type": "array"
                 }
-            }
-        ],
-        "name": "InputFields",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    "com.linkedin.pegasus2avro.common.InstitutionalMemory",
-    "com.linkedin.pegasus2avro.common.Ownership",
-    {
-        "Aspect": {
-            "name": "subTypes"
-        },
-        "doc": "Sub Types. Use this aspect to specialize a generic Entity\ne.g. Making a Dataset also be a View or also be a LookerExplore",
-        "fields": [
+            },
             {
+                "Relationship": {
+                    "/*/test": {
+                        "entityTypes": [
+                            "test"
+                        ],
+                        "name": "IsPassing"
+                    }
+                },
                 "Searchable": {
-                    "/*": {
-                        "addToFilters": true,
-                        "fieldType": "KEYWORD",
-                        "filterNameOverride": "Sub Type",
-                        "queryByDefault": true
+                    "/*/test": {
+                        "fieldName": "passingTests",
+                        "fieldType": "URN",
+                        "hasValuesFieldName": "hasPassingTests"
                     }
                 },
-                "doc": "The names of the specific types.",
-                "name": "typeNames",
+                "doc": "Results that are passing",
+                "name": "passing",
                 "type": {
-                    "items": "string",
+                    "items": "com.linkedin.pegasus2avro.test.TestResult",
                     "type": "array"
                 }
             }
         ],
-        "name": "SubTypes",
-        "namespace": "com.linkedin.pegasus2avro.common",
+        "name": "TestResults",
+        "namespace": "com.linkedin.pegasus2avro.test",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "origin"
+            "name": "testInfo"
         },
-        "doc": "Carries information about where an entity originated from.",
+        "doc": "Information about a DataHub Test",
         "fields": [
             {
-                "doc": "Where an entity originated from. Either NATIVE or EXTERNAL.",
-                "name": "type",
-                "type": {
-                    "doc": "Enum to define where an entity originated from.",
-                    "name": "OriginType",
-                    "namespace": "com.linkedin.pegasus2avro.common",
-                    "symbolDocs": {
-                        "EXTERNAL": "The entity is external to DataHub.",
-                        "NATIVE": "The entity is native to DataHub."
-                    },
-                    "symbols": [
-                        "NATIVE",
-                        "EXTERNAL"
-                    ],
-                    "type": "enum"
-                }
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "The name of the test",
+                "name": "name",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "KEYWORD"
+                },
+                "doc": "Category of the test",
+                "name": "category",
+                "type": "string"
             },
             {
+                "Searchable": {
+                    "fieldType": "TEXT"
+                },
                 "default": null,
-                "doc": "Only populated if type is EXTERNAL. The externalType of the entity, such as the name of the identity provider.",
-                "name": "externalType",
+                "doc": "Description of the test",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
-            }
-        ],
-        "name": "Origin",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "browsePathsV2"
-        },
-        "doc": "Shared aspect containing a Browse Path to be indexed for an entity.",
-        "fields": [
+            },
             {
-                "Searchable": {
-                    "/*/id": {
-                        "fieldName": "browsePathV2",
-                        "fieldType": "BROWSE_PATH_V2"
-                    }
-                },
-                "doc": "A valid browse path for the entity. This field is provided by DataHub by default.\nThis aspect is a newer version of browsePaths where we can encode more information in the path.\nThis path is also based on containers for a given entity if it has containers.\n\nThis is stored in elasticsearch as unit-separator delimited strings and only includes platform specific folders or containers.\nThese paths should not include high level info captured elsewhere ie. Platform and Environment.",
-                "name": "path",
+                "doc": "Configuration for the Test",
+                "name": "definition",
                 "type": {
-                    "items": {
-                        "doc": "Represents a single level in an entity's browsePathV2",
-                        "fields": [
-                            {
-                                "doc": "The ID of the browse path entry. This is what gets stored in the index.\nIf there's an urn associated with this entry, id and urn will be the same",
-                                "name": "id",
-                                "type": "string"
-                            },
-                            {
-                                "Urn": "Urn",
-                                "default": null,
-                                "doc": "Optional urn pointing to some entity in DataHub",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                    "fields": [
+                        {
+                            "doc": "The Test Definition Type",
+                            "name": "type",
+                            "type": {
+                                "name": "TestDefinitionType",
+                                "namespace": "com.linkedin.pegasus2avro.test",
+                                "symbolDocs": {
+                                    "JSON": "JSON / YAML test def"
                                 },
-                                "name": "urn",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
+                                "symbols": [
+                                    "JSON"
+                                ],
+                                "type": "enum"
                             }
-                        ],
-                        "name": "BrowsePathEntry",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "type": "record"
-                    },
-                    "type": "array"
+                        },
+                        {
+                            "default": null,
+                            "doc": "JSON format configuration for the test",
+                            "name": "json",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        }
+                    ],
+                    "name": "TestDefinition",
+                    "namespace": "com.linkedin.pegasus2avro.test",
+                    "type": "record"
                 }
             }
         ],
-        "name": "BrowsePathsV2",
-        "namespace": "com.linkedin.pegasus2avro.common",
+        "name": "TestInfo",
+        "namespace": "com.linkedin.pegasus2avro.test",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.common.GlobalTags",
-    "com.linkedin.pegasus2avro.common.Status",
     {
         "Aspect": {
-            "name": "operation",
-            "type": "timeseries"
+            "name": "dataPlatformInstanceProperties"
         },
-        "doc": "Operational info for an entity.",
+        "doc": "Properties associated with a Data Platform Instance",
         "fields": [
             {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
             },
             {
                 "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                    "string"
                 ]
             },
             {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
+                "default": null,
+                "doc": "Display name of the Data Platform Instance",
+                "name": "name",
                 "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
+                    "null",
+                    "string"
                 ]
             },
             {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
                 "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
+                "doc": "Documentation of the Data Platform Instance",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
+            }
+        ],
+        "name": "DataPlatformInstanceProperties",
+        "namespace": "com.linkedin.pegasus2avro.dataplatforminstance",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataHubViewInfo"
+        },
+        "doc": "Information about a DataHub View. -- TODO: Understand whether an entity type filter is required.",
+        "fields": [
+            {
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "The name of the View",
+                "name": "name",
+                "type": "string"
             },
             {
-                "TimeseriesField": {},
-                "Urn": "Urn",
                 "default": null,
-                "doc": "Actor who issued this operation.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "actor",
+                "doc": "Description of the view",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "TimeseriesField": {},
-                "doc": "Operation type of change.",
-                "name": "operationType",
+                "Searchable": {},
+                "doc": "The type of View",
+                "name": "type",
                 "type": {
-                    "doc": "Enum to define the operation type when an entity changes.",
-                    "name": "OperationType",
-                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "name": "DataHubViewType",
+                    "namespace": "com.linkedin.pegasus2avro.view",
                     "symbolDocs": {
-                        "ALTER": "Asset was altered",
-                        "CREATE": "Asset was created",
-                        "CUSTOM": "Custom asset operation",
-                        "DELETE": "Rows were deleted",
-                        "DROP": "Asset was dropped",
-                        "INSERT": "Rows were inserted",
-                        "UPDATE": "Rows were updated"
+                        "GLOBAL": "A global view, which all users can see and use.",
+                        "PERSONAL": "A view private for a specific person."
                     },
                     "symbols": [
-                        "INSERT",
-                        "UPDATE",
-                        "DELETE",
-                        "CREATE",
-                        "ALTER",
-                        "DROP",
-                        "CUSTOM",
-                        "UNKNOWN"
+                        "PERSONAL",
+                        "GLOBAL"
                     ],
                     "type": "enum"
                 }
             },
             {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "A custom type of operation. Required if operationType is CUSTOM.",
-                "name": "customOperationType",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "How many rows were affected by this operation.",
-                "name": "numAffectedRows",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            },
-            {
-                "TimeseriesFieldCollection": {
-                    "key": "datasetName"
-                },
-                "Urn": "Urn",
-                "default": null,
-                "doc": "Which other datasets were affected by this operation.",
-                "name": "affectedDatasets",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ],
-                "urn_is_array": true
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "Source Type",
-                "name": "sourceType",
-                "type": [
-                    "null",
-                    {
-                        "doc": "The source of an operation",
-                        "name": "OperationSourceType",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "symbolDocs": {
-                            "DATA_PLATFORM": "Rows were updated",
-                            "DATA_PROCESS": "Provided by a Data Process"
+                "doc": "The view itself",
+                "name": "definition",
+                "type": {
+                    "doc": "A View definition.",
+                    "fields": [
+                        {
+                            "doc": "The Entity Types in the scope of the View.",
+                            "name": "entityTypes",
+                            "type": {
+                                "items": "string",
+                                "type": "array"
+                            }
                         },
-                        "symbols": [
-                            "DATA_PROCESS",
-                            "DATA_PLATFORM"
-                        ],
-                        "type": "enum"
-                    }
-                ]
+                        {
+                            "doc": "The filter criteria, which represents the view itself",
+                            "name": "filter",
+                            "type": "com.linkedin.pegasus2avro.metadata.query.filter.Filter"
+                        }
+                    ],
+                    "name": "DataHubViewDefinition",
+                    "namespace": "com.linkedin.pegasus2avro.view",
+                    "type": "record"
+                }
             },
             {
-                "default": null,
-                "doc": "Custom properties",
-                "name": "customProperties",
-                "type": [
-                    "null",
-                    {
-                        "type": "map",
-                        "values": "string"
+                "Searchable": {
+                    "/actor": {
+                        "fieldName": "createdBy",
+                        "fieldType": "URN"
+                    },
+                    "/time": {
+                        "fieldName": "createdAt",
+                        "fieldType": "DATETIME"
                     }
-                ]
+                },
+                "doc": "Audit stamp capturing the time and actor who created the View.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             },
             {
                 "Searchable": {
-                    "fieldName": "lastOperationTime",
-                    "fieldType": "DATETIME"
+                    "/time": {
+                        "fieldName": "lastModifiedAt",
+                        "fieldType": "DATETIME"
+                    }
                 },
-                "TimeseriesField": {},
-                "doc": "The time at which the operation occurred. Would be better named 'operationTime'",
-                "name": "lastUpdatedTimestamp",
-                "type": "long"
+                "doc": "Audit stamp capturing the time and actor who last modified the View.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             }
         ],
-        "name": "Operation",
-        "namespace": "com.linkedin.pegasus2avro.common",
+        "name": "DataHubViewInfo",
+        "namespace": "com.linkedin.pegasus2avro.view",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "siblings"
+            "name": "dataHubSecretValue"
         },
-        "doc": "Siblings information of an entity.",
+        "doc": "The value of a DataHub Secret",
         "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "name": "SiblingOf"
-                    }
-                },
                 "Searchable": {
-                    "/*": {
-                        "fieldName": "siblings",
-                        "fieldType": "URN",
-                        "queryByDefault": false
-                    }
-                },
-                "Urn": "Urn",
-                "doc": "List of sibling entities",
-                "name": "siblings",
-                "type": {
-                    "items": "string",
-                    "type": "array"
+                    "fieldType": "TEXT_PARTIAL"
                 },
-                "urn_is_array": true
+                "doc": "The display name for the secret",
+                "name": "name",
+                "type": "string"
             },
             {
-                "doc": "If this is the leader entity of the set of siblings",
-                "name": "primary",
-                "type": "boolean"
-            }
-        ],
-        "name": "Siblings",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    "com.linkedin.pegasus2avro.common.BrowsePaths",
-    "com.linkedin.pegasus2avro.common.Deprecation",
-    "com.linkedin.pegasus2avro.common.Cost",
-    {
-        "Aspect": {
-            "name": "dataHubUpgradeResult"
-        },
-        "doc": "Information collected when a DataHubUpgrade successfully finishes",
-        "fields": [
-            {
-                "doc": "Timestamp when we started this DataHubUpgrade",
-                "name": "timestampMs",
-                "type": "long"
+                "doc": "The AES-encrypted value of the DataHub secret.",
+                "name": "value",
+                "type": "string"
             },
             {
                 "default": null,
-                "doc": "Result map to place helpful information about this upgrade job",
-                "name": "result",
+                "doc": "Description of the secret",
+                "name": "description",
                 "type": [
                     "null",
-                    {
-                        "type": "map",
-                        "values": "string"
-                    }
+                    "string"
                 ]
-            }
-        ],
-        "name": "DataHubUpgradeResult",
-        "namespace": "com.linkedin.pegasus2avro.upgrade",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataHubUpgradeRequest"
-        },
-        "doc": "Information collected when kicking off a DataHubUpgrade",
-        "fields": [
-            {
-                "doc": "Timestamp when we started this DataHubUpgrade",
-                "name": "timestampMs",
-                "type": "long"
             },
             {
-                "doc": "Version of this upgrade",
-                "name": "version",
-                "type": "string"
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "createdTime",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "default": null,
+                "doc": "Created Audit stamp",
+                "name": "created",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                ]
             }
         ],
-        "name": "DataHubUpgradeRequest",
-        "namespace": "com.linkedin.pegasus2avro.upgrade",
+        "name": "DataHubSecretValue",
+        "namespace": "com.linkedin.pegasus2avro.secret",
         "type": "record"
     }
 ]
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schema_classes.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schema_classes.py`

 * *Files 6% similar despite different names*

```diff
@@ -25,32 +25,32 @@
 import decimal
 import datetime
 import six
 from avrogen.dict_wrapper import DictWrapper
 from avrogen import avrojson
 from avro.schema import RecordSchema, make_avsc_object
 from avro import schema as avro_schema
-from typing import ClassVar, List, Dict, Union, Optional, Type
+from typing import List, Dict, Union, Optional
 
 
 def __read_file(file_name):
     with open(file_name, "r") as f:
         return f.read()
         
 
 def __get_names_and_schema(json_str):
     names = avro_schema.Names()
     schema = make_avsc_object(json.loads(json_str), names)
     return names, schema
 
 
-_SCHEMA_JSON_STR = __read_file(os.path.join(os.path.dirname(__file__), "schema.avsc"))
+SCHEMA_JSON_STR = __read_file(os.path.join(os.path.dirname(__file__), "schema.avsc"))
 
 
-__NAMES, _SCHEMA = __get_names_and_schema(_SCHEMA_JSON_STR)
+__NAMES, SCHEMA = __get_names_and_schema(SCHEMA_JSON_STR)
 __SCHEMAS: Dict[str, RecordSchema] = {}
 
 class _Aspect(DictWrapper):
     ASPECT_NAME: ClassVar[str] = None  # type: ignore
     ASPECT_TYPE: ClassVar[str] = "default"
     ASPECT_INFO: ClassVar[dict] = None  # type: ignore
 
@@ -69,16 +69,16 @@
 
     @classmethod
     def get_aspect_info(cls) -> dict:
         return cls.ASPECT_INFO
 
 
 
-def get_schema_type(fullname: str) -> RecordSchema:
-    return __SCHEMAS[fullname]
+def get_schema_type(fullname):
+    return __SCHEMAS.get(fullname)
     
     
 __SCHEMAS = dict((n.fullname.lstrip("."), n) for n in six.itervalues(__NAMES.names))
 
 class KafkaAuditHeaderClass(DictWrapper):
     """This header records information about the context of an event as it is emitted into kafka and is intended to be used by the kafka audit application.  For more information see go/kafkaauditheader"""
     
@@ -100,102 +100,117 @@
         self.instance = instance
         self.appName = appName
         self.messageId = messageId
         self.auditVersion = auditVersion
         self.fabricUrn = fabricUrn
         self.clusterConnectionString = clusterConnectionString
     
+    @classmethod
+    def construct_with_defaults(cls) -> "KafkaAuditHeaderClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.time = int()
         self.server = str()
         self.instance = self.RECORD_SCHEMA.fields_dict["instance"].default
         self.appName = str()
         self.messageId = bytes()
         self.auditVersion = self.RECORD_SCHEMA.fields_dict["auditVersion"].default
         self.fabricUrn = self.RECORD_SCHEMA.fields_dict["fabricUrn"].default
         self.clusterConnectionString = self.RECORD_SCHEMA.fields_dict["clusterConnectionString"].default
     
     
     @property
     def time(self) -> int:
-        """The time at which the event was emitted into kafka."""
+        """Getter: The time at which the event was emitted into kafka."""
         return self._inner_dict.get('time')  # type: ignore
     
     @time.setter
     def time(self, value: int) -> None:
+        """Setter: The time at which the event was emitted into kafka."""
         self._inner_dict['time'] = value
     
     
     @property
     def server(self) -> str:
-        """The fully qualified name of the host from which the event is being emitted."""
+        """Getter: The fully qualified name of the host from which the event is being emitted."""
         return self._inner_dict.get('server')  # type: ignore
     
     @server.setter
     def server(self, value: str) -> None:
+        """Setter: The fully qualified name of the host from which the event is being emitted."""
         self._inner_dict['server'] = value
     
     
     @property
     def instance(self) -> Union[None, str]:
-        """The instance on the server from which the event is being emitted. e.g. i001"""
+        """Getter: The instance on the server from which the event is being emitted. e.g. i001"""
         return self._inner_dict.get('instance')  # type: ignore
     
     @instance.setter
     def instance(self, value: Union[None, str]) -> None:
+        """Setter: The instance on the server from which the event is being emitted. e.g. i001"""
         self._inner_dict['instance'] = value
     
     
     @property
     def appName(self) -> str:
-        """The name of the application from which the event is being emitted. see go/appname"""
+        """Getter: The name of the application from which the event is being emitted. see go/appname"""
         return self._inner_dict.get('appName')  # type: ignore
     
     @appName.setter
     def appName(self, value: str) -> None:
+        """Setter: The name of the application from which the event is being emitted. see go/appname"""
         self._inner_dict['appName'] = value
     
     
     @property
     def messageId(self) -> bytes:
-        """A unique identifier for the message"""
+        """Getter: A unique identifier for the message"""
         return self._inner_dict.get('messageId')  # type: ignore
     
     @messageId.setter
     def messageId(self, value: bytes) -> None:
+        """Setter: A unique identifier for the message"""
         self._inner_dict['messageId'] = value
     
     
     @property
     def auditVersion(self) -> Union[None, int]:
-        """The version that is being used for auditing. In version 0, the audit trail buckets events into 10 minute audit windows based on the EventHeader timestamp. In version 1, the audit trail buckets events as follows: if the schema has an outer KafkaAuditHeader, use the outer audit header timestamp for bucketing; else if the EventHeader has an inner KafkaAuditHeader use that inner audit header's timestamp for bucketing"""
+        """Getter: The version that is being used for auditing. In version 0, the audit trail buckets events into 10 minute audit windows based on the EventHeader timestamp. In version 1, the audit trail buckets events as follows: if the schema has an outer KafkaAuditHeader, use the outer audit header timestamp for bucketing; else if the EventHeader has an inner KafkaAuditHeader use that inner audit header's timestamp for bucketing"""
         return self._inner_dict.get('auditVersion')  # type: ignore
     
     @auditVersion.setter
     def auditVersion(self, value: Union[None, int]) -> None:
+        """Setter: The version that is being used for auditing. In version 0, the audit trail buckets events into 10 minute audit windows based on the EventHeader timestamp. In version 1, the audit trail buckets events as follows: if the schema has an outer KafkaAuditHeader, use the outer audit header timestamp for bucketing; else if the EventHeader has an inner KafkaAuditHeader use that inner audit header's timestamp for bucketing"""
         self._inner_dict['auditVersion'] = value
     
     
     @property
     def fabricUrn(self) -> Union[None, str]:
-        """The fabricUrn of the host from which the event is being emitted. Fabric Urn in the format of urn:li:fabric:{fabric_name}. See go/fabric."""
+        """Getter: The fabricUrn of the host from which the event is being emitted. Fabric Urn in the format of urn:li:fabric:{fabric_name}. See go/fabric."""
         return self._inner_dict.get('fabricUrn')  # type: ignore
     
     @fabricUrn.setter
     def fabricUrn(self, value: Union[None, str]) -> None:
+        """Setter: The fabricUrn of the host from which the event is being emitted. Fabric Urn in the format of urn:li:fabric:{fabric_name}. See go/fabric."""
         self._inner_dict['fabricUrn'] = value
     
     
     @property
     def clusterConnectionString(self) -> Union[None, str]:
-        """This is a String that the client uses to establish some kind of connection with the Kafka cluster. The exact format of it depends on specific versions of clients and brokers. This information could potentially identify the fabric and cluster with which the client is producing to or consuming from."""
+        """Getter: This is a String that the client uses to establish some kind of connection with the Kafka cluster. The exact format of it depends on specific versions of clients and brokers. This information could potentially identify the fabric and cluster with which the client is producing to or consuming from."""
         return self._inner_dict.get('clusterConnectionString')  # type: ignore
     
     @clusterConnectionString.setter
     def clusterConnectionString(self, value: Union[None, str]) -> None:
+        """Setter: This is a String that the client uses to establish some kind of connection with the Kafka cluster. The exact format of it depends on specific versions of clients and brokers. This information could potentially identify the fabric and cluster with which the client is producing to or consuming from."""
         self._inner_dict['clusterConnectionString'] = value
     
     
 class DataHubAccessTokenInfoClass(_Aspect):
     """Information about a DataHub Access Token"""
 
 
@@ -216,80 +231,93 @@
         self.name = name
         self.actorUrn = actorUrn
         self.ownerUrn = ownerUrn
         self.createdAt = createdAt
         self.expiresAt = expiresAt
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubAccessTokenInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.actorUrn = str()
         self.ownerUrn = str()
         self.createdAt = int()
         self.expiresAt = self.RECORD_SCHEMA.fields_dict["expiresAt"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def name(self) -> str:
-        """User defined name for the access token if defined."""
+        """Getter: User defined name for the access token if defined."""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: User defined name for the access token if defined."""
         self._inner_dict['name'] = value
     
     
     @property
     def actorUrn(self) -> str:
-        """Urn of the actor to which this access token belongs to."""
+        """Getter: Urn of the actor to which this access token belongs to."""
         return self._inner_dict.get('actorUrn')  # type: ignore
     
     @actorUrn.setter
     def actorUrn(self, value: str) -> None:
+        """Setter: Urn of the actor to which this access token belongs to."""
         self._inner_dict['actorUrn'] = value
     
     
     @property
     def ownerUrn(self) -> str:
-        """Urn of the actor which created this access token."""
+        """Getter: Urn of the actor which created this access token."""
         return self._inner_dict.get('ownerUrn')  # type: ignore
     
     @ownerUrn.setter
     def ownerUrn(self, value: str) -> None:
+        """Setter: Urn of the actor which created this access token."""
         self._inner_dict['ownerUrn'] = value
     
     
     @property
     def createdAt(self) -> int:
-        """When the token was created."""
+        """Getter: When the token was created."""
         return self._inner_dict.get('createdAt')  # type: ignore
     
     @createdAt.setter
     def createdAt(self, value: int) -> None:
+        """Setter: When the token was created."""
         self._inner_dict['createdAt'] = value
     
     
     @property
     def expiresAt(self) -> Union[None, int]:
-        """When the token expires."""
+        """Getter: When the token expires."""
         return self._inner_dict.get('expiresAt')  # type: ignore
     
     @expiresAt.setter
     def expiresAt(self, value: Union[None, int]) -> None:
+        """Setter: When the token expires."""
         self._inner_dict['expiresAt'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Description of the token if defined."""
+        """Getter: Description of the token if defined."""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Description of the token if defined."""
         self._inner_dict['description'] = value
     
     
 class AssertionInfoClass(_Aspect):
     """Information about an assertion"""
 
 
@@ -310,58 +338,69 @@
             self.customProperties = dict()
         else:
             self.customProperties = customProperties
         self.externalUrl = externalUrl
         self.type = type
         self.datasetAssertion = datasetAssertion
     
+    @classmethod
+    def construct_with_defaults(cls) -> "AssertionInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.type = AssertionTypeClass.DATASET
         self.datasetAssertion = self.RECORD_SCHEMA.fields_dict["datasetAssertion"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def type(self) -> Union[str, "AssertionTypeClass"]:
-        """Type of assertion. Assertion types can evolve to span Datasets, Flows (Pipelines), Models, Features etc."""
+        """Getter: Type of assertion. Assertion types can evolve to span Datasets, Flows (Pipelines), Models, Features etc."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "AssertionTypeClass"]) -> None:
+        """Setter: Type of assertion. Assertion types can evolve to span Datasets, Flows (Pipelines), Models, Features etc."""
         self._inner_dict['type'] = value
     
     
     @property
     def datasetAssertion(self) -> Union[None, "DatasetAssertionInfoClass"]:
-        """Dataset Assertion information when type is DATASET"""
+        """Getter: Dataset Assertion information when type is DATASET"""
         return self._inner_dict.get('datasetAssertion')  # type: ignore
     
     @datasetAssertion.setter
     def datasetAssertion(self, value: Union[None, "DatasetAssertionInfoClass"]) -> None:
+        """Setter: Dataset Assertion information when type is DATASET"""
         self._inner_dict['datasetAssertion'] = value
     
     
 class AssertionResultClass(DictWrapper):
     """The result of running an assertion"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.assertion.AssertionResult")
@@ -380,91 +419,105 @@
         self.rowCount = rowCount
         self.missingCount = missingCount
         self.unexpectedCount = unexpectedCount
         self.actualAggValue = actualAggValue
         self.nativeResults = nativeResults
         self.externalUrl = externalUrl
     
+    @classmethod
+    def construct_with_defaults(cls) -> "AssertionResultClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = AssertionResultTypeClass.SUCCESS
         self.rowCount = self.RECORD_SCHEMA.fields_dict["rowCount"].default
         self.missingCount = self.RECORD_SCHEMA.fields_dict["missingCount"].default
         self.unexpectedCount = self.RECORD_SCHEMA.fields_dict["unexpectedCount"].default
         self.actualAggValue = self.RECORD_SCHEMA.fields_dict["actualAggValue"].default
         self.nativeResults = self.RECORD_SCHEMA.fields_dict["nativeResults"].default
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
     
     
     @property
     def type(self) -> Union[str, "AssertionResultTypeClass"]:
-        """ The final result, e.g. either SUCCESS or FAILURE."""
+        """Getter:  The final result, e.g. either SUCCESS or FAILURE."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "AssertionResultTypeClass"]) -> None:
+        """Setter:  The final result, e.g. either SUCCESS or FAILURE."""
         self._inner_dict['type'] = value
     
     
     @property
     def rowCount(self) -> Union[None, int]:
-        """Number of rows for evaluated batch"""
+        """Getter: Number of rows for evaluated batch"""
         return self._inner_dict.get('rowCount')  # type: ignore
     
     @rowCount.setter
     def rowCount(self, value: Union[None, int]) -> None:
+        """Setter: Number of rows for evaluated batch"""
         self._inner_dict['rowCount'] = value
     
     
     @property
     def missingCount(self) -> Union[None, int]:
-        """Number of rows with missing value for evaluated batch"""
+        """Getter: Number of rows with missing value for evaluated batch"""
         return self._inner_dict.get('missingCount')  # type: ignore
     
     @missingCount.setter
     def missingCount(self, value: Union[None, int]) -> None:
+        """Setter: Number of rows with missing value for evaluated batch"""
         self._inner_dict['missingCount'] = value
     
     
     @property
     def unexpectedCount(self) -> Union[None, int]:
-        """Number of rows with unexpected value for evaluated batch"""
+        """Getter: Number of rows with unexpected value for evaluated batch"""
         return self._inner_dict.get('unexpectedCount')  # type: ignore
     
     @unexpectedCount.setter
     def unexpectedCount(self, value: Union[None, int]) -> None:
+        """Setter: Number of rows with unexpected value for evaluated batch"""
         self._inner_dict['unexpectedCount'] = value
     
     
     @property
     def actualAggValue(self) -> Union[None, float]:
-        """Observed aggregate value for evaluated batch"""
+        """Getter: Observed aggregate value for evaluated batch"""
         return self._inner_dict.get('actualAggValue')  # type: ignore
     
     @actualAggValue.setter
     def actualAggValue(self, value: Union[None, float]) -> None:
+        """Setter: Observed aggregate value for evaluated batch"""
         self._inner_dict['actualAggValue'] = value
     
     
     @property
     def nativeResults(self) -> Union[None, Dict[str, str]]:
-        """Other results of evaluation"""
+        """Getter: Other results of evaluation"""
         return self._inner_dict.get('nativeResults')  # type: ignore
     
     @nativeResults.setter
     def nativeResults(self, value: Union[None, Dict[str, str]]) -> None:
+        """Setter: Other results of evaluation"""
         self._inner_dict['nativeResults'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where full results are available"""
+        """Getter: URL where full results are available"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where full results are available"""
         self._inner_dict['externalUrl'] = value
     
     
 class AssertionResultTypeClass(object):
     # No docs available.
     
     
@@ -512,14 +565,21 @@
         self.assertionUrn = assertionUrn
         self.asserteeUrn = asserteeUrn
         self.batchSpec = batchSpec
         self.status = status
         self.result = result
         self.runtimeContext = runtimeContext
     
+    @classmethod
+    def construct_with_defaults(cls) -> "AssertionRunEventClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMillis = int()
         self.eventGranularity = self.RECORD_SCHEMA.fields_dict["eventGranularity"].default
         self.partitionSpec = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["partitionSpec"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["partitionSpec"].type)
         self.messageId = self.RECORD_SCHEMA.fields_dict["messageId"].default
         self.runId = str()
         self.assertionUrn = str()
@@ -528,119 +588,130 @@
         self.status = AssertionRunStatusClass.COMPLETE
         self.result = self.RECORD_SCHEMA.fields_dict["result"].default
         self.runtimeContext = self.RECORD_SCHEMA.fields_dict["runtimeContext"].default
     
     
     @property
     def timestampMillis(self) -> int:
-        """The event timestamp field as epoch at UTC in milli seconds."""
+        """Getter: The event timestamp field as epoch at UTC in milli seconds."""
         return self._inner_dict.get('timestampMillis')  # type: ignore
     
     @timestampMillis.setter
     def timestampMillis(self, value: int) -> None:
+        """Setter: The event timestamp field as epoch at UTC in milli seconds."""
         self._inner_dict['timestampMillis'] = value
     
     
     @property
     def eventGranularity(self) -> Union[None, "TimeWindowSizeClass"]:
-        """Granularity of the event if applicable"""
+        """Getter: Granularity of the event if applicable"""
         return self._inner_dict.get('eventGranularity')  # type: ignore
     
     @eventGranularity.setter
     def eventGranularity(self, value: Union[None, "TimeWindowSizeClass"]) -> None:
+        """Setter: Granularity of the event if applicable"""
         self._inner_dict['eventGranularity'] = value
     
     
     @property
     def partitionSpec(self) -> Union["PartitionSpecClass", None]:
-        """The optional partition specification."""
+        """Getter: The optional partition specification."""
         return self._inner_dict.get('partitionSpec')  # type: ignore
     
     @partitionSpec.setter
     def partitionSpec(self, value: Union["PartitionSpecClass", None]) -> None:
+        """Setter: The optional partition specification."""
         self._inner_dict['partitionSpec'] = value
     
     
     @property
     def messageId(self) -> Union[None, str]:
-        """The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
+        """Getter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         return self._inner_dict.get('messageId')  # type: ignore
     
     @messageId.setter
     def messageId(self, value: Union[None, str]) -> None:
+        """Setter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         self._inner_dict['messageId'] = value
     
     
     @property
     def runId(self) -> str:
-        """ Native (platform-specific) identifier for this run"""
+        """Getter:  Native (platform-specific) identifier for this run"""
         return self._inner_dict.get('runId')  # type: ignore
     
     @runId.setter
     def runId(self, value: str) -> None:
+        """Setter:  Native (platform-specific) identifier for this run"""
         self._inner_dict['runId'] = value
     
     
     @property
     def assertionUrn(self) -> str:
         # No docs available.
         return self._inner_dict.get('assertionUrn')  # type: ignore
     
     @assertionUrn.setter
     def assertionUrn(self, value: str) -> None:
+        # No docs available.
         self._inner_dict['assertionUrn'] = value
     
     
     @property
     def asserteeUrn(self) -> str:
         # No docs available.
         return self._inner_dict.get('asserteeUrn')  # type: ignore
     
     @asserteeUrn.setter
     def asserteeUrn(self, value: str) -> None:
+        # No docs available.
         self._inner_dict['asserteeUrn'] = value
     
     
     @property
     def batchSpec(self) -> Union[None, "BatchSpecClass"]:
-        """Specification of the batch which this run is evaluating"""
+        """Getter: Specification of the batch which this run is evaluating"""
         return self._inner_dict.get('batchSpec')  # type: ignore
     
     @batchSpec.setter
     def batchSpec(self, value: Union[None, "BatchSpecClass"]) -> None:
+        """Setter: Specification of the batch which this run is evaluating"""
         self._inner_dict['batchSpec'] = value
     
     
     @property
     def status(self) -> Union[str, "AssertionRunStatusClass"]:
-        """The status of the assertion run as per this timeseries event."""
+        """Getter: The status of the assertion run as per this timeseries event."""
         return self._inner_dict.get('status')  # type: ignore
     
     @status.setter
     def status(self, value: Union[str, "AssertionRunStatusClass"]) -> None:
+        """Setter: The status of the assertion run as per this timeseries event."""
         self._inner_dict['status'] = value
     
     
     @property
     def result(self) -> Union[None, "AssertionResultClass"]:
-        """Results of assertion, present if the status is COMPLETE"""
+        """Getter: Results of assertion, present if the status is COMPLETE"""
         return self._inner_dict.get('result')  # type: ignore
     
     @result.setter
     def result(self, value: Union[None, "AssertionResultClass"]) -> None:
+        """Setter: Results of assertion, present if the status is COMPLETE"""
         self._inner_dict['result'] = value
     
     
     @property
     def runtimeContext(self) -> Union[None, Dict[str, str]]:
-        """Runtime parameters of evaluation"""
+        """Getter: Runtime parameters of evaluation"""
         return self._inner_dict.get('runtimeContext')  # type: ignore
     
     @runtimeContext.setter
     def runtimeContext(self, value: Union[None, Dict[str, str]]) -> None:
+        """Setter: Runtime parameters of evaluation"""
         self._inner_dict['runtimeContext'] = value
     
     
 class AssertionRunStatusClass(object):
     # No docs available.
     
     
@@ -754,36 +825,45 @@
         type: Union[str, "AssertionStdParameterTypeClass"],
     ):
         super().__init__()
         
         self.value = value
         self.type = type
     
+    @classmethod
+    def construct_with_defaults(cls) -> "AssertionStdParameterClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.value = str()
         self.type = AssertionStdParameterTypeClass.STRING
     
     
     @property
     def value(self) -> str:
-        """The parameter value"""
+        """Getter: The parameter value"""
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: str) -> None:
+        """Setter: The parameter value"""
         self._inner_dict['value'] = value
     
     
     @property
     def type(self) -> Union[str, "AssertionStdParameterTypeClass"]:
-        """The type of the parameter"""
+        """Getter: The type of the parameter"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "AssertionStdParameterTypeClass"]) -> None:
+        """Setter: The type of the parameter"""
         self._inner_dict['type'] = value
     
     
 class AssertionStdParameterTypeClass(object):
     # No docs available.
     
     STRING = "STRING"
@@ -804,47 +884,57 @@
     ):
         super().__init__()
         
         self.value = value
         self.maxValue = maxValue
         self.minValue = minValue
     
+    @classmethod
+    def construct_with_defaults(cls) -> "AssertionStdParametersClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.value = self.RECORD_SCHEMA.fields_dict["value"].default
         self.maxValue = self.RECORD_SCHEMA.fields_dict["maxValue"].default
         self.minValue = self.RECORD_SCHEMA.fields_dict["minValue"].default
     
     
     @property
     def value(self) -> Union[None, "AssertionStdParameterClass"]:
-        """The value parameter of an assertion"""
+        """Getter: The value parameter of an assertion"""
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: Union[None, "AssertionStdParameterClass"]) -> None:
+        """Setter: The value parameter of an assertion"""
         self._inner_dict['value'] = value
     
     
     @property
     def maxValue(self) -> Union[None, "AssertionStdParameterClass"]:
-        """The maxValue parameter of an assertion"""
+        """Getter: The maxValue parameter of an assertion"""
         return self._inner_dict.get('maxValue')  # type: ignore
     
     @maxValue.setter
     def maxValue(self, value: Union[None, "AssertionStdParameterClass"]) -> None:
+        """Setter: The maxValue parameter of an assertion"""
         self._inner_dict['maxValue'] = value
     
     
     @property
     def minValue(self) -> Union[None, "AssertionStdParameterClass"]:
-        """The minValue parameter of an assertion"""
+        """Getter: The minValue parameter of an assertion"""
         return self._inner_dict.get('minValue')  # type: ignore
     
     @minValue.setter
     def minValue(self, value: Union[None, "AssertionStdParameterClass"]) -> None:
+        """Setter: The minValue parameter of an assertion"""
         self._inner_dict['minValue'] = value
     
     
 class AssertionTypeClass(object):
     # No docs available.
     
     DATASET = "DATASET"
@@ -867,58 +957,69 @@
             self.customProperties = dict()
         else:
             self.customProperties = customProperties
         self.nativeBatchId = nativeBatchId
         self.query = query
         self.limit = limit
     
+    @classmethod
+    def construct_with_defaults(cls) -> "BatchSpecClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.nativeBatchId = self.RECORD_SCHEMA.fields_dict["nativeBatchId"].default
         self.query = self.RECORD_SCHEMA.fields_dict["query"].default
         self.limit = self.RECORD_SCHEMA.fields_dict["limit"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def nativeBatchId(self) -> Union[None, str]:
-        """The native identifier as specified by the system operating on the batch."""
+        """Getter: The native identifier as specified by the system operating on the batch."""
         return self._inner_dict.get('nativeBatchId')  # type: ignore
     
     @nativeBatchId.setter
     def nativeBatchId(self, value: Union[None, str]) -> None:
+        """Setter: The native identifier as specified by the system operating on the batch."""
         self._inner_dict['nativeBatchId'] = value
     
     
     @property
     def query(self) -> Union[None, str]:
-        """A query that identifies a batch of data"""
+        """Getter: A query that identifies a batch of data"""
         return self._inner_dict.get('query')  # type: ignore
     
     @query.setter
     def query(self, value: Union[None, str]) -> None:
+        """Setter: A query that identifies a batch of data"""
         self._inner_dict['query'] = value
     
     
     @property
     def limit(self) -> Union[None, int]:
-        """Any limit to the number of rows in the batch, if applied"""
+        """Getter: Any limit to the number of rows in the batch, if applied"""
         return self._inner_dict.get('limit')  # type: ignore
     
     @limit.setter
     def limit(self, value: Union[None, int]) -> None:
+        """Setter: Any limit to the number of rows in the batch, if applied"""
         self._inner_dict['limit'] = value
     
     
 class DatasetAssertionInfoClass(DictWrapper):
     """Attributes that are applicable to single-Dataset Assertions"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.assertion.DatasetAssertionInfo")
@@ -941,113 +1042,129 @@
         self.aggregation = aggregation
         self.operator = operator
         self.parameters = parameters
         self.nativeType = nativeType
         self.nativeParameters = nativeParameters
         self.logic = logic
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetAssertionInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.dataset = str()
         self.scope = DatasetAssertionScopeClass.DATASET_COLUMN
         self.fields = self.RECORD_SCHEMA.fields_dict["fields"].default
         self.aggregation = self.RECORD_SCHEMA.fields_dict["aggregation"].default
         self.operator = AssertionStdOperatorClass.BETWEEN
         self.parameters = self.RECORD_SCHEMA.fields_dict["parameters"].default
         self.nativeType = self.RECORD_SCHEMA.fields_dict["nativeType"].default
         self.nativeParameters = self.RECORD_SCHEMA.fields_dict["nativeParameters"].default
         self.logic = self.RECORD_SCHEMA.fields_dict["logic"].default
     
     
     @property
     def dataset(self) -> str:
-        """The dataset targeted by this assertion."""
+        """Getter: The dataset targeted by this assertion."""
         return self._inner_dict.get('dataset')  # type: ignore
     
     @dataset.setter
     def dataset(self, value: str) -> None:
+        """Setter: The dataset targeted by this assertion."""
         self._inner_dict['dataset'] = value
     
     
     @property
     def scope(self) -> Union[str, "DatasetAssertionScopeClass"]:
-        """Scope of the Assertion. What part of the dataset does this assertion apply to?"""
+        """Getter: Scope of the Assertion. What part of the dataset does this assertion apply to?"""
         return self._inner_dict.get('scope')  # type: ignore
     
     @scope.setter
     def scope(self, value: Union[str, "DatasetAssertionScopeClass"]) -> None:
+        """Setter: Scope of the Assertion. What part of the dataset does this assertion apply to?"""
         self._inner_dict['scope'] = value
     
     
     @property
     def fields(self) -> Union[None, List[str]]:
-        """One or more dataset schema fields that are targeted by this assertion"""
+        """Getter: One or more dataset schema fields that are targeted by this assertion"""
         return self._inner_dict.get('fields')  # type: ignore
     
     @fields.setter
     def fields(self, value: Union[None, List[str]]) -> None:
+        """Setter: One or more dataset schema fields that are targeted by this assertion"""
         self._inner_dict['fields'] = value
     
     
     @property
     def aggregation(self) -> Union[None, Union[str, "AssertionStdAggregationClass"]]:
-        """Standardized assertion operator"""
+        """Getter: Standardized assertion operator"""
         return self._inner_dict.get('aggregation')  # type: ignore
     
     @aggregation.setter
     def aggregation(self, value: Union[None, Union[str, "AssertionStdAggregationClass"]]) -> None:
+        """Setter: Standardized assertion operator"""
         self._inner_dict['aggregation'] = value
     
     
     @property
     def operator(self) -> Union[str, "AssertionStdOperatorClass"]:
-        """Standardized assertion operator"""
+        """Getter: Standardized assertion operator"""
         return self._inner_dict.get('operator')  # type: ignore
     
     @operator.setter
     def operator(self, value: Union[str, "AssertionStdOperatorClass"]) -> None:
+        """Setter: Standardized assertion operator"""
         self._inner_dict['operator'] = value
     
     
     @property
     def parameters(self) -> Union[None, "AssertionStdParametersClass"]:
-        """Standard parameters required for the assertion. e.g. min_value, max_value, value, columns"""
+        """Getter: Standard parameters required for the assertion. e.g. min_value, max_value, value, columns"""
         return self._inner_dict.get('parameters')  # type: ignore
     
     @parameters.setter
     def parameters(self, value: Union[None, "AssertionStdParametersClass"]) -> None:
+        """Setter: Standard parameters required for the assertion. e.g. min_value, max_value, value, columns"""
         self._inner_dict['parameters'] = value
     
     
     @property
     def nativeType(self) -> Union[None, str]:
-        """Native assertion type"""
+        """Getter: Native assertion type"""
         return self._inner_dict.get('nativeType')  # type: ignore
     
     @nativeType.setter
     def nativeType(self, value: Union[None, str]) -> None:
+        """Setter: Native assertion type"""
         self._inner_dict['nativeType'] = value
     
     
     @property
     def nativeParameters(self) -> Union[None, Dict[str, str]]:
-        """Native parameters required for the assertion."""
+        """Getter: Native parameters required for the assertion."""
         return self._inner_dict.get('nativeParameters')  # type: ignore
     
     @nativeParameters.setter
     def nativeParameters(self, value: Union[None, Dict[str, str]]) -> None:
+        """Setter: Native parameters required for the assertion."""
         self._inner_dict['nativeParameters'] = value
     
     
     @property
     def logic(self) -> Union[None, str]:
         # No docs available.
         return self._inner_dict.get('logic')  # type: ignore
     
     @logic.setter
     def logic(self, value: Union[None, str]) -> None:
+        # No docs available.
         self._inner_dict['logic'] = value
     
     
 class DatasetAssertionScopeClass(object):
     # No docs available.
     
     
@@ -1099,136 +1216,155 @@
         self.chartUrl = chartUrl
         self.inputs = inputs
         self.inputEdges = inputEdges
         self.type = type
         self.access = access
         self.lastRefreshed = lastRefreshed
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ChartInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.title = str()
         self.description = str()
-        self.lastModified = ChangeAuditStampsClass._construct_with_defaults()
+        self.lastModified = ChangeAuditStampsClass.construct_with_defaults()
         self.chartUrl = self.RECORD_SCHEMA.fields_dict["chartUrl"].default
         self.inputs = self.RECORD_SCHEMA.fields_dict["inputs"].default
         self.inputEdges = self.RECORD_SCHEMA.fields_dict["inputEdges"].default
         self.type = self.RECORD_SCHEMA.fields_dict["type"].default
         self.access = self.RECORD_SCHEMA.fields_dict["access"].default
         self.lastRefreshed = self.RECORD_SCHEMA.fields_dict["lastRefreshed"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def title(self) -> str:
-        """Title of the chart"""
+        """Getter: Title of the chart"""
         return self._inner_dict.get('title')  # type: ignore
     
     @title.setter
     def title(self, value: str) -> None:
+        """Setter: Title of the chart"""
         self._inner_dict['title'] = value
     
     
     @property
     def description(self) -> str:
-        """Detailed description about the chart"""
+        """Getter: Detailed description about the chart"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: str) -> None:
+        """Setter: Detailed description about the chart"""
         self._inner_dict['description'] = value
     
     
     @property
     def lastModified(self) -> "ChangeAuditStampsClass":
-        """Captures information about who created/last modified/deleted this chart and when"""
+        """Getter: Captures information about who created/last modified/deleted this chart and when"""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "ChangeAuditStampsClass") -> None:
+        """Setter: Captures information about who created/last modified/deleted this chart and when"""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def chartUrl(self) -> Union[None, str]:
-        """URL for the chart. This could be used as an external link on DataHub to allow users access/view the chart"""
+        """Getter: URL for the chart. This could be used as an external link on DataHub to allow users access/view the chart"""
         return self._inner_dict.get('chartUrl')  # type: ignore
     
     @chartUrl.setter
     def chartUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL for the chart. This could be used as an external link on DataHub to allow users access/view the chart"""
         self._inner_dict['chartUrl'] = value
     
     
     @property
     def inputs(self) -> Union[None, List[str]]:
-        """Data sources for the chart
+        """Getter: Data sources for the chart
     Deprecated! Use inputEdges instead."""
         return self._inner_dict.get('inputs')  # type: ignore
     
     @inputs.setter
     def inputs(self, value: Union[None, List[str]]) -> None:
+        """Setter: Data sources for the chart
+    Deprecated! Use inputEdges instead."""
         self._inner_dict['inputs'] = value
     
     
     @property
     def inputEdges(self) -> Union[None, List["EdgeClass"]]:
-        """Data sources for the chart"""
+        """Getter: Data sources for the chart"""
         return self._inner_dict.get('inputEdges')  # type: ignore
     
     @inputEdges.setter
     def inputEdges(self, value: Union[None, List["EdgeClass"]]) -> None:
+        """Setter: Data sources for the chart"""
         self._inner_dict['inputEdges'] = value
     
     
     @property
     def type(self) -> Union[None, Union[str, "ChartTypeClass"]]:
-        """Type of the chart"""
+        """Getter: Type of the chart"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[None, Union[str, "ChartTypeClass"]]) -> None:
+        """Setter: Type of the chart"""
         self._inner_dict['type'] = value
     
     
     @property
     def access(self) -> Union[None, Union[str, "AccessLevelClass"]]:
-        """Access level for the chart"""
+        """Getter: Access level for the chart"""
         return self._inner_dict.get('access')  # type: ignore
     
     @access.setter
     def access(self, value: Union[None, Union[str, "AccessLevelClass"]]) -> None:
+        """Setter: Access level for the chart"""
         self._inner_dict['access'] = value
     
     
     @property
     def lastRefreshed(self) -> Union[None, int]:
-        """The time when this chart last refreshed"""
+        """Getter: The time when this chart last refreshed"""
         return self._inner_dict.get('lastRefreshed')  # type: ignore
     
     @lastRefreshed.setter
     def lastRefreshed(self, value: Union[None, int]) -> None:
+        """Setter: The time when this chart last refreshed"""
         self._inner_dict['lastRefreshed'] = value
     
     
 class ChartQueryClass(_Aspect):
     """Information for chart query which is used for getting data of the chart"""
 
 
@@ -1241,36 +1377,45 @@
         type: Union[str, "ChartQueryTypeClass"],
     ):
         super().__init__()
         
         self.rawQuery = rawQuery
         self.type = type
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ChartQueryClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.rawQuery = str()
         self.type = ChartQueryTypeClass.LOOKML
     
     
     @property
     def rawQuery(self) -> str:
-        """Raw query to build a chart from input datasets"""
+        """Getter: Raw query to build a chart from input datasets"""
         return self._inner_dict.get('rawQuery')  # type: ignore
     
     @rawQuery.setter
     def rawQuery(self, value: str) -> None:
+        """Setter: Raw query to build a chart from input datasets"""
         self._inner_dict['rawQuery'] = value
     
     
     @property
     def type(self) -> Union[str, "ChartQueryTypeClass"]:
-        """Chart query type"""
+        """Getter: Chart query type"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "ChartQueryTypeClass"]) -> None:
+        """Setter: Chart query type"""
         self._inner_dict['type'] = value
     
     
 class ChartQueryTypeClass(object):
     # No docs available.
     
     
@@ -1344,91 +1489,105 @@
         else:
             self.partitionSpec = partitionSpec
         self.messageId = messageId
         self.viewsCount = viewsCount
         self.uniqueUserCount = uniqueUserCount
         self.userCounts = userCounts
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ChartUsageStatisticsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMillis = int()
         self.eventGranularity = self.RECORD_SCHEMA.fields_dict["eventGranularity"].default
         self.partitionSpec = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["partitionSpec"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["partitionSpec"].type)
         self.messageId = self.RECORD_SCHEMA.fields_dict["messageId"].default
         self.viewsCount = self.RECORD_SCHEMA.fields_dict["viewsCount"].default
         self.uniqueUserCount = self.RECORD_SCHEMA.fields_dict["uniqueUserCount"].default
         self.userCounts = self.RECORD_SCHEMA.fields_dict["userCounts"].default
     
     
     @property
     def timestampMillis(self) -> int:
-        """The event timestamp field as epoch at UTC in milli seconds."""
+        """Getter: The event timestamp field as epoch at UTC in milli seconds."""
         return self._inner_dict.get('timestampMillis')  # type: ignore
     
     @timestampMillis.setter
     def timestampMillis(self, value: int) -> None:
+        """Setter: The event timestamp field as epoch at UTC in milli seconds."""
         self._inner_dict['timestampMillis'] = value
     
     
     @property
     def eventGranularity(self) -> Union[None, "TimeWindowSizeClass"]:
-        """Granularity of the event if applicable"""
+        """Getter: Granularity of the event if applicable"""
         return self._inner_dict.get('eventGranularity')  # type: ignore
     
     @eventGranularity.setter
     def eventGranularity(self, value: Union[None, "TimeWindowSizeClass"]) -> None:
+        """Setter: Granularity of the event if applicable"""
         self._inner_dict['eventGranularity'] = value
     
     
     @property
     def partitionSpec(self) -> Union["PartitionSpecClass", None]:
-        """The optional partition specification."""
+        """Getter: The optional partition specification."""
         return self._inner_dict.get('partitionSpec')  # type: ignore
     
     @partitionSpec.setter
     def partitionSpec(self, value: Union["PartitionSpecClass", None]) -> None:
+        """Setter: The optional partition specification."""
         self._inner_dict['partitionSpec'] = value
     
     
     @property
     def messageId(self) -> Union[None, str]:
-        """The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
+        """Getter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         return self._inner_dict.get('messageId')  # type: ignore
     
     @messageId.setter
     def messageId(self, value: Union[None, str]) -> None:
+        """Setter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         self._inner_dict['messageId'] = value
     
     
     @property
     def viewsCount(self) -> Union[None, int]:
-        """The total number of times chart has been viewed"""
+        """Getter: The total number of times chart has been viewed"""
         return self._inner_dict.get('viewsCount')  # type: ignore
     
     @viewsCount.setter
     def viewsCount(self, value: Union[None, int]) -> None:
+        """Setter: The total number of times chart has been viewed"""
         self._inner_dict['viewsCount'] = value
     
     
     @property
     def uniqueUserCount(self) -> Union[None, int]:
-        """Unique user count"""
+        """Getter: Unique user count"""
         return self._inner_dict.get('uniqueUserCount')  # type: ignore
     
     @uniqueUserCount.setter
     def uniqueUserCount(self, value: Union[None, int]) -> None:
+        """Setter: Unique user count"""
         self._inner_dict['uniqueUserCount'] = value
     
     
     @property
     def userCounts(self) -> Union[None, List["ChartUserUsageCountsClass"]]:
-        """Users within this bucket, with frequency counts"""
+        """Getter: Users within this bucket, with frequency counts"""
         return self._inner_dict.get('userCounts')  # type: ignore
     
     @userCounts.setter
     def userCounts(self, value: Union[None, List["ChartUserUsageCountsClass"]]) -> None:
+        """Setter: Users within this bucket, with frequency counts"""
         self._inner_dict['userCounts'] = value
     
     
 class ChartUserUsageCountsClass(DictWrapper):
     """Records a single user's usage counts for a given resource"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.chart.ChartUserUsageCounts")
@@ -1437,36 +1596,45 @@
         viewsCount: Union[None, int]=None,
     ):
         super().__init__()
         
         self.user = user
         self.viewsCount = viewsCount
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ChartUserUsageCountsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.user = str()
         self.viewsCount = self.RECORD_SCHEMA.fields_dict["viewsCount"].default
     
     
     @property
     def user(self) -> str:
-        """The unique id of the user."""
+        """Getter: The unique id of the user."""
         return self._inner_dict.get('user')  # type: ignore
     
     @user.setter
     def user(self, value: str) -> None:
+        """Setter: The unique id of the user."""
         self._inner_dict['user'] = value
     
     
     @property
     def viewsCount(self) -> Union[None, int]:
-        """The number of times the user has viewed the chart"""
+        """Getter: The number of times the user has viewed the chart"""
         return self._inner_dict.get('viewsCount')  # type: ignore
     
     @viewsCount.setter
     def viewsCount(self, value: Union[None, int]) -> None:
+        """Setter: The number of times the user has viewed the chart"""
         self._inner_dict['viewsCount'] = value
     
     
 class EditableChartPropertiesClass(_Aspect):
     """Stores editable changes made to properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines"""
 
@@ -1492,58 +1660,69 @@
             # default: {'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'time': 0, 'message': None}
             self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         else:
             self.lastModified = lastModified
         self.deleted = deleted
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableChartPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.created = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["created"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["created"].type)
         self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         self.deleted = self.RECORD_SCHEMA.fields_dict["deleted"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def deleted(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
+        """Getter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         return self._inner_dict.get('deleted')  # type: ignore
     
     @deleted.setter
     def deleted(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         self._inner_dict['deleted'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Edited documentation of the chart """
+        """Getter: Edited documentation of the chart """
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Edited documentation of the chart """
         self._inner_dict['description'] = value
     
     
 class AccessLevelClass(object):
     """The various access levels"""
     
     
@@ -1567,100 +1746,72 @@
         super().__init__()
         
         self.time = time
         self.actor = actor
         self.impersonator = impersonator
         self.message = message
     
+    @classmethod
+    def construct_with_defaults(cls) -> "AuditStampClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.time = int()
         self.actor = str()
         self.impersonator = self.RECORD_SCHEMA.fields_dict["impersonator"].default
         self.message = self.RECORD_SCHEMA.fields_dict["message"].default
     
     
     @property
     def time(self) -> int:
-        """When did the resource/association/sub-resource move into the specific lifecycle stage represented by this AuditEvent."""
+        """Getter: When did the resource/association/sub-resource move into the specific lifecycle stage represented by this AuditEvent."""
         return self._inner_dict.get('time')  # type: ignore
     
     @time.setter
     def time(self, value: int) -> None:
+        """Setter: When did the resource/association/sub-resource move into the specific lifecycle stage represented by this AuditEvent."""
         self._inner_dict['time'] = value
     
     
     @property
     def actor(self) -> str:
-        """The entity (e.g. a member URN) which will be credited for moving the resource/association/sub-resource into the specific lifecycle stage. It is also the one used to authorize the change."""
+        """Getter: The entity (e.g. a member URN) which will be credited for moving the resource/association/sub-resource into the specific lifecycle stage. It is also the one used to authorize the change."""
         return self._inner_dict.get('actor')  # type: ignore
     
     @actor.setter
     def actor(self, value: str) -> None:
+        """Setter: The entity (e.g. a member URN) which will be credited for moving the resource/association/sub-resource into the specific lifecycle stage. It is also the one used to authorize the change."""
         self._inner_dict['actor'] = value
     
     
     @property
     def impersonator(self) -> Union[None, str]:
-        """The entity (e.g. a service URN) which performs the change on behalf of the Actor and must be authorized to act as the Actor."""
+        """Getter: The entity (e.g. a service URN) which performs the change on behalf of the Actor and must be authorized to act as the Actor."""
         return self._inner_dict.get('impersonator')  # type: ignore
     
     @impersonator.setter
     def impersonator(self, value: Union[None, str]) -> None:
+        """Setter: The entity (e.g. a service URN) which performs the change on behalf of the Actor and must be authorized to act as the Actor."""
         self._inner_dict['impersonator'] = value
     
     
     @property
     def message(self) -> Union[None, str]:
-        """Additional context around how DataHub was informed of the particular change. For example: was the change created by an automated process, or manually."""
+        """Getter: Additional context around how DataHub was informed of the particular change. For example: was the change created by an automated process, or manually."""
         return self._inner_dict.get('message')  # type: ignore
     
     @message.setter
     def message(self, value: Union[None, str]) -> None:
+        """Setter: Additional context around how DataHub was informed of the particular change. For example: was the change created by an automated process, or manually."""
         self._inner_dict['message'] = value
     
     
-class BrowsePathEntryClass(DictWrapper):
-    """Represents a single level in an entity's browsePathV2"""
-    
-    RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.BrowsePathEntry")
-    def __init__(self,
-        id: str,
-        urn: Union[None, str]=None,
-    ):
-        super().__init__()
-        
-        self.id = id
-        self.urn = urn
-    
-    def _restore_defaults(self) -> None:
-        self.id = str()
-        self.urn = self.RECORD_SCHEMA.fields_dict["urn"].default
-    
-    
-    @property
-    def id(self) -> str:
-        """The ID of the browse path entry. This is what gets stored in the index.
-    If there's an urn associated with this entry, id and urn will be the same"""
-        return self._inner_dict.get('id')  # type: ignore
-    
-    @id.setter
-    def id(self, value: str) -> None:
-        self._inner_dict['id'] = value
-    
-    
-    @property
-    def urn(self) -> Union[None, str]:
-        """Optional urn pointing to some entity in DataHub"""
-        return self._inner_dict.get('urn')  # type: ignore
-    
-    @urn.setter
-    def urn(self, value: Union[None, str]) -> None:
-        self._inner_dict['urn'] = value
-    
-    
 class BrowsePathsClass(_Aspect):
     """Shared aspect containing Browse Paths to be indexed for an entity."""
 
 
     ASPECT_NAME = 'browsePaths'
     ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.BrowsePaths")
@@ -1668,62 +1819,38 @@
     def __init__(self,
         paths: List[str],
     ):
         super().__init__()
         
         self.paths = paths
     
+    @classmethod
+    def construct_with_defaults(cls) -> "BrowsePathsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.paths = list()
     
     
     @property
     def paths(self) -> List[str]:
-        """A list of valid browse paths for the entity.
+        """Getter: A list of valid browse paths for the entity.
     
     Browse paths are expected to be forward slash-separated strings. For example: 'prod/snowflake/datasetName'"""
         return self._inner_dict.get('paths')  # type: ignore
     
     @paths.setter
     def paths(self, value: List[str]) -> None:
-        self._inner_dict['paths'] = value
-    
+        """Setter: A list of valid browse paths for the entity.
     
-class BrowsePathsV2Class(_Aspect):
-    """Shared aspect containing a Browse Path to be indexed for an entity."""
-
-
-    ASPECT_NAME = 'browsePathsV2'
-    ASPECT_INFO = {}
-    RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.BrowsePathsV2")
-
-    def __init__(self,
-        path: List["BrowsePathEntryClass"],
-    ):
-        super().__init__()
-        
-        self.path = path
-    
-    def _restore_defaults(self) -> None:
-        self.path = list()
-    
-    
-    @property
-    def path(self) -> List["BrowsePathEntryClass"]:
-        """A valid browse path for the entity. This field is provided by DataHub by default.
-    This aspect is a newer version of browsePaths where we can encode more information in the path.
-    This path is also based on containers for a given entity if it has containers.
-    
-    This is stored in elasticsearch as unit-separator delimited strings and only includes platform specific folders or containers.
-    These paths should not include high level info captured elsewhere ie. Platform and Environment."""
-        return self._inner_dict.get('path')  # type: ignore
-    
-    @path.setter
-    def path(self, value: List["BrowsePathEntryClass"]) -> None:
-        self._inner_dict['path'] = value
+    Browse paths are expected to be forward slash-separated strings. For example: 'prod/snowflake/datasetName'"""
+        self._inner_dict['paths'] = value
     
     
 class ChangeAuditStampsClass(DictWrapper):
     """Data captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into various lifecycle stages, and who acted to move it into those lifecycle stages. The recommended best practice is to include this record in your record schema, and annotate its fields as @readOnly in your resource. See https://github.com/linkedin/rest.li/wiki/Validation-in-Rest.li#restli-validation-annotations"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.ChangeAuditStamps")
     def __init__(self,
@@ -1741,47 +1868,57 @@
         if lastModified is None:
             # default: {'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'time': 0, 'message': None}
             self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         else:
             self.lastModified = lastModified
         self.deleted = deleted
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ChangeAuditStampsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.created = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["created"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["created"].type)
         self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         self.deleted = self.RECORD_SCHEMA.fields_dict["deleted"].default
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def deleted(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
+        """Getter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         return self._inner_dict.get('deleted')  # type: ignore
     
     @deleted.setter
     def deleted(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         self._inner_dict['deleted'] = value
     
     
 class CostClass(_Aspect):
     # No docs available.
 
 
@@ -1794,36 +1931,45 @@
         cost: "CostCostClass",
     ):
         super().__init__()
         
         self.costType = costType
         self.cost = cost
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CostClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.costType = CostTypeClass.ORG_COST_TYPE
-        self.cost = CostCostClass._construct_with_defaults()
+        self.cost = CostCostClass.construct_with_defaults()
     
     
     @property
     def costType(self) -> Union[str, "CostTypeClass"]:
         # No docs available.
         return self._inner_dict.get('costType')  # type: ignore
     
     @costType.setter
     def costType(self, value: Union[str, "CostTypeClass"]) -> None:
+        # No docs available.
         self._inner_dict['costType'] = value
     
     
     @property
     def cost(self) -> "CostCostClass":
         # No docs available.
         return self._inner_dict.get('cost')  # type: ignore
     
     @cost.setter
     def cost(self, value: "CostCostClass") -> None:
+        # No docs available.
         self._inner_dict['cost'] = value
     
     
 class CostCostClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.CostCost")
@@ -1834,47 +1980,57 @@
     ):
         super().__init__()
         
         self.costId = costId
         self.costCode = costCode
         self.fieldDiscriminator = fieldDiscriminator
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CostCostClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.costId = self.RECORD_SCHEMA.fields_dict["costId"].default
         self.costCode = self.RECORD_SCHEMA.fields_dict["costCode"].default
         self.fieldDiscriminator = CostCostDiscriminatorClass.costId
     
     
     @property
     def costId(self) -> Union[None, float]:
         # No docs available.
         return self._inner_dict.get('costId')  # type: ignore
     
     @costId.setter
     def costId(self, value: Union[None, float]) -> None:
+        # No docs available.
         self._inner_dict['costId'] = value
     
     
     @property
     def costCode(self) -> Union[None, str]:
         # No docs available.
         return self._inner_dict.get('costCode')  # type: ignore
     
     @costCode.setter
     def costCode(self, value: Union[None, str]) -> None:
+        # No docs available.
         self._inner_dict['costCode'] = value
     
     
     @property
     def fieldDiscriminator(self) -> Union[str, "CostCostDiscriminatorClass"]:
-        """Contains the name of the field that has its value set."""
+        """Getter: Contains the name of the field that has its value set."""
         return self._inner_dict.get('fieldDiscriminator')  # type: ignore
     
     @fieldDiscriminator.setter
     def fieldDiscriminator(self, value: Union[str, "CostCostDiscriminatorClass"]) -> None:
+        """Setter: Contains the name of the field that has its value set."""
         self._inner_dict['fieldDiscriminator'] = value
     
     
 class CostCostDiscriminatorClass(object):
     # No docs available.
     
     costId = "costId"
@@ -1902,36 +2058,45 @@
         instance: Union[None, str]=None,
     ):
         super().__init__()
         
         self.platform = platform
         self.instance = instance
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataPlatformInstanceClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.platform = str()
         self.instance = self.RECORD_SCHEMA.fields_dict["instance"].default
     
     
     @property
     def platform(self) -> str:
-        """Data Platform"""
+        """Getter: Data Platform"""
         return self._inner_dict.get('platform')  # type: ignore
     
     @platform.setter
     def platform(self, value: str) -> None:
+        """Setter: Data Platform"""
         self._inner_dict['platform'] = value
     
     
     @property
     def instance(self) -> Union[None, str]:
-        """Instance of the data platform (e.g. db instance)"""
+        """Getter: Instance of the data platform (e.g. db instance)"""
         return self._inner_dict.get('instance')  # type: ignore
     
     @instance.setter
     def instance(self, value: Union[None, str]) -> None:
+        """Setter: Instance of the data platform (e.g. db instance)"""
         self._inner_dict['instance'] = value
     
     
 class DeprecationClass(_Aspect):
     """Deprecation status of an entity"""
 
 
@@ -1948,58 +2113,69 @@
         super().__init__()
         
         self.deprecated = deprecated
         self.decommissionTime = decommissionTime
         self.note = note
         self.actor = actor
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DeprecationClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.deprecated = bool()
         self.decommissionTime = self.RECORD_SCHEMA.fields_dict["decommissionTime"].default
         self.note = str()
         self.actor = str()
     
     
     @property
     def deprecated(self) -> bool:
-        """Whether the entity is deprecated."""
+        """Getter: Whether the entity is deprecated."""
         return self._inner_dict.get('deprecated')  # type: ignore
     
     @deprecated.setter
     def deprecated(self, value: bool) -> None:
+        """Setter: Whether the entity is deprecated."""
         self._inner_dict['deprecated'] = value
     
     
     @property
     def decommissionTime(self) -> Union[None, int]:
-        """The time user plan to decommission this entity."""
+        """Getter: The time user plan to decommission this entity."""
         return self._inner_dict.get('decommissionTime')  # type: ignore
     
     @decommissionTime.setter
     def decommissionTime(self, value: Union[None, int]) -> None:
+        """Setter: The time user plan to decommission this entity."""
         self._inner_dict['decommissionTime'] = value
     
     
     @property
     def note(self) -> str:
-        """Additional information about the entity deprecation plan, such as the wiki, doc, RB."""
+        """Getter: Additional information about the entity deprecation plan, such as the wiki, doc, RB."""
         return self._inner_dict.get('note')  # type: ignore
     
     @note.setter
     def note(self, value: str) -> None:
+        """Setter: Additional information about the entity deprecation plan, such as the wiki, doc, RB."""
         self._inner_dict['note'] = value
     
     
     @property
     def actor(self) -> str:
-        """The user URN which will be credited for modifying this deprecation content."""
+        """Getter: The user URN which will be credited for modifying this deprecation content."""
         return self._inner_dict.get('actor')  # type: ignore
     
     @actor.setter
     def actor(self, value: str) -> None:
+        """Setter: The user URN which will be credited for modifying this deprecation content."""
         self._inner_dict['actor'] = value
     
     
 class EdgeClass(DictWrapper):
     """Information about a relatonship edge."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.Edge")
@@ -2014,69 +2190,81 @@
         
         self.sourceUrn = sourceUrn
         self.destinationUrn = destinationUrn
         self.created = created
         self.lastModified = lastModified
         self.properties = properties
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EdgeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.sourceUrn = str()
         self.destinationUrn = str()
-        self.created = AuditStampClass._construct_with_defaults()
-        self.lastModified = AuditStampClass._construct_with_defaults()
+        self.created = AuditStampClass.construct_with_defaults()
+        self.lastModified = AuditStampClass.construct_with_defaults()
         self.properties = self.RECORD_SCHEMA.fields_dict["properties"].default
     
     
     @property
     def sourceUrn(self) -> str:
-        """Urn of the source of this relationship edge."""
+        """Getter: Urn of the source of this relationship edge."""
         return self._inner_dict.get('sourceUrn')  # type: ignore
     
     @sourceUrn.setter
     def sourceUrn(self, value: str) -> None:
+        """Setter: Urn of the source of this relationship edge."""
         self._inner_dict['sourceUrn'] = value
     
     
     @property
     def destinationUrn(self) -> str:
-        """Urn of the destination of this relationship edge."""
+        """Getter: Urn of the destination of this relationship edge."""
         return self._inner_dict.get('destinationUrn')  # type: ignore
     
     @destinationUrn.setter
     def destinationUrn(self, value: str) -> None:
+        """Setter: Urn of the destination of this relationship edge."""
         self._inner_dict['destinationUrn'] = value
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """Audit stamp containing who created this relationship edge and when"""
+        """Getter: Audit stamp containing who created this relationship edge and when"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp containing who created this relationship edge and when"""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """Audit stamp containing who last modified this relationship edge and when"""
+        """Getter: Audit stamp containing who last modified this relationship edge and when"""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp containing who last modified this relationship edge and when"""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def properties(self) -> Union[None, Dict[str, str]]:
-        """A generic properties bag that allows us to store specific information on this graph edge."""
+        """Getter: A generic properties bag that allows us to store specific information on this graph edge."""
         return self._inner_dict.get('properties')  # type: ignore
     
     @properties.setter
     def properties(self, value: Union[None, Dict[str, str]]) -> None:
+        """Setter: A generic properties bag that allows us to store specific information on this graph edge."""
         self._inner_dict['properties'] = value
     
     
 class EmbedClass(_Aspect):
     """Information regarding rendering an embed for an asset."""
 
 
@@ -2087,25 +2275,33 @@
     def __init__(self,
         renderUrl: Union[None, str]=None,
     ):
         super().__init__()
         
         self.renderUrl = renderUrl
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EmbedClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.renderUrl = self.RECORD_SCHEMA.fields_dict["renderUrl"].default
     
     
     @property
     def renderUrl(self) -> Union[None, str]:
-        """An embed URL to be rendered inside of an iframe."""
+        """Getter: An embed URL to be rendered inside of an iframe."""
         return self._inner_dict.get('renderUrl')  # type: ignore
     
     @renderUrl.setter
     def renderUrl(self, value: Union[None, str]) -> None:
+        """Setter: An embed URL to be rendered inside of an iframe."""
         self._inner_dict['renderUrl'] = value
     
     
 class FabricTypeClass(object):
     """Fabric group type"""
     
     
@@ -2151,25 +2347,33 @@
     def __init__(self,
         tags: List["TagAssociationClass"],
     ):
         super().__init__()
         
         self.tags = tags
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlobalTagsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.tags = list()
     
     
     @property
     def tags(self) -> List["TagAssociationClass"]:
-        """Tags associated with a given entity"""
+        """Getter: Tags associated with a given entity"""
         return self._inner_dict.get('tags')  # type: ignore
     
     @tags.setter
     def tags(self, value: List["TagAssociationClass"]) -> None:
+        """Setter: Tags associated with a given entity"""
         self._inner_dict['tags'] = value
     
     
 class GlossaryTermAssociationClass(DictWrapper):
     """Properties of an applied glossary term."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.GlossaryTermAssociation")
@@ -2178,36 +2382,45 @@
         context: Union[None, str]=None,
     ):
         super().__init__()
         
         self.urn = urn
         self.context = context
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlossaryTermAssociationClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.context = self.RECORD_SCHEMA.fields_dict["context"].default
     
     
     @property
     def urn(self) -> str:
-        """Urn of the applied glossary term"""
+        """Getter: Urn of the applied glossary term"""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: Urn of the applied glossary term"""
         self._inner_dict['urn'] = value
     
     
     @property
     def context(self) -> Union[None, str]:
-        """Additional context about the association"""
+        """Getter: Additional context about the association"""
         return self._inner_dict.get('context')  # type: ignore
     
     @context.setter
     def context(self, value: Union[None, str]) -> None:
+        """Setter: Additional context about the association"""
         self._inner_dict['context'] = value
     
     
 class GlossaryTermsClass(_Aspect):
     """Related business terms information"""
 
 
@@ -2220,36 +2433,45 @@
         auditStamp: "AuditStampClass",
     ):
         super().__init__()
         
         self.terms = terms
         self.auditStamp = auditStamp
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlossaryTermsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.terms = list()
-        self.auditStamp = AuditStampClass._construct_with_defaults()
+        self.auditStamp = AuditStampClass.construct_with_defaults()
     
     
     @property
     def terms(self) -> List["GlossaryTermAssociationClass"]:
-        """The related business terms"""
+        """Getter: The related business terms"""
         return self._inner_dict.get('terms')  # type: ignore
     
     @terms.setter
     def terms(self, value: List["GlossaryTermAssociationClass"]) -> None:
+        """Setter: The related business terms"""
         self._inner_dict['terms'] = value
     
     
     @property
     def auditStamp(self) -> "AuditStampClass":
-        """Audit stamp containing who reported the related business term"""
+        """Getter: Audit stamp containing who reported the related business term"""
         return self._inner_dict.get('auditStamp')  # type: ignore
     
     @auditStamp.setter
     def auditStamp(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp containing who reported the related business term"""
         self._inner_dict['auditStamp'] = value
     
     
 class InputFieldClass(DictWrapper):
     """Information about a field a chart or dashboard references"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.InputField")
@@ -2258,36 +2480,45 @@
         schemaField: Union[None, "SchemaFieldClass"]=None,
     ):
         super().__init__()
         
         self.schemaFieldUrn = schemaFieldUrn
         self.schemaField = schemaField
     
+    @classmethod
+    def construct_with_defaults(cls) -> "InputFieldClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.schemaFieldUrn = str()
         self.schemaField = self.RECORD_SCHEMA.fields_dict["schemaField"].default
     
     
     @property
     def schemaFieldUrn(self) -> str:
-        """Urn of the schema being referenced for lineage purposes"""
+        """Getter: Urn of the schema being referenced for lineage purposes"""
         return self._inner_dict.get('schemaFieldUrn')  # type: ignore
     
     @schemaFieldUrn.setter
     def schemaFieldUrn(self, value: str) -> None:
+        """Setter: Urn of the schema being referenced for lineage purposes"""
         self._inner_dict['schemaFieldUrn'] = value
     
     
     @property
     def schemaField(self) -> Union[None, "SchemaFieldClass"]:
-        """Copied version of the referenced schema field object for indexing purposes"""
+        """Getter: Copied version of the referenced schema field object for indexing purposes"""
         return self._inner_dict.get('schemaField')  # type: ignore
     
     @schemaField.setter
     def schemaField(self, value: Union[None, "SchemaFieldClass"]) -> None:
+        """Setter: Copied version of the referenced schema field object for indexing purposes"""
         self._inner_dict['schemaField'] = value
     
     
 class InputFieldsClass(_Aspect):
     """Information about the fields a chart or dashboard references"""
 
 
@@ -2298,25 +2529,33 @@
     def __init__(self,
         fields: List["InputFieldClass"],
     ):
         super().__init__()
         
         self.fields = fields
     
+    @classmethod
+    def construct_with_defaults(cls) -> "InputFieldsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.fields = list()
     
     
     @property
     def fields(self) -> List["InputFieldClass"]:
-        """List of fields being referenced"""
+        """Getter: List of fields being referenced"""
         return self._inner_dict.get('fields')  # type: ignore
     
     @fields.setter
     def fields(self, value: List["InputFieldClass"]) -> None:
+        """Setter: List of fields being referenced"""
         self._inner_dict['fields'] = value
     
     
 class InstitutionalMemoryClass(_Aspect):
     """Institutional memory of an entity. This is a way to link to relevant documentation and provide description of the documentation. Institutional or tribal knowledge is very important for users to leverage the entity."""
 
 
@@ -2327,25 +2566,33 @@
     def __init__(self,
         elements: List["InstitutionalMemoryMetadataClass"],
     ):
         super().__init__()
         
         self.elements = elements
     
+    @classmethod
+    def construct_with_defaults(cls) -> "InstitutionalMemoryClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.elements = list()
     
     
     @property
     def elements(self) -> List["InstitutionalMemoryMetadataClass"]:
-        """List of records that represent institutional memory of an entity. Each record consists of a link, description, creator and timestamps associated with that record."""
+        """Getter: List of records that represent institutional memory of an entity. Each record consists of a link, description, creator and timestamps associated with that record."""
         return self._inner_dict.get('elements')  # type: ignore
     
     @elements.setter
     def elements(self, value: List["InstitutionalMemoryMetadataClass"]) -> None:
+        """Setter: List of records that represent institutional memory of an entity. Each record consists of a link, description, creator and timestamps associated with that record."""
         self._inner_dict['elements'] = value
     
     
 class InstitutionalMemoryMetadataClass(DictWrapper):
     """Metadata corresponding to a record of institutional memory."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.InstitutionalMemoryMetadata")
@@ -2356,47 +2603,57 @@
     ):
         super().__init__()
         
         self.url = url
         self.description = description
         self.createStamp = createStamp
     
+    @classmethod
+    def construct_with_defaults(cls) -> "InstitutionalMemoryMetadataClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.url = str()
         self.description = str()
-        self.createStamp = AuditStampClass._construct_with_defaults()
+        self.createStamp = AuditStampClass.construct_with_defaults()
     
     
     @property
     def url(self) -> str:
-        """Link to an engineering design document or a wiki page."""
+        """Getter: Link to an engineering design document or a wiki page."""
         return self._inner_dict.get('url')  # type: ignore
     
     @url.setter
     def url(self, value: str) -> None:
+        """Setter: Link to an engineering design document or a wiki page."""
         self._inner_dict['url'] = value
     
     
     @property
     def description(self) -> str:
-        """Description of the link."""
+        """Getter: Description of the link."""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: str) -> None:
+        """Setter: Description of the link."""
         self._inner_dict['description'] = value
     
     
     @property
     def createStamp(self) -> "AuditStampClass":
-        """Audit stamp associated with creation of this record"""
+        """Getter: Audit stamp associated with creation of this record"""
         return self._inner_dict.get('createStamp')  # type: ignore
     
     @createStamp.setter
     def createStamp(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp associated with creation of this record"""
         self._inner_dict['createStamp'] = value
     
     
 class MLFeatureDataTypeClass(object):
     """MLFeature Data Type"""
     
     
@@ -2467,36 +2724,45 @@
         location: str,
     ):
         super().__init__()
         
         self.type = type
         self.location = location
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MediaClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = MediaTypeClass.IMAGE
         self.location = str()
     
     
     @property
     def type(self) -> Union[str, "MediaTypeClass"]:
-        """Type of content the Media is storing, e.g. image, video, etc."""
+        """Getter: Type of content the Media is storing, e.g. image, video, etc."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "MediaTypeClass"]) -> None:
+        """Setter: Type of content the Media is storing, e.g. image, video, etc."""
         self._inner_dict['type'] = value
     
     
     @property
     def location(self) -> str:
-        """Where the media content is stored."""
+        """Getter: Where the media content is stored."""
         return self._inner_dict.get('location')  # type: ignore
     
     @location.setter
     def location(self, value: str) -> None:
+        """Setter: Where the media content is stored."""
         self._inner_dict['location'] = value
     
     
 class MediaTypeClass(object):
     """Enum defining the type of content a Media object holds."""
     
     
@@ -2542,14 +2808,21 @@
         self.customOperationType = customOperationType
         self.numAffectedRows = numAffectedRows
         self.affectedDatasets = affectedDatasets
         self.sourceType = sourceType
         self.customProperties = customProperties
         self.lastUpdatedTimestamp = lastUpdatedTimestamp
     
+    @classmethod
+    def construct_with_defaults(cls) -> "OperationClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMillis = int()
         self.eventGranularity = self.RECORD_SCHEMA.fields_dict["eventGranularity"].default
         self.partitionSpec = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["partitionSpec"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["partitionSpec"].type)
         self.messageId = self.RECORD_SCHEMA.fields_dict["messageId"].default
         self.actor = self.RECORD_SCHEMA.fields_dict["actor"].default
         self.operationType = OperationTypeClass.INSERT
@@ -2559,129 +2832,141 @@
         self.sourceType = self.RECORD_SCHEMA.fields_dict["sourceType"].default
         self.customProperties = self.RECORD_SCHEMA.fields_dict["customProperties"].default
         self.lastUpdatedTimestamp = int()
     
     
     @property
     def timestampMillis(self) -> int:
-        """The event timestamp field as epoch at UTC in milli seconds."""
+        """Getter: The event timestamp field as epoch at UTC in milli seconds."""
         return self._inner_dict.get('timestampMillis')  # type: ignore
     
     @timestampMillis.setter
     def timestampMillis(self, value: int) -> None:
+        """Setter: The event timestamp field as epoch at UTC in milli seconds."""
         self._inner_dict['timestampMillis'] = value
     
     
     @property
     def eventGranularity(self) -> Union[None, "TimeWindowSizeClass"]:
-        """Granularity of the event if applicable"""
+        """Getter: Granularity of the event if applicable"""
         return self._inner_dict.get('eventGranularity')  # type: ignore
     
     @eventGranularity.setter
     def eventGranularity(self, value: Union[None, "TimeWindowSizeClass"]) -> None:
+        """Setter: Granularity of the event if applicable"""
         self._inner_dict['eventGranularity'] = value
     
     
     @property
     def partitionSpec(self) -> Union["PartitionSpecClass", None]:
-        """The optional partition specification."""
+        """Getter: The optional partition specification."""
         return self._inner_dict.get('partitionSpec')  # type: ignore
     
     @partitionSpec.setter
     def partitionSpec(self, value: Union["PartitionSpecClass", None]) -> None:
+        """Setter: The optional partition specification."""
         self._inner_dict['partitionSpec'] = value
     
     
     @property
     def messageId(self) -> Union[None, str]:
-        """The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
+        """Getter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         return self._inner_dict.get('messageId')  # type: ignore
     
     @messageId.setter
     def messageId(self, value: Union[None, str]) -> None:
+        """Setter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         self._inner_dict['messageId'] = value
     
     
     @property
     def actor(self) -> Union[None, str]:
-        """Actor who issued this operation."""
+        """Getter: Actor who issued this operation."""
         return self._inner_dict.get('actor')  # type: ignore
     
     @actor.setter
     def actor(self, value: Union[None, str]) -> None:
+        """Setter: Actor who issued this operation."""
         self._inner_dict['actor'] = value
     
     
     @property
     def operationType(self) -> Union[str, "OperationTypeClass"]:
-        """Operation type of change."""
+        """Getter: Operation type of change."""
         return self._inner_dict.get('operationType')  # type: ignore
     
     @operationType.setter
     def operationType(self, value: Union[str, "OperationTypeClass"]) -> None:
+        """Setter: Operation type of change."""
         self._inner_dict['operationType'] = value
     
     
     @property
     def customOperationType(self) -> Union[None, str]:
-        """A custom type of operation. Required if operationType is CUSTOM."""
+        """Getter: A custom type of operation. Required if operationType is CUSTOM."""
         return self._inner_dict.get('customOperationType')  # type: ignore
     
     @customOperationType.setter
     def customOperationType(self, value: Union[None, str]) -> None:
+        """Setter: A custom type of operation. Required if operationType is CUSTOM."""
         self._inner_dict['customOperationType'] = value
     
     
     @property
     def numAffectedRows(self) -> Union[None, int]:
-        """How many rows were affected by this operation."""
+        """Getter: How many rows were affected by this operation."""
         return self._inner_dict.get('numAffectedRows')  # type: ignore
     
     @numAffectedRows.setter
     def numAffectedRows(self, value: Union[None, int]) -> None:
+        """Setter: How many rows were affected by this operation."""
         self._inner_dict['numAffectedRows'] = value
     
     
     @property
     def affectedDatasets(self) -> Union[None, List[str]]:
-        """Which other datasets were affected by this operation."""
+        """Getter: Which other datasets were affected by this operation."""
         return self._inner_dict.get('affectedDatasets')  # type: ignore
     
     @affectedDatasets.setter
     def affectedDatasets(self, value: Union[None, List[str]]) -> None:
+        """Setter: Which other datasets were affected by this operation."""
         self._inner_dict['affectedDatasets'] = value
     
     
     @property
     def sourceType(self) -> Union[None, Union[str, "OperationSourceTypeClass"]]:
-        """Source Type"""
+        """Getter: Source Type"""
         return self._inner_dict.get('sourceType')  # type: ignore
     
     @sourceType.setter
     def sourceType(self, value: Union[None, Union[str, "OperationSourceTypeClass"]]) -> None:
+        """Setter: Source Type"""
         self._inner_dict['sourceType'] = value
     
     
     @property
     def customProperties(self) -> Union[None, Dict[str, str]]:
-        """Custom properties"""
+        """Getter: Custom properties"""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Union[None, Dict[str, str]]) -> None:
+        """Setter: Custom properties"""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def lastUpdatedTimestamp(self) -> int:
-        """The time at which the operation occurred. Would be better named 'operationTime'"""
+        """Getter: The time at which the operation occurred. Would be better named 'operationTime'"""
         return self._inner_dict.get('lastUpdatedTimestamp')  # type: ignore
     
     @lastUpdatedTimestamp.setter
     def lastUpdatedTimestamp(self, value: int) -> None:
+        """Setter: The time at which the operation occurred. Would be better named 'operationTime'"""
         self._inner_dict['lastUpdatedTimestamp'] = value
     
     
 class OperationSourceTypeClass(object):
     """The source of an operation"""
     
     
@@ -2733,36 +3018,45 @@
         externalType: Union[None, str]=None,
     ):
         super().__init__()
         
         self.type = type
         self.externalType = externalType
     
+    @classmethod
+    def construct_with_defaults(cls) -> "OriginClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = OriginTypeClass.NATIVE
         self.externalType = self.RECORD_SCHEMA.fields_dict["externalType"].default
     
     
     @property
     def type(self) -> Union[str, "OriginTypeClass"]:
-        """Where an entity originated from. Either NATIVE or EXTERNAL."""
+        """Getter: Where an entity originated from. Either NATIVE or EXTERNAL."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "OriginTypeClass"]) -> None:
+        """Setter: Where an entity originated from. Either NATIVE or EXTERNAL."""
         self._inner_dict['type'] = value
     
     
     @property
     def externalType(self) -> Union[None, str]:
-        """Only populated if type is EXTERNAL. The externalType of the entity, such as the name of the identity provider."""
+        """Getter: Only populated if type is EXTERNAL. The externalType of the entity, such as the name of the identity provider."""
         return self._inner_dict.get('externalType')  # type: ignore
     
     @externalType.setter
     def externalType(self, value: Union[None, str]) -> None:
+        """Setter: Only populated if type is EXTERNAL. The externalType of the entity, such as the name of the identity provider."""
         self._inner_dict['externalType'] = value
     
     
 class OriginTypeClass(object):
     """Enum to define where an entity originated from."""
     
     
@@ -2784,48 +3078,59 @@
     ):
         super().__init__()
         
         self.owner = owner
         self.type = type
         self.source = source
     
+    @classmethod
+    def construct_with_defaults(cls) -> "OwnerClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.owner = str()
         self.type = OwnershipTypeClass.TECHNICAL_OWNER
         self.source = self.RECORD_SCHEMA.fields_dict["source"].default
     
     
     @property
     def owner(self) -> str:
-        """Owner URN, e.g. urn:li:corpuser:ldap, urn:li:corpGroup:group_name, and urn:li:multiProduct:mp_name
+        """Getter: Owner URN, e.g. urn:li:corpuser:ldap, urn:li:corpGroup:group_name, and urn:li:multiProduct:mp_name
     (Caveat: only corpuser is currently supported in the frontend.)"""
         return self._inner_dict.get('owner')  # type: ignore
     
     @owner.setter
     def owner(self, value: str) -> None:
+        """Setter: Owner URN, e.g. urn:li:corpuser:ldap, urn:li:corpGroup:group_name, and urn:li:multiProduct:mp_name
+    (Caveat: only corpuser is currently supported in the frontend.)"""
         self._inner_dict['owner'] = value
     
     
     @property
     def type(self) -> Union[str, "OwnershipTypeClass"]:
-        """The type of the ownership"""
+        """Getter: The type of the ownership"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "OwnershipTypeClass"]) -> None:
+        """Setter: The type of the ownership"""
         self._inner_dict['type'] = value
     
     
     @property
     def source(self) -> Union[None, "OwnershipSourceClass"]:
-        """Source information for the ownership"""
+        """Getter: Source information for the ownership"""
         return self._inner_dict.get('source')  # type: ignore
     
     @source.setter
     def source(self, value: Union[None, "OwnershipSourceClass"]) -> None:
+        """Setter: Source information for the ownership"""
         self._inner_dict['source'] = value
     
     
 class OwnershipClass(_Aspect):
     """Ownership information of an entity."""
 
 
@@ -2842,36 +3147,45 @@
         self.owners = owners
         if lastModified is None:
             # default: {'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'time': 0, 'message': None}
             self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         else:
             self.lastModified = lastModified
     
+    @classmethod
+    def construct_with_defaults(cls) -> "OwnershipClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.owners = list()
         self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
     
     
     @property
     def owners(self) -> List["OwnerClass"]:
-        """List of owners of the entity."""
+        """Getter: List of owners of the entity."""
         return self._inner_dict.get('owners')  # type: ignore
     
     @owners.setter
     def owners(self, value: List["OwnerClass"]) -> None:
+        """Setter: List of owners of the entity."""
         self._inner_dict['owners'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """Audit stamp containing who last modified the record and when. A value of 0 in the time field indicates missing data."""
+        """Getter: Audit stamp containing who last modified the record and when. A value of 0 in the time field indicates missing data."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp containing who last modified the record and when. A value of 0 in the time field indicates missing data."""
         self._inner_dict['lastModified'] = value
     
     
 class OwnershipSourceClass(DictWrapper):
     """Source/provider of the ownership information"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.OwnershipSource")
@@ -2880,36 +3194,45 @@
         url: Union[None, str]=None,
     ):
         super().__init__()
         
         self.type = type
         self.url = url
     
+    @classmethod
+    def construct_with_defaults(cls) -> "OwnershipSourceClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = OwnershipSourceTypeClass.AUDIT
         self.url = self.RECORD_SCHEMA.fields_dict["url"].default
     
     
     @property
     def type(self) -> Union[str, "OwnershipSourceTypeClass"]:
-        """The type of the source"""
+        """Getter: The type of the source"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "OwnershipSourceTypeClass"]) -> None:
+        """Setter: The type of the source"""
         self._inner_dict['type'] = value
     
     
     @property
     def url(self) -> Union[None, str]:
-        """A reference URL for the source"""
+        """Getter: A reference URL for the source"""
         return self._inner_dict.get('url')  # type: ignore
     
     @url.setter
     def url(self, value: Union[None, str]) -> None:
+        """Setter: A reference URL for the source"""
         self._inner_dict['url'] = value
     
     
 class OwnershipSourceTypeClass(object):
     # No docs available.
     
     
@@ -2992,36 +3315,45 @@
         primary: bool,
     ):
         super().__init__()
         
         self.siblings = siblings
         self.primary = primary
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SiblingsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.siblings = list()
         self.primary = bool()
     
     
     @property
     def siblings(self) -> List[str]:
-        """List of sibling entities"""
+        """Getter: List of sibling entities"""
         return self._inner_dict.get('siblings')  # type: ignore
     
     @siblings.setter
     def siblings(self, value: List[str]) -> None:
+        """Setter: List of sibling entities"""
         self._inner_dict['siblings'] = value
     
     
     @property
     def primary(self) -> bool:
-        """If this is the leader entity of the set of siblings"""
+        """Getter: If this is the leader entity of the set of siblings"""
         return self._inner_dict.get('primary')  # type: ignore
     
     @primary.setter
     def primary(self, value: bool) -> None:
+        """Setter: If this is the leader entity of the set of siblings"""
         self._inner_dict['primary'] = value
     
     
 class StatusClass(_Aspect):
     """The lifecycle status metadata of an entity, e.g. dataset, metric, feature, etc.
     This aspect is used to represent soft deletes conventionally."""
 
@@ -3037,25 +3369,33 @@
         
         if removed is None:
             # default: False
             self.removed = self.RECORD_SCHEMA.fields_dict["removed"].default
         else:
             self.removed = removed
     
+    @classmethod
+    def construct_with_defaults(cls) -> "StatusClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.removed = self.RECORD_SCHEMA.fields_dict["removed"].default
     
     
     @property
     def removed(self) -> bool:
-        """Whether the entity has been removed (soft-deleted)."""
+        """Getter: Whether the entity has been removed (soft-deleted)."""
         return self._inner_dict.get('removed')  # type: ignore
     
     @removed.setter
     def removed(self, value: bool) -> None:
+        """Setter: Whether the entity has been removed (soft-deleted)."""
         self._inner_dict['removed'] = value
     
     
 class SubTypesClass(_Aspect):
     """Sub Types. Use this aspect to specialize a generic Entity
     e.g. Making a Dataset also be a View or also be a LookerExplore"""
 
@@ -3067,25 +3407,33 @@
     def __init__(self,
         typeNames: List[str],
     ):
         super().__init__()
         
         self.typeNames = typeNames
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SubTypesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.typeNames = list()
     
     
     @property
     def typeNames(self) -> List[str]:
-        """The names of the specific types."""
+        """Getter: The names of the specific types."""
         return self._inner_dict.get('typeNames')  # type: ignore
     
     @typeNames.setter
     def typeNames(self, value: List[str]) -> None:
+        """Setter: The names of the specific types."""
         self._inner_dict['typeNames'] = value
     
     
 class TagAssociationClass(DictWrapper):
     """Properties of an applied tag. For now, just an Urn. In the future we can extend this with other properties, e.g.
     propagation parameters."""
     
@@ -3095,36 +3443,45 @@
         context: Union[None, str]=None,
     ):
         super().__init__()
         
         self.tag = tag
         self.context = context
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TagAssociationClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.tag = str()
         self.context = self.RECORD_SCHEMA.fields_dict["context"].default
     
     
     @property
     def tag(self) -> str:
-        """Urn of the applied tag"""
+        """Getter: Urn of the applied tag"""
         return self._inner_dict.get('tag')  # type: ignore
     
     @tag.setter
     def tag(self, value: str) -> None:
+        """Setter: Urn of the applied tag"""
         self._inner_dict['tag'] = value
     
     
     @property
     def context(self) -> Union[None, str]:
-        """Additional context about the association"""
+        """Getter: Additional context about the association"""
         return self._inner_dict.get('context')  # type: ignore
     
     @context.setter
     def context(self, value: Union[None, str]) -> None:
+        """Setter: Additional context about the association"""
         self._inner_dict['context'] = value
     
     
 class TimeStampClass(DictWrapper):
     """A standard event timestamp"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.TimeStamp")
@@ -3133,61 +3490,78 @@
         actor: Union[None, str]=None,
     ):
         super().__init__()
         
         self.time = time
         self.actor = actor
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TimeStampClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.time = int()
         self.actor = self.RECORD_SCHEMA.fields_dict["actor"].default
     
     
     @property
     def time(self) -> int:
-        """When did the event occur"""
+        """Getter: When did the event occur"""
         return self._inner_dict.get('time')  # type: ignore
     
     @time.setter
     def time(self, value: int) -> None:
+        """Setter: When did the event occur"""
         self._inner_dict['time'] = value
     
     
     @property
     def actor(self) -> Union[None, str]:
-        """Optional: The actor urn involved in the event."""
+        """Getter: Optional: The actor urn involved in the event."""
         return self._inner_dict.get('actor')  # type: ignore
     
     @actor.setter
     def actor(self, value: Union[None, str]) -> None:
+        """Setter: Optional: The actor urn involved in the event."""
         self._inner_dict['actor'] = value
     
     
 class VersionTagClass(DictWrapper):
     """A resource-defined string representing the resource state for the purpose of concurrency control"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.VersionTag")
     def __init__(self,
         versionTag: Union[None, str]=None,
     ):
         super().__init__()
         
         self.versionTag = versionTag
     
+    @classmethod
+    def construct_with_defaults(cls) -> "VersionTagClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.versionTag = self.RECORD_SCHEMA.fields_dict["versionTag"].default
     
     
     @property
     def versionTag(self) -> Union[None, str]:
         # No docs available.
         return self._inner_dict.get('versionTag')  # type: ignore
     
     @versionTag.setter
     def versionTag(self, value: Union[None, str]) -> None:
+        # No docs available.
         self._inner_dict['versionTag'] = value
     
     
 class WindowDurationClass(object):
     """Enum to define the length of a bucket when doing aggregations"""
     
     YEAR = "YEAR"
@@ -3215,25 +3589,33 @@
     def __init__(self,
         udf: str,
     ):
         super().__init__()
         
         self.udf = udf
     
+    @classmethod
+    def construct_with_defaults(cls) -> "UDFTransformerClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.udf = str()
     
     
     @property
     def udf(self) -> str:
-        """A UDF mentioning how the source fields got transformed to destination field. This is the FQCN(Fully Qualified Class Name) of the udf."""
+        """Getter: A UDF mentioning how the source fields got transformed to destination field. This is the FQCN(Fully Qualified Class Name) of the udf."""
         return self._inner_dict.get('udf')  # type: ignore
     
     @udf.setter
     def udf(self, value: str) -> None:
+        """Setter: A UDF mentioning how the source fields got transformed to destination field. This is the FQCN(Fully Qualified Class Name) of the udf."""
         self._inner_dict['udf'] = value
     
     
 class ContainerClass(_Aspect):
     """Link from an asset to its parent container"""
 
 
@@ -3244,25 +3626,33 @@
     def __init__(self,
         container: str,
     ):
         super().__init__()
         
         self.container = container
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ContainerClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.container = str()
     
     
     @property
     def container(self) -> str:
-        """The parent container of an asset"""
+        """Getter: The parent container of an asset"""
         return self._inner_dict.get('container')  # type: ignore
     
     @container.setter
     def container(self, value: str) -> None:
+        """Setter: The parent container of an asset"""
         self._inner_dict['container'] = value
     
     
 class ContainerPropertiesClass(_Aspect):
     """Information about a Asset Container as received from a 3rd party source system"""
 
 
@@ -3289,91 +3679,105 @@
         self.externalUrl = externalUrl
         self.name = name
         self.qualifiedName = qualifiedName
         self.description = description
         self.created = created
         self.lastModified = lastModified
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ContainerPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.name = str()
         self.qualifiedName = self.RECORD_SCHEMA.fields_dict["qualifiedName"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.created = self.RECORD_SCHEMA.fields_dict["created"].default
         self.lastModified = self.RECORD_SCHEMA.fields_dict["lastModified"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def name(self) -> str:
-        """Display name of the Asset Container"""
+        """Getter: Display name of the Asset Container"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Display name of the Asset Container"""
         self._inner_dict['name'] = value
     
     
     @property
     def qualifiedName(self) -> Union[None, str]:
-        """Fully-qualified name of the Container"""
+        """Getter: Fully-qualified name of the Container"""
         return self._inner_dict.get('qualifiedName')  # type: ignore
     
     @qualifiedName.setter
     def qualifiedName(self, value: Union[None, str]) -> None:
+        """Setter: Fully-qualified name of the Container"""
         self._inner_dict['qualifiedName'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Description of the Asset Container as it exists inside a source system"""
+        """Getter: Description of the Asset Container as it exists inside a source system"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Description of the Asset Container as it exists inside a source system"""
         self._inner_dict['description'] = value
     
     
     @property
     def created(self) -> Union[None, "TimeStampClass"]:
-        """A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
+        """Getter: A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: Union[None, "TimeStampClass"]) -> None:
+        """Setter: A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> Union[None, "TimeStampClass"]:
-        """A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
+        """Getter: A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: Union[None, "TimeStampClass"]) -> None:
+        """Setter: A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
         self._inner_dict['lastModified'] = value
     
     
 class EditableContainerPropertiesClass(_Aspect):
     """Editable information about an Asset Container as defined on the DataHub Platform"""
 
 
@@ -3384,25 +3788,33 @@
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableContainerPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Description of the Asset Container as its received on the DataHub Platform"""
+        """Getter: Description of the Asset Container as its received on the DataHub Platform"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Description of the Asset Container as its received on the DataHub Platform"""
         self._inner_dict['description'] = value
     
     
 class DashboardInfoClass(_Aspect):
     """Information about a dashboard"""
 
 
@@ -3447,148 +3859,169 @@
             self.datasets = datasets
         self.datasetEdges = datasetEdges
         self.lastModified = lastModified
         self.dashboardUrl = dashboardUrl
         self.access = access
         self.lastRefreshed = lastRefreshed
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DashboardInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.title = str()
         self.description = str()
         self.charts = list()
         self.chartEdges = self.RECORD_SCHEMA.fields_dict["chartEdges"].default
         self.datasets = list()
         self.datasetEdges = self.RECORD_SCHEMA.fields_dict["datasetEdges"].default
-        self.lastModified = ChangeAuditStampsClass._construct_with_defaults()
+        self.lastModified = ChangeAuditStampsClass.construct_with_defaults()
         self.dashboardUrl = self.RECORD_SCHEMA.fields_dict["dashboardUrl"].default
         self.access = self.RECORD_SCHEMA.fields_dict["access"].default
         self.lastRefreshed = self.RECORD_SCHEMA.fields_dict["lastRefreshed"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def title(self) -> str:
-        """Title of the dashboard"""
+        """Getter: Title of the dashboard"""
         return self._inner_dict.get('title')  # type: ignore
     
     @title.setter
     def title(self, value: str) -> None:
+        """Setter: Title of the dashboard"""
         self._inner_dict['title'] = value
     
     
     @property
     def description(self) -> str:
-        """Detailed description about the dashboard"""
+        """Getter: Detailed description about the dashboard"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: str) -> None:
+        """Setter: Detailed description about the dashboard"""
         self._inner_dict['description'] = value
     
     
     @property
     def charts(self) -> List[str]:
-        """Charts in a dashboard
+        """Getter: Charts in a dashboard
     Deprecated! Use chartEdges instead."""
         return self._inner_dict.get('charts')  # type: ignore
     
     @charts.setter
     def charts(self, value: List[str]) -> None:
+        """Setter: Charts in a dashboard
+    Deprecated! Use chartEdges instead."""
         self._inner_dict['charts'] = value
     
     
     @property
     def chartEdges(self) -> Union[None, List["EdgeClass"]]:
-        """Charts in a dashboard"""
+        """Getter: Charts in a dashboard"""
         return self._inner_dict.get('chartEdges')  # type: ignore
     
     @chartEdges.setter
     def chartEdges(self, value: Union[None, List["EdgeClass"]]) -> None:
+        """Setter: Charts in a dashboard"""
         self._inner_dict['chartEdges'] = value
     
     
     @property
     def datasets(self) -> List[str]:
-        """Datasets consumed by a dashboard
+        """Getter: Datasets consumed by a dashboard
     Deprecated! Use datasetEdges instead."""
         return self._inner_dict.get('datasets')  # type: ignore
     
     @datasets.setter
     def datasets(self, value: List[str]) -> None:
+        """Setter: Datasets consumed by a dashboard
+    Deprecated! Use datasetEdges instead."""
         self._inner_dict['datasets'] = value
     
     
     @property
     def datasetEdges(self) -> Union[None, List["EdgeClass"]]:
-        """Datasets consumed by a dashboard"""
+        """Getter: Datasets consumed by a dashboard"""
         return self._inner_dict.get('datasetEdges')  # type: ignore
     
     @datasetEdges.setter
     def datasetEdges(self, value: Union[None, List["EdgeClass"]]) -> None:
+        """Setter: Datasets consumed by a dashboard"""
         self._inner_dict['datasetEdges'] = value
     
     
     @property
     def lastModified(self) -> "ChangeAuditStampsClass":
-        """Captures information about who created/last modified/deleted this dashboard and when"""
+        """Getter: Captures information about who created/last modified/deleted this dashboard and when"""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "ChangeAuditStampsClass") -> None:
+        """Setter: Captures information about who created/last modified/deleted this dashboard and when"""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def dashboardUrl(self) -> Union[None, str]:
-        """URL for the dashboard. This could be used as an external link on DataHub to allow users access/view the dashboard"""
+        """Getter: URL for the dashboard. This could be used as an external link on DataHub to allow users access/view the dashboard"""
         return self._inner_dict.get('dashboardUrl')  # type: ignore
     
     @dashboardUrl.setter
     def dashboardUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL for the dashboard. This could be used as an external link on DataHub to allow users access/view the dashboard"""
         self._inner_dict['dashboardUrl'] = value
     
     
     @property
     def access(self) -> Union[None, Union[str, "AccessLevelClass"]]:
-        """Access level for the dashboard"""
+        """Getter: Access level for the dashboard"""
         return self._inner_dict.get('access')  # type: ignore
     
     @access.setter
     def access(self, value: Union[None, Union[str, "AccessLevelClass"]]) -> None:
+        """Setter: Access level for the dashboard"""
         self._inner_dict['access'] = value
     
     
     @property
     def lastRefreshed(self) -> Union[None, int]:
-        """The time when this dashboard last refreshed"""
+        """Getter: The time when this dashboard last refreshed"""
         return self._inner_dict.get('lastRefreshed')  # type: ignore
     
     @lastRefreshed.setter
     def lastRefreshed(self, value: Union[None, int]) -> None:
+        """Setter: The time when this dashboard last refreshed"""
         self._inner_dict['lastRefreshed'] = value
     
     
 class DashboardUsageStatisticsClass(_Aspect):
     """Experimental (Subject to breaking change) -- Stats corresponding to dashboard's usage.
     
     If this aspect represents the latest snapshot of the statistics about a Dashboard, the eventGranularity field should be null. 
@@ -3625,14 +4058,21 @@
         self.viewsCount = viewsCount
         self.executionsCount = executionsCount
         self.uniqueUserCount = uniqueUserCount
         self.userCounts = userCounts
         self.favoritesCount = favoritesCount
         self.lastViewedAt = lastViewedAt
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DashboardUsageStatisticsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMillis = int()
         self.eventGranularity = self.RECORD_SCHEMA.fields_dict["eventGranularity"].default
         self.partitionSpec = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["partitionSpec"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["partitionSpec"].type)
         self.messageId = self.RECORD_SCHEMA.fields_dict["messageId"].default
         self.viewsCount = self.RECORD_SCHEMA.fields_dict["viewsCount"].default
         self.executionsCount = self.RECORD_SCHEMA.fields_dict["executionsCount"].default
@@ -3640,111 +4080,123 @@
         self.userCounts = self.RECORD_SCHEMA.fields_dict["userCounts"].default
         self.favoritesCount = self.RECORD_SCHEMA.fields_dict["favoritesCount"].default
         self.lastViewedAt = self.RECORD_SCHEMA.fields_dict["lastViewedAt"].default
     
     
     @property
     def timestampMillis(self) -> int:
-        """The event timestamp field as epoch at UTC in milli seconds."""
+        """Getter: The event timestamp field as epoch at UTC in milli seconds."""
         return self._inner_dict.get('timestampMillis')  # type: ignore
     
     @timestampMillis.setter
     def timestampMillis(self, value: int) -> None:
+        """Setter: The event timestamp field as epoch at UTC in milli seconds."""
         self._inner_dict['timestampMillis'] = value
     
     
     @property
     def eventGranularity(self) -> Union[None, "TimeWindowSizeClass"]:
-        """Granularity of the event if applicable"""
+        """Getter: Granularity of the event if applicable"""
         return self._inner_dict.get('eventGranularity')  # type: ignore
     
     @eventGranularity.setter
     def eventGranularity(self, value: Union[None, "TimeWindowSizeClass"]) -> None:
+        """Setter: Granularity of the event if applicable"""
         self._inner_dict['eventGranularity'] = value
     
     
     @property
     def partitionSpec(self) -> Union["PartitionSpecClass", None]:
-        """The optional partition specification."""
+        """Getter: The optional partition specification."""
         return self._inner_dict.get('partitionSpec')  # type: ignore
     
     @partitionSpec.setter
     def partitionSpec(self, value: Union["PartitionSpecClass", None]) -> None:
+        """Setter: The optional partition specification."""
         self._inner_dict['partitionSpec'] = value
     
     
     @property
     def messageId(self) -> Union[None, str]:
-        """The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
+        """Getter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         return self._inner_dict.get('messageId')  # type: ignore
     
     @messageId.setter
     def messageId(self, value: Union[None, str]) -> None:
+        """Setter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         self._inner_dict['messageId'] = value
     
     
     @property
     def viewsCount(self) -> Union[None, int]:
-        """The total number of times dashboard has been viewed"""
+        """Getter: The total number of times dashboard has been viewed"""
         return self._inner_dict.get('viewsCount')  # type: ignore
     
     @viewsCount.setter
     def viewsCount(self, value: Union[None, int]) -> None:
+        """Setter: The total number of times dashboard has been viewed"""
         self._inner_dict['viewsCount'] = value
     
     
     @property
     def executionsCount(self) -> Union[None, int]:
-        """The total number of dashboard executions (refreshes / syncs) """
+        """Getter: The total number of dashboard executions (refreshes / syncs) """
         return self._inner_dict.get('executionsCount')  # type: ignore
     
     @executionsCount.setter
     def executionsCount(self, value: Union[None, int]) -> None:
+        """Setter: The total number of dashboard executions (refreshes / syncs) """
         self._inner_dict['executionsCount'] = value
     
     
     @property
     def uniqueUserCount(self) -> Union[None, int]:
-        """Unique user count"""
+        """Getter: Unique user count"""
         return self._inner_dict.get('uniqueUserCount')  # type: ignore
     
     @uniqueUserCount.setter
     def uniqueUserCount(self, value: Union[None, int]) -> None:
+        """Setter: Unique user count"""
         self._inner_dict['uniqueUserCount'] = value
     
     
     @property
     def userCounts(self) -> Union[None, List["DashboardUserUsageCountsClass"]]:
-        """Users within this bucket, with frequency counts"""
+        """Getter: Users within this bucket, with frequency counts"""
         return self._inner_dict.get('userCounts')  # type: ignore
     
     @userCounts.setter
     def userCounts(self, value: Union[None, List["DashboardUserUsageCountsClass"]]) -> None:
+        """Setter: Users within this bucket, with frequency counts"""
         self._inner_dict['userCounts'] = value
     
     
     @property
     def favoritesCount(self) -> Union[None, int]:
-        """The total number of times that the dashboard has been favorited """
+        """Getter: The total number of times that the dashboard has been favorited """
         return self._inner_dict.get('favoritesCount')  # type: ignore
     
     @favoritesCount.setter
     def favoritesCount(self, value: Union[None, int]) -> None:
+        """Setter: The total number of times that the dashboard has been favorited """
         self._inner_dict['favoritesCount'] = value
     
     
     @property
     def lastViewedAt(self) -> Union[None, int]:
-        """Last viewed at
+        """Getter: Last viewed at
     
     This should not be set in cases where statistics are windowed. """
         return self._inner_dict.get('lastViewedAt')  # type: ignore
     
     @lastViewedAt.setter
     def lastViewedAt(self, value: Union[None, int]) -> None:
+        """Setter: Last viewed at
+    
+    This should not be set in cases where statistics are windowed. """
         self._inner_dict['lastViewedAt'] = value
     
     
 class DashboardUserUsageCountsClass(DictWrapper):
     """Records a single user's usage counts for a given resource"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dashboard.DashboardUserUsageCounts")
@@ -3759,69 +4211,81 @@
         
         self.user = user
         self.viewsCount = viewsCount
         self.executionsCount = executionsCount
         self.usageCount = usageCount
         self.userEmail = userEmail
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DashboardUserUsageCountsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.user = str()
         self.viewsCount = self.RECORD_SCHEMA.fields_dict["viewsCount"].default
         self.executionsCount = self.RECORD_SCHEMA.fields_dict["executionsCount"].default
         self.usageCount = self.RECORD_SCHEMA.fields_dict["usageCount"].default
         self.userEmail = self.RECORD_SCHEMA.fields_dict["userEmail"].default
     
     
     @property
     def user(self) -> str:
-        """The unique id of the user."""
+        """Getter: The unique id of the user."""
         return self._inner_dict.get('user')  # type: ignore
     
     @user.setter
     def user(self, value: str) -> None:
+        """Setter: The unique id of the user."""
         self._inner_dict['user'] = value
     
     
     @property
     def viewsCount(self) -> Union[None, int]:
-        """The number of times the user has viewed the dashboard"""
+        """Getter: The number of times the user has viewed the dashboard"""
         return self._inner_dict.get('viewsCount')  # type: ignore
     
     @viewsCount.setter
     def viewsCount(self, value: Union[None, int]) -> None:
+        """Setter: The number of times the user has viewed the dashboard"""
         self._inner_dict['viewsCount'] = value
     
     
     @property
     def executionsCount(self) -> Union[None, int]:
-        """The number of times the user has executed (refreshed) the dashboard"""
+        """Getter: The number of times the user has executed (refreshed) the dashboard"""
         return self._inner_dict.get('executionsCount')  # type: ignore
     
     @executionsCount.setter
     def executionsCount(self, value: Union[None, int]) -> None:
+        """Setter: The number of times the user has executed (refreshed) the dashboard"""
         self._inner_dict['executionsCount'] = value
     
     
     @property
     def usageCount(self) -> Union[None, int]:
-        """Normalized numeric metric representing user's dashboard usage -- the number of times the user executed or viewed the dashboard. """
+        """Getter: Normalized numeric metric representing user's dashboard usage -- the number of times the user executed or viewed the dashboard. """
         return self._inner_dict.get('usageCount')  # type: ignore
     
     @usageCount.setter
     def usageCount(self, value: Union[None, int]) -> None:
+        """Setter: Normalized numeric metric representing user's dashboard usage -- the number of times the user executed or viewed the dashboard. """
         self._inner_dict['usageCount'] = value
     
     
     @property
     def userEmail(self) -> Union[None, str]:
-        """If user_email is set, we attempt to resolve the user's urn upon ingest"""
+        """Getter: If user_email is set, we attempt to resolve the user's urn upon ingest"""
         return self._inner_dict.get('userEmail')  # type: ignore
     
     @userEmail.setter
     def userEmail(self, value: Union[None, str]) -> None:
+        """Setter: If user_email is set, we attempt to resolve the user's urn upon ingest"""
         self._inner_dict['userEmail'] = value
     
     
 class EditableDashboardPropertiesClass(_Aspect):
     """Stores editable changes made to properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines"""
 
@@ -3847,58 +4311,69 @@
             # default: {'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'time': 0, 'message': None}
             self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         else:
             self.lastModified = lastModified
         self.deleted = deleted
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableDashboardPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.created = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["created"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["created"].type)
         self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         self.deleted = self.RECORD_SCHEMA.fields_dict["deleted"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def deleted(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
+        """Getter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         return self._inner_dict.get('deleted')  # type: ignore
     
     @deleted.setter
     def deleted(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         self._inner_dict['deleted'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Edited documentation of the dashboard"""
+        """Getter: Edited documentation of the dashboard"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Edited documentation of the dashboard"""
         self._inner_dict['description'] = value
     
     
 class DataFlowInfoClass(_Aspect):
     """Information about a Data processing flow"""
 
 
@@ -3925,91 +4400,105 @@
         self.externalUrl = externalUrl
         self.name = name
         self.description = description
         self.project = project
         self.created = created
         self.lastModified = lastModified
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataFlowInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.name = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.project = self.RECORD_SCHEMA.fields_dict["project"].default
         self.created = self.RECORD_SCHEMA.fields_dict["created"].default
         self.lastModified = self.RECORD_SCHEMA.fields_dict["lastModified"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def name(self) -> str:
-        """Flow name"""
+        """Getter: Flow name"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Flow name"""
         self._inner_dict['name'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Flow description"""
+        """Getter: Flow description"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Flow description"""
         self._inner_dict['description'] = value
     
     
     @property
     def project(self) -> Union[None, str]:
-        """Optional project/namespace associated with the flow"""
+        """Getter: Optional project/namespace associated with the flow"""
         return self._inner_dict.get('project')  # type: ignore
     
     @project.setter
     def project(self, value: Union[None, str]) -> None:
+        """Setter: Optional project/namespace associated with the flow"""
         self._inner_dict['project'] = value
     
     
     @property
     def created(self) -> Union[None, "TimeStampClass"]:
-        """A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
+        """Getter: A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: Union[None, "TimeStampClass"]) -> None:
+        """Setter: A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> Union[None, "TimeStampClass"]:
-        """A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
+        """Getter: A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: Union[None, "TimeStampClass"]) -> None:
+        """Setter: A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
         self._inner_dict['lastModified'] = value
     
     
 class DataJobInfoClass(_Aspect):
     """Information about a Data processing job"""
 
 
@@ -4040,114 +4529,131 @@
         self.description = description
         self.type = type
         self.flowUrn = flowUrn
         self.created = created
         self.lastModified = lastModified
         self.status = status
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataJobInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.name = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.type = AzkabanJobTypeClass.COMMAND
         self.flowUrn = self.RECORD_SCHEMA.fields_dict["flowUrn"].default
         self.created = self.RECORD_SCHEMA.fields_dict["created"].default
         self.lastModified = self.RECORD_SCHEMA.fields_dict["lastModified"].default
         self.status = self.RECORD_SCHEMA.fields_dict["status"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def name(self) -> str:
-        """Job name"""
+        """Getter: Job name"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Job name"""
         self._inner_dict['name'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Job description"""
+        """Getter: Job description"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Job description"""
         self._inner_dict['description'] = value
     
     
     @property
     def type(self) -> Union[Union[str, "AzkabanJobTypeClass"], str]:
-        """Datajob type
+        """Getter: Datajob type
     *NOTE**: AzkabanJobType is deprecated. Please use strings instead."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[Union[str, "AzkabanJobTypeClass"], str]) -> None:
+        """Setter: Datajob type
+    *NOTE**: AzkabanJobType is deprecated. Please use strings instead."""
         self._inner_dict['type'] = value
     
     
     @property
     def flowUrn(self) -> Union[None, str]:
-        """DataFlow urn that this job is part of"""
+        """Getter: DataFlow urn that this job is part of"""
         return self._inner_dict.get('flowUrn')  # type: ignore
     
     @flowUrn.setter
     def flowUrn(self, value: Union[None, str]) -> None:
+        """Setter: DataFlow urn that this job is part of"""
         self._inner_dict['flowUrn'] = value
     
     
     @property
     def created(self) -> Union[None, "TimeStampClass"]:
-        """A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
+        """Getter: A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: Union[None, "TimeStampClass"]) -> None:
+        """Setter: A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> Union[None, "TimeStampClass"]:
-        """A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
+        """Getter: A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: Union[None, "TimeStampClass"]) -> None:
+        """Setter: A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def status(self) -> Union[None, Union[str, "JobStatusClass"]]:
-        """Status of the job - Deprecated for Data Process Instance model."""
+        """Getter: Status of the job - Deprecated for Data Process Instance model."""
         return self._inner_dict.get('status')  # type: ignore
     
     @status.setter
     def status(self, value: Union[None, Union[str, "JobStatusClass"]]) -> None:
+        """Setter: Status of the job - Deprecated for Data Process Instance model."""
         self._inner_dict['status'] = value
     
     
 class DataJobInputOutputClass(_Aspect):
     """Information about the inputs and outputs of a Data processing job"""
 
 
@@ -4174,118 +4680,135 @@
         self.outputDatasetEdges = outputDatasetEdges
         self.inputDatajobs = inputDatajobs
         self.inputDatajobEdges = inputDatajobEdges
         self.inputDatasetFields = inputDatasetFields
         self.outputDatasetFields = outputDatasetFields
         self.fineGrainedLineages = fineGrainedLineages
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataJobInputOutputClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.inputDatasets = list()
         self.inputDatasetEdges = self.RECORD_SCHEMA.fields_dict["inputDatasetEdges"].default
         self.outputDatasets = list()
         self.outputDatasetEdges = self.RECORD_SCHEMA.fields_dict["outputDatasetEdges"].default
         self.inputDatajobs = self.RECORD_SCHEMA.fields_dict["inputDatajobs"].default
         self.inputDatajobEdges = self.RECORD_SCHEMA.fields_dict["inputDatajobEdges"].default
         self.inputDatasetFields = self.RECORD_SCHEMA.fields_dict["inputDatasetFields"].default
         self.outputDatasetFields = self.RECORD_SCHEMA.fields_dict["outputDatasetFields"].default
         self.fineGrainedLineages = self.RECORD_SCHEMA.fields_dict["fineGrainedLineages"].default
     
     
     @property
     def inputDatasets(self) -> List[str]:
-        """Input datasets consumed by the data job during processing
+        """Getter: Input datasets consumed by the data job during processing
     Deprecated! Use inputDatasetEdges instead."""
         return self._inner_dict.get('inputDatasets')  # type: ignore
     
     @inputDatasets.setter
     def inputDatasets(self, value: List[str]) -> None:
+        """Setter: Input datasets consumed by the data job during processing
+    Deprecated! Use inputDatasetEdges instead."""
         self._inner_dict['inputDatasets'] = value
     
     
     @property
     def inputDatasetEdges(self) -> Union[None, List["EdgeClass"]]:
-        """Input datasets consumed by the data job during processing"""
+        """Getter: Input datasets consumed by the data job during processing"""
         return self._inner_dict.get('inputDatasetEdges')  # type: ignore
     
     @inputDatasetEdges.setter
     def inputDatasetEdges(self, value: Union[None, List["EdgeClass"]]) -> None:
+        """Setter: Input datasets consumed by the data job during processing"""
         self._inner_dict['inputDatasetEdges'] = value
     
     
     @property
     def outputDatasets(self) -> List[str]:
-        """Output datasets produced by the data job during processing
+        """Getter: Output datasets produced by the data job during processing
     Deprecated! Use outputDatasetEdges instead."""
         return self._inner_dict.get('outputDatasets')  # type: ignore
     
     @outputDatasets.setter
     def outputDatasets(self, value: List[str]) -> None:
+        """Setter: Output datasets produced by the data job during processing
+    Deprecated! Use outputDatasetEdges instead."""
         self._inner_dict['outputDatasets'] = value
     
     
     @property
     def outputDatasetEdges(self) -> Union[None, List["EdgeClass"]]:
-        """Output datasets produced by the data job during processing"""
+        """Getter: Output datasets produced by the data job during processing"""
         return self._inner_dict.get('outputDatasetEdges')  # type: ignore
     
     @outputDatasetEdges.setter
     def outputDatasetEdges(self, value: Union[None, List["EdgeClass"]]) -> None:
+        """Setter: Output datasets produced by the data job during processing"""
         self._inner_dict['outputDatasetEdges'] = value
     
     
     @property
     def inputDatajobs(self) -> Union[None, List[str]]:
-        """Input datajobs that this data job depends on
+        """Getter: Input datajobs that this data job depends on
     Deprecated! Use inputDatajobEdges instead."""
         return self._inner_dict.get('inputDatajobs')  # type: ignore
     
     @inputDatajobs.setter
     def inputDatajobs(self, value: Union[None, List[str]]) -> None:
+        """Setter: Input datajobs that this data job depends on
+    Deprecated! Use inputDatajobEdges instead."""
         self._inner_dict['inputDatajobs'] = value
     
     
     @property
     def inputDatajobEdges(self) -> Union[None, List["EdgeClass"]]:
-        """Input datajobs that this data job depends on"""
+        """Getter: Input datajobs that this data job depends on"""
         return self._inner_dict.get('inputDatajobEdges')  # type: ignore
     
     @inputDatajobEdges.setter
     def inputDatajobEdges(self, value: Union[None, List["EdgeClass"]]) -> None:
+        """Setter: Input datajobs that this data job depends on"""
         self._inner_dict['inputDatajobEdges'] = value
     
     
     @property
     def inputDatasetFields(self) -> Union[None, List[str]]:
-        """Fields of the input datasets used by this job"""
+        """Getter: Fields of the input datasets used by this job"""
         return self._inner_dict.get('inputDatasetFields')  # type: ignore
     
     @inputDatasetFields.setter
     def inputDatasetFields(self, value: Union[None, List[str]]) -> None:
+        """Setter: Fields of the input datasets used by this job"""
         self._inner_dict['inputDatasetFields'] = value
     
     
     @property
     def outputDatasetFields(self) -> Union[None, List[str]]:
-        """Fields of the output datasets this job writes to"""
+        """Getter: Fields of the output datasets this job writes to"""
         return self._inner_dict.get('outputDatasetFields')  # type: ignore
     
     @outputDatasetFields.setter
     def outputDatasetFields(self, value: Union[None, List[str]]) -> None:
+        """Setter: Fields of the output datasets this job writes to"""
         self._inner_dict['outputDatasetFields'] = value
     
     
     @property
     def fineGrainedLineages(self) -> Union[None, List["FineGrainedLineageClass"]]:
-        """Fine-grained column-level lineages
-    Not currently supported in the UI
-    Use UpstreamLineage aspect for datasets to express Column Level Lineage for the UI"""
+        """Getter: Fine-grained column-level lineages"""
         return self._inner_dict.get('fineGrainedLineages')  # type: ignore
     
     @fineGrainedLineages.setter
     def fineGrainedLineages(self, value: Union[None, List["FineGrainedLineageClass"]]) -> None:
+        """Setter: Fine-grained column-level lineages"""
         self._inner_dict['fineGrainedLineages'] = value
     
     
 class EditableDataFlowPropertiesClass(_Aspect):
     """Stores editable changes made to properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines"""
 
@@ -4311,58 +4834,69 @@
             # default: {'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'time': 0, 'message': None}
             self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         else:
             self.lastModified = lastModified
         self.deleted = deleted
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableDataFlowPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.created = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["created"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["created"].type)
         self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         self.deleted = self.RECORD_SCHEMA.fields_dict["deleted"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def deleted(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
+        """Getter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         return self._inner_dict.get('deleted')  # type: ignore
     
     @deleted.setter
     def deleted(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         self._inner_dict['deleted'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Edited documentation of the data flow"""
+        """Getter: Edited documentation of the data flow"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Edited documentation of the data flow"""
         self._inner_dict['description'] = value
     
     
 class EditableDataJobPropertiesClass(_Aspect):
     """Stores editable changes made to properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines"""
 
@@ -4388,58 +4922,69 @@
             # default: {'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'time': 0, 'message': None}
             self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         else:
             self.lastModified = lastModified
         self.deleted = deleted
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableDataJobPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.created = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["created"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["created"].type)
         self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         self.deleted = self.RECORD_SCHEMA.fields_dict["deleted"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def deleted(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
+        """Getter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         return self._inner_dict.get('deleted')  # type: ignore
     
     @deleted.setter
     def deleted(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         self._inner_dict['deleted'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Edited documentation of the data job """
+        """Getter: Edited documentation of the data job """
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Edited documentation of the data job """
         self._inner_dict['description'] = value
     
     
 class JobStatusClass(object):
     """Job statuses"""
     
     
@@ -4489,58 +5034,69 @@
             self.customProperties = dict()
         else:
             self.customProperties = customProperties
         self.externalUrl = externalUrl
         self.version = version
         self.versionType = versionType
     
+    @classmethod
+    def construct_with_defaults(cls) -> "VersionInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.version = str()
         self.versionType = str()
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def version(self) -> str:
-        """The version which can indentify a job version like a commit hash or md5 hash"""
+        """Getter: The version which can indentify a job version like a commit hash or md5 hash"""
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: str) -> None:
+        """Setter: The version which can indentify a job version like a commit hash or md5 hash"""
         self._inner_dict['version'] = value
     
     
     @property
     def versionType(self) -> str:
-        """The type of the version like git hash or md5 hash"""
+        """Getter: The type of the version like git hash or md5 hash"""
         return self._inner_dict.get('versionType')  # type: ignore
     
     @versionType.setter
     def versionType(self, value: str) -> None:
+        """Setter: The type of the version like git hash or md5 hash"""
         self._inner_dict['versionType'] = value
     
     
 class AzkabanJobTypeClass(object):
     """The various types of support azkaban jobs"""
     
     
@@ -4601,115 +5157,133 @@
         self.messageId = messageId
         self.pipelineName = pipelineName
         self.platformInstanceId = platformInstanceId
         self.config = config
         self.state = state
         self.runId = runId
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatahubIngestionCheckpointClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMillis = int()
         self.eventGranularity = self.RECORD_SCHEMA.fields_dict["eventGranularity"].default
         self.partitionSpec = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["partitionSpec"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["partitionSpec"].type)
         self.messageId = self.RECORD_SCHEMA.fields_dict["messageId"].default
         self.pipelineName = str()
         self.platformInstanceId = str()
         self.config = str()
-        self.state = IngestionCheckpointStateClass._construct_with_defaults()
+        self.state = IngestionCheckpointStateClass.construct_with_defaults()
         self.runId = str()
     
     
     @property
     def timestampMillis(self) -> int:
-        """The event timestamp field as epoch at UTC in milli seconds."""
+        """Getter: The event timestamp field as epoch at UTC in milli seconds."""
         return self._inner_dict.get('timestampMillis')  # type: ignore
     
     @timestampMillis.setter
     def timestampMillis(self, value: int) -> None:
+        """Setter: The event timestamp field as epoch at UTC in milli seconds."""
         self._inner_dict['timestampMillis'] = value
     
     
     @property
     def eventGranularity(self) -> Union[None, "TimeWindowSizeClass"]:
-        """Granularity of the event if applicable"""
+        """Getter: Granularity of the event if applicable"""
         return self._inner_dict.get('eventGranularity')  # type: ignore
     
     @eventGranularity.setter
     def eventGranularity(self, value: Union[None, "TimeWindowSizeClass"]) -> None:
+        """Setter: Granularity of the event if applicable"""
         self._inner_dict['eventGranularity'] = value
     
     
     @property
     def partitionSpec(self) -> Union["PartitionSpecClass", None]:
-        """The optional partition specification."""
+        """Getter: The optional partition specification."""
         return self._inner_dict.get('partitionSpec')  # type: ignore
     
     @partitionSpec.setter
     def partitionSpec(self, value: Union["PartitionSpecClass", None]) -> None:
+        """Setter: The optional partition specification."""
         self._inner_dict['partitionSpec'] = value
     
     
     @property
     def messageId(self) -> Union[None, str]:
-        """The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
+        """Getter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         return self._inner_dict.get('messageId')  # type: ignore
     
     @messageId.setter
     def messageId(self, value: Union[None, str]) -> None:
+        """Setter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         self._inner_dict['messageId'] = value
     
     
     @property
     def pipelineName(self) -> str:
-        """The name of the pipeline that ran ingestion, a stable unique user provided identifier.
+        """Getter: The name of the pipeline that ran ingestion, a stable unique user provided identifier.
      e.g. my_snowflake1-to-datahub."""
         return self._inner_dict.get('pipelineName')  # type: ignore
     
     @pipelineName.setter
     def pipelineName(self, value: str) -> None:
+        """Setter: The name of the pipeline that ran ingestion, a stable unique user provided identifier.
+     e.g. my_snowflake1-to-datahub."""
         self._inner_dict['pipelineName'] = value
     
     
     @property
     def platformInstanceId(self) -> str:
-        """The id of the instance against which the ingestion pipeline ran.
+        """Getter: The id of the instance against which the ingestion pipeline ran.
     e.g.: Bigquery project ids, MySQL hostnames etc."""
         return self._inner_dict.get('platformInstanceId')  # type: ignore
     
     @platformInstanceId.setter
     def platformInstanceId(self, value: str) -> None:
+        """Setter: The id of the instance against which the ingestion pipeline ran.
+    e.g.: Bigquery project ids, MySQL hostnames etc."""
         self._inner_dict['platformInstanceId'] = value
     
     
     @property
     def config(self) -> str:
-        """Json-encoded string representation of the non-secret members of the config ."""
+        """Getter: Json-encoded string representation of the non-secret members of the config ."""
         return self._inner_dict.get('config')  # type: ignore
     
     @config.setter
     def config(self, value: str) -> None:
+        """Setter: Json-encoded string representation of the non-secret members of the config ."""
         self._inner_dict['config'] = value
     
     
     @property
     def state(self) -> "IngestionCheckpointStateClass":
-        """Opaque blob of the state representation."""
+        """Getter: Opaque blob of the state representation."""
         return self._inner_dict.get('state')  # type: ignore
     
     @state.setter
     def state(self, value: "IngestionCheckpointStateClass") -> None:
+        """Setter: Opaque blob of the state representation."""
         self._inner_dict['state'] = value
     
     
     @property
     def runId(self) -> str:
-        """The run identifier of this job."""
+        """Getter: The run identifier of this job."""
         return self._inner_dict.get('runId')  # type: ignore
     
     @runId.setter
     def runId(self, value: str) -> None:
+        """Setter: The run identifier of this job."""
         self._inner_dict['runId'] = value
     
     
 class DatahubIngestionRunSummaryClass(_Aspect):
     """Summary of a datahub ingestion run for a given platform."""
 
 
@@ -4779,14 +5353,21 @@
         self.softwareVersion = softwareVersion
         self.systemHostName = systemHostName
         self.operatingSystemName = operatingSystemName
         self.numProcessors = numProcessors
         self.totalMemory = totalMemory
         self.availableMemory = availableMemory
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatahubIngestionRunSummaryClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMillis = int()
         self.eventGranularity = self.RECORD_SCHEMA.fields_dict["eventGranularity"].default
         self.partitionSpec = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["partitionSpec"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["partitionSpec"].type)
         self.messageId = self.RECORD_SCHEMA.fields_dict["messageId"].default
         self.pipelineName = str()
         self.platformInstanceId = str()
@@ -4812,291 +5393,321 @@
         self.numProcessors = self.RECORD_SCHEMA.fields_dict["numProcessors"].default
         self.totalMemory = self.RECORD_SCHEMA.fields_dict["totalMemory"].default
         self.availableMemory = self.RECORD_SCHEMA.fields_dict["availableMemory"].default
     
     
     @property
     def timestampMillis(self) -> int:
-        """The event timestamp field as epoch at UTC in milli seconds."""
+        """Getter: The event timestamp field as epoch at UTC in milli seconds."""
         return self._inner_dict.get('timestampMillis')  # type: ignore
     
     @timestampMillis.setter
     def timestampMillis(self, value: int) -> None:
+        """Setter: The event timestamp field as epoch at UTC in milli seconds."""
         self._inner_dict['timestampMillis'] = value
     
     
     @property
     def eventGranularity(self) -> Union[None, "TimeWindowSizeClass"]:
-        """Granularity of the event if applicable"""
+        """Getter: Granularity of the event if applicable"""
         return self._inner_dict.get('eventGranularity')  # type: ignore
     
     @eventGranularity.setter
     def eventGranularity(self, value: Union[None, "TimeWindowSizeClass"]) -> None:
+        """Setter: Granularity of the event if applicable"""
         self._inner_dict['eventGranularity'] = value
     
     
     @property
     def partitionSpec(self) -> Union["PartitionSpecClass", None]:
-        """The optional partition specification."""
+        """Getter: The optional partition specification."""
         return self._inner_dict.get('partitionSpec')  # type: ignore
     
     @partitionSpec.setter
     def partitionSpec(self, value: Union["PartitionSpecClass", None]) -> None:
+        """Setter: The optional partition specification."""
         self._inner_dict['partitionSpec'] = value
     
     
     @property
     def messageId(self) -> Union[None, str]:
-        """The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
+        """Getter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         return self._inner_dict.get('messageId')  # type: ignore
     
     @messageId.setter
     def messageId(self, value: Union[None, str]) -> None:
+        """Setter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         self._inner_dict['messageId'] = value
     
     
     @property
     def pipelineName(self) -> str:
-        """The name of the pipeline that ran ingestion, a stable unique user provided identifier.
+        """Getter: The name of the pipeline that ran ingestion, a stable unique user provided identifier.
      e.g. my_snowflake1-to-datahub."""
         return self._inner_dict.get('pipelineName')  # type: ignore
     
     @pipelineName.setter
     def pipelineName(self, value: str) -> None:
+        """Setter: The name of the pipeline that ran ingestion, a stable unique user provided identifier.
+     e.g. my_snowflake1-to-datahub."""
         self._inner_dict['pipelineName'] = value
     
     
     @property
     def platformInstanceId(self) -> str:
-        """The id of the instance against which the ingestion pipeline ran.
+        """Getter: The id of the instance against which the ingestion pipeline ran.
     e.g.: Bigquery project ids, MySQL hostnames etc."""
         return self._inner_dict.get('platformInstanceId')  # type: ignore
     
     @platformInstanceId.setter
     def platformInstanceId(self, value: str) -> None:
+        """Setter: The id of the instance against which the ingestion pipeline ran.
+    e.g.: Bigquery project ids, MySQL hostnames etc."""
         self._inner_dict['platformInstanceId'] = value
     
     
     @property
     def runId(self) -> str:
-        """The runId for this pipeline instance."""
+        """Getter: The runId for this pipeline instance."""
         return self._inner_dict.get('runId')  # type: ignore
     
     @runId.setter
     def runId(self, value: str) -> None:
+        """Setter: The runId for this pipeline instance."""
         self._inner_dict['runId'] = value
     
     
     @property
     def runStatus(self) -> Union[str, "JobStatusClass"]:
-        """Run Status - Succeeded/Skipped/Failed etc."""
+        """Getter: Run Status - Succeeded/Skipped/Failed etc."""
         return self._inner_dict.get('runStatus')  # type: ignore
     
     @runStatus.setter
     def runStatus(self, value: Union[str, "JobStatusClass"]) -> None:
+        """Setter: Run Status - Succeeded/Skipped/Failed etc."""
         self._inner_dict['runStatus'] = value
     
     
     @property
     def numWorkUnitsCommitted(self) -> Union[None, int]:
-        """The number of workunits written to sink."""
+        """Getter: The number of workunits written to sink."""
         return self._inner_dict.get('numWorkUnitsCommitted')  # type: ignore
     
     @numWorkUnitsCommitted.setter
     def numWorkUnitsCommitted(self, value: Union[None, int]) -> None:
+        """Setter: The number of workunits written to sink."""
         self._inner_dict['numWorkUnitsCommitted'] = value
     
     
     @property
     def numWorkUnitsCreated(self) -> Union[None, int]:
-        """The number of workunits that are produced."""
+        """Getter: The number of workunits that are produced."""
         return self._inner_dict.get('numWorkUnitsCreated')  # type: ignore
     
     @numWorkUnitsCreated.setter
     def numWorkUnitsCreated(self, value: Union[None, int]) -> None:
+        """Setter: The number of workunits that are produced."""
         self._inner_dict['numWorkUnitsCreated'] = value
     
     
     @property
     def numEvents(self) -> Union[None, int]:
-        """The number of events produced (MCE + MCP)."""
+        """Getter: The number of events produced (MCE + MCP)."""
         return self._inner_dict.get('numEvents')  # type: ignore
     
     @numEvents.setter
     def numEvents(self, value: Union[None, int]) -> None:
+        """Setter: The number of events produced (MCE + MCP)."""
         self._inner_dict['numEvents'] = value
     
     
     @property
     def numEntities(self) -> Union[None, int]:
-        """The total number of entities produced (unique entity urns)."""
+        """Getter: The total number of entities produced (unique entity urns)."""
         return self._inner_dict.get('numEntities')  # type: ignore
     
     @numEntities.setter
     def numEntities(self, value: Union[None, int]) -> None:
+        """Setter: The total number of entities produced (unique entity urns)."""
         self._inner_dict['numEntities'] = value
     
     
     @property
     def numAspects(self) -> Union[None, int]:
-        """The total number of aspects produced across all entities."""
+        """Getter: The total number of aspects produced across all entities."""
         return self._inner_dict.get('numAspects')  # type: ignore
     
     @numAspects.setter
     def numAspects(self, value: Union[None, int]) -> None:
+        """Setter: The total number of aspects produced across all entities."""
         self._inner_dict['numAspects'] = value
     
     
     @property
     def numSourceAPICalls(self) -> Union[None, int]:
-        """Total number of source API calls."""
+        """Getter: Total number of source API calls."""
         return self._inner_dict.get('numSourceAPICalls')  # type: ignore
     
     @numSourceAPICalls.setter
     def numSourceAPICalls(self, value: Union[None, int]) -> None:
+        """Setter: Total number of source API calls."""
         self._inner_dict['numSourceAPICalls'] = value
     
     
     @property
     def totalLatencySourceAPICalls(self) -> Union[None, int]:
-        """Total latency across all source API calls."""
+        """Getter: Total latency across all source API calls."""
         return self._inner_dict.get('totalLatencySourceAPICalls')  # type: ignore
     
     @totalLatencySourceAPICalls.setter
     def totalLatencySourceAPICalls(self, value: Union[None, int]) -> None:
+        """Setter: Total latency across all source API calls."""
         self._inner_dict['totalLatencySourceAPICalls'] = value
     
     
     @property
     def numSinkAPICalls(self) -> Union[None, int]:
-        """Total number of sink API calls."""
+        """Getter: Total number of sink API calls."""
         return self._inner_dict.get('numSinkAPICalls')  # type: ignore
     
     @numSinkAPICalls.setter
     def numSinkAPICalls(self, value: Union[None, int]) -> None:
+        """Setter: Total number of sink API calls."""
         self._inner_dict['numSinkAPICalls'] = value
     
     
     @property
     def totalLatencySinkAPICalls(self) -> Union[None, int]:
-        """Total latency across all sink API calls."""
+        """Getter: Total latency across all sink API calls."""
         return self._inner_dict.get('totalLatencySinkAPICalls')  # type: ignore
     
     @totalLatencySinkAPICalls.setter
     def totalLatencySinkAPICalls(self, value: Union[None, int]) -> None:
+        """Setter: Total latency across all sink API calls."""
         self._inner_dict['totalLatencySinkAPICalls'] = value
     
     
     @property
     def numWarnings(self) -> Union[None, int]:
-        """Number of warnings generated."""
+        """Getter: Number of warnings generated."""
         return self._inner_dict.get('numWarnings')  # type: ignore
     
     @numWarnings.setter
     def numWarnings(self, value: Union[None, int]) -> None:
+        """Setter: Number of warnings generated."""
         self._inner_dict['numWarnings'] = value
     
     
     @property
     def numErrors(self) -> Union[None, int]:
-        """Number of errors generated."""
+        """Getter: Number of errors generated."""
         return self._inner_dict.get('numErrors')  # type: ignore
     
     @numErrors.setter
     def numErrors(self, value: Union[None, int]) -> None:
+        """Setter: Number of errors generated."""
         self._inner_dict['numErrors'] = value
     
     
     @property
     def numEntitiesSkipped(self) -> Union[None, int]:
-        """Number of entities skipped."""
+        """Getter: Number of entities skipped."""
         return self._inner_dict.get('numEntitiesSkipped')  # type: ignore
     
     @numEntitiesSkipped.setter
     def numEntitiesSkipped(self, value: Union[None, int]) -> None:
+        """Setter: Number of entities skipped."""
         self._inner_dict['numEntitiesSkipped'] = value
     
     
     @property
     def config(self) -> Union[None, str]:
-        """The non-sensitive key-value pairs of the yaml config used as json string."""
+        """Getter: The non-sensitive key-value pairs of the yaml config used as json string."""
         return self._inner_dict.get('config')  # type: ignore
     
     @config.setter
     def config(self, value: Union[None, str]) -> None:
+        """Setter: The non-sensitive key-value pairs of the yaml config used as json string."""
         self._inner_dict['config'] = value
     
     
     @property
     def custom_summary(self) -> Union[None, str]:
-        """Custom value."""
+        """Getter: Custom value."""
         return self._inner_dict.get('custom_summary')  # type: ignore
     
     @custom_summary.setter
     def custom_summary(self, value: Union[None, str]) -> None:
+        """Setter: Custom value."""
         self._inner_dict['custom_summary'] = value
     
     
     @property
     def softwareVersion(self) -> Union[None, str]:
-        """The software version of this ingestion."""
+        """Getter: The software version of this ingestion."""
         return self._inner_dict.get('softwareVersion')  # type: ignore
     
     @softwareVersion.setter
     def softwareVersion(self, value: Union[None, str]) -> None:
+        """Setter: The software version of this ingestion."""
         self._inner_dict['softwareVersion'] = value
     
     
     @property
     def systemHostName(self) -> Union[None, str]:
-        """The hostname the ingestion pipeline ran on."""
+        """Getter: The hostname the ingestion pipeline ran on."""
         return self._inner_dict.get('systemHostName')  # type: ignore
     
     @systemHostName.setter
     def systemHostName(self, value: Union[None, str]) -> None:
+        """Setter: The hostname the ingestion pipeline ran on."""
         self._inner_dict['systemHostName'] = value
     
     
     @property
     def operatingSystemName(self) -> Union[None, str]:
-        """The os the ingestion pipeline ran on."""
+        """Getter: The os the ingestion pipeline ran on."""
         return self._inner_dict.get('operatingSystemName')  # type: ignore
     
     @operatingSystemName.setter
     def operatingSystemName(self, value: Union[None, str]) -> None:
+        """Setter: The os the ingestion pipeline ran on."""
         self._inner_dict['operatingSystemName'] = value
     
     
     @property
     def numProcessors(self) -> Union[None, int]:
-        """The number of processors on the host the ingestion pipeline ran on."""
+        """Getter: The number of processors on the host the ingestion pipeline ran on."""
         return self._inner_dict.get('numProcessors')  # type: ignore
     
     @numProcessors.setter
     def numProcessors(self, value: Union[None, int]) -> None:
+        """Setter: The number of processors on the host the ingestion pipeline ran on."""
         self._inner_dict['numProcessors'] = value
     
     
     @property
     def totalMemory(self) -> Union[None, int]:
-        """The total amount of memory on the host the ingestion pipeline ran on."""
+        """Getter: The total amount of memory on the host the ingestion pipeline ran on."""
         return self._inner_dict.get('totalMemory')  # type: ignore
     
     @totalMemory.setter
     def totalMemory(self, value: Union[None, int]) -> None:
+        """Setter: The total amount of memory on the host the ingestion pipeline ran on."""
         self._inner_dict['totalMemory'] = value
     
     
     @property
     def availableMemory(self) -> Union[None, int]:
-        """The available memory on the host the ingestion pipeline ran on."""
+        """Getter: The available memory on the host the ingestion pipeline ran on."""
         return self._inner_dict.get('availableMemory')  # type: ignore
     
     @availableMemory.setter
     def availableMemory(self, value: Union[None, int]) -> None:
+        """Setter: The available memory on the host the ingestion pipeline ran on."""
         self._inner_dict['availableMemory'] = value
     
     
 class IngestionCheckpointStateClass(DictWrapper):
     """The checkpoint state object of a datahub ingestion run for a given job."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.datajob.datahub.IngestionCheckpointState")
@@ -5107,47 +5718,57 @@
     ):
         super().__init__()
         
         self.formatVersion = formatVersion
         self.serde = serde
         self.payload = payload
     
+    @classmethod
+    def construct_with_defaults(cls) -> "IngestionCheckpointStateClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.formatVersion = str()
         self.serde = str()
         self.payload = self.RECORD_SCHEMA.fields_dict["payload"].default
     
     
     @property
     def formatVersion(self) -> str:
-        """The version of the state format."""
+        """Getter: The version of the state format."""
         return self._inner_dict.get('formatVersion')  # type: ignore
     
     @formatVersion.setter
     def formatVersion(self, value: str) -> None:
+        """Setter: The version of the state format."""
         self._inner_dict['formatVersion'] = value
     
     
     @property
     def serde(self) -> str:
-        """The serialization/deserialization protocol."""
+        """Getter: The serialization/deserialization protocol."""
         return self._inner_dict.get('serde')  # type: ignore
     
     @serde.setter
     def serde(self, value: str) -> None:
+        """Setter: The serialization/deserialization protocol."""
         self._inner_dict['serde'] = value
     
     
     @property
     def payload(self) -> Union[None, bytes]:
-        """Opaque blob of the state representation."""
+        """Getter: Opaque blob of the state representation."""
         return self._inner_dict.get('payload')  # type: ignore
     
     @payload.setter
     def payload(self, value: Union[None, bytes]) -> None:
+        """Setter: Opaque blob of the state representation."""
         self._inner_dict['payload'] = value
     
     
 class DataPlatformInfoClass(_Aspect):
     """Information about a data platform"""
 
 
@@ -5166,69 +5787,81 @@
         
         self.name = name
         self.displayName = displayName
         self.type = type
         self.datasetNameDelimiter = datasetNameDelimiter
         self.logoUrl = logoUrl
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataPlatformInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.displayName = self.RECORD_SCHEMA.fields_dict["displayName"].default
         self.type = PlatformTypeClass.FILE_SYSTEM
         self.datasetNameDelimiter = str()
         self.logoUrl = self.RECORD_SCHEMA.fields_dict["logoUrl"].default
     
     
     @property
     def name(self) -> str:
-        """Name of the data platform"""
+        """Getter: Name of the data platform"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the data platform"""
         self._inner_dict['name'] = value
     
     
     @property
     def displayName(self) -> Union[None, str]:
-        """The name that will be used for displaying a platform type."""
+        """Getter: The name that will be used for displaying a platform type."""
         return self._inner_dict.get('displayName')  # type: ignore
     
     @displayName.setter
     def displayName(self, value: Union[None, str]) -> None:
+        """Setter: The name that will be used for displaying a platform type."""
         self._inner_dict['displayName'] = value
     
     
     @property
     def type(self) -> Union[str, "PlatformTypeClass"]:
-        """Platform type this data platform describes"""
+        """Getter: Platform type this data platform describes"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "PlatformTypeClass"]) -> None:
+        """Setter: Platform type this data platform describes"""
         self._inner_dict['type'] = value
     
     
     @property
     def datasetNameDelimiter(self) -> str:
-        """The delimiter in the dataset names on the data platform, e.g. '/' for HDFS and '.' for Oracle"""
+        """Getter: The delimiter in the dataset names on the data platform, e.g. '/' for HDFS and '.' for Oracle"""
         return self._inner_dict.get('datasetNameDelimiter')  # type: ignore
     
     @datasetNameDelimiter.setter
     def datasetNameDelimiter(self, value: str) -> None:
+        """Setter: The delimiter in the dataset names on the data platform, e.g. '/' for HDFS and '.' for Oracle"""
         self._inner_dict['datasetNameDelimiter'] = value
     
     
     @property
     def logoUrl(self) -> Union[None, str]:
-        """The URL for a logo associated with the platform"""
+        """Getter: The URL for a logo associated with the platform"""
         return self._inner_dict.get('logoUrl')  # type: ignore
     
     @logoUrl.setter
     def logoUrl(self, value: Union[None, str]) -> None:
+        """Setter: The URL for a logo associated with the platform"""
         self._inner_dict['logoUrl'] = value
     
     
 class PlatformTypeClass(object):
     """Platform types available at LinkedIn"""
     
     
@@ -5281,58 +5914,69 @@
             self.customProperties = dict()
         else:
             self.customProperties = customProperties
         self.externalUrl = externalUrl
         self.name = name
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataPlatformInstancePropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.name = self.RECORD_SCHEMA.fields_dict["name"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def name(self) -> Union[None, str]:
-        """Display name of the Data Platform Instance"""
+        """Getter: Display name of the Data Platform Instance"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: Union[None, str]) -> None:
+        """Setter: Display name of the Data Platform Instance"""
         self._inner_dict['name'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the Data Platform Instance"""
+        """Getter: Documentation of the Data Platform Instance"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the Data Platform Instance"""
         self._inner_dict['description'] = value
     
     
 class DataProcessInfoClass(_Aspect):
     """The inputs and outputs of this data process"""
 
 
@@ -5345,36 +5989,45 @@
         outputs: Union[None, List[str]]=None,
     ):
         super().__init__()
         
         self.inputs = inputs
         self.outputs = outputs
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataProcessInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.inputs = self.RECORD_SCHEMA.fields_dict["inputs"].default
         self.outputs = self.RECORD_SCHEMA.fields_dict["outputs"].default
     
     
     @property
     def inputs(self) -> Union[None, List[str]]:
-        """the inputs of the data process"""
+        """Getter: the inputs of the data process"""
         return self._inner_dict.get('inputs')  # type: ignore
     
     @inputs.setter
     def inputs(self, value: Union[None, List[str]]) -> None:
+        """Setter: the inputs of the data process"""
         self._inner_dict['inputs'] = value
     
     
     @property
     def outputs(self) -> Union[None, List[str]]:
-        """the outputs of the data process"""
+        """Getter: the outputs of the data process"""
         return self._inner_dict.get('outputs')  # type: ignore
     
     @outputs.setter
     def outputs(self, value: Union[None, List[str]]) -> None:
+        """Setter: the outputs of the data process"""
         self._inner_dict['outputs'] = value
     
     
 class DataProcessInstanceInputClass(_Aspect):
     """Information about the inputs datasets of a Data process"""
 
 
@@ -5385,25 +6038,33 @@
     def __init__(self,
         inputs: List[str],
     ):
         super().__init__()
         
         self.inputs = inputs
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataProcessInstanceInputClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.inputs = list()
     
     
     @property
     def inputs(self) -> List[str]:
-        """Input datasets to be consumed"""
+        """Getter: Input datasets to be consumed"""
         return self._inner_dict.get('inputs')  # type: ignore
     
     @inputs.setter
     def inputs(self, value: List[str]) -> None:
+        """Setter: Input datasets to be consumed"""
         self._inner_dict['inputs'] = value
     
     
 class DataProcessInstanceOutputClass(_Aspect):
     """Information about the outputs of a Data process"""
 
 
@@ -5414,25 +6075,33 @@
     def __init__(self,
         outputs: List[str],
     ):
         super().__init__()
         
         self.outputs = outputs
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataProcessInstanceOutputClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.outputs = list()
     
     
     @property
     def outputs(self) -> List[str]:
-        """Output datasets to be produced"""
+        """Getter: Output datasets to be produced"""
         return self._inner_dict.get('outputs')  # type: ignore
     
     @outputs.setter
     def outputs(self, value: List[str]) -> None:
+        """Setter: Output datasets to be produced"""
         self._inner_dict['outputs'] = value
     
     
 class DataProcessInstancePropertiesClass(_Aspect):
     """The inputs and outputs of this data process"""
 
 
@@ -5455,69 +6124,81 @@
         else:
             self.customProperties = customProperties
         self.externalUrl = externalUrl
         self.name = name
         self.type = type
         self.created = created
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataProcessInstancePropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.name = str()
         self.type = self.RECORD_SCHEMA.fields_dict["type"].default
-        self.created = AuditStampClass._construct_with_defaults()
+        self.created = AuditStampClass.construct_with_defaults()
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def name(self) -> str:
-        """Process name"""
+        """Getter: Process name"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Process name"""
         self._inner_dict['name'] = value
     
     
     @property
     def type(self) -> Union[None, Union[str, "DataProcessTypeClass"]]:
-        """Process type"""
+        """Getter: Process type"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[None, Union[str, "DataProcessTypeClass"]]) -> None:
+        """Setter: Process type"""
         self._inner_dict['type'] = value
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """Audit stamp containing who reported the lineage and when"""
+        """Getter: Audit stamp containing who reported the lineage and when"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp containing who reported the lineage and when"""
         self._inner_dict['created'] = value
     
     
 class DataProcessInstanceRelationshipsClass(_Aspect):
     """Information about Data process relationships"""
 
 
@@ -5532,49 +6213,61 @@
     ):
         super().__init__()
         
         self.parentTemplate = parentTemplate
         self.parentInstance = parentInstance
         self.upstreamInstances = upstreamInstances
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataProcessInstanceRelationshipsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.parentTemplate = self.RECORD_SCHEMA.fields_dict["parentTemplate"].default
         self.parentInstance = self.RECORD_SCHEMA.fields_dict["parentInstance"].default
         self.upstreamInstances = list()
     
     
     @property
     def parentTemplate(self) -> Union[None, str]:
-        """The parent entity whose run instance it is"""
+        """Getter: The parent entity whose run instance it is"""
         return self._inner_dict.get('parentTemplate')  # type: ignore
     
     @parentTemplate.setter
     def parentTemplate(self, value: Union[None, str]) -> None:
+        """Setter: The parent entity whose run instance it is"""
         self._inner_dict['parentTemplate'] = value
     
     
     @property
     def parentInstance(self) -> Union[None, str]:
-        """The parent DataProcessInstance where it belongs to.
+        """Getter: The parent DataProcessInstance where it belongs to.
     If it is a Airflow Task then it should belong to an Airflow Dag run as well
     which will be another DataProcessInstance"""
         return self._inner_dict.get('parentInstance')  # type: ignore
     
     @parentInstance.setter
     def parentInstance(self, value: Union[None, str]) -> None:
+        """Setter: The parent DataProcessInstance where it belongs to.
+    If it is a Airflow Task then it should belong to an Airflow Dag run as well
+    which will be another DataProcessInstance"""
         self._inner_dict['parentInstance'] = value
     
     
     @property
     def upstreamInstances(self) -> List[str]:
-        """Input DataProcessInstance which triggered this dataprocess instance"""
+        """Getter: Input DataProcessInstance which triggered this dataprocess instance"""
         return self._inner_dict.get('upstreamInstances')  # type: ignore
     
     @upstreamInstances.setter
     def upstreamInstances(self, value: List[str]) -> None:
+        """Setter: Input DataProcessInstance which triggered this dataprocess instance"""
         self._inner_dict['upstreamInstances'] = value
     
     
 class DataProcessInstanceRunEventClass(_Aspect):
     """An event representing the current status of data process run.
     DataProcessRunEvent should be used for reporting the status of a dataProcess' run."""
 
@@ -5605,102 +6298,117 @@
             self.partitionSpec = partitionSpec
         self.messageId = messageId
         self.externalUrl = externalUrl
         self.status = status
         self.attempt = attempt
         self.result = result
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataProcessInstanceRunEventClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMillis = int()
         self.eventGranularity = self.RECORD_SCHEMA.fields_dict["eventGranularity"].default
         self.partitionSpec = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["partitionSpec"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["partitionSpec"].type)
         self.messageId = self.RECORD_SCHEMA.fields_dict["messageId"].default
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.status = DataProcessRunStatusClass.STARTED
         self.attempt = self.RECORD_SCHEMA.fields_dict["attempt"].default
         self.result = self.RECORD_SCHEMA.fields_dict["result"].default
     
     
     @property
     def timestampMillis(self) -> int:
-        """The event timestamp field as epoch at UTC in milli seconds."""
+        """Getter: The event timestamp field as epoch at UTC in milli seconds."""
         return self._inner_dict.get('timestampMillis')  # type: ignore
     
     @timestampMillis.setter
     def timestampMillis(self, value: int) -> None:
+        """Setter: The event timestamp field as epoch at UTC in milli seconds."""
         self._inner_dict['timestampMillis'] = value
     
     
     @property
     def eventGranularity(self) -> Union[None, "TimeWindowSizeClass"]:
-        """Granularity of the event if applicable"""
+        """Getter: Granularity of the event if applicable"""
         return self._inner_dict.get('eventGranularity')  # type: ignore
     
     @eventGranularity.setter
     def eventGranularity(self, value: Union[None, "TimeWindowSizeClass"]) -> None:
+        """Setter: Granularity of the event if applicable"""
         self._inner_dict['eventGranularity'] = value
     
     
     @property
     def partitionSpec(self) -> Union["PartitionSpecClass", None]:
-        """The optional partition specification."""
+        """Getter: The optional partition specification."""
         return self._inner_dict.get('partitionSpec')  # type: ignore
     
     @partitionSpec.setter
     def partitionSpec(self, value: Union["PartitionSpecClass", None]) -> None:
+        """Setter: The optional partition specification."""
         self._inner_dict['partitionSpec'] = value
     
     
     @property
     def messageId(self) -> Union[None, str]:
-        """The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
+        """Getter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         return self._inner_dict.get('messageId')  # type: ignore
     
     @messageId.setter
     def messageId(self, value: Union[None, str]) -> None:
+        """Setter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         self._inner_dict['messageId'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def status(self) -> Union[str, "DataProcessRunStatusClass"]:
         # No docs available.
         return self._inner_dict.get('status')  # type: ignore
     
     @status.setter
     def status(self, value: Union[str, "DataProcessRunStatusClass"]) -> None:
+        # No docs available.
         self._inner_dict['status'] = value
     
     
     @property
     def attempt(self) -> Union[None, int]:
-        """Return the try number that this Instance Run is in"""
+        """Getter: Return the try number that this Instance Run is in"""
         return self._inner_dict.get('attempt')  # type: ignore
     
     @attempt.setter
     def attempt(self, value: Union[None, int]) -> None:
+        """Setter: Return the try number that this Instance Run is in"""
         self._inner_dict['attempt'] = value
     
     
     @property
     def result(self) -> Union[None, "DataProcessInstanceRunResultClass"]:
-        """The final result of the Data Processing run."""
+        """Getter: The final result of the Data Processing run."""
         return self._inner_dict.get('result')  # type: ignore
     
     @result.setter
     def result(self, value: Union[None, "DataProcessInstanceRunResultClass"]) -> None:
+        """Setter: The final result of the Data Processing run."""
         self._inner_dict['result'] = value
     
     
 class DataProcessInstanceRunResultClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataprocess.DataProcessInstanceRunResult")
@@ -5709,36 +6417,45 @@
         nativeResultType: str,
     ):
         super().__init__()
         
         self.type = type
         self.nativeResultType = nativeResultType
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataProcessInstanceRunResultClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = RunResultTypeClass.SUCCESS
         self.nativeResultType = str()
     
     
     @property
     def type(self) -> Union[str, "RunResultTypeClass"]:
-        """ The final result, e.g. SUCCESS, FAILURE, SKIPPED, or UP_FOR_RETRY."""
+        """Getter:  The final result, e.g. SUCCESS, FAILURE, SKIPPED, or UP_FOR_RETRY."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "RunResultTypeClass"]) -> None:
+        """Setter:  The final result, e.g. SUCCESS, FAILURE, SKIPPED, or UP_FOR_RETRY."""
         self._inner_dict['type'] = value
     
     
     @property
     def nativeResultType(self) -> str:
-        """It identifies the system where the native result comes from like Airflow, Azkaban, etc.."""
+        """Getter: It identifies the system where the native result comes from like Airflow, Azkaban, etc.."""
         return self._inner_dict.get('nativeResultType')  # type: ignore
     
     @nativeResultType.setter
     def nativeResultType(self, value: str) -> None:
+        """Setter: It identifies the system where the native result comes from like Airflow, Azkaban, etc.."""
         self._inner_dict['nativeResultType'] = value
     
     
 class DataProcessRunStatusClass(object):
     # No docs available.
     
     
@@ -5791,58 +6508,69 @@
         super().__init__()
         
         self.deprecated = deprecated
         self.decommissionTime = decommissionTime
         self.note = note
         self.actor = actor
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetDeprecationClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.deprecated = bool()
         self.decommissionTime = self.RECORD_SCHEMA.fields_dict["decommissionTime"].default
         self.note = str()
         self.actor = self.RECORD_SCHEMA.fields_dict["actor"].default
     
     
     @property
     def deprecated(self) -> bool:
-        """Whether the dataset is deprecated by owner."""
+        """Getter: Whether the dataset is deprecated by owner."""
         return self._inner_dict.get('deprecated')  # type: ignore
     
     @deprecated.setter
     def deprecated(self, value: bool) -> None:
+        """Setter: Whether the dataset is deprecated by owner."""
         self._inner_dict['deprecated'] = value
     
     
     @property
     def decommissionTime(self) -> Union[None, int]:
-        """The time user plan to decommission this dataset."""
+        """Getter: The time user plan to decommission this dataset."""
         return self._inner_dict.get('decommissionTime')  # type: ignore
     
     @decommissionTime.setter
     def decommissionTime(self, value: Union[None, int]) -> None:
+        """Setter: The time user plan to decommission this dataset."""
         self._inner_dict['decommissionTime'] = value
     
     
     @property
     def note(self) -> str:
-        """Additional information about the dataset deprecation plan, such as the wiki, doc, RB."""
+        """Getter: Additional information about the dataset deprecation plan, such as the wiki, doc, RB."""
         return self._inner_dict.get('note')  # type: ignore
     
     @note.setter
     def note(self, value: str) -> None:
+        """Setter: Additional information about the dataset deprecation plan, such as the wiki, doc, RB."""
         self._inner_dict['note'] = value
     
     
     @property
     def actor(self) -> Union[None, str]:
-        """The corpuser URN which will be credited for modifying this deprecation content."""
+        """Getter: The corpuser URN which will be credited for modifying this deprecation content."""
         return self._inner_dict.get('actor')  # type: ignore
     
     @actor.setter
     def actor(self, value: Union[None, str]) -> None:
+        """Setter: The corpuser URN which will be credited for modifying this deprecation content."""
         self._inner_dict['actor'] = value
     
     
 class DatasetFieldMappingClass(DictWrapper):
     """Representation of mapping between fields in source dataset to the field in destination dataset"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.DatasetFieldMapping")
@@ -5855,58 +6583,69 @@
         super().__init__()
         
         self.created = created
         self.transformation = transformation
         self.sourceFields = sourceFields
         self.destinationField = destinationField
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetFieldMappingClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
-        self.created = AuditStampClass._construct_with_defaults()
+        self.created = AuditStampClass.construct_with_defaults()
         self.transformation = TransformationTypeClass.BLACKBOX
         self.sourceFields = list()
         self.destinationField = str()
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """Audit stamp containing who reported the field mapping and when"""
+        """Getter: Audit stamp containing who reported the field mapping and when"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp containing who reported the field mapping and when"""
         self._inner_dict['created'] = value
     
     
     @property
     def transformation(self) -> Union[Union[str, "TransformationTypeClass"], "UDFTransformerClass"]:
-        """Transfomration function between the fields involved"""
+        """Getter: Transfomration function between the fields involved"""
         return self._inner_dict.get('transformation')  # type: ignore
     
     @transformation.setter
     def transformation(self, value: Union[Union[str, "TransformationTypeClass"], "UDFTransformerClass"]) -> None:
+        """Setter: Transfomration function between the fields involved"""
         self._inner_dict['transformation'] = value
     
     
     @property
     def sourceFields(self) -> List[str]:
-        """Source fields from which the fine grained lineage is derived"""
+        """Getter: Source fields from which the fine grained lineage is derived"""
         return self._inner_dict.get('sourceFields')  # type: ignore
     
     @sourceFields.setter
     def sourceFields(self, value: List[str]) -> None:
+        """Setter: Source fields from which the fine grained lineage is derived"""
         self._inner_dict['sourceFields'] = value
     
     
     @property
     def destinationField(self) -> str:
-        """Destination field which is derived from source fields"""
+        """Getter: Destination field which is derived from source fields"""
         return self._inner_dict.get('destinationField')  # type: ignore
     
     @destinationField.setter
     def destinationField(self, value: str) -> None:
+        """Setter: Destination field which is derived from source fields"""
         self._inner_dict['destinationField'] = value
     
     
 class DatasetFieldProfileClass(DictWrapper):
     """Stats corresponding to fields in a dataset"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.DatasetFieldProfile")
@@ -5939,14 +6678,21 @@
         self.median = median
         self.stdev = stdev
         self.quantiles = quantiles
         self.distinctValueFrequencies = distinctValueFrequencies
         self.histogram = histogram
         self.sampleValues = sampleValues
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetFieldProfileClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.fieldPath = str()
         self.uniqueCount = self.RECORD_SCHEMA.fields_dict["uniqueCount"].default
         self.uniqueProportion = self.RECORD_SCHEMA.fields_dict["uniqueProportion"].default
         self.nullCount = self.RECORD_SCHEMA.fields_dict["nullCount"].default
         self.nullProportion = self.RECORD_SCHEMA.fields_dict["nullProportion"].default
         self.min = self.RECORD_SCHEMA.fields_dict["min"].default
@@ -5963,144 +6709,158 @@
     @property
     def fieldPath(self) -> str:
         # No docs available.
         return self._inner_dict.get('fieldPath')  # type: ignore
     
     @fieldPath.setter
     def fieldPath(self, value: str) -> None:
+        # No docs available.
         self._inner_dict['fieldPath'] = value
     
     
     @property
     def uniqueCount(self) -> Union[None, int]:
         # No docs available.
         return self._inner_dict.get('uniqueCount')  # type: ignore
     
     @uniqueCount.setter
     def uniqueCount(self, value: Union[None, int]) -> None:
+        # No docs available.
         self._inner_dict['uniqueCount'] = value
     
     
     @property
     def uniqueProportion(self) -> Union[None, float]:
         # No docs available.
         return self._inner_dict.get('uniqueProportion')  # type: ignore
     
     @uniqueProportion.setter
     def uniqueProportion(self, value: Union[None, float]) -> None:
+        # No docs available.
         self._inner_dict['uniqueProportion'] = value
     
     
     @property
     def nullCount(self) -> Union[None, int]:
         # No docs available.
         return self._inner_dict.get('nullCount')  # type: ignore
     
     @nullCount.setter
     def nullCount(self, value: Union[None, int]) -> None:
+        # No docs available.
         self._inner_dict['nullCount'] = value
     
     
     @property
     def nullProportion(self) -> Union[None, float]:
         # No docs available.
         return self._inner_dict.get('nullProportion')  # type: ignore
     
     @nullProportion.setter
     def nullProportion(self, value: Union[None, float]) -> None:
+        # No docs available.
         self._inner_dict['nullProportion'] = value
     
     
     @property
     def min(self) -> Union[None, str]:
         # No docs available.
         return self._inner_dict.get('min')  # type: ignore
     
     @min.setter
     def min(self, value: Union[None, str]) -> None:
+        # No docs available.
         self._inner_dict['min'] = value
     
     
     @property
     def max(self) -> Union[None, str]:
         # No docs available.
         return self._inner_dict.get('max')  # type: ignore
     
     @max.setter
     def max(self, value: Union[None, str]) -> None:
+        # No docs available.
         self._inner_dict['max'] = value
     
     
     @property
     def mean(self) -> Union[None, str]:
         # No docs available.
         return self._inner_dict.get('mean')  # type: ignore
     
     @mean.setter
     def mean(self, value: Union[None, str]) -> None:
+        # No docs available.
         self._inner_dict['mean'] = value
     
     
     @property
     def median(self) -> Union[None, str]:
         # No docs available.
         return self._inner_dict.get('median')  # type: ignore
     
     @median.setter
     def median(self, value: Union[None, str]) -> None:
+        # No docs available.
         self._inner_dict['median'] = value
     
     
     @property
     def stdev(self) -> Union[None, str]:
         # No docs available.
         return self._inner_dict.get('stdev')  # type: ignore
     
     @stdev.setter
     def stdev(self, value: Union[None, str]) -> None:
+        # No docs available.
         self._inner_dict['stdev'] = value
     
     
     @property
     def quantiles(self) -> Union[None, List["QuantileClass"]]:
         # No docs available.
         return self._inner_dict.get('quantiles')  # type: ignore
     
     @quantiles.setter
     def quantiles(self, value: Union[None, List["QuantileClass"]]) -> None:
+        # No docs available.
         self._inner_dict['quantiles'] = value
     
     
     @property
     def distinctValueFrequencies(self) -> Union[None, List["ValueFrequencyClass"]]:
         # No docs available.
         return self._inner_dict.get('distinctValueFrequencies')  # type: ignore
     
     @distinctValueFrequencies.setter
     def distinctValueFrequencies(self, value: Union[None, List["ValueFrequencyClass"]]) -> None:
+        # No docs available.
         self._inner_dict['distinctValueFrequencies'] = value
     
     
     @property
     def histogram(self) -> Union[None, "HistogramClass"]:
         # No docs available.
         return self._inner_dict.get('histogram')  # type: ignore
     
     @histogram.setter
     def histogram(self, value: Union[None, "HistogramClass"]) -> None:
+        # No docs available.
         self._inner_dict['histogram'] = value
     
     
     @property
     def sampleValues(self) -> Union[None, List[str]]:
         # No docs available.
         return self._inner_dict.get('sampleValues')  # type: ignore
     
     @sampleValues.setter
     def sampleValues(self, value: Union[None, List[str]]) -> None:
+        # No docs available.
         self._inner_dict['sampleValues'] = value
     
     
 class DatasetFieldUsageCountsClass(DictWrapper):
     """Records field-level usage counts for a given dataset"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.DatasetFieldUsageCounts")
@@ -6109,36 +6869,45 @@
         count: int,
     ):
         super().__init__()
         
         self.fieldPath = fieldPath
         self.count = count
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetFieldUsageCountsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.fieldPath = str()
         self.count = int()
     
     
     @property
     def fieldPath(self) -> str:
-        """The name of the field."""
+        """Getter: The name of the field."""
         return self._inner_dict.get('fieldPath')  # type: ignore
     
     @fieldPath.setter
     def fieldPath(self, value: str) -> None:
+        """Setter: The name of the field."""
         self._inner_dict['fieldPath'] = value
     
     
     @property
     def count(self) -> int:
-        """Number of times the field has been used."""
+        """Getter: Number of times the field has been used."""
         return self._inner_dict.get('count')  # type: ignore
     
     @count.setter
     def count(self, value: int) -> None:
+        """Setter: Number of times the field has been used."""
         self._inner_dict['count'] = value
     
     
 class DatasetLineageTypeClass(object):
     """The various types of supported dataset lineage"""
     
     
@@ -6182,102 +6951,117 @@
             self.partitionSpec = partitionSpec
         self.messageId = messageId
         self.rowCount = rowCount
         self.columnCount = columnCount
         self.fieldProfiles = fieldProfiles
         self.sizeInBytes = sizeInBytes
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetProfileClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMillis = int()
         self.eventGranularity = self.RECORD_SCHEMA.fields_dict["eventGranularity"].default
         self.partitionSpec = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["partitionSpec"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["partitionSpec"].type)
         self.messageId = self.RECORD_SCHEMA.fields_dict["messageId"].default
         self.rowCount = self.RECORD_SCHEMA.fields_dict["rowCount"].default
         self.columnCount = self.RECORD_SCHEMA.fields_dict["columnCount"].default
         self.fieldProfiles = self.RECORD_SCHEMA.fields_dict["fieldProfiles"].default
         self.sizeInBytes = self.RECORD_SCHEMA.fields_dict["sizeInBytes"].default
     
     
     @property
     def timestampMillis(self) -> int:
-        """The event timestamp field as epoch at UTC in milli seconds."""
+        """Getter: The event timestamp field as epoch at UTC in milli seconds."""
         return self._inner_dict.get('timestampMillis')  # type: ignore
     
     @timestampMillis.setter
     def timestampMillis(self, value: int) -> None:
+        """Setter: The event timestamp field as epoch at UTC in milli seconds."""
         self._inner_dict['timestampMillis'] = value
     
     
     @property
     def eventGranularity(self) -> Union[None, "TimeWindowSizeClass"]:
-        """Granularity of the event if applicable"""
+        """Getter: Granularity of the event if applicable"""
         return self._inner_dict.get('eventGranularity')  # type: ignore
     
     @eventGranularity.setter
     def eventGranularity(self, value: Union[None, "TimeWindowSizeClass"]) -> None:
+        """Setter: Granularity of the event if applicable"""
         self._inner_dict['eventGranularity'] = value
     
     
     @property
     def partitionSpec(self) -> Union["PartitionSpecClass", None]:
-        """The optional partition specification."""
+        """Getter: The optional partition specification."""
         return self._inner_dict.get('partitionSpec')  # type: ignore
     
     @partitionSpec.setter
     def partitionSpec(self, value: Union["PartitionSpecClass", None]) -> None:
+        """Setter: The optional partition specification."""
         self._inner_dict['partitionSpec'] = value
     
     
     @property
     def messageId(self) -> Union[None, str]:
-        """The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
+        """Getter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         return self._inner_dict.get('messageId')  # type: ignore
     
     @messageId.setter
     def messageId(self, value: Union[None, str]) -> None:
+        """Setter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         self._inner_dict['messageId'] = value
     
     
     @property
     def rowCount(self) -> Union[None, int]:
-        """The total number of rows"""
+        """Getter: The total number of rows"""
         return self._inner_dict.get('rowCount')  # type: ignore
     
     @rowCount.setter
     def rowCount(self, value: Union[None, int]) -> None:
+        """Setter: The total number of rows"""
         self._inner_dict['rowCount'] = value
     
     
     @property
     def columnCount(self) -> Union[None, int]:
-        """The total number of columns (or schema fields)"""
+        """Getter: The total number of columns (or schema fields)"""
         return self._inner_dict.get('columnCount')  # type: ignore
     
     @columnCount.setter
     def columnCount(self, value: Union[None, int]) -> None:
+        """Setter: The total number of columns (or schema fields)"""
         self._inner_dict['columnCount'] = value
     
     
     @property
     def fieldProfiles(self) -> Union[None, List["DatasetFieldProfileClass"]]:
-        """Profiles for each column (or schema field)"""
+        """Getter: Profiles for each column (or schema field)"""
         return self._inner_dict.get('fieldProfiles')  # type: ignore
     
     @fieldProfiles.setter
     def fieldProfiles(self, value: Union[None, List["DatasetFieldProfileClass"]]) -> None:
+        """Setter: Profiles for each column (or schema field)"""
         self._inner_dict['fieldProfiles'] = value
     
     
     @property
     def sizeInBytes(self) -> Union[None, int]:
-        """Storage size in bytes"""
+        """Getter: Storage size in bytes"""
         return self._inner_dict.get('sizeInBytes')  # type: ignore
     
     @sizeInBytes.setter
     def sizeInBytes(self, value: Union[None, int]) -> None:
+        """Setter: Storage size in bytes"""
         self._inner_dict['sizeInBytes'] = value
     
     
 class DatasetPropertiesClass(_Aspect):
     """Properties associated with a Dataset"""
 
 
@@ -6312,114 +7096,131 @@
         self.lastModified = lastModified
         if tags is None:
             # default: []
             self.tags = list()
         else:
             self.tags = tags
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.name = self.RECORD_SCHEMA.fields_dict["name"].default
         self.qualifiedName = self.RECORD_SCHEMA.fields_dict["qualifiedName"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.uri = self.RECORD_SCHEMA.fields_dict["uri"].default
         self.created = self.RECORD_SCHEMA.fields_dict["created"].default
         self.lastModified = self.RECORD_SCHEMA.fields_dict["lastModified"].default
         self.tags = list()
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def name(self) -> Union[None, str]:
-        """Display name of the Dataset"""
+        """Getter: Display name of the Dataset"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: Union[None, str]) -> None:
+        """Setter: Display name of the Dataset"""
         self._inner_dict['name'] = value
     
     
     @property
     def qualifiedName(self) -> Union[None, str]:
-        """Fully-qualified name of the Dataset"""
+        """Getter: Fully-qualified name of the Dataset"""
         return self._inner_dict.get('qualifiedName')  # type: ignore
     
     @qualifiedName.setter
     def qualifiedName(self, value: Union[None, str]) -> None:
+        """Setter: Fully-qualified name of the Dataset"""
         self._inner_dict['qualifiedName'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the dataset"""
+        """Getter: Documentation of the dataset"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the dataset"""
         self._inner_dict['description'] = value
     
     
     @property
     def uri(self) -> Union[None, str]:
-        """The abstracted URI such as hdfs:///data/tracking/PageViewEvent, file:///dir/file_name. Uri should not include any environment specific properties. Some datasets might not have a standardized uri, which makes this field optional (i.e. kafka topic)."""
+        """Getter: The abstracted URI such as hdfs:///data/tracking/PageViewEvent, file:///dir/file_name. Uri should not include any environment specific properties. Some datasets might not have a standardized uri, which makes this field optional (i.e. kafka topic)."""
         return self._inner_dict.get('uri')  # type: ignore
     
     @uri.setter
     def uri(self, value: Union[None, str]) -> None:
+        """Setter: The abstracted URI such as hdfs:///data/tracking/PageViewEvent, file:///dir/file_name. Uri should not include any environment specific properties. Some datasets might not have a standardized uri, which makes this field optional (i.e. kafka topic)."""
         self._inner_dict['uri'] = value
     
     
     @property
     def created(self) -> Union[None, "TimeStampClass"]:
-        """A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
+        """Getter: A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: Union[None, "TimeStampClass"]) -> None:
+        """Setter: A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)"""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> Union[None, "TimeStampClass"]:
-        """A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
+        """Getter: A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: Union[None, "TimeStampClass"]) -> None:
+        """Setter: A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)"""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def tags(self) -> List[str]:
-        """[Legacy] Unstructured tags for the dataset. Structured tags can be applied via the `GlobalTags` aspect.
+        """Getter: [Legacy] Unstructured tags for the dataset. Structured tags can be applied via the `GlobalTags` aspect.
     This is now deprecated."""
         return self._inner_dict.get('tags')  # type: ignore
     
     @tags.setter
     def tags(self, value: List[str]) -> None:
+        """Setter: [Legacy] Unstructured tags for the dataset. Structured tags can be applied via the `GlobalTags` aspect.
+    This is now deprecated."""
         self._inner_dict['tags'] = value
     
     
 class DatasetUpstreamLineageClass(_Aspect):
     """Fine Grained upstream lineage for fields in a dataset"""
 
 
@@ -6430,25 +7231,33 @@
     def __init__(self,
         fieldMappings: List["DatasetFieldMappingClass"],
     ):
         super().__init__()
         
         self.fieldMappings = fieldMappings
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetUpstreamLineageClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.fieldMappings = list()
     
     
     @property
     def fieldMappings(self) -> List["DatasetFieldMappingClass"]:
-        """Upstream to downstream field level lineage mappings"""
+        """Getter: Upstream to downstream field level lineage mappings"""
         return self._inner_dict.get('fieldMappings')  # type: ignore
     
     @fieldMappings.setter
     def fieldMappings(self, value: List["DatasetFieldMappingClass"]) -> None:
+        """Setter: Upstream to downstream field level lineage mappings"""
         self._inner_dict['fieldMappings'] = value
     
     
 class DatasetUsageStatisticsClass(_Aspect):
     """Stats corresponding to dataset's usage."""
 
 
@@ -6480,113 +7289,129 @@
         self.messageId = messageId
         self.uniqueUserCount = uniqueUserCount
         self.totalSqlQueries = totalSqlQueries
         self.topSqlQueries = topSqlQueries
         self.userCounts = userCounts
         self.fieldCounts = fieldCounts
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetUsageStatisticsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMillis = int()
         self.eventGranularity = self.RECORD_SCHEMA.fields_dict["eventGranularity"].default
         self.partitionSpec = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["partitionSpec"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["partitionSpec"].type)
         self.messageId = self.RECORD_SCHEMA.fields_dict["messageId"].default
         self.uniqueUserCount = self.RECORD_SCHEMA.fields_dict["uniqueUserCount"].default
         self.totalSqlQueries = self.RECORD_SCHEMA.fields_dict["totalSqlQueries"].default
         self.topSqlQueries = self.RECORD_SCHEMA.fields_dict["topSqlQueries"].default
         self.userCounts = self.RECORD_SCHEMA.fields_dict["userCounts"].default
         self.fieldCounts = self.RECORD_SCHEMA.fields_dict["fieldCounts"].default
     
     
     @property
     def timestampMillis(self) -> int:
-        """The event timestamp field as epoch at UTC in milli seconds."""
+        """Getter: The event timestamp field as epoch at UTC in milli seconds."""
         return self._inner_dict.get('timestampMillis')  # type: ignore
     
     @timestampMillis.setter
     def timestampMillis(self, value: int) -> None:
+        """Setter: The event timestamp field as epoch at UTC in milli seconds."""
         self._inner_dict['timestampMillis'] = value
     
     
     @property
     def eventGranularity(self) -> Union[None, "TimeWindowSizeClass"]:
-        """Granularity of the event if applicable"""
+        """Getter: Granularity of the event if applicable"""
         return self._inner_dict.get('eventGranularity')  # type: ignore
     
     @eventGranularity.setter
     def eventGranularity(self, value: Union[None, "TimeWindowSizeClass"]) -> None:
+        """Setter: Granularity of the event if applicable"""
         self._inner_dict['eventGranularity'] = value
     
     
     @property
     def partitionSpec(self) -> Union["PartitionSpecClass", None]:
-        """The optional partition specification."""
+        """Getter: The optional partition specification."""
         return self._inner_dict.get('partitionSpec')  # type: ignore
     
     @partitionSpec.setter
     def partitionSpec(self, value: Union["PartitionSpecClass", None]) -> None:
+        """Setter: The optional partition specification."""
         self._inner_dict['partitionSpec'] = value
     
     
     @property
     def messageId(self) -> Union[None, str]:
-        """The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
+        """Getter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         return self._inner_dict.get('messageId')  # type: ignore
     
     @messageId.setter
     def messageId(self, value: Union[None, str]) -> None:
+        """Setter: The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value."""
         self._inner_dict['messageId'] = value
     
     
     @property
     def uniqueUserCount(self) -> Union[None, int]:
-        """Unique user count"""
+        """Getter: Unique user count"""
         return self._inner_dict.get('uniqueUserCount')  # type: ignore
     
     @uniqueUserCount.setter
     def uniqueUserCount(self, value: Union[None, int]) -> None:
+        """Setter: Unique user count"""
         self._inner_dict['uniqueUserCount'] = value
     
     
     @property
     def totalSqlQueries(self) -> Union[None, int]:
-        """Total SQL query count"""
+        """Getter: Total SQL query count"""
         return self._inner_dict.get('totalSqlQueries')  # type: ignore
     
     @totalSqlQueries.setter
     def totalSqlQueries(self, value: Union[None, int]) -> None:
+        """Setter: Total SQL query count"""
         self._inner_dict['totalSqlQueries'] = value
     
     
     @property
     def topSqlQueries(self) -> Union[None, List[str]]:
-        """Frequent SQL queries; mostly makes sense for datasets in SQL databases"""
+        """Getter: Frequent SQL queries; mostly makes sense for datasets in SQL databases"""
         return self._inner_dict.get('topSqlQueries')  # type: ignore
     
     @topSqlQueries.setter
     def topSqlQueries(self, value: Union[None, List[str]]) -> None:
+        """Setter: Frequent SQL queries; mostly makes sense for datasets in SQL databases"""
         self._inner_dict['topSqlQueries'] = value
     
     
     @property
     def userCounts(self) -> Union[None, List["DatasetUserUsageCountsClass"]]:
-        """Users within this bucket, with frequency counts"""
+        """Getter: Users within this bucket, with frequency counts"""
         return self._inner_dict.get('userCounts')  # type: ignore
     
     @userCounts.setter
     def userCounts(self, value: Union[None, List["DatasetUserUsageCountsClass"]]) -> None:
+        """Setter: Users within this bucket, with frequency counts"""
         self._inner_dict['userCounts'] = value
     
     
     @property
     def fieldCounts(self) -> Union[None, List["DatasetFieldUsageCountsClass"]]:
-        """Field-level usage stats"""
+        """Getter: Field-level usage stats"""
         return self._inner_dict.get('fieldCounts')  # type: ignore
     
     @fieldCounts.setter
     def fieldCounts(self, value: Union[None, List["DatasetFieldUsageCountsClass"]]) -> None:
+        """Setter: Field-level usage stats"""
         self._inner_dict['fieldCounts'] = value
     
     
 class DatasetUserUsageCountsClass(DictWrapper):
     """Records a single user's usage counts for a given resource"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.DatasetUserUsageCounts")
@@ -6597,47 +7422,57 @@
     ):
         super().__init__()
         
         self.user = user
         self.count = count
         self.userEmail = userEmail
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetUserUsageCountsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.user = str()
         self.count = int()
         self.userEmail = self.RECORD_SCHEMA.fields_dict["userEmail"].default
     
     
     @property
     def user(self) -> str:
-        """The unique id of the user."""
+        """Getter: The unique id of the user."""
         return self._inner_dict.get('user')  # type: ignore
     
     @user.setter
     def user(self, value: str) -> None:
+        """Setter: The unique id of the user."""
         self._inner_dict['user'] = value
     
     
     @property
     def count(self) -> int:
-        """Number of times the dataset has been used by the user."""
+        """Getter: Number of times the dataset has been used by the user."""
         return self._inner_dict.get('count')  # type: ignore
     
     @count.setter
     def count(self, value: int) -> None:
+        """Setter: Number of times the dataset has been used by the user."""
         self._inner_dict['count'] = value
     
     
     @property
     def userEmail(self) -> Union[None, str]:
-        """If user_email is set, we attempt to resolve the user's urn upon ingest"""
+        """Getter: If user_email is set, we attempt to resolve the user's urn upon ingest"""
         return self._inner_dict.get('userEmail')  # type: ignore
     
     @userEmail.setter
     def userEmail(self, value: Union[None, str]) -> None:
+        """Setter: If user_email is set, we attempt to resolve the user's urn upon ingest"""
         self._inner_dict['userEmail'] = value
     
     
 class EditableDatasetPropertiesClass(_Aspect):
     """EditableDatasetProperties stores editable changes made to dataset properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines"""
 
@@ -6663,58 +7498,69 @@
             # default: {'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'time': 0, 'message': None}
             self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         else:
             self.lastModified = lastModified
         self.deleted = deleted
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableDatasetPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.created = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["created"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["created"].type)
         self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         self.deleted = self.RECORD_SCHEMA.fields_dict["deleted"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def deleted(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
+        """Getter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         return self._inner_dict.get('deleted')  # type: ignore
     
     @deleted.setter
     def deleted(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         self._inner_dict['deleted'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the dataset"""
+        """Getter: Documentation of the dataset"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the dataset"""
         self._inner_dict['description'] = value
     
     
 class FineGrainedLineageClass(DictWrapper):
     """A fine-grained lineage from upstream fields/datasets to downstream field(s)"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.FineGrainedLineage")
@@ -6735,80 +7581,93 @@
         self.transformOperation = transformOperation
         if confidenceScore is None:
             # default: 1.0
             self.confidenceScore = self.RECORD_SCHEMA.fields_dict["confidenceScore"].default
         else:
             self.confidenceScore = confidenceScore
     
+    @classmethod
+    def construct_with_defaults(cls) -> "FineGrainedLineageClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.upstreamType = FineGrainedLineageUpstreamTypeClass.FIELD_SET
         self.upstreams = self.RECORD_SCHEMA.fields_dict["upstreams"].default
         self.downstreamType = FineGrainedLineageDownstreamTypeClass.FIELD
         self.downstreams = self.RECORD_SCHEMA.fields_dict["downstreams"].default
         self.transformOperation = self.RECORD_SCHEMA.fields_dict["transformOperation"].default
         self.confidenceScore = self.RECORD_SCHEMA.fields_dict["confidenceScore"].default
     
     
     @property
     def upstreamType(self) -> Union[str, "FineGrainedLineageUpstreamTypeClass"]:
-        """The type of upstream entity"""
+        """Getter: The type of upstream entity"""
         return self._inner_dict.get('upstreamType')  # type: ignore
     
     @upstreamType.setter
     def upstreamType(self, value: Union[str, "FineGrainedLineageUpstreamTypeClass"]) -> None:
+        """Setter: The type of upstream entity"""
         self._inner_dict['upstreamType'] = value
     
     
     @property
     def upstreams(self) -> Union[None, List[str]]:
-        """Upstream entities in the lineage"""
+        """Getter: Upstream entities in the lineage"""
         return self._inner_dict.get('upstreams')  # type: ignore
     
     @upstreams.setter
     def upstreams(self, value: Union[None, List[str]]) -> None:
+        """Setter: Upstream entities in the lineage"""
         self._inner_dict['upstreams'] = value
     
     
     @property
     def downstreamType(self) -> Union[str, "FineGrainedLineageDownstreamTypeClass"]:
-        """The type of downstream field(s)"""
+        """Getter: The type of downstream field(s)"""
         return self._inner_dict.get('downstreamType')  # type: ignore
     
     @downstreamType.setter
     def downstreamType(self, value: Union[str, "FineGrainedLineageDownstreamTypeClass"]) -> None:
+        """Setter: The type of downstream field(s)"""
         self._inner_dict['downstreamType'] = value
     
     
     @property
     def downstreams(self) -> Union[None, List[str]]:
-        """Downstream fields in the lineage"""
+        """Getter: Downstream fields in the lineage"""
         return self._inner_dict.get('downstreams')  # type: ignore
     
     @downstreams.setter
     def downstreams(self, value: Union[None, List[str]]) -> None:
+        """Setter: Downstream fields in the lineage"""
         self._inner_dict['downstreams'] = value
     
     
     @property
     def transformOperation(self) -> Union[None, str]:
-        """The transform operation applied to the upstream entities to produce the downstream field(s)"""
+        """Getter: The transform operation applied to the upstream entities to produce the downstream field(s)"""
         return self._inner_dict.get('transformOperation')  # type: ignore
     
     @transformOperation.setter
     def transformOperation(self, value: Union[None, str]) -> None:
+        """Setter: The transform operation applied to the upstream entities to produce the downstream field(s)"""
         self._inner_dict['transformOperation'] = value
     
     
     @property
     def confidenceScore(self) -> float:
-        """The confidence in this lineage between 0 (low confidence) and 1 (high confidence)"""
+        """Getter: The confidence in this lineage between 0 (low confidence) and 1 (high confidence)"""
         return self._inner_dict.get('confidenceScore')  # type: ignore
     
     @confidenceScore.setter
     def confidenceScore(self, value: float) -> None:
+        """Setter: The confidence in this lineage between 0 (low confidence) and 1 (high confidence)"""
         self._inner_dict['confidenceScore'] = value
     
     
 class FineGrainedLineageDownstreamTypeClass(object):
     """The type of downstream field(s) in a fine-grained lineage"""
     
     
@@ -6842,36 +7701,45 @@
         heights: List[float],
     ):
         super().__init__()
         
         self.boundaries = boundaries
         self.heights = heights
     
+    @classmethod
+    def construct_with_defaults(cls) -> "HistogramClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.boundaries = list()
         self.heights = list()
     
     
     @property
     def boundaries(self) -> List[str]:
         # No docs available.
         return self._inner_dict.get('boundaries')  # type: ignore
     
     @boundaries.setter
     def boundaries(self, value: List[str]) -> None:
+        # No docs available.
         self._inner_dict['boundaries'] = value
     
     
     @property
     def heights(self) -> List[float]:
         # No docs available.
         return self._inner_dict.get('heights')  # type: ignore
     
     @heights.setter
     def heights(self, value: List[float]) -> None:
+        # No docs available.
         self._inner_dict['heights'] = value
     
     
 class QuantileClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.Quantile")
@@ -6880,36 +7748,45 @@
         value: str,
     ):
         super().__init__()
         
         self.quantile = quantile
         self.value = value
     
+    @classmethod
+    def construct_with_defaults(cls) -> "QuantileClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.quantile = str()
         self.value = str()
     
     
     @property
     def quantile(self) -> str:
         # No docs available.
         return self._inner_dict.get('quantile')  # type: ignore
     
     @quantile.setter
     def quantile(self, value: str) -> None:
+        # No docs available.
         self._inner_dict['quantile'] = value
     
     
     @property
     def value(self) -> str:
         # No docs available.
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: str) -> None:
+        # No docs available.
         self._inner_dict['value'] = value
     
     
 class UpstreamClass(DictWrapper):
     """Upstream lineage information about a dataset including the source reporting the lineage"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.Upstream")
@@ -6928,69 +7805,81 @@
         else:
             self.auditStamp = auditStamp
         self.created = created
         self.dataset = dataset
         self.type = type
         self.properties = properties
     
+    @classmethod
+    def construct_with_defaults(cls) -> "UpstreamClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.auditStamp = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["auditStamp"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["auditStamp"].type)
         self.created = self.RECORD_SCHEMA.fields_dict["created"].default
         self.dataset = str()
         self.type = DatasetLineageTypeClass.COPY
         self.properties = self.RECORD_SCHEMA.fields_dict["properties"].default
     
     
     @property
     def auditStamp(self) -> "AuditStampClass":
-        """Audit stamp containing who reported the lineage and when."""
+        """Getter: Audit stamp containing who reported the lineage and when."""
         return self._inner_dict.get('auditStamp')  # type: ignore
     
     @auditStamp.setter
     def auditStamp(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp containing who reported the lineage and when."""
         self._inner_dict['auditStamp'] = value
     
     
     @property
     def created(self) -> Union[None, "AuditStampClass"]:
-        """Audit stamp containing who created the lineage and when."""
+        """Getter: Audit stamp containing who created the lineage and when."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: Audit stamp containing who created the lineage and when."""
         self._inner_dict['created'] = value
     
     
     @property
     def dataset(self) -> str:
-        """The upstream dataset the lineage points to"""
+        """Getter: The upstream dataset the lineage points to"""
         return self._inner_dict.get('dataset')  # type: ignore
     
     @dataset.setter
     def dataset(self, value: str) -> None:
+        """Setter: The upstream dataset the lineage points to"""
         self._inner_dict['dataset'] = value
     
     
     @property
     def type(self) -> Union[str, "DatasetLineageTypeClass"]:
-        """The type of the lineage"""
+        """Getter: The type of the lineage"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "DatasetLineageTypeClass"]) -> None:
+        """Setter: The type of the lineage"""
         self._inner_dict['type'] = value
     
     
     @property
     def properties(self) -> Union[None, Dict[str, str]]:
-        """A generic properties bag that allows us to store specific information on this graph edge."""
+        """Getter: A generic properties bag that allows us to store specific information on this graph edge."""
         return self._inner_dict.get('properties')  # type: ignore
     
     @properties.setter
     def properties(self, value: Union[None, Dict[str, str]]) -> None:
+        """Setter: A generic properties bag that allows us to store specific information on this graph edge."""
         self._inner_dict['properties'] = value
     
     
 class UpstreamLineageClass(_Aspect):
     """Upstream lineage of a dataset"""
 
 
@@ -7003,36 +7892,45 @@
         fineGrainedLineages: Union[None, List["FineGrainedLineageClass"]]=None,
     ):
         super().__init__()
         
         self.upstreams = upstreams
         self.fineGrainedLineages = fineGrainedLineages
     
+    @classmethod
+    def construct_with_defaults(cls) -> "UpstreamLineageClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.upstreams = list()
         self.fineGrainedLineages = self.RECORD_SCHEMA.fields_dict["fineGrainedLineages"].default
     
     
     @property
     def upstreams(self) -> List["UpstreamClass"]:
-        """List of upstream dataset lineage information"""
+        """Getter: List of upstream dataset lineage information"""
         return self._inner_dict.get('upstreams')  # type: ignore
     
     @upstreams.setter
     def upstreams(self, value: List["UpstreamClass"]) -> None:
+        """Setter: List of upstream dataset lineage information"""
         self._inner_dict['upstreams'] = value
     
     
     @property
     def fineGrainedLineages(self) -> Union[None, List["FineGrainedLineageClass"]]:
-        """ List of fine-grained lineage information, including field-level lineage"""
+        """Getter:  List of fine-grained lineage information, including field-level lineage"""
         return self._inner_dict.get('fineGrainedLineages')  # type: ignore
     
     @fineGrainedLineages.setter
     def fineGrainedLineages(self, value: Union[None, List["FineGrainedLineageClass"]]) -> None:
+        """Setter:  List of fine-grained lineage information, including field-level lineage"""
         self._inner_dict['fineGrainedLineages'] = value
     
     
 class ValueFrequencyClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.ValueFrequency")
@@ -7041,36 +7939,45 @@
         frequency: int,
     ):
         super().__init__()
         
         self.value = value
         self.frequency = frequency
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ValueFrequencyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.value = str()
         self.frequency = int()
     
     
     @property
     def value(self) -> str:
         # No docs available.
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: str) -> None:
+        # No docs available.
         self._inner_dict['value'] = value
     
     
     @property
     def frequency(self) -> int:
         # No docs available.
         return self._inner_dict.get('frequency')  # type: ignore
     
     @frequency.setter
     def frequency(self, value: int) -> None:
+        # No docs available.
         self._inner_dict['frequency'] = value
     
     
 class ViewPropertiesClass(_Aspect):
     """Details about a View. 
     e.g. Gets activated when subTypes is view"""
 
@@ -7086,47 +7993,57 @@
     ):
         super().__init__()
         
         self.materialized = materialized
         self.viewLogic = viewLogic
         self.viewLanguage = viewLanguage
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ViewPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.materialized = bool()
         self.viewLogic = str()
         self.viewLanguage = str()
     
     
     @property
     def materialized(self) -> bool:
-        """Whether the view is materialized"""
+        """Getter: Whether the view is materialized"""
         return self._inner_dict.get('materialized')  # type: ignore
     
     @materialized.setter
     def materialized(self, value: bool) -> None:
+        """Setter: Whether the view is materialized"""
         self._inner_dict['materialized'] = value
     
     
     @property
     def viewLogic(self) -> str:
-        """The view logic"""
+        """Getter: The view logic"""
         return self._inner_dict.get('viewLogic')  # type: ignore
     
     @viewLogic.setter
     def viewLogic(self, value: str) -> None:
+        """Setter: The view logic"""
         self._inner_dict['viewLogic'] = value
     
     
     @property
     def viewLanguage(self) -> str:
-        """The view logic language / dialect"""
+        """Getter: The view logic language / dialect"""
         return self._inner_dict.get('viewLanguage')  # type: ignore
     
     @viewLanguage.setter
     def viewLanguage(self, value: str) -> None:
+        """Setter: The view logic language / dialect"""
         self._inner_dict['viewLanguage'] = value
     
     
 class DomainPropertiesClass(_Aspect):
     """Information about a Domain"""
 
 
@@ -7141,47 +8058,57 @@
     ):
         super().__init__()
         
         self.name = name
         self.description = description
         self.created = created
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DomainPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.created = self.RECORD_SCHEMA.fields_dict["created"].default
     
     
     @property
     def name(self) -> str:
-        """Display name of the Domain"""
+        """Getter: Display name of the Domain"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Display name of the Domain"""
         self._inner_dict['name'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Description of the Domain"""
+        """Getter: Description of the Domain"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Description of the Domain"""
         self._inner_dict['description'] = value
     
     
     @property
     def created(self) -> Union[None, "AuditStampClass"]:
-        """Created Audit stamp"""
+        """Getter: Created Audit stamp"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: Created Audit stamp"""
         self._inner_dict['created'] = value
     
     
 class DomainsClass(_Aspect):
     """Links from an Asset to its Domains"""
 
 
@@ -7192,25 +8119,33 @@
     def __init__(self,
         domains: List[str],
     ):
         super().__init__()
         
         self.domains = domains
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DomainsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.domains = list()
     
     
     @property
     def domains(self) -> List[str]:
-        """The Domains attached to an Asset"""
+        """Getter: The Domains attached to an Asset"""
         return self._inner_dict.get('domains')  # type: ignore
     
     @domains.setter
     def domains(self, value: List[str]) -> None:
+        """Setter: The Domains attached to an Asset"""
         self._inner_dict['domains'] = value
     
     
 class ChangeTypeClass(object):
     """Descriptor for a change action"""
     
     
@@ -7257,69 +8192,81 @@
         
         self.task = task
         self.args = args
         self.executorId = executorId
         self.source = source
         self.requestedAt = requestedAt
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ExecutionRequestInputClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.task = str()
         self.args = dict()
         self.executorId = str()
-        self.source = ExecutionRequestSourceClass._construct_with_defaults()
+        self.source = ExecutionRequestSourceClass.construct_with_defaults()
         self.requestedAt = int()
     
     
     @property
     def task(self) -> str:
-        """The name of the task to execute, for example RUN_INGEST"""
+        """Getter: The name of the task to execute, for example RUN_INGEST"""
         return self._inner_dict.get('task')  # type: ignore
     
     @task.setter
     def task(self, value: str) -> None:
+        """Setter: The name of the task to execute, for example RUN_INGEST"""
         self._inner_dict['task'] = value
     
     
     @property
     def args(self) -> Dict[str, str]:
-        """Arguments provided to the task"""
+        """Getter: Arguments provided to the task"""
         return self._inner_dict.get('args')  # type: ignore
     
     @args.setter
     def args(self, value: Dict[str, str]) -> None:
+        """Setter: Arguments provided to the task"""
         self._inner_dict['args'] = value
     
     
     @property
     def executorId(self) -> str:
-        """Advanced: specify a specific executor to route the request to. If none is provided, a "default" executor is used."""
+        """Getter: Advanced: specify a specific executor to route the request to. If none is provided, a "default" executor is used."""
         return self._inner_dict.get('executorId')  # type: ignore
     
     @executorId.setter
     def executorId(self, value: str) -> None:
+        """Setter: Advanced: specify a specific executor to route the request to. If none is provided, a "default" executor is used."""
         self._inner_dict['executorId'] = value
     
     
     @property
     def source(self) -> "ExecutionRequestSourceClass":
-        """Source which created the execution request"""
+        """Getter: Source which created the execution request"""
         return self._inner_dict.get('source')  # type: ignore
     
     @source.setter
     def source(self, value: "ExecutionRequestSourceClass") -> None:
+        """Setter: Source which created the execution request"""
         self._inner_dict['source'] = value
     
     
     @property
     def requestedAt(self) -> int:
-        """Time at which the execution request input was created"""
+        """Getter: Time at which the execution request input was created"""
         return self._inner_dict.get('requestedAt')  # type: ignore
     
     @requestedAt.setter
     def requestedAt(self, value: int) -> None:
+        """Setter: Time at which the execution request input was created"""
         self._inner_dict['requestedAt'] = value
     
     
 class ExecutionRequestResultClass(_Aspect):
     """The result of an execution request"""
 
 
@@ -7338,69 +8285,81 @@
         
         self.status = status
         self.report = report
         self.structuredReport = structuredReport
         self.startTimeMs = startTimeMs
         self.durationMs = durationMs
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ExecutionRequestResultClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.status = str()
         self.report = self.RECORD_SCHEMA.fields_dict["report"].default
         self.structuredReport = self.RECORD_SCHEMA.fields_dict["structuredReport"].default
         self.startTimeMs = self.RECORD_SCHEMA.fields_dict["startTimeMs"].default
         self.durationMs = self.RECORD_SCHEMA.fields_dict["durationMs"].default
     
     
     @property
     def status(self) -> str:
-        """The status of the execution request"""
+        """Getter: The status of the execution request"""
         return self._inner_dict.get('status')  # type: ignore
     
     @status.setter
     def status(self, value: str) -> None:
+        """Setter: The status of the execution request"""
         self._inner_dict['status'] = value
     
     
     @property
     def report(self) -> Union[None, str]:
-        """The pretty-printed execution report."""
+        """Getter: The pretty-printed execution report."""
         return self._inner_dict.get('report')  # type: ignore
     
     @report.setter
     def report(self, value: Union[None, str]) -> None:
+        """Setter: The pretty-printed execution report."""
         self._inner_dict['report'] = value
     
     
     @property
     def structuredReport(self) -> Union[None, "StructuredExecutionReportClass"]:
-        """A structured report if available."""
+        """Getter: A structured report if available."""
         return self._inner_dict.get('structuredReport')  # type: ignore
     
     @structuredReport.setter
     def structuredReport(self, value: Union[None, "StructuredExecutionReportClass"]) -> None:
+        """Setter: A structured report if available."""
         self._inner_dict['structuredReport'] = value
     
     
     @property
     def startTimeMs(self) -> Union[None, int]:
-        """Time at which the request was created"""
+        """Getter: Time at which the request was created"""
         return self._inner_dict.get('startTimeMs')  # type: ignore
     
     @startTimeMs.setter
     def startTimeMs(self, value: Union[None, int]) -> None:
+        """Setter: Time at which the request was created"""
         self._inner_dict['startTimeMs'] = value
     
     
     @property
     def durationMs(self) -> Union[None, int]:
-        """Duration in milliseconds"""
+        """Getter: Duration in milliseconds"""
         return self._inner_dict.get('durationMs')  # type: ignore
     
     @durationMs.setter
     def durationMs(self, value: Union[None, int]) -> None:
+        """Setter: Duration in milliseconds"""
         self._inner_dict['durationMs'] = value
     
     
 class ExecutionRequestSignalClass(_Aspect):
     """An signal sent to a running execution request"""
 
 
@@ -7415,47 +8374,57 @@
     ):
         super().__init__()
         
         self.signal = signal
         self.executorId = executorId
         self.createdAt = createdAt
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ExecutionRequestSignalClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.signal = str()
         self.executorId = self.RECORD_SCHEMA.fields_dict["executorId"].default
-        self.createdAt = AuditStampClass._construct_with_defaults()
+        self.createdAt = AuditStampClass.construct_with_defaults()
     
     
     @property
     def signal(self) -> str:
-        """The signal to issue, e.g. KILL"""
+        """Getter: The signal to issue, e.g. KILL"""
         return self._inner_dict.get('signal')  # type: ignore
     
     @signal.setter
     def signal(self, value: str) -> None:
+        """Setter: The signal to issue, e.g. KILL"""
         self._inner_dict['signal'] = value
     
     
     @property
     def executorId(self) -> Union[None, str]:
-        """Advanced: specify a specific executor to route the request to. If none is provided, a "default" executor is used."""
+        """Getter: Advanced: specify a specific executor to route the request to. If none is provided, a "default" executor is used."""
         return self._inner_dict.get('executorId')  # type: ignore
     
     @executorId.setter
     def executorId(self, value: Union[None, str]) -> None:
+        """Setter: Advanced: specify a specific executor to route the request to. If none is provided, a "default" executor is used."""
         self._inner_dict['executorId'] = value
     
     
     @property
     def createdAt(self) -> "AuditStampClass":
-        """Audit Stamp"""
+        """Getter: Audit Stamp"""
         return self._inner_dict.get('createdAt')  # type: ignore
     
     @createdAt.setter
     def createdAt(self, value: "AuditStampClass") -> None:
+        """Setter: Audit Stamp"""
         self._inner_dict['createdAt'] = value
     
     
 class ExecutionRequestSourceClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.execution.ExecutionRequestSource")
@@ -7464,36 +8433,45 @@
         ingestionSource: Union[None, str]=None,
     ):
         super().__init__()
         
         self.type = type
         self.ingestionSource = ingestionSource
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ExecutionRequestSourceClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = str()
         self.ingestionSource = self.RECORD_SCHEMA.fields_dict["ingestionSource"].default
     
     
     @property
     def type(self) -> str:
-        """The type of the execution request source, e.g. INGESTION_SOURCE"""
+        """Getter: The type of the execution request source, e.g. INGESTION_SOURCE"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: str) -> None:
+        """Setter: The type of the execution request source, e.g. INGESTION_SOURCE"""
         self._inner_dict['type'] = value
     
     
     @property
     def ingestionSource(self) -> Union[None, str]:
-        """The urn of the ingestion source associated with the ingestion request. Present if type is INGESTION_SOURCE"""
+        """Getter: The urn of the ingestion source associated with the ingestion request. Present if type is INGESTION_SOURCE"""
         return self._inner_dict.get('ingestionSource')  # type: ignore
     
     @ingestionSource.setter
     def ingestionSource(self, value: Union[None, str]) -> None:
+        """Setter: The urn of the ingestion source associated with the ingestion request. Present if type is INGESTION_SOURCE"""
         self._inner_dict['ingestionSource'] = value
     
     
 class StructuredExecutionReportClass(DictWrapper):
     """A flexible carrier for structured results of an execution request.
     The goal is to allow for free flow of structured responses from execution tasks to the orchestrator or observer.
     The full spectrum of different execution report types is not intended to be modeled by this object."""
@@ -7506,47 +8484,57 @@
     ):
         super().__init__()
         
         self.type = type
         self.serializedValue = serializedValue
         self.contentType = contentType
     
+    @classmethod
+    def construct_with_defaults(cls) -> "StructuredExecutionReportClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = str()
         self.serializedValue = str()
         self.contentType = str()
     
     
     @property
     def type(self) -> str:
-        """The type of the structured report. (e.g. INGESTION_REPORT, TEST_CONNECTION_REPORT, etc.)"""
+        """Getter: The type of the structured report. (e.g. INGESTION_REPORT, TEST_CONNECTION_REPORT, etc.)"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: str) -> None:
+        """Setter: The type of the structured report. (e.g. INGESTION_REPORT, TEST_CONNECTION_REPORT, etc.)"""
         self._inner_dict['type'] = value
     
     
     @property
     def serializedValue(self) -> str:
-        """The serialized value of the structured report"""
+        """Getter: The serialized value of the structured report"""
         return self._inner_dict.get('serializedValue')  # type: ignore
     
     @serializedValue.setter
     def serializedValue(self, value: str) -> None:
+        """Setter: The serialized value of the structured report"""
         self._inner_dict['serializedValue'] = value
     
     
     @property
     def contentType(self) -> str:
-        """The content-type of the serialized value (e.g. application/json, application/json;gzip etc.)"""
+        """Getter: The content-type of the serialized value (e.g. application/json, application/json;gzip etc.)"""
         return self._inner_dict.get('contentType')  # type: ignore
     
     @contentType.setter
     def contentType(self, value: str) -> None:
+        """Setter: The content-type of the serialized value (e.g. application/json, application/json;gzip etc.)"""
         self._inner_dict['contentType'] = value
     
     
 class GlossaryNodeInfoClass(_Aspect):
     """Properties associated with a GlossaryNode"""
 
 
@@ -7563,58 +8551,69 @@
         super().__init__()
         
         self.definition = definition
         self.parentNode = parentNode
         self.name = name
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlossaryNodeInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.definition = str()
         self.parentNode = self.RECORD_SCHEMA.fields_dict["parentNode"].default
         self.name = self.RECORD_SCHEMA.fields_dict["name"].default
         self.id = self.RECORD_SCHEMA.fields_dict["id"].default
     
     
     @property
     def definition(self) -> str:
-        """Definition of business node"""
+        """Getter: Definition of business node"""
         return self._inner_dict.get('definition')  # type: ignore
     
     @definition.setter
     def definition(self, value: str) -> None:
+        """Setter: Definition of business node"""
         self._inner_dict['definition'] = value
     
     
     @property
     def parentNode(self) -> Union[None, str]:
-        """Parent node of the glossary term"""
+        """Getter: Parent node of the glossary term"""
         return self._inner_dict.get('parentNode')  # type: ignore
     
     @parentNode.setter
     def parentNode(self, value: Union[None, str]) -> None:
+        """Setter: Parent node of the glossary term"""
         self._inner_dict['parentNode'] = value
     
     
     @property
     def name(self) -> Union[None, str]:
-        """Display name of the node"""
+        """Getter: Display name of the node"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: Union[None, str]) -> None:
+        """Setter: Display name of the node"""
         self._inner_dict['name'] = value
     
     
     @property
     def id(self) -> Union[None, str]:
-        """Optional id for the GlossaryNode"""
+        """Getter: Optional id for the GlossaryNode"""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: Union[None, str]) -> None:
+        """Setter: Optional id for the GlossaryNode"""
         self._inner_dict['id'] = value
     
     
 class GlossaryRelatedTermsClass(_Aspect):
     """Has A / Is A lineage information about a glossary Term reporting the lineage"""
 
 
@@ -7631,59 +8630,71 @@
         super().__init__()
         
         self.isRelatedTerms = isRelatedTerms
         self.hasRelatedTerms = hasRelatedTerms
         self.values = values
         self.relatedTerms = relatedTerms
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlossaryRelatedTermsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.isRelatedTerms = self.RECORD_SCHEMA.fields_dict["isRelatedTerms"].default
         self.hasRelatedTerms = self.RECORD_SCHEMA.fields_dict["hasRelatedTerms"].default
         self.values = self.RECORD_SCHEMA.fields_dict["values"].default
         self.relatedTerms = self.RECORD_SCHEMA.fields_dict["relatedTerms"].default
     
     
     @property
     def isRelatedTerms(self) -> Union[None, List[str]]:
-        """The relationship Is A with glossary term"""
+        """Getter: The relationship Is A with glossary term"""
         return self._inner_dict.get('isRelatedTerms')  # type: ignore
     
     @isRelatedTerms.setter
     def isRelatedTerms(self, value: Union[None, List[str]]) -> None:
+        """Setter: The relationship Is A with glossary term"""
         self._inner_dict['isRelatedTerms'] = value
     
     
     @property
     def hasRelatedTerms(self) -> Union[None, List[str]]:
-        """The relationship Has A with glossary term"""
+        """Getter: The relationship Has A with glossary term"""
         return self._inner_dict.get('hasRelatedTerms')  # type: ignore
     
     @hasRelatedTerms.setter
     def hasRelatedTerms(self, value: Union[None, List[str]]) -> None:
+        """Setter: The relationship Has A with glossary term"""
         self._inner_dict['hasRelatedTerms'] = value
     
     
     @property
     def values(self) -> Union[None, List[str]]:
-        """The relationship Has Value with glossary term.
+        """Getter: The relationship Has Value with glossary term.
     These are fixed value a term has. For example a ColorEnum where RED, GREEN and YELLOW are fixed values."""
         return self._inner_dict.get('values')  # type: ignore
     
     @values.setter
     def values(self, value: Union[None, List[str]]) -> None:
+        """Setter: The relationship Has Value with glossary term.
+    These are fixed value a term has. For example a ColorEnum where RED, GREEN and YELLOW are fixed values."""
         self._inner_dict['values'] = value
     
     
     @property
     def relatedTerms(self) -> Union[None, List[str]]:
-        """The relationship isRelatedTo with glossary term"""
+        """Getter: The relationship isRelatedTo with glossary term"""
         return self._inner_dict.get('relatedTerms')  # type: ignore
     
     @relatedTerms.setter
     def relatedTerms(self, value: Union[None, List[str]]) -> None:
+        """Setter: The relationship isRelatedTo with glossary term"""
         self._inner_dict['relatedTerms'] = value
     
     
 class GlossaryTermInfoClass(_Aspect):
     """Properties associated with a GlossaryTerm"""
 
 
@@ -7714,113 +8725,129 @@
         self.definition = definition
         self.parentNode = parentNode
         self.termSource = termSource
         self.sourceRef = sourceRef
         self.sourceUrl = sourceUrl
         self.rawSchema = rawSchema
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlossaryTermInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.id = self.RECORD_SCHEMA.fields_dict["id"].default
         self.name = self.RECORD_SCHEMA.fields_dict["name"].default
         self.definition = str()
         self.parentNode = self.RECORD_SCHEMA.fields_dict["parentNode"].default
         self.termSource = str()
         self.sourceRef = self.RECORD_SCHEMA.fields_dict["sourceRef"].default
         self.sourceUrl = self.RECORD_SCHEMA.fields_dict["sourceUrl"].default
         self.rawSchema = self.RECORD_SCHEMA.fields_dict["rawSchema"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def id(self) -> Union[None, str]:
-        """Optional id for the term"""
+        """Getter: Optional id for the term"""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: Union[None, str]) -> None:
+        """Setter: Optional id for the term"""
         self._inner_dict['id'] = value
     
     
     @property
     def name(self) -> Union[None, str]:
-        """Display name of the term"""
+        """Getter: Display name of the term"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: Union[None, str]) -> None:
+        """Setter: Display name of the term"""
         self._inner_dict['name'] = value
     
     
     @property
     def definition(self) -> str:
-        """Definition of business term."""
+        """Getter: Definition of business term."""
         return self._inner_dict.get('definition')  # type: ignore
     
     @definition.setter
     def definition(self, value: str) -> None:
+        """Setter: Definition of business term."""
         self._inner_dict['definition'] = value
     
     
     @property
     def parentNode(self) -> Union[None, str]:
-        """Parent node of the glossary term"""
+        """Getter: Parent node of the glossary term"""
         return self._inner_dict.get('parentNode')  # type: ignore
     
     @parentNode.setter
     def parentNode(self, value: Union[None, str]) -> None:
+        """Setter: Parent node of the glossary term"""
         self._inner_dict['parentNode'] = value
     
     
     @property
     def termSource(self) -> str:
-        """Source of the Business Term (INTERNAL or EXTERNAL) with default value as INTERNAL"""
+        """Getter: Source of the Business Term (INTERNAL or EXTERNAL) with default value as INTERNAL"""
         return self._inner_dict.get('termSource')  # type: ignore
     
     @termSource.setter
     def termSource(self, value: str) -> None:
+        """Setter: Source of the Business Term (INTERNAL or EXTERNAL) with default value as INTERNAL"""
         self._inner_dict['termSource'] = value
     
     
     @property
     def sourceRef(self) -> Union[None, str]:
-        """External Reference to the business-term"""
+        """Getter: External Reference to the business-term"""
         return self._inner_dict.get('sourceRef')  # type: ignore
     
     @sourceRef.setter
     def sourceRef(self, value: Union[None, str]) -> None:
+        """Setter: External Reference to the business-term"""
         self._inner_dict['sourceRef'] = value
     
     
     @property
     def sourceUrl(self) -> Union[None, str]:
-        """The abstracted URL such as https://spec.edmcouncil.org/fibo/ontology/FBC/FinancialInstruments/FinancialInstruments/CashInstrument."""
+        """Getter: The abstracted URL such as https://spec.edmcouncil.org/fibo/ontology/FBC/FinancialInstruments/FinancialInstruments/CashInstrument."""
         return self._inner_dict.get('sourceUrl')  # type: ignore
     
     @sourceUrl.setter
     def sourceUrl(self, value: Union[None, str]) -> None:
+        """Setter: The abstracted URL such as https://spec.edmcouncil.org/fibo/ontology/FBC/FinancialInstruments/FinancialInstruments/CashInstrument."""
         self._inner_dict['sourceUrl'] = value
     
     
     @property
     def rawSchema(self) -> Union[None, str]:
-        """Schema definition of the glossary term"""
+        """Getter: Schema definition of the glossary term"""
         return self._inner_dict.get('rawSchema')  # type: ignore
     
     @rawSchema.setter
     def rawSchema(self, value: Union[None, str]) -> None:
+        """Setter: Schema definition of the glossary term"""
         self._inner_dict['rawSchema'] = value
     
     
 class CorpGroupEditableInfoClass(_Aspect):
     """Group information that can be edited from UI"""
 
 
@@ -7841,58 +8868,69 @@
             # default: 'https://raw.githubusercontent.com/datahub-project/datahub/master/datahub-web-react/src/images/default_avatar.png'
             self.pictureLink = self.RECORD_SCHEMA.fields_dict["pictureLink"].default
         else:
             self.pictureLink = pictureLink
         self.slack = slack
         self.email = email
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpGroupEditableInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.pictureLink = self.RECORD_SCHEMA.fields_dict["pictureLink"].default
         self.slack = self.RECORD_SCHEMA.fields_dict["slack"].default
         self.email = self.RECORD_SCHEMA.fields_dict["email"].default
     
     
     @property
     def description(self) -> Union[None, str]:
-        """A description of the group"""
+        """Getter: A description of the group"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: A description of the group"""
         self._inner_dict['description'] = value
     
     
     @property
     def pictureLink(self) -> str:
-        """A URL which points to a picture which user wants to set as the photo for the group"""
+        """Getter: A URL which points to a picture which user wants to set as the photo for the group"""
         return self._inner_dict.get('pictureLink')  # type: ignore
     
     @pictureLink.setter
     def pictureLink(self, value: str) -> None:
+        """Setter: A URL which points to a picture which user wants to set as the photo for the group"""
         self._inner_dict['pictureLink'] = value
     
     
     @property
     def slack(self) -> Union[None, str]:
-        """Slack channel for the group"""
+        """Getter: Slack channel for the group"""
         return self._inner_dict.get('slack')  # type: ignore
     
     @slack.setter
     def slack(self, value: Union[None, str]) -> None:
+        """Setter: Slack channel for the group"""
         self._inner_dict['slack'] = value
     
     
     @property
     def email(self) -> Union[None, str]:
-        """Email address to contact the group"""
+        """Getter: Email address to contact the group"""
         return self._inner_dict.get('email')  # type: ignore
     
     @email.setter
     def email(self, value: Union[None, str]) -> None:
+        """Setter: Email address to contact the group"""
         self._inner_dict['email'] = value
     
     
 class CorpGroupInfoClass(_Aspect):
     """Information about a Corp Group ingested from a third party source"""
 
 
@@ -7917,131 +8955,158 @@
         self.admins = admins
         self.members = members
         self.groups = groups
         self.description = description
         self.slack = slack
         self.created = created
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpGroupInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.displayName = self.RECORD_SCHEMA.fields_dict["displayName"].default
         self.email = self.RECORD_SCHEMA.fields_dict["email"].default
         self.admins = list()
         self.members = list()
         self.groups = list()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.slack = self.RECORD_SCHEMA.fields_dict["slack"].default
         self.created = self.RECORD_SCHEMA.fields_dict["created"].default
     
     
     @property
     def displayName(self) -> Union[None, str]:
-        """The name of the group."""
+        """Getter: The name of the group."""
         return self._inner_dict.get('displayName')  # type: ignore
     
     @displayName.setter
     def displayName(self, value: Union[None, str]) -> None:
+        """Setter: The name of the group."""
         self._inner_dict['displayName'] = value
     
     
     @property
     def email(self) -> Union[None, str]:
-        """email of this group"""
+        """Getter: email of this group"""
         return self._inner_dict.get('email')  # type: ignore
     
     @email.setter
     def email(self, value: Union[None, str]) -> None:
+        """Setter: email of this group"""
         self._inner_dict['email'] = value
     
     
     @property
     def admins(self) -> List[str]:
-        """owners of this group
+        """Getter: owners of this group
     Deprecated! Replaced by Ownership aspect."""
         return self._inner_dict.get('admins')  # type: ignore
     
     @admins.setter
     def admins(self, value: List[str]) -> None:
+        """Setter: owners of this group
+    Deprecated! Replaced by Ownership aspect."""
         self._inner_dict['admins'] = value
     
     
     @property
     def members(self) -> List[str]:
-        """List of ldap urn in this group.
+        """Getter: List of ldap urn in this group.
     Deprecated! Replaced by GroupMembership aspect."""
         return self._inner_dict.get('members')  # type: ignore
     
     @members.setter
     def members(self, value: List[str]) -> None:
+        """Setter: List of ldap urn in this group.
+    Deprecated! Replaced by GroupMembership aspect."""
         self._inner_dict['members'] = value
     
     
     @property
     def groups(self) -> List[str]:
-        """List of groups in this group.
+        """Getter: List of groups in this group.
     Deprecated! This field is unused."""
         return self._inner_dict.get('groups')  # type: ignore
     
     @groups.setter
     def groups(self, value: List[str]) -> None:
+        """Setter: List of groups in this group.
+    Deprecated! This field is unused."""
         self._inner_dict['groups'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """A description of the group."""
+        """Getter: A description of the group."""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: A description of the group."""
         self._inner_dict['description'] = value
     
     
     @property
     def slack(self) -> Union[None, str]:
-        """Slack channel for the group"""
+        """Getter: Slack channel for the group"""
         return self._inner_dict.get('slack')  # type: ignore
     
     @slack.setter
     def slack(self, value: Union[None, str]) -> None:
+        """Setter: Slack channel for the group"""
         self._inner_dict['slack'] = value
     
     
     @property
     def created(self) -> Union[None, "AuditStampClass"]:
-        """Created Audit stamp"""
+        """Getter: Created Audit stamp"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: Created Audit stamp"""
         self._inner_dict['created'] = value
     
     
 class CorpUserAppearanceSettingsClass(DictWrapper):
     """Settings for a user around the appearance of their DataHub UI"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.CorpUserAppearanceSettings")
     def __init__(self,
         showSimplifiedHomepage: Union[None, bool]=None,
     ):
         super().__init__()
         
         self.showSimplifiedHomepage = showSimplifiedHomepage
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpUserAppearanceSettingsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.showSimplifiedHomepage = self.RECORD_SCHEMA.fields_dict["showSimplifiedHomepage"].default
     
     
     @property
     def showSimplifiedHomepage(self) -> Union[None, bool]:
-        """Flag whether the user should see a homepage with only datasets, charts and dashboards. Intended for users
+        """Getter: Flag whether the user should see a homepage with only datasets, charts and dashboards. Intended for users
     who have less operational use cases for the datahub tool."""
         return self._inner_dict.get('showSimplifiedHomepage')  # type: ignore
     
     @showSimplifiedHomepage.setter
     def showSimplifiedHomepage(self, value: Union[None, bool]) -> None:
+        """Setter: Flag whether the user should see a homepage with only datasets, charts and dashboards. Intended for users
+    who have less operational use cases for the datahub tool."""
         self._inner_dict['showSimplifiedHomepage'] = value
     
     
 class CorpUserCredentialsClass(_Aspect):
     """Corp user credentials"""
 
 
@@ -8058,58 +9123,69 @@
         super().__init__()
         
         self.salt = salt
         self.hashedPassword = hashedPassword
         self.passwordResetToken = passwordResetToken
         self.passwordResetTokenExpirationTimeMillis = passwordResetTokenExpirationTimeMillis
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpUserCredentialsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.salt = str()
         self.hashedPassword = str()
         self.passwordResetToken = self.RECORD_SCHEMA.fields_dict["passwordResetToken"].default
         self.passwordResetTokenExpirationTimeMillis = self.RECORD_SCHEMA.fields_dict["passwordResetTokenExpirationTimeMillis"].default
     
     
     @property
     def salt(self) -> str:
-        """Salt used to hash password"""
+        """Getter: Salt used to hash password"""
         return self._inner_dict.get('salt')  # type: ignore
     
     @salt.setter
     def salt(self, value: str) -> None:
+        """Setter: Salt used to hash password"""
         self._inner_dict['salt'] = value
     
     
     @property
     def hashedPassword(self) -> str:
-        """Hashed password generated by concatenating salt and password, then hashing"""
+        """Getter: Hashed password generated by concatenating salt and password, then hashing"""
         return self._inner_dict.get('hashedPassword')  # type: ignore
     
     @hashedPassword.setter
     def hashedPassword(self, value: str) -> None:
+        """Setter: Hashed password generated by concatenating salt and password, then hashing"""
         self._inner_dict['hashedPassword'] = value
     
     
     @property
     def passwordResetToken(self) -> Union[None, str]:
-        """Optional token needed to reset a user's password. Can only be set by the admin."""
+        """Getter: Optional token needed to reset a user's password. Can only be set by the admin."""
         return self._inner_dict.get('passwordResetToken')  # type: ignore
     
     @passwordResetToken.setter
     def passwordResetToken(self, value: Union[None, str]) -> None:
+        """Setter: Optional token needed to reset a user's password. Can only be set by the admin."""
         self._inner_dict['passwordResetToken'] = value
     
     
     @property
     def passwordResetTokenExpirationTimeMillis(self) -> Union[None, int]:
-        """When the password reset token expires."""
+        """Getter: When the password reset token expires."""
         return self._inner_dict.get('passwordResetTokenExpirationTimeMillis')  # type: ignore
     
     @passwordResetTokenExpirationTimeMillis.setter
     def passwordResetTokenExpirationTimeMillis(self, value: Union[None, int]) -> None:
+        """Setter: When the password reset token expires."""
         self._inner_dict['passwordResetTokenExpirationTimeMillis'] = value
     
     
 class CorpUserEditableInfoClass(_Aspect):
     """Linkedin corp user information that can be edited from UI"""
 
 
@@ -8148,113 +9224,129 @@
             self.pictureLink = pictureLink
         self.displayName = displayName
         self.title = title
         self.slack = slack
         self.phone = phone
         self.email = email
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpUserEditableInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.aboutMe = self.RECORD_SCHEMA.fields_dict["aboutMe"].default
         self.teams = list()
         self.skills = list()
         self.pictureLink = self.RECORD_SCHEMA.fields_dict["pictureLink"].default
         self.displayName = self.RECORD_SCHEMA.fields_dict["displayName"].default
         self.title = self.RECORD_SCHEMA.fields_dict["title"].default
         self.slack = self.RECORD_SCHEMA.fields_dict["slack"].default
         self.phone = self.RECORD_SCHEMA.fields_dict["phone"].default
         self.email = self.RECORD_SCHEMA.fields_dict["email"].default
     
     
     @property
     def aboutMe(self) -> Union[None, str]:
-        """About me section of the user"""
+        """Getter: About me section of the user"""
         return self._inner_dict.get('aboutMe')  # type: ignore
     
     @aboutMe.setter
     def aboutMe(self, value: Union[None, str]) -> None:
+        """Setter: About me section of the user"""
         self._inner_dict['aboutMe'] = value
     
     
     @property
     def teams(self) -> List[str]:
-        """Teams that the user belongs to e.g. Metadata"""
+        """Getter: Teams that the user belongs to e.g. Metadata"""
         return self._inner_dict.get('teams')  # type: ignore
     
     @teams.setter
     def teams(self, value: List[str]) -> None:
+        """Setter: Teams that the user belongs to e.g. Metadata"""
         self._inner_dict['teams'] = value
     
     
     @property
     def skills(self) -> List[str]:
-        """Skills that the user possesses e.g. Machine Learning"""
+        """Getter: Skills that the user possesses e.g. Machine Learning"""
         return self._inner_dict.get('skills')  # type: ignore
     
     @skills.setter
     def skills(self, value: List[str]) -> None:
+        """Setter: Skills that the user possesses e.g. Machine Learning"""
         self._inner_dict['skills'] = value
     
     
     @property
     def pictureLink(self) -> str:
-        """A URL which points to a picture which user wants to set as a profile photo"""
+        """Getter: A URL which points to a picture which user wants to set as a profile photo"""
         return self._inner_dict.get('pictureLink')  # type: ignore
     
     @pictureLink.setter
     def pictureLink(self, value: str) -> None:
+        """Setter: A URL which points to a picture which user wants to set as a profile photo"""
         self._inner_dict['pictureLink'] = value
     
     
     @property
     def displayName(self) -> Union[None, str]:
-        """DataHub-native display name"""
+        """Getter: DataHub-native display name"""
         return self._inner_dict.get('displayName')  # type: ignore
     
     @displayName.setter
     def displayName(self, value: Union[None, str]) -> None:
+        """Setter: DataHub-native display name"""
         self._inner_dict['displayName'] = value
     
     
     @property
     def title(self) -> Union[None, str]:
-        """DataHub-native Title, e.g. 'Software Engineer'"""
+        """Getter: DataHub-native Title, e.g. 'Software Engineer'"""
         return self._inner_dict.get('title')  # type: ignore
     
     @title.setter
     def title(self, value: Union[None, str]) -> None:
+        """Setter: DataHub-native Title, e.g. 'Software Engineer'"""
         self._inner_dict['title'] = value
     
     
     @property
     def slack(self) -> Union[None, str]:
-        """Slack handle for the user"""
+        """Getter: Slack handle for the user"""
         return self._inner_dict.get('slack')  # type: ignore
     
     @slack.setter
     def slack(self, value: Union[None, str]) -> None:
+        """Setter: Slack handle for the user"""
         self._inner_dict['slack'] = value
     
     
     @property
     def phone(self) -> Union[None, str]:
-        """Phone number to contact the user"""
+        """Getter: Phone number to contact the user"""
         return self._inner_dict.get('phone')  # type: ignore
     
     @phone.setter
     def phone(self, value: Union[None, str]) -> None:
+        """Setter: Phone number to contact the user"""
         self._inner_dict['phone'] = value
     
     
     @property
     def email(self) -> Union[None, str]:
-        """Email address to contact the user"""
+        """Getter: Email address to contact the user"""
         return self._inner_dict.get('email')  # type: ignore
     
     @email.setter
     def email(self, value: Union[None, str]) -> None:
+        """Setter: Email address to contact the user"""
         self._inner_dict['email'] = value
     
     
 class CorpUserInfoClass(_Aspect):
     """Linkedin corp user information"""
 
 
@@ -8291,14 +9383,21 @@
         self.departmentId = departmentId
         self.departmentName = departmentName
         self.firstName = firstName
         self.lastName = lastName
         self.fullName = fullName
         self.countryCode = countryCode
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpUserInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.active = bool()
         self.displayName = self.RECORD_SCHEMA.fields_dict["displayName"].default
         self.email = self.RECORD_SCHEMA.fields_dict["email"].default
         self.title = self.RECORD_SCHEMA.fields_dict["title"].default
         self.managerUrn = self.RECORD_SCHEMA.fields_dict["managerUrn"].default
@@ -8308,129 +9407,141 @@
         self.lastName = self.RECORD_SCHEMA.fields_dict["lastName"].default
         self.fullName = self.RECORD_SCHEMA.fields_dict["fullName"].default
         self.countryCode = self.RECORD_SCHEMA.fields_dict["countryCode"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def active(self) -> bool:
-        """Deprecated! Use CorpUserStatus instead. Whether the corpUser is active, ref: https://iwww.corp.linkedin.com/wiki/cf/display/GTSD/Accessing+Active+Directory+via+LDAP+tools"""
+        """Getter: Deprecated! Use CorpUserStatus instead. Whether the corpUser is active, ref: https://iwww.corp.linkedin.com/wiki/cf/display/GTSD/Accessing+Active+Directory+via+LDAP+tools"""
         return self._inner_dict.get('active')  # type: ignore
     
     @active.setter
     def active(self, value: bool) -> None:
+        """Setter: Deprecated! Use CorpUserStatus instead. Whether the corpUser is active, ref: https://iwww.corp.linkedin.com/wiki/cf/display/GTSD/Accessing+Active+Directory+via+LDAP+tools"""
         self._inner_dict['active'] = value
     
     
     @property
     def displayName(self) -> Union[None, str]:
-        """displayName of this user ,  e.g.  Hang Zhang(DataHQ)"""
+        """Getter: displayName of this user ,  e.g.  Hang Zhang(DataHQ)"""
         return self._inner_dict.get('displayName')  # type: ignore
     
     @displayName.setter
     def displayName(self, value: Union[None, str]) -> None:
+        """Setter: displayName of this user ,  e.g.  Hang Zhang(DataHQ)"""
         self._inner_dict['displayName'] = value
     
     
     @property
     def email(self) -> Union[None, str]:
-        """email address of this user"""
+        """Getter: email address of this user"""
         return self._inner_dict.get('email')  # type: ignore
     
     @email.setter
     def email(self, value: Union[None, str]) -> None:
+        """Setter: email address of this user"""
         self._inner_dict['email'] = value
     
     
     @property
     def title(self) -> Union[None, str]:
-        """title of this user"""
+        """Getter: title of this user"""
         return self._inner_dict.get('title')  # type: ignore
     
     @title.setter
     def title(self, value: Union[None, str]) -> None:
+        """Setter: title of this user"""
         self._inner_dict['title'] = value
     
     
     @property
     def managerUrn(self) -> Union[None, str]:
-        """direct manager of this user"""
+        """Getter: direct manager of this user"""
         return self._inner_dict.get('managerUrn')  # type: ignore
     
     @managerUrn.setter
     def managerUrn(self, value: Union[None, str]) -> None:
+        """Setter: direct manager of this user"""
         self._inner_dict['managerUrn'] = value
     
     
     @property
     def departmentId(self) -> Union[None, int]:
-        """department id this user belong to"""
+        """Getter: department id this user belong to"""
         return self._inner_dict.get('departmentId')  # type: ignore
     
     @departmentId.setter
     def departmentId(self, value: Union[None, int]) -> None:
+        """Setter: department id this user belong to"""
         self._inner_dict['departmentId'] = value
     
     
     @property
     def departmentName(self) -> Union[None, str]:
-        """department name this user belong to"""
+        """Getter: department name this user belong to"""
         return self._inner_dict.get('departmentName')  # type: ignore
     
     @departmentName.setter
     def departmentName(self, value: Union[None, str]) -> None:
+        """Setter: department name this user belong to"""
         self._inner_dict['departmentName'] = value
     
     
     @property
     def firstName(self) -> Union[None, str]:
-        """first name of this user"""
+        """Getter: first name of this user"""
         return self._inner_dict.get('firstName')  # type: ignore
     
     @firstName.setter
     def firstName(self, value: Union[None, str]) -> None:
+        """Setter: first name of this user"""
         self._inner_dict['firstName'] = value
     
     
     @property
     def lastName(self) -> Union[None, str]:
-        """last name of this user"""
+        """Getter: last name of this user"""
         return self._inner_dict.get('lastName')  # type: ignore
     
     @lastName.setter
     def lastName(self, value: Union[None, str]) -> None:
+        """Setter: last name of this user"""
         self._inner_dict['lastName'] = value
     
     
     @property
     def fullName(self) -> Union[None, str]:
-        """Common name of this user, format is firstName + lastName (split by a whitespace)"""
+        """Getter: Common name of this user, format is firstName + lastName (split by a whitespace)"""
         return self._inner_dict.get('fullName')  # type: ignore
     
     @fullName.setter
     def fullName(self, value: Union[None, str]) -> None:
+        """Setter: Common name of this user, format is firstName + lastName (split by a whitespace)"""
         self._inner_dict['fullName'] = value
     
     
     @property
     def countryCode(self) -> Union[None, str]:
-        """two uppercase letters country code. e.g.  US"""
+        """Getter: two uppercase letters country code. e.g.  US"""
         return self._inner_dict.get('countryCode')  # type: ignore
     
     @countryCode.setter
     def countryCode(self, value: Union[None, str]) -> None:
+        """Setter: two uppercase letters country code. e.g.  US"""
         self._inner_dict['countryCode'] = value
     
     
 class CorpUserSettingsClass(_Aspect):
     """Settings that a user can customize through the datahub ui"""
 
 
@@ -8443,36 +9554,45 @@
         views: Union[None, "CorpUserViewsSettingsClass"]=None,
     ):
         super().__init__()
         
         self.appearance = appearance
         self.views = views
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpUserSettingsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
-        self.appearance = CorpUserAppearanceSettingsClass._construct_with_defaults()
+        self.appearance = CorpUserAppearanceSettingsClass.construct_with_defaults()
         self.views = self.RECORD_SCHEMA.fields_dict["views"].default
     
     
     @property
     def appearance(self) -> "CorpUserAppearanceSettingsClass":
-        """Settings for a user around the appearance of their DataHub U"""
+        """Getter: Settings for a user around the appearance of their DataHub U"""
         return self._inner_dict.get('appearance')  # type: ignore
     
     @appearance.setter
     def appearance(self, value: "CorpUserAppearanceSettingsClass") -> None:
+        """Setter: Settings for a user around the appearance of their DataHub U"""
         self._inner_dict['appearance'] = value
     
     
     @property
     def views(self) -> Union[None, "CorpUserViewsSettingsClass"]:
-        """User preferences for the Views feature."""
+        """Getter: User preferences for the Views feature."""
         return self._inner_dict.get('views')  # type: ignore
     
     @views.setter
     def views(self, value: Union[None, "CorpUserViewsSettingsClass"]) -> None:
+        """Setter: User preferences for the Views feature."""
         self._inner_dict['views'] = value
     
     
 class CorpUserStatusClass(_Aspect):
     """The status of the user, e.g. provisioned, active, suspended, etc."""
 
 
@@ -8485,62 +9605,80 @@
         lastModified: "AuditStampClass",
     ):
         super().__init__()
         
         self.status = status
         self.lastModified = lastModified
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpUserStatusClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.status = str()
-        self.lastModified = AuditStampClass._construct_with_defaults()
+        self.lastModified = AuditStampClass.construct_with_defaults()
     
     
     @property
     def status(self) -> str:
-        """Status of the user, e.g. PROVISIONED / ACTIVE / SUSPENDED"""
+        """Getter: Status of the user, e.g. PROVISIONED / ACTIVE / SUSPENDED"""
         return self._inner_dict.get('status')  # type: ignore
     
     @status.setter
     def status(self, value: str) -> None:
+        """Setter: Status of the user, e.g. PROVISIONED / ACTIVE / SUSPENDED"""
         self._inner_dict['status'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """Audit stamp containing who last modified the status and when."""
+        """Getter: Audit stamp containing who last modified the status and when."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp containing who last modified the status and when."""
         self._inner_dict['lastModified'] = value
     
     
 class CorpUserViewsSettingsClass(DictWrapper):
     """Settings related to the 'Views' feature."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.CorpUserViewsSettings")
     def __init__(self,
         defaultView: Union[None, str]=None,
     ):
         super().__init__()
         
         self.defaultView = defaultView
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpUserViewsSettingsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.defaultView = self.RECORD_SCHEMA.fields_dict["defaultView"].default
     
     
     @property
     def defaultView(self) -> Union[None, str]:
-        """The default View which is selected for the user.
+        """Getter: The default View which is selected for the user.
     If none is chosen, then this value will be left blank."""
         return self._inner_dict.get('defaultView')  # type: ignore
     
     @defaultView.setter
     def defaultView(self, value: Union[None, str]) -> None:
+        """Setter: The default View which is selected for the user.
+    If none is chosen, then this value will be left blank."""
         self._inner_dict['defaultView'] = value
     
     
 class GroupMembershipClass(_Aspect):
     """Carries information about the CorpGroups a user is in."""
 
 
@@ -8551,25 +9689,33 @@
     def __init__(self,
         groups: List[str],
     ):
         super().__init__()
         
         self.groups = groups
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GroupMembershipClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.groups = list()
     
     
     @property
     def groups(self) -> List[str]:
         # No docs available.
         return self._inner_dict.get('groups')  # type: ignore
     
     @groups.setter
     def groups(self, value: List[str]) -> None:
+        # No docs available.
         self._inner_dict['groups'] = value
     
     
 class InviteTokenClass(_Aspect):
     """Aspect used to store invite tokens."""
 
 
@@ -8582,36 +9728,45 @@
         role: Union[None, str]=None,
     ):
         super().__init__()
         
         self.token = token
         self.role = role
     
+    @classmethod
+    def construct_with_defaults(cls) -> "InviteTokenClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.token = str()
         self.role = self.RECORD_SCHEMA.fields_dict["role"].default
     
     
     @property
     def token(self) -> str:
-        """The encrypted invite token."""
+        """Getter: The encrypted invite token."""
         return self._inner_dict.get('token')  # type: ignore
     
     @token.setter
     def token(self, value: str) -> None:
+        """Setter: The encrypted invite token."""
         self._inner_dict['token'] = value
     
     
     @property
     def role(self) -> Union[None, str]:
-        """The role that this invite token may be associated with"""
+        """Getter: The role that this invite token may be associated with"""
         return self._inner_dict.get('role')  # type: ignore
     
     @role.setter
     def role(self, value: Union[None, str]) -> None:
+        """Setter: The role that this invite token may be associated with"""
         self._inner_dict['role'] = value
     
     
 class NativeGroupMembershipClass(_Aspect):
     """Carries information about the native CorpGroups a user is in."""
 
 
@@ -8622,25 +9777,33 @@
     def __init__(self,
         nativeGroups: List[str],
     ):
         super().__init__()
         
         self.nativeGroups = nativeGroups
     
+    @classmethod
+    def construct_with_defaults(cls) -> "NativeGroupMembershipClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.nativeGroups = list()
     
     
     @property
     def nativeGroups(self) -> List[str]:
         # No docs available.
         return self._inner_dict.get('nativeGroups')  # type: ignore
     
     @nativeGroups.setter
     def nativeGroups(self, value: List[str]) -> None:
+        # No docs available.
         self._inner_dict['nativeGroups'] = value
     
     
 class RoleMembershipClass(_Aspect):
     """Carries information about which roles a user is assigned to."""
 
 
@@ -8651,25 +9814,33 @@
     def __init__(self,
         roles: List[str],
     ):
         super().__init__()
         
         self.roles = roles
     
+    @classmethod
+    def construct_with_defaults(cls) -> "RoleMembershipClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.roles = list()
     
     
     @property
     def roles(self) -> List[str]:
         # No docs available.
         return self._inner_dict.get('roles')  # type: ignore
     
     @roles.setter
     def roles(self, value: List[str]) -> None:
+        # No docs available.
         self._inner_dict['roles'] = value
     
     
 class DataHubIngestionSourceConfigClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ingestion.DataHubIngestionSourceConfig")
@@ -8682,58 +9853,69 @@
         super().__init__()
         
         self.recipe = recipe
         self.version = version
         self.executorId = executorId
         self.debugMode = debugMode
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubIngestionSourceConfigClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.recipe = str()
         self.version = self.RECORD_SCHEMA.fields_dict["version"].default
         self.executorId = self.RECORD_SCHEMA.fields_dict["executorId"].default
         self.debugMode = self.RECORD_SCHEMA.fields_dict["debugMode"].default
     
     
     @property
     def recipe(self) -> str:
-        """The JSON recipe to use for ingestion"""
+        """Getter: The JSON recipe to use for ingestion"""
         return self._inner_dict.get('recipe')  # type: ignore
     
     @recipe.setter
     def recipe(self, value: str) -> None:
+        """Setter: The JSON recipe to use for ingestion"""
         self._inner_dict['recipe'] = value
     
     
     @property
     def version(self) -> Union[None, str]:
-        """The PyPI version of the datahub CLI to use when executing a recipe"""
+        """Getter: The PyPI version of the datahub CLI to use when executing a recipe"""
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: Union[None, str]) -> None:
+        """Setter: The PyPI version of the datahub CLI to use when executing a recipe"""
         self._inner_dict['version'] = value
     
     
     @property
     def executorId(self) -> Union[None, str]:
-        """The id of the executor to use to execute the ingestion run"""
+        """Getter: The id of the executor to use to execute the ingestion run"""
         return self._inner_dict.get('executorId')  # type: ignore
     
     @executorId.setter
     def executorId(self, value: Union[None, str]) -> None:
+        """Setter: The id of the executor to use to execute the ingestion run"""
         self._inner_dict['executorId'] = value
     
     
     @property
     def debugMode(self) -> Union[None, bool]:
-        """Whether or not to run this ingestion source in debug mode"""
+        """Getter: Whether or not to run this ingestion source in debug mode"""
         return self._inner_dict.get('debugMode')  # type: ignore
     
     @debugMode.setter
     def debugMode(self, value: Union[None, bool]) -> None:
+        """Setter: Whether or not to run this ingestion source in debug mode"""
         self._inner_dict['debugMode'] = value
     
     
 class DataHubIngestionSourceInfoClass(_Aspect):
     """Info about a DataHub ingestion source"""
 
 
@@ -8752,69 +9934,81 @@
         
         self.name = name
         self.type = type
         self.platform = platform
         self.schedule = schedule
         self.config = config
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubIngestionSourceInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.type = str()
         self.platform = self.RECORD_SCHEMA.fields_dict["platform"].default
         self.schedule = self.RECORD_SCHEMA.fields_dict["schedule"].default
-        self.config = DataHubIngestionSourceConfigClass._construct_with_defaults()
+        self.config = DataHubIngestionSourceConfigClass.construct_with_defaults()
     
     
     @property
     def name(self) -> str:
-        """The display name of the ingestion source"""
+        """Getter: The display name of the ingestion source"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: The display name of the ingestion source"""
         self._inner_dict['name'] = value
     
     
     @property
     def type(self) -> str:
-        """The type of the source itself, e.g. mysql, bigquery, bigquery-usage. Should match the recipe."""
+        """Getter: The type of the source itself, e.g. mysql, bigquery, bigquery-usage. Should match the recipe."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: str) -> None:
+        """Setter: The type of the source itself, e.g. mysql, bigquery, bigquery-usage. Should match the recipe."""
         self._inner_dict['type'] = value
     
     
     @property
     def platform(self) -> Union[None, str]:
-        """Data Platform URN associated with the source"""
+        """Getter: Data Platform URN associated with the source"""
         return self._inner_dict.get('platform')  # type: ignore
     
     @platform.setter
     def platform(self, value: Union[None, str]) -> None:
+        """Setter: Data Platform URN associated with the source"""
         self._inner_dict['platform'] = value
     
     
     @property
     def schedule(self) -> Union[None, "DataHubIngestionSourceScheduleClass"]:
-        """The schedule on which the ingestion source is executed"""
+        """Getter: The schedule on which the ingestion source is executed"""
         return self._inner_dict.get('schedule')  # type: ignore
     
     @schedule.setter
     def schedule(self, value: Union[None, "DataHubIngestionSourceScheduleClass"]) -> None:
+        """Setter: The schedule on which the ingestion source is executed"""
         self._inner_dict['schedule'] = value
     
     
     @property
     def config(self) -> "DataHubIngestionSourceConfigClass":
-        """Parameters associated with the Ingestion Source"""
+        """Getter: Parameters associated with the Ingestion Source"""
         return self._inner_dict.get('config')  # type: ignore
     
     @config.setter
     def config(self, value: "DataHubIngestionSourceConfigClass") -> None:
+        """Setter: Parameters associated with the Ingestion Source"""
         self._inner_dict['config'] = value
     
     
 class DataHubIngestionSourceScheduleClass(DictWrapper):
     """The schedule associated with an ingestion source."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ingestion.DataHubIngestionSourceSchedule")
@@ -8823,36 +10017,45 @@
         timezone: str,
     ):
         super().__init__()
         
         self.interval = interval
         self.timezone = timezone
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubIngestionSourceScheduleClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.interval = str()
         self.timezone = str()
     
     
     @property
     def interval(self) -> str:
-        """A cron-formatted execution interval, as a cron string, e.g. * * * * *"""
+        """Getter: A cron-formatted execution interval, as a cron string, e.g. * * * * *"""
         return self._inner_dict.get('interval')  # type: ignore
     
     @interval.setter
     def interval(self, value: str) -> None:
+        """Setter: A cron-formatted execution interval, as a cron string, e.g. * * * * *"""
         self._inner_dict['interval'] = value
     
     
     @property
     def timezone(self) -> str:
-        """Timezone in which the cron interval applies, e.g. America/Los Angeles"""
+        """Getter: Timezone in which the cron interval applies, e.g. America/Los Angeles"""
         return self._inner_dict.get('timezone')  # type: ignore
     
     @timezone.setter
     def timezone(self, value: str) -> None:
+        """Setter: Timezone in which the cron interval applies, e.g. America/Los Angeles"""
         self._inner_dict['timezone'] = value
     
     
 class AssertionKeyClass(_Aspect):
     """Key for a Assertion"""
 
 
@@ -8863,96 +10066,121 @@
     def __init__(self,
         assertionId: str,
     ):
         super().__init__()
         
         self.assertionId = assertionId
     
+    @classmethod
+    def construct_with_defaults(cls) -> "AssertionKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.assertionId = str()
     
     
     @property
     def assertionId(self) -> str:
-        """Unique id for the assertion."""
+        """Getter: Unique id for the assertion."""
         return self._inner_dict.get('assertionId')  # type: ignore
     
     @assertionId.setter
     def assertionId(self, value: str) -> None:
+        """Setter: Unique id for the assertion."""
         self._inner_dict['assertionId'] = value
     
     
 class ChartKeyClass(_Aspect):
     """Key for a Chart"""
 
 
     ASPECT_NAME = 'chartKey'
-    ASPECT_INFO = {'keyForEntity': 'chart', 'entityCategory': '_unset_', 'entityAspects': ['chartInfo', 'editableChartProperties', 'chartQuery', 'inputFields', 'chartUsageStatistics', 'embed', 'browsePaths', 'domains', 'container', 'deprecation', 'ownership', 'status', 'institutionalMemory', 'dataPlatformInstance', 'globalTags', 'glossaryTerms', 'browsePathsV2']}
+    ASPECT_INFO = {'keyForEntity': 'chart', 'entityCategory': '_unset_', 'entityAspects': ['domains', 'container', 'deprecation', 'inputFields', 'chartUsageStatistics', 'embed']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.ChartKey")
 
     def __init__(self,
         dashboardTool: str,
         chartId: str,
     ):
         super().__init__()
         
         self.dashboardTool = dashboardTool
         self.chartId = chartId
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ChartKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.dashboardTool = str()
         self.chartId = str()
     
     
     @property
     def dashboardTool(self) -> str:
-        """The name of the dashboard tool such as looker, redash etc."""
+        """Getter: The name of the dashboard tool such as looker, redash etc."""
         return self._inner_dict.get('dashboardTool')  # type: ignore
     
     @dashboardTool.setter
     def dashboardTool(self, value: str) -> None:
+        """Setter: The name of the dashboard tool such as looker, redash etc."""
         self._inner_dict['dashboardTool'] = value
     
     
     @property
     def chartId(self) -> str:
-        """Unique id for the chart. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, chart URL could be used here for Looker such as 'looker.linkedin.com/looks/1234'"""
+        """Getter: Unique id for the chart. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, chart URL could be used here for Looker such as 'looker.linkedin.com/looks/1234'"""
         return self._inner_dict.get('chartId')  # type: ignore
     
     @chartId.setter
     def chartId(self, value: str) -> None:
+        """Setter: Unique id for the chart. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, chart URL could be used here for Looker such as 'looker.linkedin.com/looks/1234'"""
         self._inner_dict['chartId'] = value
     
     
 class ContainerKeyClass(_Aspect):
     """Key for an Asset Container"""
 
 
     ASPECT_NAME = 'containerKey'
-    ASPECT_INFO = {'keyForEntity': 'container', 'entityCategory': '_unset_', 'entityAspects': ['containerProperties', 'editableContainerProperties', 'dataPlatformInstance', 'subTypes', 'ownership', 'container', 'globalTags', 'glossaryTerms', 'institutionalMemory', 'browsePaths', 'status', 'domains', 'browsePathsV2'], 'entityDoc': 'A container of related data assets.'}
+    ASPECT_INFO = {'keyForEntity': 'container', 'entityCategory': '_unset_', 'entityAspects': ['containerProperties', 'editableContainerProperties', 'dataPlatformInstance', 'subTypes', 'ownership', 'container', 'globalTags', 'glossaryTerms', 'institutionalMemory', 'browsePaths', 'status', 'domains'], 'entityDoc': 'A container of related data assets.'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.ContainerKey")
 
     def __init__(self,
         guid: Union[None, str]=None,
     ):
         super().__init__()
         
         self.guid = guid
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ContainerKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.guid = self.RECORD_SCHEMA.fields_dict["guid"].default
     
     
     @property
     def guid(self) -> Union[None, str]:
-        """Unique guid for container"""
+        """Getter: Unique guid for container"""
         return self._inner_dict.get('guid')  # type: ignore
     
     @guid.setter
     def guid(self, value: Union[None, str]) -> None:
+        """Setter: Unique guid for container"""
         self._inner_dict['guid'] = value
     
     
 class CorpGroupKeyClass(_Aspect):
     """Key for a CorpGroup"""
 
 
@@ -8963,25 +10191,33 @@
     def __init__(self,
         name: str,
     ):
         super().__init__()
         
         self.name = name
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpGroupKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
     
     
     @property
     def name(self) -> str:
-        """The URL-encoded name of the AD/LDAP group. Serves as a globally unique identifier within DataHub."""
+        """Getter: The URL-encoded name of the AD/LDAP group. Serves as a globally unique identifier within DataHub."""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: The URL-encoded name of the AD/LDAP group. Serves as a globally unique identifier within DataHub."""
         self._inner_dict['name'] = value
     
     
 class CorpUserKeyClass(_Aspect):
     """Key for a CorpUser"""
 
 
@@ -8992,122 +10228,149 @@
     def __init__(self,
         username: str,
     ):
         super().__init__()
         
         self.username = username
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpUserKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.username = str()
     
     
     @property
     def username(self) -> str:
-        """The name of the AD/LDAP user."""
+        """Getter: The name of the AD/LDAP user."""
         return self._inner_dict.get('username')  # type: ignore
     
     @username.setter
     def username(self, value: str) -> None:
+        """Setter: The name of the AD/LDAP user."""
         self._inner_dict['username'] = value
     
     
 class DashboardKeyClass(_Aspect):
     """Key for a Dashboard"""
 
 
     ASPECT_NAME = 'dashboardKey'
-    ASPECT_INFO = {'keyForEntity': 'dashboard', 'entityCategory': '_unset_', 'entityAspects': ['domains', 'container', 'deprecation', 'dashboardUsageStatistics', 'inputFields', 'subTypes', 'embed', 'dashboardInfo', 'editableDashboardProperties', 'ownership', 'status', 'globalTags', 'browsePaths', 'glossaryTerms', 'institutionalMemory', 'dataPlatformInstance', 'browsePathsV2']}
+    ASPECT_INFO = {'keyForEntity': 'dashboard', 'entityCategory': '_unset_', 'entityAspects': ['domains', 'container', 'deprecation', 'dashboardUsageStatistics', 'inputFields', 'subTypes', 'embed']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DashboardKey")
 
     def __init__(self,
         dashboardTool: str,
         dashboardId: str,
     ):
         super().__init__()
         
         self.dashboardTool = dashboardTool
         self.dashboardId = dashboardId
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DashboardKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.dashboardTool = str()
         self.dashboardId = str()
     
     
     @property
     def dashboardTool(self) -> str:
-        """The name of the dashboard tool such as looker, redash etc."""
+        """Getter: The name of the dashboard tool such as looker, redash etc."""
         return self._inner_dict.get('dashboardTool')  # type: ignore
     
     @dashboardTool.setter
     def dashboardTool(self, value: str) -> None:
+        """Setter: The name of the dashboard tool such as looker, redash etc."""
         self._inner_dict['dashboardTool'] = value
     
     
     @property
     def dashboardId(self) -> str:
-        """Unique id for the dashboard. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, dashboard URL could be used here for Looker such as 'looker.linkedin.com/dashboards/1234'"""
+        """Getter: Unique id for the dashboard. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, dashboard URL could be used here for Looker such as 'looker.linkedin.com/dashboards/1234'"""
         return self._inner_dict.get('dashboardId')  # type: ignore
     
     @dashboardId.setter
     def dashboardId(self, value: str) -> None:
+        """Setter: Unique id for the dashboard. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, dashboard URL could be used here for Looker such as 'looker.linkedin.com/dashboards/1234'"""
         self._inner_dict['dashboardId'] = value
     
     
 class DataFlowKeyClass(_Aspect):
     """Key for a Data Flow"""
 
 
     ASPECT_NAME = 'dataFlowKey'
-    ASPECT_INFO = {'keyForEntity': 'dataFlow', 'entityCategory': 'core', 'entityAspects': ['domains', 'deprecation', 'versionInfo', 'dataFlowInfo', 'editableDataFlowProperties', 'ownership', 'status', 'globalTags', 'browsePaths', 'glossaryTerms', 'institutionalMemory', 'dataPlatformInstance', 'browsePathsV2']}
+    ASPECT_INFO = {'keyForEntity': 'dataFlow', 'entityCategory': 'core', 'entityAspects': ['domains', 'deprecation', 'versionInfo']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataFlowKey")
 
     def __init__(self,
         orchestrator: str,
         flowId: str,
         cluster: str,
     ):
         super().__init__()
         
         self.orchestrator = orchestrator
         self.flowId = flowId
         self.cluster = cluster
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataFlowKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.orchestrator = str()
         self.flowId = str()
         self.cluster = str()
     
     
     @property
     def orchestrator(self) -> str:
-        """Workflow manager like azkaban, airflow which orchestrates the flow"""
+        """Getter: Workflow manager like azkaban, airflow which orchestrates the flow"""
         return self._inner_dict.get('orchestrator')  # type: ignore
     
     @orchestrator.setter
     def orchestrator(self, value: str) -> None:
+        """Setter: Workflow manager like azkaban, airflow which orchestrates the flow"""
         self._inner_dict['orchestrator'] = value
     
     
     @property
     def flowId(self) -> str:
-        """Unique Identifier of the data flow"""
+        """Getter: Unique Identifier of the data flow"""
         return self._inner_dict.get('flowId')  # type: ignore
     
     @flowId.setter
     def flowId(self, value: str) -> None:
+        """Setter: Unique Identifier of the data flow"""
         self._inner_dict['flowId'] = value
     
     
     @property
     def cluster(self) -> str:
-        """Cluster where the flow is executed"""
+        """Getter: Cluster where the flow is executed"""
         return self._inner_dict.get('cluster')  # type: ignore
     
     @cluster.setter
     def cluster(self, value: str) -> None:
+        """Setter: Cluster where the flow is executed"""
         self._inner_dict['cluster'] = value
     
     
 class DataHubAccessTokenKeyClass(_Aspect):
     """Key for a DataHub Access Token"""
 
 
@@ -9118,25 +10381,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubAccessTokenKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """Access token's SHA-256 hashed JWT signature"""
+        """Getter: Access token's SHA-256 hashed JWT signature"""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: Access token's SHA-256 hashed JWT signature"""
         self._inner_dict['id'] = value
     
     
 class DataHubIngestionSourceKeyClass(_Aspect):
     """Key for a DataHub ingestion source"""
 
 
@@ -9147,25 +10418,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubIngestionSourceKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the Ingestion Source, either generated or provided"""
+        """Getter: A unique id for the Ingestion Source, either generated or provided"""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the Ingestion Source, either generated or provided"""
         self._inner_dict['id'] = value
     
     
 class DataHubPolicyKeyClass(_Aspect):
     """Key for a DataHub Policy"""
 
 
@@ -9176,25 +10455,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubPolicyKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the DataHub access policy record. Generated on the server side at policy creation time."""
+        """Getter: A unique id for the DataHub access policy record. Generated on the server side at policy creation time."""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the DataHub access policy record. Generated on the server side at policy creation time."""
         self._inner_dict['id'] = value
     
     
 class DataHubRetentionKeyClass(_Aspect):
     """Key for a DataHub Retention"""
 
 
@@ -9207,36 +10494,45 @@
         aspectName: str,
     ):
         super().__init__()
         
         self.entityName = entityName
         self.aspectName = aspectName
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubRetentionKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.entityName = str()
         self.aspectName = str()
     
     
     @property
     def entityName(self) -> str:
-        """Entity name to apply retention to. * (or empty) for applying defaults."""
+        """Getter: Entity name to apply retention to. * (or empty) for applying defaults."""
         return self._inner_dict.get('entityName')  # type: ignore
     
     @entityName.setter
     def entityName(self, value: str) -> None:
+        """Setter: Entity name to apply retention to. * (or empty) for applying defaults."""
         self._inner_dict['entityName'] = value
     
     
     @property
     def aspectName(self) -> str:
-        """Aspect name to apply retention to. * (or empty) for applying defaults."""
+        """Getter: Aspect name to apply retention to. * (or empty) for applying defaults."""
         return self._inner_dict.get('aspectName')  # type: ignore
     
     @aspectName.setter
     def aspectName(self, value: str) -> None:
+        """Setter: Aspect name to apply retention to. * (or empty) for applying defaults."""
         self._inner_dict['aspectName'] = value
     
     
 class DataHubRoleKeyClass(_Aspect):
     """Key for a DataHub Role"""
 
 
@@ -9247,25 +10543,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubRoleKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the DataHub role record. Generated on the server side at role creation time."""
+        """Getter: A unique id for the DataHub role record. Generated on the server side at role creation time."""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the DataHub role record. Generated on the server side at role creation time."""
         self._inner_dict['id'] = value
     
     
 class DataHubSecretKeyClass(_Aspect):
     """Key for a DataHub Secret"""
 
 
@@ -9276,25 +10580,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubSecretKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the Secret"""
+        """Getter: A unique id for the Secret"""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the Secret"""
         self._inner_dict['id'] = value
     
     
 class DataHubStepStateKeyClass(_Aspect):
     """Key for a DataHub Step State"""
 
 
@@ -9305,25 +10617,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubStepStateKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the state"""
+        """Getter: A unique id for the state"""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the state"""
         self._inner_dict['id'] = value
     
     
 class DataHubUpgradeKeyClass(_Aspect):
     """Key for a DataHubUpgrade"""
 
 
@@ -9334,25 +10654,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubUpgradeKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
         # No docs available.
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        # No docs available.
         self._inner_dict['id'] = value
     
     
 class DataHubViewKeyClass(_Aspect):
     """Key for a DataHub View"""
 
 
@@ -9363,67 +10691,84 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubViewKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the View"""
+        """Getter: A unique id for the View"""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the View"""
         self._inner_dict['id'] = value
     
     
 class DataJobKeyClass(_Aspect):
     """Key for a Data Job"""
 
 
     ASPECT_NAME = 'dataJobKey'
-    ASPECT_INFO = {'keyForEntity': 'dataJob', 'entityCategory': '_unset_', 'entityAspects': ['datahubIngestionRunSummary', 'datahubIngestionCheckpoint', 'domains', 'deprecation', 'versionInfo', 'dataJobInfo', 'dataJobInputOutput', 'editableDataJobProperties', 'ownership', 'status', 'globalTags', 'browsePaths', 'glossaryTerms', 'institutionalMemory', 'dataPlatformInstance', 'browsePathsV2']}
+    ASPECT_INFO = {'keyForEntity': 'dataJob', 'entityCategory': '_unset_', 'entityAspects': ['datahubIngestionRunSummary', 'datahubIngestionCheckpoint', 'domains', 'deprecation', 'versionInfo']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataJobKey")
 
     def __init__(self,
         flow: str,
         jobId: str,
     ):
         super().__init__()
         
         self.flow = flow
         self.jobId = jobId
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataJobKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.flow = str()
         self.jobId = str()
     
     
     @property
     def flow(self) -> str:
-        """Standardized data processing flow urn representing the flow for the job"""
+        """Getter: Standardized data processing flow urn representing the flow for the job"""
         return self._inner_dict.get('flow')  # type: ignore
     
     @flow.setter
     def flow(self, value: str) -> None:
+        """Setter: Standardized data processing flow urn representing the flow for the job"""
         self._inner_dict['flow'] = value
     
     
     @property
     def jobId(self) -> str:
-        """Unique Identifier of the data job"""
+        """Getter: Unique Identifier of the data job"""
         return self._inner_dict.get('jobId')  # type: ignore
     
     @jobId.setter
     def jobId(self, value: str) -> None:
+        """Setter: Unique Identifier of the data job"""
         self._inner_dict['jobId'] = value
     
     
 class DataPlatformInstanceKeyClass(_Aspect):
     """Key for a Dataset"""
 
 
@@ -9436,36 +10781,45 @@
         instance: str,
     ):
         super().__init__()
         
         self.platform = platform
         self.instance = instance
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataPlatformInstanceKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.platform = str()
         self.instance = str()
     
     
     @property
     def platform(self) -> str:
-        """Data platform urn associated with the instance"""
+        """Getter: Data platform urn associated with the instance"""
         return self._inner_dict.get('platform')  # type: ignore
     
     @platform.setter
     def platform(self, value: str) -> None:
+        """Setter: Data platform urn associated with the instance"""
         self._inner_dict['platform'] = value
     
     
     @property
     def instance(self) -> str:
-        """Unique instance id"""
+        """Getter: Unique instance id"""
         return self._inner_dict.get('instance')  # type: ignore
     
     @instance.setter
     def instance(self, value: str) -> None:
+        """Setter: Unique instance id"""
         self._inner_dict['instance'] = value
     
     
 class DataPlatformKeyClass(_Aspect):
     """Key for a Data Platform"""
 
 
@@ -9476,25 +10830,33 @@
     def __init__(self,
         platformName: str,
     ):
         super().__init__()
         
         self.platformName = platformName
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataPlatformKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.platformName = str()
     
     
     @property
     def platformName(self) -> str:
-        """Data platform name i.e. hdfs, oracle, espresso"""
+        """Getter: Data platform name i.e. hdfs, oracle, espresso"""
         return self._inner_dict.get('platformName')  # type: ignore
     
     @platformName.setter
     def platformName(self, value: str) -> None:
+        """Setter: Data platform name i.e. hdfs, oracle, espresso"""
         self._inner_dict['platformName'] = value
     
     
 class DataProcessInstanceKeyClass(_Aspect):
     """Key for an Asset DataProcessInstance"""
 
 
@@ -9505,136 +10867,165 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataProcessInstanceKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the DataProcessInstance . Should be separate from the name used for displaying a DataProcessInstance."""
+        """Getter: A unique id for the DataProcessInstance . Should be separate from the name used for displaying a DataProcessInstance."""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the DataProcessInstance . Should be separate from the name used for displaying a DataProcessInstance."""
         self._inner_dict['id'] = value
     
     
 class DataProcessKeyClass(_Aspect):
     """Key for a Data Process"""
 
 
     ASPECT_NAME = 'dataProcessKey'
-    ASPECT_INFO = {'keyForEntity': 'dataProcess', 'entityCategory': '_unset_', 'entityAspects': ['dataProcessInfo', 'ownership', 'status']}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataProcessKey")
 
     def __init__(self,
         name: str,
         orchestrator: str,
         origin: Union[str, "FabricTypeClass"],
     ):
         super().__init__()
         
         self.name = name
         self.orchestrator = orchestrator
         self.origin = origin
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataProcessKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.orchestrator = str()
         self.origin = FabricTypeClass.DEV
     
     
     @property
     def name(self) -> str:
-        """Process name i.e. an ETL job name"""
+        """Getter: Process name i.e. an ETL job name"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Process name i.e. an ETL job name"""
         self._inner_dict['name'] = value
     
     
     @property
     def orchestrator(self) -> str:
-        """Standardized Orchestrator where data process is defined.
+        """Getter: Standardized Orchestrator where data process is defined.
     TODO: Migrate towards something that can be validated like DataPlatform urn"""
         return self._inner_dict.get('orchestrator')  # type: ignore
     
     @orchestrator.setter
     def orchestrator(self, value: str) -> None:
+        """Setter: Standardized Orchestrator where data process is defined.
+    TODO: Migrate towards something that can be validated like DataPlatform urn"""
         self._inner_dict['orchestrator'] = value
     
     
     @property
     def origin(self) -> Union[str, "FabricTypeClass"]:
-        """Fabric type where dataset belongs to or where it was generated."""
+        """Getter: Fabric type where dataset belongs to or where it was generated."""
         return self._inner_dict.get('origin')  # type: ignore
     
     @origin.setter
     def origin(self, value: Union[str, "FabricTypeClass"]) -> None:
+        """Setter: Fabric type where dataset belongs to or where it was generated."""
         self._inner_dict['origin'] = value
     
     
 class DatasetKeyClass(_Aspect):
     """Key for a Dataset"""
 
 
     ASPECT_NAME = 'datasetKey'
-    ASPECT_INFO = {'keyForEntity': 'dataset', 'entityCategory': 'core', 'entityAspects': ['viewProperties', 'subTypes', 'datasetProfile', 'datasetUsageStatistics', 'operation', 'domains', 'schemaMetadata', 'status', 'container', 'deprecation', 'testResults', 'siblings', 'embed', 'datasetProperties', 'editableDatasetProperties', 'datasetDeprecation', 'datasetUpstreamLineage', 'upstreamLineage', 'institutionalMemory', 'ownership', 'editableSchemaMetadata', 'globalTags', 'glossaryTerms', 'browsePaths', 'dataPlatformInstance', 'browsePathsV2'], 'entityDoc': 'Datasets represent logical or physical data assets stored or represented in various data platforms. Tables, Views, Streams are all instances of datasets.'}
+    ASPECT_INFO = {'keyForEntity': 'dataset', 'entityCategory': 'core', 'entityAspects': ['viewProperties', 'subTypes', 'datasetProfile', 'datasetUsageStatistics', 'operation', 'domains', 'schemaMetadata', 'status', 'container', 'deprecation', 'testResults', 'siblings', 'embed'], 'entityDoc': 'Datasets represent logical or physical data assets stored or represented in various data platforms. Tables, Views, Streams are all instances of datasets.'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DatasetKey")
 
     def __init__(self,
         platform: str,
         name: str,
         origin: Union[str, "FabricTypeClass"],
     ):
         super().__init__()
         
         self.platform = platform
         self.name = name
         self.origin = origin
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.platform = str()
         self.name = str()
         self.origin = FabricTypeClass.DEV
     
     
     @property
     def platform(self) -> str:
-        """Data platform urn associated with the dataset"""
+        """Getter: Data platform urn associated with the dataset"""
         return self._inner_dict.get('platform')  # type: ignore
     
     @platform.setter
     def platform(self, value: str) -> None:
+        """Setter: Data platform urn associated with the dataset"""
         self._inner_dict['platform'] = value
     
     
     @property
     def name(self) -> str:
-        """Unique guid for dataset"""
+        """Getter: Unique guid for dataset"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Unique guid for dataset"""
         self._inner_dict['name'] = value
     
     
     @property
     def origin(self) -> Union[str, "FabricTypeClass"]:
-        """Fabric type where dataset belongs to or where it was generated."""
+        """Getter: Fabric type where dataset belongs to or where it was generated."""
         return self._inner_dict.get('origin')  # type: ignore
     
     @origin.setter
     def origin(self, value: Union[str, "FabricTypeClass"]) -> None:
+        """Setter: Fabric type where dataset belongs to or where it was generated."""
         self._inner_dict['origin'] = value
     
     
 class DomainKeyClass(_Aspect):
     """Key for an Asset Domain"""
 
 
@@ -9645,25 +11036,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DomainKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the domain. Should be separate from the name used for displaying a Domain."""
+        """Getter: A unique id for the domain. Should be separate from the name used for displaying a Domain."""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the domain. Should be separate from the name used for displaying a Domain."""
         self._inner_dict['id'] = value
     
     
 class ExecutionRequestKeyClass(_Aspect):
     """Key for an DataHub Execution Request"""
 
 
@@ -9674,25 +11073,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ExecutionRequestKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the DataHub execution request."""
+        """Getter: A unique id for the DataHub execution request."""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the DataHub execution request."""
         self._inner_dict['id'] = value
     
     
 class GlobalSettingsKeyClass(_Aspect):
     """Key for a Global Settings"""
 
 
@@ -9703,25 +11110,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlobalSettingsKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """Id for the settings. There should be only 1 global settings urn: urn:li:globalSettings:0"""
+        """Getter: Id for the settings. There should be only 1 global settings urn: urn:li:globalSettings:0"""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: Id for the settings. There should be only 1 global settings urn: urn:li:globalSettings:0"""
         self._inner_dict['id'] = value
     
     
 class GlossaryNodeKeyClass(_Aspect):
     """Key for a GlossaryNode"""
 
 
@@ -9732,54 +11147,70 @@
     def __init__(self,
         name: str,
     ):
         super().__init__()
         
         self.name = name
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlossaryNodeKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
     
     
     @property
     def name(self) -> str:
         # No docs available.
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        # No docs available.
         self._inner_dict['name'] = value
     
     
 class GlossaryTermKeyClass(_Aspect):
     """Key for a GlossaryTerm"""
 
 
     ASPECT_NAME = 'glossaryTermKey'
-    ASPECT_INFO = {'keyForEntity': 'glossaryTerm', 'entityCategory': '_unset_', 'entityAspects': ['glossaryTermInfo', 'glossaryRelatedTerms', 'institutionalMemory', 'schemaMetadata', 'ownership', 'deprecation', 'domains', 'status', 'browsePaths']}
+    ASPECT_INFO = {'keyForEntity': 'glossaryTerm', 'entityCategory': '_unset_', 'entityAspects': ['glossaryTermInfo', 'institutionalMemory', 'schemaMetadata', 'ownership', 'deprecation', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.GlossaryTermKey")
 
     def __init__(self,
         name: str,
     ):
         super().__init__()
         
         self.name = name
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlossaryTermKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
     
     
     @property
     def name(self) -> str:
-        """The term name, which serves as a unique id"""
+        """Getter: The term name, which serves as a unique id"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: The term name, which serves as a unique id"""
         self._inner_dict['name'] = value
     
     
 class InviteTokenKeyClass(_Aspect):
     """Key for an InviteToken."""
 
 
@@ -9790,358 +11221,432 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "InviteTokenKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the invite token."""
+        """Getter: A unique id for the invite token."""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the invite token."""
         self._inner_dict['id'] = value
     
     
 class MLFeatureKeyClass(_Aspect):
     """Key for an MLFeature"""
 
 
     ASPECT_NAME = 'mlFeatureKey'
-    ASPECT_INFO = {'keyForEntity': 'mlFeature', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlFeatureProperties', 'domains', 'mlFeatureProperties', 'ownership', 'institutionalMemory', 'status', 'deprecation', 'browsePaths', 'globalTags', 'dataPlatformInstance', 'browsePathsV2']}
+    ASPECT_INFO = {'keyForEntity': 'mlFeature', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlFeatureProperties', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLFeatureKey")
 
     def __init__(self,
         featureNamespace: str,
         name: str,
     ):
         super().__init__()
         
         self.featureNamespace = featureNamespace
         self.name = name
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLFeatureKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.featureNamespace = str()
         self.name = str()
     
     
     @property
     def featureNamespace(self) -> str:
-        """Namespace for the feature"""
+        """Getter: Namespace for the feature"""
         return self._inner_dict.get('featureNamespace')  # type: ignore
     
     @featureNamespace.setter
     def featureNamespace(self, value: str) -> None:
+        """Setter: Namespace for the feature"""
         self._inner_dict['featureNamespace'] = value
     
     
     @property
     def name(self) -> str:
-        """Name of the feature"""
+        """Getter: Name of the feature"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the feature"""
         self._inner_dict['name'] = value
     
     
 class MLFeatureTableKeyClass(_Aspect):
     """Key for an MLFeatureTable"""
 
 
     ASPECT_NAME = 'mlFeatureTableKey'
-    ASPECT_INFO = {'keyForEntity': 'mlFeatureTable', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlFeatureTableProperties', 'domains', 'mlFeatureTableProperties', 'ownership', 'institutionalMemory', 'status', 'deprecation', 'browsePaths', 'globalTags', 'dataPlatformInstance', 'browsePathsV2']}
+    ASPECT_INFO = {'keyForEntity': 'mlFeatureTable', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlFeatureTableProperties', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLFeatureTableKey")
 
     def __init__(self,
         platform: str,
         name: str,
     ):
         super().__init__()
         
         self.platform = platform
         self.name = name
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLFeatureTableKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.platform = str()
         self.name = str()
     
     
     @property
     def platform(self) -> str:
-        """Data platform urn associated with the feature table"""
+        """Getter: Data platform urn associated with the feature table"""
         return self._inner_dict.get('platform')  # type: ignore
     
     @platform.setter
     def platform(self, value: str) -> None:
+        """Setter: Data platform urn associated with the feature table"""
         self._inner_dict['platform'] = value
     
     
     @property
     def name(self) -> str:
-        """Name of the feature table"""
+        """Getter: Name of the feature table"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the feature table"""
         self._inner_dict['name'] = value
     
     
 class MLModelDeploymentKeyClass(_Aspect):
     """Key for an ML model deployment"""
 
 
     ASPECT_NAME = 'mlModelDeploymentKey'
-    ASPECT_INFO = {'keyForEntity': 'mlModelDeployment', 'entityCategory': 'core', 'entityAspects': ['mlModelDeploymentProperties', 'ownership', 'status', 'deprecation', 'globalTags', 'dataPlatformInstance']}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLModelDeploymentKey")
 
     def __init__(self,
         platform: str,
         name: str,
         origin: Union[str, "FabricTypeClass"],
     ):
         super().__init__()
         
         self.platform = platform
         self.name = name
         self.origin = origin
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelDeploymentKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.platform = str()
         self.name = str()
         self.origin = FabricTypeClass.DEV
     
     
     @property
     def platform(self) -> str:
-        """Standardized platform urn for the model Deployment"""
+        """Getter: Standardized platform urn for the model Deployment"""
         return self._inner_dict.get('platform')  # type: ignore
     
     @platform.setter
     def platform(self, value: str) -> None:
+        """Setter: Standardized platform urn for the model Deployment"""
         self._inner_dict['platform'] = value
     
     
     @property
     def name(self) -> str:
-        """Name of the MLModelDeployment"""
+        """Getter: Name of the MLModelDeployment"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the MLModelDeployment"""
         self._inner_dict['name'] = value
     
     
     @property
     def origin(self) -> Union[str, "FabricTypeClass"]:
-        """Fabric type where model Deployment belongs to or where it was generated"""
+        """Getter: Fabric type where model Deployment belongs to or where it was generated"""
         return self._inner_dict.get('origin')  # type: ignore
     
     @origin.setter
     def origin(self, value: Union[str, "FabricTypeClass"]) -> None:
+        """Setter: Fabric type where model Deployment belongs to or where it was generated"""
         self._inner_dict['origin'] = value
     
     
 class MLModelGroupKeyClass(_Aspect):
     """Key for an ML model group"""
 
 
     ASPECT_NAME = 'mlModelGroupKey'
-    ASPECT_INFO = {'keyForEntity': 'mlModelGroup', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlModelGroupProperties', 'domains', 'mlModelGroupProperties', 'ownership', 'status', 'deprecation', 'browsePaths', 'globalTags', 'dataPlatformInstance', 'browsePathsV2']}
+    ASPECT_INFO = {'keyForEntity': 'mlModelGroup', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlModelGroupProperties', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLModelGroupKey")
 
     def __init__(self,
         platform: str,
         name: str,
         origin: Union[str, "FabricTypeClass"],
     ):
         super().__init__()
         
         self.platform = platform
         self.name = name
         self.origin = origin
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelGroupKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.platform = str()
         self.name = str()
         self.origin = FabricTypeClass.DEV
     
     
     @property
     def platform(self) -> str:
-        """Standardized platform urn for the model group"""
+        """Getter: Standardized platform urn for the model group"""
         return self._inner_dict.get('platform')  # type: ignore
     
     @platform.setter
     def platform(self, value: str) -> None:
+        """Setter: Standardized platform urn for the model group"""
         self._inner_dict['platform'] = value
     
     
     @property
     def name(self) -> str:
-        """Name of the MLModelGroup"""
+        """Getter: Name of the MLModelGroup"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the MLModelGroup"""
         self._inner_dict['name'] = value
     
     
     @property
     def origin(self) -> Union[str, "FabricTypeClass"]:
-        """Fabric type where model group belongs to or where it was generated"""
+        """Getter: Fabric type where model group belongs to or where it was generated"""
         return self._inner_dict.get('origin')  # type: ignore
     
     @origin.setter
     def origin(self, value: Union[str, "FabricTypeClass"]) -> None:
+        """Setter: Fabric type where model group belongs to or where it was generated"""
         self._inner_dict['origin'] = value
     
     
 class MLModelKeyClass(_Aspect):
     """Key for an ML model"""
 
 
     ASPECT_NAME = 'mlModelKey'
-    ASPECT_INFO = {'keyForEntity': 'mlModel', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlModelProperties', 'domains', 'ownership', 'mlModelProperties', 'intendedUse', 'mlModelFactorPrompts', 'mlModelMetrics', 'mlModelEvaluationData', 'mlModelTrainingData', 'mlModelQuantitativeAnalyses', 'mlModelEthicalConsiderations', 'mlModelCaveatsAndRecommendations', 'institutionalMemory', 'sourceCode', 'status', 'cost', 'deprecation', 'browsePaths', 'globalTags', 'dataPlatformInstance', 'browsePathsV2']}
+    ASPECT_INFO = {'keyForEntity': 'mlModel', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlModelProperties', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLModelKey")
 
     def __init__(self,
         platform: str,
         name: str,
         origin: Union[str, "FabricTypeClass"],
     ):
         super().__init__()
         
         self.platform = platform
         self.name = name
         self.origin = origin
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.platform = str()
         self.name = str()
         self.origin = FabricTypeClass.DEV
     
     
     @property
     def platform(self) -> str:
-        """Standardized platform urn for the model"""
+        """Getter: Standardized platform urn for the model"""
         return self._inner_dict.get('platform')  # type: ignore
     
     @platform.setter
     def platform(self, value: str) -> None:
+        """Setter: Standardized platform urn for the model"""
         self._inner_dict['platform'] = value
     
     
     @property
     def name(self) -> str:
-        """Name of the MLModel"""
+        """Getter: Name of the MLModel"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the MLModel"""
         self._inner_dict['name'] = value
     
     
     @property
     def origin(self) -> Union[str, "FabricTypeClass"]:
-        """Fabric type where model belongs to or where it was generated"""
+        """Getter: Fabric type where model belongs to or where it was generated"""
         return self._inner_dict.get('origin')  # type: ignore
     
     @origin.setter
     def origin(self, value: Union[str, "FabricTypeClass"]) -> None:
+        """Setter: Fabric type where model belongs to or where it was generated"""
         self._inner_dict['origin'] = value
     
     
 class MLPrimaryKeyKeyClass(_Aspect):
     """Key for an MLPrimaryKey"""
 
 
     ASPECT_NAME = 'mlPrimaryKeyKey'
-    ASPECT_INFO = {'keyForEntity': 'mlPrimaryKey', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlPrimaryKeyProperties', 'domains', 'mlPrimaryKeyProperties', 'ownership', 'institutionalMemory', 'status', 'deprecation', 'globalTags', 'dataPlatformInstance']}
+    ASPECT_INFO = {'keyForEntity': 'mlPrimaryKey', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlPrimaryKeyProperties', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLPrimaryKeyKey")
 
     def __init__(self,
         featureNamespace: str,
         name: str,
     ):
         super().__init__()
         
         self.featureNamespace = featureNamespace
         self.name = name
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLPrimaryKeyKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.featureNamespace = str()
         self.name = str()
     
     
     @property
     def featureNamespace(self) -> str:
-        """Namespace for the primary key"""
+        """Getter: Namespace for the primary key"""
         return self._inner_dict.get('featureNamespace')  # type: ignore
     
     @featureNamespace.setter
     def featureNamespace(self, value: str) -> None:
+        """Setter: Namespace for the primary key"""
         self._inner_dict['featureNamespace'] = value
     
     
     @property
     def name(self) -> str:
-        """Name of the primary key"""
+        """Getter: Name of the primary key"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the primary key"""
         self._inner_dict['name'] = value
     
     
 class NotebookKeyClass(_Aspect):
     """Key for a Notebook"""
 
 
     ASPECT_NAME = 'notebookKey'
-    ASPECT_INFO = {'keyForEntity': 'notebook', 'entityCategory': '_unset_', 'entityAspects': ['notebookInfo', 'notebookContent', 'editableNotebookProperties', 'ownership', 'status', 'globalTags', 'glossaryTerms', 'browsePaths', 'institutionalMemory', 'domains', 'subTypes', 'dataPlatformInstance', 'browsePathsV2'], 'entityDoc': 'Notebook represents a combination of query, text, chart and etc. This is in BETA version'}
+    ASPECT_INFO = {'keyForEntity': 'notebook', 'entityCategory': '_unset_', 'entityAspects': ['notebookInfo', 'notebookContent', 'editableNotebookProperties', 'ownership', 'status', 'globalTags', 'glossaryTerms', 'browsePaths', 'institutionalMemory', 'domains', 'subTypes', 'dataPlatformInstance'], 'entityDoc': 'Notebook represents a combination of query, text, chart and etc. This is in BETA version'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.NotebookKey")
 
     def __init__(self,
         notebookTool: str,
         notebookId: str,
     ):
         super().__init__()
         
         self.notebookTool = notebookTool
         self.notebookId = notebookId
     
+    @classmethod
+    def construct_with_defaults(cls) -> "NotebookKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.notebookTool = str()
         self.notebookId = str()
     
     
     @property
     def notebookTool(self) -> str:
-        """The name of the Notebook tool such as QueryBook, etc."""
+        """Getter: The name of the Notebook tool such as QueryBook, etc."""
         return self._inner_dict.get('notebookTool')  # type: ignore
     
     @notebookTool.setter
     def notebookTool(self, value: str) -> None:
+        """Setter: The name of the Notebook tool such as QueryBook, etc."""
         self._inner_dict['notebookTool'] = value
     
     
     @property
     def notebookId(self) -> str:
-        """Unique id for the Notebook. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773'"""
+        """Getter: Unique id for the Notebook. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773'"""
         return self._inner_dict.get('notebookId')  # type: ignore
     
     @notebookId.setter
     def notebookId(self, value: str) -> None:
+        """Setter: Unique id for the Notebook. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773'"""
         self._inner_dict['notebookId'] = value
     
     
 class PostKeyClass(_Aspect):
     """Key for a Post."""
 
 
@@ -10152,25 +11657,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "PostKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the DataHub Post record. Generated on the server side at Post creation time."""
+        """Getter: A unique id for the DataHub Post record. Generated on the server side at Post creation time."""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the DataHub Post record. Generated on the server side at Post creation time."""
         self._inner_dict['id'] = value
     
     
 class QueryKeyClass(_Aspect):
     """Key for a Query"""
 
 
@@ -10181,25 +11694,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "QueryKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """A unique id for the Query."""
+        """Getter: A unique id for the Query."""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: A unique id for the Query."""
         self._inner_dict['id'] = value
     
     
 class SchemaFieldKeyClass(_Aspect):
     """Key for a SchemaField"""
 
 
@@ -10212,65 +11733,82 @@
         fieldPath: str,
     ):
         super().__init__()
         
         self.parent = parent
         self.fieldPath = fieldPath
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SchemaFieldKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.parent = str()
         self.fieldPath = str()
     
     
     @property
     def parent(self) -> str:
-        """Parent associated with the schema field"""
+        """Getter: Parent associated with the schema field"""
         return self._inner_dict.get('parent')  # type: ignore
     
     @parent.setter
     def parent(self, value: str) -> None:
+        """Setter: Parent associated with the schema field"""
         self._inner_dict['parent'] = value
     
     
     @property
     def fieldPath(self) -> str:
-        """fieldPath identifying the schema field"""
+        """Getter: fieldPath identifying the schema field"""
         return self._inner_dict.get('fieldPath')  # type: ignore
     
     @fieldPath.setter
     def fieldPath(self, value: str) -> None:
+        """Setter: fieldPath identifying the schema field"""
         self._inner_dict['fieldPath'] = value
     
     
 class TagKeyClass(_Aspect):
     """Key for a Tag"""
 
 
     ASPECT_NAME = 'tagKey'
-    ASPECT_INFO = {'keyForEntity': 'tag', 'entityCategory': '_unset_', 'entityAspects': ['tagProperties', 'ownership', 'deprecation', 'status']}
+    ASPECT_INFO = {'keyForEntity': 'tag', 'entityCategory': '_unset_', 'entityAspects': ['tagProperties', 'ownership', 'deprecation']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.TagKey")
 
     def __init__(self,
         name: str,
     ):
         super().__init__()
         
         self.name = name
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TagKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
     
     
     @property
     def name(self) -> str:
-        """The tag name, which serves as a unique id"""
+        """Getter: The tag name, which serves as a unique id"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: The tag name, which serves as a unique id"""
         self._inner_dict['name'] = value
     
     
 class TelemetryKeyClass(_Aspect):
     """Key for the telemetry client ID, only one should ever exist"""
 
 
@@ -10281,25 +11819,33 @@
     def __init__(self,
         name: str,
     ):
         super().__init__()
         
         self.name = name
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TelemetryKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
     
     
     @property
     def name(self) -> str:
-        """The telemetry entity name, which serves as a unique id"""
+        """Getter: The telemetry entity name, which serves as a unique id"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: The telemetry entity name, which serves as a unique id"""
         self._inner_dict['name'] = value
     
     
 class TestKeyClass(_Aspect):
     """Key for a Test"""
 
 
@@ -10310,25 +11856,33 @@
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
         self.id = id
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TestKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.id = str()
     
     
     @property
     def id(self) -> str:
-        """Unique id for the test"""
+        """Getter: Unique id for the test"""
         return self._inner_dict.get('id')  # type: ignore
     
     @id.setter
     def id(self, value: str) -> None:
+        """Setter: Unique id for the test"""
         self._inner_dict['id'] = value
     
     
 class ConditionClass(object):
     """The matching condition in a filter criterion"""
     
     
@@ -10340,17 +11894,14 @@
     
     """Represent the relation: field = value, e.g. platform = hdfs"""
     EQUAL = "EQUAL"
     
     """Represent the relation: field is null, e.g. platform is null"""
     IS_NULL = "IS_NULL"
     
-    """Represents the relation: field exists and is non-empty, e.g. owners is not null and != [] (empty)"""
-    EXISTS = "EXISTS"
-    
     """Represent the relation greater than, e.g. ownerCount > 5"""
     GREATER_THAN = "GREATER_THAN"
     
     """Represent the relation greater than or equal to, e.g. ownerCount >= 5"""
     GREATER_THAN_OR_EQUAL_TO = "GREATER_THAN_OR_EQUAL_TO"
     
     """Represent the relation: String field is one of the array values to, e.g. name in ["Profile", "Event"]"""
@@ -10373,25 +11924,33 @@
     def __init__(self,
         and_: List["CriterionClass"],
     ):
         super().__init__()
         
         self.and_ = and_
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ConjunctiveCriterionClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.and_ = list()
     
     
     @property
     def and_(self) -> List["CriterionClass"]:
-        """A list of and criteria the filter applies to the query"""
+        """Getter: A list of and criteria the filter applies to the query"""
         return self._inner_dict.get('and')  # type: ignore
     
     @and_.setter
     def and_(self, value: List["CriterionClass"]) -> None:
+        """Setter: A list of and criteria the filter applies to the query"""
         self._inner_dict['and'] = value
     
     
 class CriterionClass(DictWrapper):
     """A criterion for matching a field with given value"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.query.filter.Criterion")
@@ -10418,70 +11977,83 @@
             self.condition = condition
         if negated is None:
             # default: False
             self.negated = self.RECORD_SCHEMA.fields_dict["negated"].default
         else:
             self.negated = negated
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CriterionClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.field = str()
         self.value = str()
         self.values = list()
         self.condition = self.RECORD_SCHEMA.fields_dict["condition"].default
         self.negated = self.RECORD_SCHEMA.fields_dict["negated"].default
     
     
     @property
     def field(self) -> str:
-        """The name of the field that the criterion refers to"""
+        """Getter: The name of the field that the criterion refers to"""
         return self._inner_dict.get('field')  # type: ignore
     
     @field.setter
     def field(self, value: str) -> None:
+        """Setter: The name of the field that the criterion refers to"""
         self._inner_dict['field'] = value
     
     
     @property
     def value(self) -> str:
-        """The value of the intended field"""
+        """Getter: The value of the intended field"""
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: str) -> None:
+        """Setter: The value of the intended field"""
         self._inner_dict['value'] = value
     
     
     @property
     def values(self) -> List[str]:
-        """Values. one of which the intended field should match
+        """Getter: Values. one of which the intended field should match
     Note, if values is set, the above "value" field will be ignored"""
         return self._inner_dict.get('values')  # type: ignore
     
     @values.setter
     def values(self, value: List[str]) -> None:
+        """Setter: Values. one of which the intended field should match
+    Note, if values is set, the above "value" field will be ignored"""
         self._inner_dict['values'] = value
     
     
     @property
     def condition(self) -> Union[str, "ConditionClass"]:
-        """The condition for the criterion, e.g. EQUAL, START_WITH"""
+        """Getter: The condition for the criterion, e.g. EQUAL, START_WITH"""
         return self._inner_dict.get('condition')  # type: ignore
     
     @condition.setter
     def condition(self, value: Union[str, "ConditionClass"]) -> None:
+        """Setter: The condition for the criterion, e.g. EQUAL, START_WITH"""
         self._inner_dict['condition'] = value
     
     
     @property
     def negated(self) -> bool:
-        """Whether the condition should be negated"""
+        """Getter: Whether the condition should be negated"""
         return self._inner_dict.get('negated')  # type: ignore
     
     @negated.setter
     def negated(self, value: bool) -> None:
+        """Setter: Whether the condition should be negated"""
         self._inner_dict['negated'] = value
     
     
 class FilterClass(DictWrapper):
     """The filter for finding a record or a collection of records"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.query.filter.Filter")
@@ -10490,36 +12062,45 @@
         criteria: Union[None, List["CriterionClass"]]=None,
     ):
         super().__init__()
         
         self.or_ = or_
         self.criteria = criteria
     
+    @classmethod
+    def construct_with_defaults(cls) -> "FilterClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.or_ = self.RECORD_SCHEMA.fields_dict["or"].default
         self.criteria = self.RECORD_SCHEMA.fields_dict["criteria"].default
     
     
     @property
     def or_(self) -> Union[None, List["ConjunctiveCriterionClass"]]:
-        """A list of disjunctive criterion for the filter. (or operation to combine filters)"""
+        """Getter: A list of disjunctive criterion for the filter. (or operation to combine filters)"""
         return self._inner_dict.get('or')  # type: ignore
     
     @or_.setter
     def or_(self, value: Union[None, List["ConjunctiveCriterionClass"]]) -> None:
+        """Setter: A list of disjunctive criterion for the filter. (or operation to combine filters)"""
         self._inner_dict['or'] = value
     
     
     @property
     def criteria(self) -> Union[None, List["CriterionClass"]]:
-        """Deprecated! A list of conjunctive criterion for the filter. If "or" field is provided, then this field is ignored."""
+        """Getter: Deprecated! A list of conjunctive criterion for the filter. If "or" field is provided, then this field is ignored."""
         return self._inner_dict.get('criteria')  # type: ignore
     
     @criteria.setter
     def criteria(self, value: Union[None, List["CriterionClass"]]) -> None:
+        """Setter: Deprecated! A list of conjunctive criterion for the filter. If "or" field is provided, then this field is ignored."""
         self._inner_dict['criteria'] = value
     
     
 class ChartSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific Chart entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.ChartSnapshot")
@@ -10528,36 +12109,45 @@
         aspects: List[Union["ChartKeyClass", "ChartInfoClass", "ChartQueryClass", "EditableChartPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ChartSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["ChartKeyClass", "ChartInfoClass", "ChartQueryClass", "EditableChartPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]]:
-        """The list of metadata aspects associated with the chart. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the chart. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["ChartKeyClass", "ChartInfoClass", "ChartQueryClass", "EditableChartPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the chart. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class CorpGroupSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific CorpGroup entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.CorpGroupSnapshot")
@@ -10566,36 +12156,45 @@
         aspects: List[Union["CorpGroupKeyClass", "CorpGroupInfoClass", "GlobalTagsClass", "StatusClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpGroupSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["CorpGroupKeyClass", "CorpGroupInfoClass", "GlobalTagsClass", "StatusClass"]]:
-        """The list of metadata aspects associated with the LdapUser. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the LdapUser. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["CorpGroupKeyClass", "CorpGroupInfoClass", "GlobalTagsClass", "StatusClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the LdapUser. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class CorpUserSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific CorpUser entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.CorpUserSnapshot")
@@ -10604,36 +12203,45 @@
         aspects: List[Union["CorpUserKeyClass", "CorpUserInfoClass", "CorpUserEditableInfoClass", "CorpUserStatusClass", "GroupMembershipClass", "GlobalTagsClass", "StatusClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CorpUserSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["CorpUserKeyClass", "CorpUserInfoClass", "CorpUserEditableInfoClass", "CorpUserStatusClass", "GroupMembershipClass", "GlobalTagsClass", "StatusClass"]]:
-        """The list of metadata aspects associated with the CorpUser. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the CorpUser. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["CorpUserKeyClass", "CorpUserInfoClass", "CorpUserEditableInfoClass", "CorpUserStatusClass", "GroupMembershipClass", "GlobalTagsClass", "StatusClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the CorpUser. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class DashboardSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific Dashboard entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.DashboardSnapshot")
@@ -10642,36 +12250,45 @@
         aspects: List[Union["DashboardKeyClass", "DashboardInfoClass", "EditableDashboardPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DashboardSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["DashboardKeyClass", "DashboardInfoClass", "EditableDashboardPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]]:
-        """The list of metadata aspects associated with the dashboard. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the dashboard. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["DashboardKeyClass", "DashboardInfoClass", "EditableDashboardPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the dashboard. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class DataFlowSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific DataFlow entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.DataFlowSnapshot")
@@ -10680,36 +12297,45 @@
         aspects: List[Union["DataFlowKeyClass", "DataFlowInfoClass", "EditableDataFlowPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataFlowSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["DataFlowKeyClass", "DataFlowInfoClass", "EditableDataFlowPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]]:
-        """The list of metadata aspects associated with the data flow. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the data flow. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["DataFlowKeyClass", "DataFlowInfoClass", "EditableDataFlowPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the data flow. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class DataHubPolicySnapshotClass(DictWrapper):
     """A metadata snapshot for DataHub Access Policy data."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.DataHubPolicySnapshot")
@@ -10718,36 +12344,45 @@
         aspects: List[Union["DataHubPolicyKeyClass", "DataHubPolicyInfoClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubPolicySnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["DataHubPolicyKeyClass", "DataHubPolicyInfoClass"]]:
-        """The list of metadata aspects associated with the DataHub access policy."""
+        """Getter: The list of metadata aspects associated with the DataHub access policy."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["DataHubPolicyKeyClass", "DataHubPolicyInfoClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the DataHub access policy."""
         self._inner_dict['aspects'] = value
     
     
 class DataHubRetentionSnapshotClass(DictWrapper):
     """A metadata snapshot for DataHub Access Policy data."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.DataHubRetentionSnapshot")
@@ -10756,36 +12391,45 @@
         aspects: List[Union["DataHubRetentionKeyClass", "DataHubRetentionConfigClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubRetentionSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["DataHubRetentionKeyClass", "DataHubRetentionConfigClass"]]:
-        """The list of metadata aspects associated with the DataHub access policy."""
+        """Getter: The list of metadata aspects associated with the DataHub access policy."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["DataHubRetentionKeyClass", "DataHubRetentionConfigClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the DataHub access policy."""
         self._inner_dict['aspects'] = value
     
     
 class DataJobSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific DataJob entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.DataJobSnapshot")
@@ -10794,36 +12438,45 @@
         aspects: List[Union["DataJobKeyClass", "DataJobInfoClass", "DataJobInputOutputClass", "EditableDataJobPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataJobSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["DataJobKeyClass", "DataJobInfoClass", "DataJobInputOutputClass", "EditableDataJobPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]]:
-        """The list of metadata aspects associated with the data job. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the data job. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["DataJobKeyClass", "DataJobInfoClass", "DataJobInputOutputClass", "EditableDataJobPropertiesClass", "OwnershipClass", "StatusClass", "GlobalTagsClass", "BrowsePathsClass", "GlossaryTermsClass", "InstitutionalMemoryClass", "DataPlatformInstanceClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the data job. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class DataPlatformSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific dataplatform entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.DataPlatformSnapshot")
@@ -10832,36 +12485,45 @@
         aspects: List[Union["DataPlatformKeyClass", "DataPlatformInfoClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataPlatformSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["DataPlatformKeyClass", "DataPlatformInfoClass"]]:
-        """The list of metadata aspects associated with the data platform. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the data platform. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["DataPlatformKeyClass", "DataPlatformInfoClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the data platform. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class DataProcessSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific Data process entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.DataProcessSnapshot")
@@ -10870,36 +12532,45 @@
         aspects: List[Union["DataProcessKeyClass", "OwnershipClass", "DataProcessInfoClass", "StatusClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataProcessSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["DataProcessKeyClass", "OwnershipClass", "DataProcessInfoClass", "StatusClass"]]:
-        """The list of metadata aspects associated with the data process. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the data process. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["DataProcessKeyClass", "OwnershipClass", "DataProcessInfoClass", "StatusClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the data process. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class DatasetSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific dataset entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.DatasetSnapshot")
@@ -10908,36 +12579,45 @@
         aspects: List[Union["DatasetKeyClass", "DatasetPropertiesClass", "EditableDatasetPropertiesClass", "DatasetDeprecationClass", "DatasetUpstreamLineageClass", "UpstreamLineageClass", "InstitutionalMemoryClass", "OwnershipClass", "StatusClass", "SchemaMetadataClass", "EditableSchemaMetadataClass", "GlobalTagsClass", "GlossaryTermsClass", "BrowsePathsClass", "DataPlatformInstanceClass", "ViewPropertiesClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["DatasetKeyClass", "DatasetPropertiesClass", "EditableDatasetPropertiesClass", "DatasetDeprecationClass", "DatasetUpstreamLineageClass", "UpstreamLineageClass", "InstitutionalMemoryClass", "OwnershipClass", "StatusClass", "SchemaMetadataClass", "EditableSchemaMetadataClass", "GlobalTagsClass", "GlossaryTermsClass", "BrowsePathsClass", "DataPlatformInstanceClass", "ViewPropertiesClass"]]:
-        """The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["DatasetKeyClass", "DatasetPropertiesClass", "EditableDatasetPropertiesClass", "DatasetDeprecationClass", "DatasetUpstreamLineageClass", "UpstreamLineageClass", "InstitutionalMemoryClass", "OwnershipClass", "StatusClass", "SchemaMetadataClass", "EditableSchemaMetadataClass", "GlobalTagsClass", "GlossaryTermsClass", "BrowsePathsClass", "DataPlatformInstanceClass", "ViewPropertiesClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class GlossaryNodeSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific GlossaryNode entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.GlossaryNodeSnapshot")
@@ -10946,36 +12626,45 @@
         aspects: List[Union["GlossaryNodeKeyClass", "GlossaryNodeInfoClass", "OwnershipClass", "StatusClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlossaryNodeSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["GlossaryNodeKeyClass", "GlossaryNodeInfoClass", "OwnershipClass", "StatusClass"]]:
-        """The list of metadata aspects associated with the GlossaryNode. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the GlossaryNode. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["GlossaryNodeKeyClass", "GlossaryNodeInfoClass", "OwnershipClass", "StatusClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the GlossaryNode. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class GlossaryTermSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific GlossaryTerm entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.GlossaryTermSnapshot")
@@ -10984,36 +12673,45 @@
         aspects: List[Union["GlossaryTermKeyClass", "GlossaryTermInfoClass", "OwnershipClass", "StatusClass", "BrowsePathsClass", "GlossaryRelatedTermsClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlossaryTermSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["GlossaryTermKeyClass", "GlossaryTermInfoClass", "OwnershipClass", "StatusClass", "BrowsePathsClass", "GlossaryRelatedTermsClass"]]:
-        """The list of metadata aspects associated with the GlossaryTerm. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the GlossaryTerm. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["GlossaryTermKeyClass", "GlossaryTermInfoClass", "OwnershipClass", "StatusClass", "BrowsePathsClass", "GlossaryRelatedTermsClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the GlossaryTerm. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class MLFeatureSnapshotClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.MLFeatureSnapshot")
@@ -11022,36 +12720,45 @@
         aspects: List[Union["MLFeatureKeyClass", "MLFeaturePropertiesClass", "OwnershipClass", "InstitutionalMemoryClass", "StatusClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLFeatureSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["MLFeatureKeyClass", "MLFeaturePropertiesClass", "OwnershipClass", "InstitutionalMemoryClass", "StatusClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]:
-        """The list of metadata aspects associated with the MLFeature. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the MLFeature. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["MLFeatureKeyClass", "MLFeaturePropertiesClass", "OwnershipClass", "InstitutionalMemoryClass", "StatusClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the MLFeature. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class MLFeatureTableSnapshotClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.MLFeatureTableSnapshot")
@@ -11060,36 +12767,45 @@
         aspects: List[Union["MLFeatureTableKeyClass", "MLFeatureTablePropertiesClass", "OwnershipClass", "InstitutionalMemoryClass", "StatusClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLFeatureTableSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["MLFeatureTableKeyClass", "MLFeatureTablePropertiesClass", "OwnershipClass", "InstitutionalMemoryClass", "StatusClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]:
-        """The list of metadata aspects associated with the MLFeatureTable. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the MLFeatureTable. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["MLFeatureTableKeyClass", "MLFeatureTablePropertiesClass", "OwnershipClass", "InstitutionalMemoryClass", "StatusClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the MLFeatureTable. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class MLModelDeploymentSnapshotClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.MLModelDeploymentSnapshot")
@@ -11098,36 +12814,45 @@
         aspects: List[Union["MLModelDeploymentKeyClass", "MLModelDeploymentPropertiesClass", "OwnershipClass", "StatusClass", "DeprecationClass", "GlobalTagsClass", "DataPlatformInstanceClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelDeploymentSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["MLModelDeploymentKeyClass", "MLModelDeploymentPropertiesClass", "OwnershipClass", "StatusClass", "DeprecationClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]:
-        """The list of metadata aspects associated with the MLModelDeployment. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the MLModelDeployment. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["MLModelDeploymentKeyClass", "MLModelDeploymentPropertiesClass", "OwnershipClass", "StatusClass", "DeprecationClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the MLModelDeployment. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class MLModelGroupSnapshotClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.MLModelGroupSnapshot")
@@ -11136,36 +12861,45 @@
         aspects: List[Union["MLModelGroupKeyClass", "MLModelGroupPropertiesClass", "OwnershipClass", "StatusClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelGroupSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["MLModelGroupKeyClass", "MLModelGroupPropertiesClass", "OwnershipClass", "StatusClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]:
-        """The list of metadata aspects associated with the MLModelGroup. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the MLModelGroup. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["MLModelGroupKeyClass", "MLModelGroupPropertiesClass", "OwnershipClass", "StatusClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the MLModelGroup. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class MLModelSnapshotClass(DictWrapper):
     """MLModel Snapshot entity details."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.MLModelSnapshot")
@@ -11174,36 +12908,45 @@
         aspects: List[Union["MLModelKeyClass", "OwnershipClass", "MLModelPropertiesClass", "IntendedUseClass", "MLModelFactorPromptsClass", "MetricsClass", "EvaluationDataClass", "TrainingDataClass", "QuantitativeAnalysesClass", "EthicalConsiderationsClass", "CaveatsAndRecommendationsClass", "InstitutionalMemoryClass", "SourceCodeClass", "StatusClass", "CostClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["MLModelKeyClass", "OwnershipClass", "MLModelPropertiesClass", "IntendedUseClass", "MLModelFactorPromptsClass", "MetricsClass", "EvaluationDataClass", "TrainingDataClass", "QuantitativeAnalysesClass", "EthicalConsiderationsClass", "CaveatsAndRecommendationsClass", "InstitutionalMemoryClass", "SourceCodeClass", "StatusClass", "CostClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]:
-        """The list of metadata aspects associated with the MLModel. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the MLModel. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["MLModelKeyClass", "OwnershipClass", "MLModelPropertiesClass", "IntendedUseClass", "MLModelFactorPromptsClass", "MetricsClass", "EvaluationDataClass", "TrainingDataClass", "QuantitativeAnalysesClass", "EthicalConsiderationsClass", "CaveatsAndRecommendationsClass", "InstitutionalMemoryClass", "SourceCodeClass", "StatusClass", "CostClass", "DeprecationClass", "BrowsePathsClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the MLModel. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class MLPrimaryKeySnapshotClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.MLPrimaryKeySnapshot")
@@ -11212,36 +12955,45 @@
         aspects: List[Union["MLPrimaryKeyKeyClass", "MLPrimaryKeyPropertiesClass", "OwnershipClass", "InstitutionalMemoryClass", "StatusClass", "DeprecationClass", "GlobalTagsClass", "DataPlatformInstanceClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLPrimaryKeySnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["MLPrimaryKeyKeyClass", "MLPrimaryKeyPropertiesClass", "OwnershipClass", "InstitutionalMemoryClass", "StatusClass", "DeprecationClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]:
-        """The list of metadata aspects associated with the MLPrimaryKey. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the MLPrimaryKey. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["MLPrimaryKeyKeyClass", "MLPrimaryKeyPropertiesClass", "OwnershipClass", "InstitutionalMemoryClass", "StatusClass", "DeprecationClass", "GlobalTagsClass", "DataPlatformInstanceClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the MLPrimaryKey. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class SchemaFieldSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific schema field entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.SchemaFieldSnapshot")
@@ -11250,36 +13002,45 @@
         aspects: List["SchemaFieldKeyClass"],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SchemaFieldSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List["SchemaFieldKeyClass"]:
-        """The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List["SchemaFieldKeyClass"]) -> None:
+        """Setter: The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class TagSnapshotClass(DictWrapper):
     """A metadata snapshot for a specific dataset entity."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.snapshot.TagSnapshot")
@@ -11288,36 +13049,45 @@
         aspects: List[Union["TagKeyClass", "OwnershipClass", "TagPropertiesClass", "StatusClass"]],
     ):
         super().__init__()
         
         self.urn = urn
         self.aspects = aspects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TagSnapshotClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.urn = str()
         self.aspects = list()
     
     
     @property
     def urn(self) -> str:
-        """URN for the entity the metadata snapshot is associated with."""
+        """Getter: URN for the entity the metadata snapshot is associated with."""
         return self._inner_dict.get('urn')  # type: ignore
     
     @urn.setter
     def urn(self, value: str) -> None:
+        """Setter: URN for the entity the metadata snapshot is associated with."""
         self._inner_dict['urn'] = value
     
     
     @property
     def aspects(self) -> List[Union["TagKeyClass", "OwnershipClass", "TagPropertiesClass", "StatusClass"]]:
-        """The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects."""
+        """Getter: The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         return self._inner_dict.get('aspects')  # type: ignore
     
     @aspects.setter
     def aspects(self, value: List[Union["TagKeyClass", "OwnershipClass", "TagPropertiesClass", "StatusClass"]]) -> None:
+        """Setter: The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects."""
         self._inner_dict['aspects'] = value
     
     
 class BaseDataClass(DictWrapper):
     """BaseData record"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.BaseData")
@@ -11328,47 +13098,57 @@
     ):
         super().__init__()
         
         self.dataset = dataset
         self.motivation = motivation
         self.preProcessing = preProcessing
     
+    @classmethod
+    def construct_with_defaults(cls) -> "BaseDataClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.dataset = str()
         self.motivation = self.RECORD_SCHEMA.fields_dict["motivation"].default
         self.preProcessing = self.RECORD_SCHEMA.fields_dict["preProcessing"].default
     
     
     @property
     def dataset(self) -> str:
-        """What dataset were used in the MLModel?"""
+        """Getter: What dataset were used in the MLModel?"""
         return self._inner_dict.get('dataset')  # type: ignore
     
     @dataset.setter
     def dataset(self, value: str) -> None:
+        """Setter: What dataset were used in the MLModel?"""
         self._inner_dict['dataset'] = value
     
     
     @property
     def motivation(self) -> Union[None, str]:
-        """Why was this dataset chosen?"""
+        """Getter: Why was this dataset chosen?"""
         return self._inner_dict.get('motivation')  # type: ignore
     
     @motivation.setter
     def motivation(self, value: Union[None, str]) -> None:
+        """Setter: Why was this dataset chosen?"""
         self._inner_dict['motivation'] = value
     
     
     @property
     def preProcessing(self) -> Union[None, List[str]]:
-        """How was the data preprocessed (e.g., tokenization of sentences, cropping of images, any filtering such as dropping images without faces)?"""
+        """Getter: How was the data preprocessed (e.g., tokenization of sentences, cropping of images, any filtering such as dropping images without faces)?"""
         return self._inner_dict.get('preProcessing')  # type: ignore
     
     @preProcessing.setter
     def preProcessing(self, value: Union[None, List[str]]) -> None:
+        """Setter: How was the data preprocessed (e.g., tokenization of sentences, cropping of images, any filtering such as dropping images without faces)?"""
         self._inner_dict['preProcessing'] = value
     
     
 class CaveatDetailsClass(DictWrapper):
     """This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.CaveatDetails")
@@ -11379,48 +13159,59 @@
     ):
         super().__init__()
         
         self.needsFurtherTesting = needsFurtherTesting
         self.caveatDescription = caveatDescription
         self.groupsNotRepresented = groupsNotRepresented
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CaveatDetailsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.needsFurtherTesting = self.RECORD_SCHEMA.fields_dict["needsFurtherTesting"].default
         self.caveatDescription = self.RECORD_SCHEMA.fields_dict["caveatDescription"].default
         self.groupsNotRepresented = self.RECORD_SCHEMA.fields_dict["groupsNotRepresented"].default
     
     
     @property
     def needsFurtherTesting(self) -> Union[None, bool]:
-        """Did the results suggest any further testing?"""
+        """Getter: Did the results suggest any further testing?"""
         return self._inner_dict.get('needsFurtherTesting')  # type: ignore
     
     @needsFurtherTesting.setter
     def needsFurtherTesting(self, value: Union[None, bool]) -> None:
+        """Setter: Did the results suggest any further testing?"""
         self._inner_dict['needsFurtherTesting'] = value
     
     
     @property
     def caveatDescription(self) -> Union[None, str]:
-        """Caveat Description
+        """Getter: Caveat Description
     For ex: Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders."""
         return self._inner_dict.get('caveatDescription')  # type: ignore
     
     @caveatDescription.setter
     def caveatDescription(self, value: Union[None, str]) -> None:
+        """Setter: Caveat Description
+    For ex: Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders."""
         self._inner_dict['caveatDescription'] = value
     
     
     @property
     def groupsNotRepresented(self) -> Union[None, List[str]]:
-        """Relevant groups that were not represented in the evaluation dataset?"""
+        """Getter: Relevant groups that were not represented in the evaluation dataset?"""
         return self._inner_dict.get('groupsNotRepresented')  # type: ignore
     
     @groupsNotRepresented.setter
     def groupsNotRepresented(self, value: Union[None, List[str]]) -> None:
+        """Setter: Relevant groups that were not represented in the evaluation dataset?"""
         self._inner_dict['groupsNotRepresented'] = value
     
     
 class CaveatsAndRecommendationsClass(_Aspect):
     """This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?"""
 
 
@@ -11435,47 +13226,57 @@
     ):
         super().__init__()
         
         self.caveats = caveats
         self.recommendations = recommendations
         self.idealDatasetCharacteristics = idealDatasetCharacteristics
     
+    @classmethod
+    def construct_with_defaults(cls) -> "CaveatsAndRecommendationsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.caveats = self.RECORD_SCHEMA.fields_dict["caveats"].default
         self.recommendations = self.RECORD_SCHEMA.fields_dict["recommendations"].default
         self.idealDatasetCharacteristics = self.RECORD_SCHEMA.fields_dict["idealDatasetCharacteristics"].default
     
     
     @property
     def caveats(self) -> Union[None, "CaveatDetailsClass"]:
-        """This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?"""
+        """Getter: This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?"""
         return self._inner_dict.get('caveats')  # type: ignore
     
     @caveats.setter
     def caveats(self, value: Union[None, "CaveatDetailsClass"]) -> None:
+        """Setter: This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?"""
         self._inner_dict['caveats'] = value
     
     
     @property
     def recommendations(self) -> Union[None, str]:
-        """Recommendations on where this MLModel should be used."""
+        """Getter: Recommendations on where this MLModel should be used."""
         return self._inner_dict.get('recommendations')  # type: ignore
     
     @recommendations.setter
     def recommendations(self, value: Union[None, str]) -> None:
+        """Setter: Recommendations on where this MLModel should be used."""
         self._inner_dict['recommendations'] = value
     
     
     @property
     def idealDatasetCharacteristics(self) -> Union[None, List[str]]:
-        """Ideal characteristics of an evaluation dataset for this MLModel"""
+        """Getter: Ideal characteristics of an evaluation dataset for this MLModel"""
         return self._inner_dict.get('idealDatasetCharacteristics')  # type: ignore
     
     @idealDatasetCharacteristics.setter
     def idealDatasetCharacteristics(self, value: Union[None, List[str]]) -> None:
+        """Setter: Ideal characteristics of an evaluation dataset for this MLModel"""
         self._inner_dict['idealDatasetCharacteristics'] = value
     
     
 class DeploymentStatusClass(object):
     """Model endpoint statuses"""
     
     
@@ -11515,25 +13316,33 @@
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableMLFeaturePropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the MLFeature"""
+        """Getter: Documentation of the MLFeature"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the MLFeature"""
         self._inner_dict['description'] = value
     
     
 class EditableMLFeatureTablePropertiesClass(_Aspect):
     """Properties associated with a MLFeatureTable editable from the ui"""
 
 
@@ -11544,25 +13353,33 @@
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableMLFeatureTablePropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the MLFeatureTable"""
+        """Getter: Documentation of the MLFeatureTable"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the MLFeatureTable"""
         self._inner_dict['description'] = value
     
     
 class EditableMLModelGroupPropertiesClass(_Aspect):
     """Properties associated with an ML Model Group editable from the UI"""
 
 
@@ -11573,25 +13390,33 @@
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableMLModelGroupPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the ml model group"""
+        """Getter: Documentation of the ml model group"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the ml model group"""
         self._inner_dict['description'] = value
     
     
 class EditableMLModelPropertiesClass(_Aspect):
     """Properties associated with a ML Model editable from the UI"""
 
 
@@ -11602,25 +13427,33 @@
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableMLModelPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the ml model"""
+        """Getter: Documentation of the ml model"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the ml model"""
         self._inner_dict['description'] = value
     
     
 class EditableMLPrimaryKeyPropertiesClass(_Aspect):
     """Properties associated with a MLPrimaryKey editable from the UI"""
 
 
@@ -11631,25 +13464,33 @@
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableMLPrimaryKeyPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the MLPrimaryKey"""
+        """Getter: Documentation of the MLPrimaryKey"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the MLPrimaryKey"""
         self._inner_dict['description'] = value
     
     
 class EthicalConsiderationsClass(_Aspect):
     """This section is intended to demonstrate the ethical considerations that went into MLModel development, surfacing ethical challenges and solutions to stakeholders."""
 
 
@@ -11668,69 +13509,81 @@
         
         self.data = data
         self.humanLife = humanLife
         self.mitigations = mitigations
         self.risksAndHarms = risksAndHarms
         self.useCases = useCases
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EthicalConsiderationsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.data = self.RECORD_SCHEMA.fields_dict["data"].default
         self.humanLife = self.RECORD_SCHEMA.fields_dict["humanLife"].default
         self.mitigations = self.RECORD_SCHEMA.fields_dict["mitigations"].default
         self.risksAndHarms = self.RECORD_SCHEMA.fields_dict["risksAndHarms"].default
         self.useCases = self.RECORD_SCHEMA.fields_dict["useCases"].default
     
     
     @property
     def data(self) -> Union[None, List[str]]:
-        """Does the MLModel use any sensitive data (e.g., protected classes)?"""
+        """Getter: Does the MLModel use any sensitive data (e.g., protected classes)?"""
         return self._inner_dict.get('data')  # type: ignore
     
     @data.setter
     def data(self, value: Union[None, List[str]]) -> None:
+        """Setter: Does the MLModel use any sensitive data (e.g., protected classes)?"""
         self._inner_dict['data'] = value
     
     
     @property
     def humanLife(self) -> Union[None, List[str]]:
-        """ Is the MLModel intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?"""
+        """Getter:  Is the MLModel intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?"""
         return self._inner_dict.get('humanLife')  # type: ignore
     
     @humanLife.setter
     def humanLife(self, value: Union[None, List[str]]) -> None:
+        """Setter:  Is the MLModel intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?"""
         self._inner_dict['humanLife'] = value
     
     
     @property
     def mitigations(self) -> Union[None, List[str]]:
-        """What risk mitigation strategies were used during MLModel development?"""
+        """Getter: What risk mitigation strategies were used during MLModel development?"""
         return self._inner_dict.get('mitigations')  # type: ignore
     
     @mitigations.setter
     def mitigations(self, value: Union[None, List[str]]) -> None:
+        """Setter: What risk mitigation strategies were used during MLModel development?"""
         self._inner_dict['mitigations'] = value
     
     
     @property
     def risksAndHarms(self) -> Union[None, List[str]]:
-        """What risks may be present in MLModel usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown."""
+        """Getter: What risks may be present in MLModel usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown."""
         return self._inner_dict.get('risksAndHarms')  # type: ignore
     
     @risksAndHarms.setter
     def risksAndHarms(self, value: Union[None, List[str]]) -> None:
+        """Setter: What risks may be present in MLModel usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown."""
         self._inner_dict['risksAndHarms'] = value
     
     
     @property
     def useCases(self) -> Union[None, List[str]]:
-        """Are there any known MLModel use cases that are especially fraught? This may connect directly to the intended use section"""
+        """Getter: Are there any known MLModel use cases that are especially fraught? This may connect directly to the intended use section"""
         return self._inner_dict.get('useCases')  # type: ignore
     
     @useCases.setter
     def useCases(self, value: Union[None, List[str]]) -> None:
+        """Setter: Are there any known MLModel use cases that are especially fraught? This may connect directly to the intended use section"""
         self._inner_dict['useCases'] = value
     
     
 class EvaluationDataClass(_Aspect):
     """All referenced datasets would ideally point to any set of documents that provide visibility into the source and composition of the dataset."""
 
 
@@ -11741,25 +13594,33 @@
     def __init__(self,
         evaluationData: List["BaseDataClass"],
     ):
         super().__init__()
         
         self.evaluationData = evaluationData
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EvaluationDataClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.evaluationData = list()
     
     
     @property
     def evaluationData(self) -> List["BaseDataClass"]:
-        """Details on the dataset(s) used for the quantitative analyses in the MLModel"""
+        """Getter: Details on the dataset(s) used for the quantitative analyses in the MLModel"""
         return self._inner_dict.get('evaluationData')  # type: ignore
     
     @evaluationData.setter
     def evaluationData(self, value: List["BaseDataClass"]) -> None:
+        """Setter: Details on the dataset(s) used for the quantitative analyses in the MLModel"""
         self._inner_dict['evaluationData'] = value
     
     
 class IntendedUseClass(_Aspect):
     """Intended Use for the ML Model"""
 
 
@@ -11774,47 +13635,57 @@
     ):
         super().__init__()
         
         self.primaryUses = primaryUses
         self.primaryUsers = primaryUsers
         self.outOfScopeUses = outOfScopeUses
     
+    @classmethod
+    def construct_with_defaults(cls) -> "IntendedUseClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.primaryUses = self.RECORD_SCHEMA.fields_dict["primaryUses"].default
         self.primaryUsers = self.RECORD_SCHEMA.fields_dict["primaryUsers"].default
         self.outOfScopeUses = self.RECORD_SCHEMA.fields_dict["outOfScopeUses"].default
     
     
     @property
     def primaryUses(self) -> Union[None, List[str]]:
-        """Primary Use cases for the MLModel."""
+        """Getter: Primary Use cases for the MLModel."""
         return self._inner_dict.get('primaryUses')  # type: ignore
     
     @primaryUses.setter
     def primaryUses(self, value: Union[None, List[str]]) -> None:
+        """Setter: Primary Use cases for the MLModel."""
         self._inner_dict['primaryUses'] = value
     
     
     @property
     def primaryUsers(self) -> Union[None, List[Union[str, "IntendedUserTypeClass"]]]:
-        """Primary Intended Users - For example, was the MLModel developed for entertainment purposes, for hobbyists, or enterprise solutions?"""
+        """Getter: Primary Intended Users - For example, was the MLModel developed for entertainment purposes, for hobbyists, or enterprise solutions?"""
         return self._inner_dict.get('primaryUsers')  # type: ignore
     
     @primaryUsers.setter
     def primaryUsers(self, value: Union[None, List[Union[str, "IntendedUserTypeClass"]]]) -> None:
+        """Setter: Primary Intended Users - For example, was the MLModel developed for entertainment purposes, for hobbyists, or enterprise solutions?"""
         self._inner_dict['primaryUsers'] = value
     
     
     @property
     def outOfScopeUses(self) -> Union[None, List[str]]:
-        """Highlight technology that the MLModel might easily be confused with, or related contexts that users could try to apply the MLModel to."""
+        """Getter: Highlight technology that the MLModel might easily be confused with, or related contexts that users could try to apply the MLModel to."""
         return self._inner_dict.get('outOfScopeUses')  # type: ignore
     
     @outOfScopeUses.setter
     def outOfScopeUses(self, value: Union[None, List[str]]) -> None:
+        """Setter: Highlight technology that the MLModel might easily be confused with, or related contexts that users could try to apply the MLModel to."""
         self._inner_dict['outOfScopeUses'] = value
     
     
 class IntendedUserTypeClass(object):
     # No docs available.
     
     ENTERPRISE = "ENTERPRISE"
@@ -11839,58 +13710,69 @@
         super().__init__()
         
         self.description = description
         self.dataType = dataType
         self.version = version
         self.sources = sources
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLFeaturePropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.dataType = self.RECORD_SCHEMA.fields_dict["dataType"].default
         self.version = self.RECORD_SCHEMA.fields_dict["version"].default
         self.sources = self.RECORD_SCHEMA.fields_dict["sources"].default
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the MLFeature"""
+        """Getter: Documentation of the MLFeature"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the MLFeature"""
         self._inner_dict['description'] = value
     
     
     @property
     def dataType(self) -> Union[None, Union[str, "MLFeatureDataTypeClass"]]:
-        """Data Type of the MLFeature"""
+        """Getter: Data Type of the MLFeature"""
         return self._inner_dict.get('dataType')  # type: ignore
     
     @dataType.setter
     def dataType(self, value: Union[None, Union[str, "MLFeatureDataTypeClass"]]) -> None:
+        """Setter: Data Type of the MLFeature"""
         self._inner_dict['dataType'] = value
     
     
     @property
     def version(self) -> Union[None, "VersionTagClass"]:
-        """Version of the MLFeature"""
+        """Getter: Version of the MLFeature"""
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: Union[None, "VersionTagClass"]) -> None:
+        """Setter: Version of the MLFeature"""
         self._inner_dict['version'] = value
     
     
     @property
     def sources(self) -> Union[None, List[str]]:
-        """Source of the MLFeature"""
+        """Getter: Source of the MLFeature"""
         return self._inner_dict.get('sources')  # type: ignore
     
     @sources.setter
     def sources(self, value: Union[None, List[str]]) -> None:
+        """Setter: Source of the MLFeature"""
         self._inner_dict['sources'] = value
     
     
 class MLFeatureTablePropertiesClass(_Aspect):
     """Properties associated with a MLFeatureTable"""
 
 
@@ -11911,58 +13793,69 @@
             self.customProperties = dict()
         else:
             self.customProperties = customProperties
         self.description = description
         self.mlFeatures = mlFeatures
         self.mlPrimaryKeys = mlPrimaryKeys
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLFeatureTablePropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.mlFeatures = self.RECORD_SCHEMA.fields_dict["mlFeatures"].default
         self.mlPrimaryKeys = self.RECORD_SCHEMA.fields_dict["mlPrimaryKeys"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the MLFeatureTable"""
+        """Getter: Documentation of the MLFeatureTable"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the MLFeatureTable"""
         self._inner_dict['description'] = value
     
     
     @property
     def mlFeatures(self) -> Union[None, List[str]]:
-        """List of features contained in the feature table"""
+        """Getter: List of features contained in the feature table"""
         return self._inner_dict.get('mlFeatures')  # type: ignore
     
     @mlFeatures.setter
     def mlFeatures(self, value: Union[None, List[str]]) -> None:
+        """Setter: List of features contained in the feature table"""
         self._inner_dict['mlFeatures'] = value
     
     
     @property
     def mlPrimaryKeys(self) -> Union[None, List[str]]:
-        """List of primary keys in the feature table (if multiple, assumed to act as a composite key)"""
+        """Getter: List of primary keys in the feature table (if multiple, assumed to act as a composite key)"""
         return self._inner_dict.get('mlPrimaryKeys')  # type: ignore
     
     @mlPrimaryKeys.setter
     def mlPrimaryKeys(self, value: Union[None, List[str]]) -> None:
+        """Setter: List of primary keys in the feature table (if multiple, assumed to act as a composite key)"""
         self._inner_dict['mlPrimaryKeys'] = value
     
     
 class MLHyperParamClass(_Aspect):
     """Properties associated with an ML Hyper Param"""
 
 
@@ -11979,58 +13872,69 @@
         super().__init__()
         
         self.name = name
         self.description = description
         self.value = value
         self.createdAt = createdAt
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLHyperParamClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.value = self.RECORD_SCHEMA.fields_dict["value"].default
         self.createdAt = self.RECORD_SCHEMA.fields_dict["createdAt"].default
     
     
     @property
     def name(self) -> str:
-        """Name of the MLHyperParam"""
+        """Getter: Name of the MLHyperParam"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the MLHyperParam"""
         self._inner_dict['name'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the MLHyperParam"""
+        """Getter: Documentation of the MLHyperParam"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the MLHyperParam"""
         self._inner_dict['description'] = value
     
     
     @property
     def value(self) -> Union[None, str]:
-        """The value of the MLHyperParam"""
+        """Getter: The value of the MLHyperParam"""
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: Union[None, str]) -> None:
+        """Setter: The value of the MLHyperParam"""
         self._inner_dict['value'] = value
     
     
     @property
     def createdAt(self) -> Union[None, int]:
-        """Date when the MLHyperParam was developed"""
+        """Getter: Date when the MLHyperParam was developed"""
         return self._inner_dict.get('createdAt')  # type: ignore
     
     @createdAt.setter
     def createdAt(self, value: Union[None, int]) -> None:
+        """Setter: Date when the MLHyperParam was developed"""
         self._inner_dict['createdAt'] = value
     
     
 class MLMetricClass(_Aspect):
     """Properties associated with an ML Metric"""
 
 
@@ -12047,58 +13951,69 @@
         super().__init__()
         
         self.name = name
         self.description = description
         self.value = value
         self.createdAt = createdAt
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLMetricClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.value = self.RECORD_SCHEMA.fields_dict["value"].default
         self.createdAt = self.RECORD_SCHEMA.fields_dict["createdAt"].default
     
     
     @property
     def name(self) -> str:
-        """Name of the mlMetric"""
+        """Getter: Name of the mlMetric"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the mlMetric"""
         self._inner_dict['name'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the mlMetric"""
+        """Getter: Documentation of the mlMetric"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the mlMetric"""
         self._inner_dict['description'] = value
     
     
     @property
     def value(self) -> Union[None, str]:
-        """The value of the mlMetric"""
+        """Getter: The value of the mlMetric"""
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: Union[None, str]) -> None:
+        """Setter: The value of the mlMetric"""
         self._inner_dict['value'] = value
     
     
     @property
     def createdAt(self) -> Union[None, int]:
-        """Date when the mlMetric was developed"""
+        """Getter: Date when the mlMetric was developed"""
         return self._inner_dict.get('createdAt')  # type: ignore
     
     @createdAt.setter
     def createdAt(self, value: Union[None, int]) -> None:
+        """Setter: Date when the mlMetric was developed"""
         self._inner_dict['createdAt'] = value
     
     
 class MLModelDeploymentPropertiesClass(_Aspect):
     """Properties associated with an ML Model Deployment"""
 
 
@@ -12123,80 +14038,93 @@
             self.customProperties = customProperties
         self.externalUrl = externalUrl
         self.description = description
         self.createdAt = createdAt
         self.version = version
         self.status = status
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelDeploymentPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.createdAt = self.RECORD_SCHEMA.fields_dict["createdAt"].default
         self.version = self.RECORD_SCHEMA.fields_dict["version"].default
         self.status = self.RECORD_SCHEMA.fields_dict["status"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the MLModelDeployment"""
+        """Getter: Documentation of the MLModelDeployment"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the MLModelDeployment"""
         self._inner_dict['description'] = value
     
     
     @property
     def createdAt(self) -> Union[None, int]:
-        """Date when the MLModelDeployment was developed"""
+        """Getter: Date when the MLModelDeployment was developed"""
         return self._inner_dict.get('createdAt')  # type: ignore
     
     @createdAt.setter
     def createdAt(self, value: Union[None, int]) -> None:
+        """Setter: Date when the MLModelDeployment was developed"""
         self._inner_dict['createdAt'] = value
     
     
     @property
     def version(self) -> Union[None, "VersionTagClass"]:
-        """Version of the MLModelDeployment"""
+        """Getter: Version of the MLModelDeployment"""
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: Union[None, "VersionTagClass"]) -> None:
+        """Setter: Version of the MLModelDeployment"""
         self._inner_dict['version'] = value
     
     
     @property
     def status(self) -> Union[None, Union[str, "DeploymentStatusClass"]]:
-        """Status of the deployment"""
+        """Getter: Status of the deployment"""
         return self._inner_dict.get('status')  # type: ignore
     
     @status.setter
     def status(self, value: Union[None, Union[str, "DeploymentStatusClass"]]) -> None:
+        """Setter: Status of the deployment"""
         self._inner_dict['status'] = value
     
     
 class MLModelFactorPromptsClass(_Aspect):
     """Prompts which affect the performance of the MLModel"""
 
 
@@ -12209,36 +14137,45 @@
         evaluationFactors: Union[None, List["MLModelFactorsClass"]]=None,
     ):
         super().__init__()
         
         self.relevantFactors = relevantFactors
         self.evaluationFactors = evaluationFactors
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelFactorPromptsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.relevantFactors = self.RECORD_SCHEMA.fields_dict["relevantFactors"].default
         self.evaluationFactors = self.RECORD_SCHEMA.fields_dict["evaluationFactors"].default
     
     
     @property
     def relevantFactors(self) -> Union[None, List["MLModelFactorsClass"]]:
-        """What are foreseeable salient factors for which MLModel performance may vary, and how were these determined?"""
+        """Getter: What are foreseeable salient factors for which MLModel performance may vary, and how were these determined?"""
         return self._inner_dict.get('relevantFactors')  # type: ignore
     
     @relevantFactors.setter
     def relevantFactors(self, value: Union[None, List["MLModelFactorsClass"]]) -> None:
+        """Setter: What are foreseeable salient factors for which MLModel performance may vary, and how were these determined?"""
         self._inner_dict['relevantFactors'] = value
     
     
     @property
     def evaluationFactors(self) -> Union[None, List["MLModelFactorsClass"]]:
-        """Which factors are being reported, and why were these chosen?"""
+        """Getter: Which factors are being reported, and why were these chosen?"""
         return self._inner_dict.get('evaluationFactors')  # type: ignore
     
     @evaluationFactors.setter
     def evaluationFactors(self, value: Union[None, List["MLModelFactorsClass"]]) -> None:
+        """Setter: Which factors are being reported, and why were these chosen?"""
         self._inner_dict['evaluationFactors'] = value
     
     
 class MLModelFactorsClass(DictWrapper):
     """Factors affecting the performance of the MLModel."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.MLModelFactors")
@@ -12249,50 +14186,63 @@
     ):
         super().__init__()
         
         self.groups = groups
         self.instrumentation = instrumentation
         self.environment = environment
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelFactorsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.groups = self.RECORD_SCHEMA.fields_dict["groups"].default
         self.instrumentation = self.RECORD_SCHEMA.fields_dict["instrumentation"].default
         self.environment = self.RECORD_SCHEMA.fields_dict["environment"].default
     
     
     @property
     def groups(self) -> Union[None, List[str]]:
-        """Groups refers to distinct categories with similar characteristics that are present in the evaluation data instances.
+        """Getter: Groups refers to distinct categories with similar characteristics that are present in the evaluation data instances.
     For human-centric machine learning MLModels, groups are people who share one or multiple characteristics."""
         return self._inner_dict.get('groups')  # type: ignore
     
     @groups.setter
     def groups(self, value: Union[None, List[str]]) -> None:
+        """Setter: Groups refers to distinct categories with similar characteristics that are present in the evaluation data instances.
+    For human-centric machine learning MLModels, groups are people who share one or multiple characteristics."""
         self._inner_dict['groups'] = value
     
     
     @property
     def instrumentation(self) -> Union[None, List[str]]:
-        """The performance of a MLModel can vary depending on what instruments were used to capture the input to the MLModel.
+        """Getter: The performance of a MLModel can vary depending on what instruments were used to capture the input to the MLModel.
     For example, a face detection model may perform differently depending on the camera's hardware and software,
     including lens, image stabilization, high dynamic range techniques, and background blurring for portrait mode."""
         return self._inner_dict.get('instrumentation')  # type: ignore
     
     @instrumentation.setter
     def instrumentation(self, value: Union[None, List[str]]) -> None:
+        """Setter: The performance of a MLModel can vary depending on what instruments were used to capture the input to the MLModel.
+    For example, a face detection model may perform differently depending on the camera's hardware and software,
+    including lens, image stabilization, high dynamic range techniques, and background blurring for portrait mode."""
         self._inner_dict['instrumentation'] = value
     
     
     @property
     def environment(self) -> Union[None, List[str]]:
-        """A further factor affecting MLModel performance is the environment in which it is deployed."""
+        """Getter: A further factor affecting MLModel performance is the environment in which it is deployed."""
         return self._inner_dict.get('environment')  # type: ignore
     
     @environment.setter
     def environment(self, value: Union[None, List[str]]) -> None:
+        """Setter: A further factor affecting MLModel performance is the environment in which it is deployed."""
         self._inner_dict['environment'] = value
     
     
 class MLModelGroupPropertiesClass(_Aspect):
     """Properties associated with an ML Model Group"""
 
 
@@ -12313,58 +14263,69 @@
             self.customProperties = dict()
         else:
             self.customProperties = customProperties
         self.description = description
         self.createdAt = createdAt
         self.version = version
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelGroupPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.createdAt = self.RECORD_SCHEMA.fields_dict["createdAt"].default
         self.version = self.RECORD_SCHEMA.fields_dict["version"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the MLModelGroup"""
+        """Getter: Documentation of the MLModelGroup"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the MLModelGroup"""
         self._inner_dict['description'] = value
     
     
     @property
     def createdAt(self) -> Union[None, int]:
-        """Date when the MLModelGroup was developed"""
+        """Getter: Date when the MLModelGroup was developed"""
         return self._inner_dict.get('createdAt')  # type: ignore
     
     @createdAt.setter
     def createdAt(self, value: Union[None, int]) -> None:
+        """Setter: Date when the MLModelGroup was developed"""
         self._inner_dict['createdAt'] = value
     
     
     @property
     def version(self) -> Union[None, "VersionTagClass"]:
-        """Version of the MLModelGroup"""
+        """Getter: Version of the MLModelGroup"""
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: Union[None, "VersionTagClass"]) -> None:
+        """Setter: Version of the MLModelGroup"""
         self._inner_dict['version'] = value
     
     
 class MLModelPropertiesClass(_Aspect):
     """Properties associated with a ML Model"""
 
 
@@ -12413,14 +14374,21 @@
         else:
             self.tags = tags
         self.deployments = deployments
         self.trainingJobs = trainingJobs
         self.downstreamJobs = downstreamJobs
         self.groups = groups
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLModelPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.date = self.RECORD_SCHEMA.fields_dict["date"].default
         self.version = self.RECORD_SCHEMA.fields_dict["version"].default
         self.type = self.RECORD_SCHEMA.fields_dict["type"].default
@@ -12434,171 +14402,189 @@
         self.trainingJobs = self.RECORD_SCHEMA.fields_dict["trainingJobs"].default
         self.downstreamJobs = self.RECORD_SCHEMA.fields_dict["downstreamJobs"].default
         self.groups = self.RECORD_SCHEMA.fields_dict["groups"].default
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the MLModel"""
+        """Getter: Documentation of the MLModel"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the MLModel"""
         self._inner_dict['description'] = value
     
     
     @property
     def date(self) -> Union[None, int]:
-        """Date when the MLModel was developed"""
+        """Getter: Date when the MLModel was developed"""
         return self._inner_dict.get('date')  # type: ignore
     
     @date.setter
     def date(self, value: Union[None, int]) -> None:
+        """Setter: Date when the MLModel was developed"""
         self._inner_dict['date'] = value
     
     
     @property
     def version(self) -> Union[None, "VersionTagClass"]:
-        """Version of the MLModel"""
+        """Getter: Version of the MLModel"""
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: Union[None, "VersionTagClass"]) -> None:
+        """Setter: Version of the MLModel"""
         self._inner_dict['version'] = value
     
     
     @property
     def type(self) -> Union[None, str]:
-        """Type of Algorithm or MLModel such as whether it is a Naive Bayes classifier, Convolutional Neural Network, etc"""
+        """Getter: Type of Algorithm or MLModel such as whether it is a Naive Bayes classifier, Convolutional Neural Network, etc"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[None, str]) -> None:
+        """Setter: Type of Algorithm or MLModel such as whether it is a Naive Bayes classifier, Convolutional Neural Network, etc"""
         self._inner_dict['type'] = value
     
     
     @property
     def hyperParameters(self) -> Union[None, Dict[str, Union[str, int, float, float, bool]]]:
-        """Hyper Parameters of the MLModel
+        """Getter: Hyper Parameters of the MLModel
     
     NOTE: these are deprecated in favor of hyperParams"""
         return self._inner_dict.get('hyperParameters')  # type: ignore
     
     @hyperParameters.setter
     def hyperParameters(self, value: Union[None, Dict[str, Union[str, int, float, float, bool]]]) -> None:
+        """Setter: Hyper Parameters of the MLModel
+    
+    NOTE: these are deprecated in favor of hyperParams"""
         self._inner_dict['hyperParameters'] = value
     
     
     @property
     def hyperParams(self) -> Union[None, List["MLHyperParamClass"]]:
-        """Hyperparameters of the MLModel"""
+        """Getter: Hyperparameters of the MLModel"""
         return self._inner_dict.get('hyperParams')  # type: ignore
     
     @hyperParams.setter
     def hyperParams(self, value: Union[None, List["MLHyperParamClass"]]) -> None:
+        """Setter: Hyperparameters of the MLModel"""
         self._inner_dict['hyperParams'] = value
     
     
     @property
     def trainingMetrics(self) -> Union[None, List["MLMetricClass"]]:
-        """Metrics of the MLModel used in training"""
+        """Getter: Metrics of the MLModel used in training"""
         return self._inner_dict.get('trainingMetrics')  # type: ignore
     
     @trainingMetrics.setter
     def trainingMetrics(self, value: Union[None, List["MLMetricClass"]]) -> None:
+        """Setter: Metrics of the MLModel used in training"""
         self._inner_dict['trainingMetrics'] = value
     
     
     @property
     def onlineMetrics(self) -> Union[None, List["MLMetricClass"]]:
-        """Metrics of the MLModel used in production"""
+        """Getter: Metrics of the MLModel used in production"""
         return self._inner_dict.get('onlineMetrics')  # type: ignore
     
     @onlineMetrics.setter
     def onlineMetrics(self, value: Union[None, List["MLMetricClass"]]) -> None:
+        """Setter: Metrics of the MLModel used in production"""
         self._inner_dict['onlineMetrics'] = value
     
     
     @property
     def mlFeatures(self) -> Union[None, List[str]]:
-        """List of features used for MLModel training"""
+        """Getter: List of features used for MLModel training"""
         return self._inner_dict.get('mlFeatures')  # type: ignore
     
     @mlFeatures.setter
     def mlFeatures(self, value: Union[None, List[str]]) -> None:
+        """Setter: List of features used for MLModel training"""
         self._inner_dict['mlFeatures'] = value
     
     
     @property
     def tags(self) -> List[str]:
-        """Tags for the MLModel"""
+        """Getter: Tags for the MLModel"""
         return self._inner_dict.get('tags')  # type: ignore
     
     @tags.setter
     def tags(self, value: List[str]) -> None:
+        """Setter: Tags for the MLModel"""
         self._inner_dict['tags'] = value
     
     
     @property
     def deployments(self) -> Union[None, List[str]]:
-        """Deployments for the MLModel"""
+        """Getter: Deployments for the MLModel"""
         return self._inner_dict.get('deployments')  # type: ignore
     
     @deployments.setter
     def deployments(self, value: Union[None, List[str]]) -> None:
+        """Setter: Deployments for the MLModel"""
         self._inner_dict['deployments'] = value
     
     
     @property
     def trainingJobs(self) -> Union[None, List[str]]:
-        """List of jobs (if any) used to train the model"""
+        """Getter: List of jobs (if any) used to train the model"""
         return self._inner_dict.get('trainingJobs')  # type: ignore
     
     @trainingJobs.setter
     def trainingJobs(self, value: Union[None, List[str]]) -> None:
+        """Setter: List of jobs (if any) used to train the model"""
         self._inner_dict['trainingJobs'] = value
     
     
     @property
     def downstreamJobs(self) -> Union[None, List[str]]:
-        """List of jobs (if any) that use the model"""
+        """Getter: List of jobs (if any) that use the model"""
         return self._inner_dict.get('downstreamJobs')  # type: ignore
     
     @downstreamJobs.setter
     def downstreamJobs(self, value: Union[None, List[str]]) -> None:
+        """Setter: List of jobs (if any) that use the model"""
         self._inner_dict['downstreamJobs'] = value
     
     
     @property
     def groups(self) -> Union[None, List[str]]:
-        """Groups the model belongs to"""
+        """Getter: Groups the model belongs to"""
         return self._inner_dict.get('groups')  # type: ignore
     
     @groups.setter
     def groups(self, value: Union[None, List[str]]) -> None:
+        """Setter: Groups the model belongs to"""
         self._inner_dict['groups'] = value
     
     
 class MLPrimaryKeyPropertiesClass(_Aspect):
     """Properties associated with a MLPrimaryKey"""
 
 
@@ -12615,58 +14601,69 @@
         super().__init__()
         
         self.description = description
         self.dataType = dataType
         self.version = version
         self.sources = sources
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MLPrimaryKeyPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.dataType = self.RECORD_SCHEMA.fields_dict["dataType"].default
         self.version = self.RECORD_SCHEMA.fields_dict["version"].default
         self.sources = list()
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the MLPrimaryKey"""
+        """Getter: Documentation of the MLPrimaryKey"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the MLPrimaryKey"""
         self._inner_dict['description'] = value
     
     
     @property
     def dataType(self) -> Union[None, Union[str, "MLFeatureDataTypeClass"]]:
-        """Data Type of the MLPrimaryKey"""
+        """Getter: Data Type of the MLPrimaryKey"""
         return self._inner_dict.get('dataType')  # type: ignore
     
     @dataType.setter
     def dataType(self, value: Union[None, Union[str, "MLFeatureDataTypeClass"]]) -> None:
+        """Setter: Data Type of the MLPrimaryKey"""
         self._inner_dict['dataType'] = value
     
     
     @property
     def version(self) -> Union[None, "VersionTagClass"]:
-        """Version of the MLPrimaryKey"""
+        """Getter: Version of the MLPrimaryKey"""
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: Union[None, "VersionTagClass"]) -> None:
+        """Setter: Version of the MLPrimaryKey"""
         self._inner_dict['version'] = value
     
     
     @property
     def sources(self) -> List[str]:
-        """Source of the MLPrimaryKey"""
+        """Getter: Source of the MLPrimaryKey"""
         return self._inner_dict.get('sources')  # type: ignore
     
     @sources.setter
     def sources(self, value: List[str]) -> None:
+        """Setter: Source of the MLPrimaryKey"""
         self._inner_dict['sources'] = value
     
     
 class MetricsClass(_Aspect):
     """Metrics to be featured for the MLModel."""
 
 
@@ -12679,36 +14676,45 @@
         decisionThreshold: Union[None, List[str]]=None,
     ):
         super().__init__()
         
         self.performanceMeasures = performanceMeasures
         self.decisionThreshold = decisionThreshold
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MetricsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.performanceMeasures = self.RECORD_SCHEMA.fields_dict["performanceMeasures"].default
         self.decisionThreshold = self.RECORD_SCHEMA.fields_dict["decisionThreshold"].default
     
     
     @property
     def performanceMeasures(self) -> Union[None, List[str]]:
-        """Measures of MLModel performance"""
+        """Getter: Measures of MLModel performance"""
         return self._inner_dict.get('performanceMeasures')  # type: ignore
     
     @performanceMeasures.setter
     def performanceMeasures(self, value: Union[None, List[str]]) -> None:
+        """Setter: Measures of MLModel performance"""
         self._inner_dict['performanceMeasures'] = value
     
     
     @property
     def decisionThreshold(self) -> Union[None, List[str]]:
-        """Decision Thresholds used (if any)?"""
+        """Getter: Decision Thresholds used (if any)?"""
         return self._inner_dict.get('decisionThreshold')  # type: ignore
     
     @decisionThreshold.setter
     def decisionThreshold(self, value: Union[None, List[str]]) -> None:
+        """Setter: Decision Thresholds used (if any)?"""
         self._inner_dict['decisionThreshold'] = value
     
     
 class QuantitativeAnalysesClass(_Aspect):
     """Quantitative analyses should be disaggregated, that is, broken down by the chosen factors. Quantitative analyses should provide the results of evaluating the MLModel according to the chosen metrics, providing confidence interval values when possible."""
 
 
@@ -12721,36 +14727,45 @@
         intersectionalResults: Union[None, str]=None,
     ):
         super().__init__()
         
         self.unitaryResults = unitaryResults
         self.intersectionalResults = intersectionalResults
     
+    @classmethod
+    def construct_with_defaults(cls) -> "QuantitativeAnalysesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.unitaryResults = self.RECORD_SCHEMA.fields_dict["unitaryResults"].default
         self.intersectionalResults = self.RECORD_SCHEMA.fields_dict["intersectionalResults"].default
     
     
     @property
     def unitaryResults(self) -> Union[None, str]:
-        """Link to a dashboard with results showing how the MLModel performed with respect to each factor"""
+        """Getter: Link to a dashboard with results showing how the MLModel performed with respect to each factor"""
         return self._inner_dict.get('unitaryResults')  # type: ignore
     
     @unitaryResults.setter
     def unitaryResults(self, value: Union[None, str]) -> None:
+        """Setter: Link to a dashboard with results showing how the MLModel performed with respect to each factor"""
         self._inner_dict['unitaryResults'] = value
     
     
     @property
     def intersectionalResults(self) -> Union[None, str]:
-        """Link to a dashboard with results showing how the MLModel performed with respect to the intersection of evaluated factors?"""
+        """Getter: Link to a dashboard with results showing how the MLModel performed with respect to the intersection of evaluated factors?"""
         return self._inner_dict.get('intersectionalResults')  # type: ignore
     
     @intersectionalResults.setter
     def intersectionalResults(self, value: Union[None, str]) -> None:
+        """Setter: Link to a dashboard with results showing how the MLModel performed with respect to the intersection of evaluated factors?"""
         self._inner_dict['intersectionalResults'] = value
     
     
 class SourceCodeClass(_Aspect):
     """Source Code"""
 
 
@@ -12761,25 +14776,33 @@
     def __init__(self,
         sourceCode: List["SourceCodeUrlClass"],
     ):
         super().__init__()
         
         self.sourceCode = sourceCode
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SourceCodeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.sourceCode = list()
     
     
     @property
     def sourceCode(self) -> List["SourceCodeUrlClass"]:
-        """Source Code along with types"""
+        """Getter: Source Code along with types"""
         return self._inner_dict.get('sourceCode')  # type: ignore
     
     @sourceCode.setter
     def sourceCode(self, value: List["SourceCodeUrlClass"]) -> None:
+        """Setter: Source Code along with types"""
         self._inner_dict['sourceCode'] = value
     
     
 class SourceCodeUrlClass(DictWrapper):
     """Source Code Url Entity"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.SourceCodeUrl")
@@ -12788,36 +14811,45 @@
         sourceCodeUrl: str,
     ):
         super().__init__()
         
         self.type = type
         self.sourceCodeUrl = sourceCodeUrl
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SourceCodeUrlClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = SourceCodeUrlTypeClass.ML_MODEL_SOURCE_CODE
         self.sourceCodeUrl = str()
     
     
     @property
     def type(self) -> Union[str, "SourceCodeUrlTypeClass"]:
-        """Source Code Url Types"""
+        """Getter: Source Code Url Types"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "SourceCodeUrlTypeClass"]) -> None:
+        """Setter: Source Code Url Types"""
         self._inner_dict['type'] = value
     
     
     @property
     def sourceCodeUrl(self) -> str:
-        """Source Code Url"""
+        """Getter: Source Code Url"""
         return self._inner_dict.get('sourceCodeUrl')  # type: ignore
     
     @sourceCodeUrl.setter
     def sourceCodeUrl(self, value: str) -> None:
+        """Setter: Source Code Url"""
         self._inner_dict['sourceCodeUrl'] = value
     
     
 class SourceCodeUrlTypeClass(object):
     # No docs available.
     
     ML_MODEL_SOURCE_CODE = "ML_MODEL_SOURCE_CODE"
@@ -12836,25 +14868,33 @@
     def __init__(self,
         trainingData: List["BaseDataClass"],
     ):
         super().__init__()
         
         self.trainingData = trainingData
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TrainingDataClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.trainingData = list()
     
     
     @property
     def trainingData(self) -> List["BaseDataClass"]:
-        """Details on the dataset(s) used for training the MLModel"""
+        """Getter: Details on the dataset(s) used for training the MLModel"""
         return self._inner_dict.get('trainingData')  # type: ignore
     
     @trainingData.setter
     def trainingData(self, value: List["BaseDataClass"]) -> None:
+        """Setter: Details on the dataset(s) used for training the MLModel"""
         self._inner_dict['trainingData'] = value
     
     
 class GenericAspectClass(DictWrapper):
     """Generic record structure for serializing an Aspect"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.mxe.GenericAspect")
@@ -12863,37 +14903,47 @@
         contentType: str,
     ):
         super().__init__()
         
         self.value = value
         self.contentType = contentType
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GenericAspectClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.value = bytes()
         self.contentType = str()
     
     
     @property
     def value(self) -> bytes:
-        """The value of the aspect, serialized as bytes."""
+        """Getter: The value of the aspect, serialized as bytes."""
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: bytes) -> None:
+        """Setter: The value of the aspect, serialized as bytes."""
         self._inner_dict['value'] = value
     
     
     @property
     def contentType(self) -> str:
-        """The content type, which represents the fashion in which the aspect was serialized.
+        """Getter: The content type, which represents the fashion in which the aspect was serialized.
     The only type currently supported is application/json."""
         return self._inner_dict.get('contentType')  # type: ignore
     
     @contentType.setter
     def contentType(self, value: str) -> None:
+        """Setter: The content type, which represents the fashion in which the aspect was serialized.
+    The only type currently supported is application/json."""
         self._inner_dict['contentType'] = value
     
     
 class GenericPayloadClass(DictWrapper):
     """Generic payload record structure for serializing a Platform Event."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.mxe.GenericPayload")
@@ -12902,37 +14952,47 @@
         contentType: str,
     ):
         super().__init__()
         
         self.value = value
         self.contentType = contentType
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GenericPayloadClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.value = bytes()
         self.contentType = str()
     
     
     @property
     def value(self) -> bytes:
-        """The value of the event, serialized as bytes."""
+        """Getter: The value of the event, serialized as bytes."""
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: bytes) -> None:
+        """Setter: The value of the event, serialized as bytes."""
         self._inner_dict['value'] = value
     
     
     @property
     def contentType(self) -> str:
-        """The content type, which represents the fashion in which the event was serialized.
+        """Getter: The content type, which represents the fashion in which the event was serialized.
     The only type currently supported is application/json."""
         return self._inner_dict.get('contentType')  # type: ignore
     
     @contentType.setter
     def contentType(self, value: str) -> None:
+        """Setter: The content type, which represents the fashion in which the event was serialized.
+    The only type currently supported is application/json."""
         self._inner_dict['contentType'] = value
     
     
 class MetadataChangeEventClass(DictWrapper):
     """Kafka event for proposing a metadata change for an entity. A corresponding MetadataAuditEvent is emitted when the change is accepted and committed, otherwise a FailedMetadataChangeEvent will be emitted instead."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.mxe.MetadataChangeEvent")
@@ -12945,58 +15005,69 @@
         super().__init__()
         
         self.auditHeader = auditHeader
         self.proposedSnapshot = proposedSnapshot
         self.proposedDelta = proposedDelta
         self.systemMetadata = systemMetadata
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MetadataChangeEventClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.auditHeader = self.RECORD_SCHEMA.fields_dict["auditHeader"].default
-        self.proposedSnapshot = ChartSnapshotClass._construct_with_defaults()
+        self.proposedSnapshot = ChartSnapshotClass.construct_with_defaults()
         self.proposedDelta = self.RECORD_SCHEMA.fields_dict["proposedDelta"].default
         self.systemMetadata = self.RECORD_SCHEMA.fields_dict["systemMetadata"].default
     
     
     @property
     def auditHeader(self) -> Union[None, "KafkaAuditHeaderClass"]:
-        """Kafka audit header. See go/kafkaauditheader for more info."""
+        """Getter: Kafka audit header. See go/kafkaauditheader for more info."""
         return self._inner_dict.get('auditHeader')  # type: ignore
     
     @auditHeader.setter
     def auditHeader(self, value: Union[None, "KafkaAuditHeaderClass"]) -> None:
+        """Setter: Kafka audit header. See go/kafkaauditheader for more info."""
         self._inner_dict['auditHeader'] = value
     
     
     @property
     def proposedSnapshot(self) -> Union["ChartSnapshotClass", "CorpGroupSnapshotClass", "CorpUserSnapshotClass", "DashboardSnapshotClass", "DataFlowSnapshotClass", "DataJobSnapshotClass", "DatasetSnapshotClass", "DataProcessSnapshotClass", "DataPlatformSnapshotClass", "MLModelSnapshotClass", "MLPrimaryKeySnapshotClass", "MLFeatureSnapshotClass", "MLFeatureTableSnapshotClass", "MLModelDeploymentSnapshotClass", "MLModelGroupSnapshotClass", "TagSnapshotClass", "GlossaryTermSnapshotClass", "GlossaryNodeSnapshotClass", "DataHubPolicySnapshotClass", "SchemaFieldSnapshotClass", "DataHubRetentionSnapshotClass"]:
-        """Snapshot of the proposed metadata change. Include only the aspects affected by the change in the snapshot."""
+        """Getter: Snapshot of the proposed metadata change. Include only the aspects affected by the change in the snapshot."""
         return self._inner_dict.get('proposedSnapshot')  # type: ignore
     
     @proposedSnapshot.setter
     def proposedSnapshot(self, value: Union["ChartSnapshotClass", "CorpGroupSnapshotClass", "CorpUserSnapshotClass", "DashboardSnapshotClass", "DataFlowSnapshotClass", "DataJobSnapshotClass", "DatasetSnapshotClass", "DataProcessSnapshotClass", "DataPlatformSnapshotClass", "MLModelSnapshotClass", "MLPrimaryKeySnapshotClass", "MLFeatureSnapshotClass", "MLFeatureTableSnapshotClass", "MLModelDeploymentSnapshotClass", "MLModelGroupSnapshotClass", "TagSnapshotClass", "GlossaryTermSnapshotClass", "GlossaryNodeSnapshotClass", "DataHubPolicySnapshotClass", "SchemaFieldSnapshotClass", "DataHubRetentionSnapshotClass"]) -> None:
+        """Setter: Snapshot of the proposed metadata change. Include only the aspects affected by the change in the snapshot."""
         self._inner_dict['proposedSnapshot'] = value
     
     
     @property
     def proposedDelta(self) -> None:
-        """Delta of the proposed metadata partial update."""
+        """Getter: Delta of the proposed metadata partial update."""
         return self._inner_dict.get('proposedDelta')  # type: ignore
     
     @proposedDelta.setter
     def proposedDelta(self, value: None) -> None:
+        """Setter: Delta of the proposed metadata partial update."""
         self._inner_dict['proposedDelta'] = value
     
     
     @property
     def systemMetadata(self) -> Union[None, "SystemMetadataClass"]:
-        """Metadata around how the snapshot was ingested"""
+        """Getter: Metadata around how the snapshot was ingested"""
         return self._inner_dict.get('systemMetadata')  # type: ignore
     
     @systemMetadata.setter
     def systemMetadata(self, value: Union[None, "SystemMetadataClass"]) -> None:
+        """Setter: Metadata around how the snapshot was ingested"""
         self._inner_dict['systemMetadata'] = value
     
     
 class MetadataChangeLogClass(DictWrapper):
     """Kafka event for capturing update made to an entity's metadata."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.mxe.MetadataChangeLog")
@@ -13023,14 +15094,21 @@
         self.aspectName = aspectName
         self.aspect = aspect
         self.systemMetadata = systemMetadata
         self.previousAspectValue = previousAspectValue
         self.previousSystemMetadata = previousSystemMetadata
         self.created = created
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MetadataChangeLogClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.auditHeader = self.RECORD_SCHEMA.fields_dict["auditHeader"].default
         self.entityType = str()
         self.entityUrn = self.RECORD_SCHEMA.fields_dict["entityUrn"].default
         self.entityKeyAspect = self.RECORD_SCHEMA.fields_dict["entityKeyAspect"].default
         self.changeType = ChangeTypeClass.UPSERT
         self.aspectName = self.RECORD_SCHEMA.fields_dict["aspectName"].default
@@ -13039,121 +15117,134 @@
         self.previousAspectValue = self.RECORD_SCHEMA.fields_dict["previousAspectValue"].default
         self.previousSystemMetadata = self.RECORD_SCHEMA.fields_dict["previousSystemMetadata"].default
         self.created = self.RECORD_SCHEMA.fields_dict["created"].default
     
     
     @property
     def auditHeader(self) -> Union[None, "KafkaAuditHeaderClass"]:
-        """Kafka audit header. Currently remains unused in the open source."""
+        """Getter: Kafka audit header. Currently remains unused in the open source."""
         return self._inner_dict.get('auditHeader')  # type: ignore
     
     @auditHeader.setter
     def auditHeader(self, value: Union[None, "KafkaAuditHeaderClass"]) -> None:
+        """Setter: Kafka audit header. Currently remains unused in the open source."""
         self._inner_dict['auditHeader'] = value
     
     
     @property
     def entityType(self) -> str:
-        """Type of the entity being written to"""
+        """Getter: Type of the entity being written to"""
         return self._inner_dict.get('entityType')  # type: ignore
     
     @entityType.setter
     def entityType(self, value: str) -> None:
+        """Setter: Type of the entity being written to"""
         self._inner_dict['entityType'] = value
     
     
     @property
     def entityUrn(self) -> Union[None, str]:
-        """Urn of the entity being written"""
+        """Getter: Urn of the entity being written"""
         return self._inner_dict.get('entityUrn')  # type: ignore
     
     @entityUrn.setter
     def entityUrn(self, value: Union[None, str]) -> None:
+        """Setter: Urn of the entity being written"""
         self._inner_dict['entityUrn'] = value
     
     
     @property
     def entityKeyAspect(self) -> Union[None, "GenericAspectClass"]:
-        """Key aspect of the entity being written"""
+        """Getter: Key aspect of the entity being written"""
         return self._inner_dict.get('entityKeyAspect')  # type: ignore
     
     @entityKeyAspect.setter
     def entityKeyAspect(self, value: Union[None, "GenericAspectClass"]) -> None:
+        """Setter: Key aspect of the entity being written"""
         self._inner_dict['entityKeyAspect'] = value
     
     
     @property
     def changeType(self) -> Union[str, "ChangeTypeClass"]:
-        """Type of change being proposed"""
+        """Getter: Type of change being proposed"""
         return self._inner_dict.get('changeType')  # type: ignore
     
     @changeType.setter
     def changeType(self, value: Union[str, "ChangeTypeClass"]) -> None:
+        """Setter: Type of change being proposed"""
         self._inner_dict['changeType'] = value
     
     
     @property
     def aspectName(self) -> Union[None, str]:
-        """Aspect of the entity being written to
+        """Getter: Aspect of the entity being written to
     Not filling this out implies that the writer wants to affect the entire entity
     Note: This is only valid for CREATE, UPSERT, and DELETE operations."""
         return self._inner_dict.get('aspectName')  # type: ignore
     
     @aspectName.setter
     def aspectName(self, value: Union[None, str]) -> None:
+        """Setter: Aspect of the entity being written to
+    Not filling this out implies that the writer wants to affect the entire entity
+    Note: This is only valid for CREATE, UPSERT, and DELETE operations."""
         self._inner_dict['aspectName'] = value
     
     
     @property
     def aspect(self) -> Union[None, "GenericAspectClass"]:
-        """The value of the new aspect."""
+        """Getter: The value of the new aspect."""
         return self._inner_dict.get('aspect')  # type: ignore
     
     @aspect.setter
     def aspect(self, value: Union[None, "GenericAspectClass"]) -> None:
+        """Setter: The value of the new aspect."""
         self._inner_dict['aspect'] = value
     
     
     @property
     def systemMetadata(self) -> Union[None, "SystemMetadataClass"]:
-        """A string->string map of custom properties that one might want to attach to an event"""
+        """Getter: A string->string map of custom properties that one might want to attach to an event"""
         return self._inner_dict.get('systemMetadata')  # type: ignore
     
     @systemMetadata.setter
     def systemMetadata(self, value: Union[None, "SystemMetadataClass"]) -> None:
+        """Setter: A string->string map of custom properties that one might want to attach to an event"""
         self._inner_dict['systemMetadata'] = value
     
     
     @property
     def previousAspectValue(self) -> Union[None, "GenericAspectClass"]:
-        """The previous value of the aspect that has changed."""
+        """Getter: The previous value of the aspect that has changed."""
         return self._inner_dict.get('previousAspectValue')  # type: ignore
     
     @previousAspectValue.setter
     def previousAspectValue(self, value: Union[None, "GenericAspectClass"]) -> None:
+        """Setter: The previous value of the aspect that has changed."""
         self._inner_dict['previousAspectValue'] = value
     
     
     @property
     def previousSystemMetadata(self) -> Union[None, "SystemMetadataClass"]:
-        """The previous value of the system metadata field that has changed."""
+        """Getter: The previous value of the system metadata field that has changed."""
         return self._inner_dict.get('previousSystemMetadata')  # type: ignore
     
     @previousSystemMetadata.setter
     def previousSystemMetadata(self, value: Union[None, "SystemMetadataClass"]) -> None:
+        """Setter: The previous value of the system metadata field that has changed."""
         self._inner_dict['previousSystemMetadata'] = value
     
     
     @property
     def created(self) -> Union[None, "AuditStampClass"]:
-        """An audit stamp detailing who and when the aspect was changed by. Required for all intents and purposes."""
+        """Getter: An audit stamp detailing who and when the aspect was changed by. Required for all intents and purposes."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An audit stamp detailing who and when the aspect was changed by. Required for all intents and purposes."""
         self._inner_dict['created'] = value
     
     
 class MetadataChangeProposalClass(DictWrapper):
     """Kafka event for proposing a metadata change for an entity. A corresponding MetadataChangeLog is emitted when the change is accepted and committed, otherwise a FailedMetadataChangeProposal will be emitted instead."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.mxe.MetadataChangeProposal")
@@ -13174,104 +15265,121 @@
         self.entityUrn = entityUrn
         self.entityKeyAspect = entityKeyAspect
         self.changeType = changeType
         self.aspectName = aspectName
         self.aspect = aspect
         self.systemMetadata = systemMetadata
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MetadataChangeProposalClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.auditHeader = self.RECORD_SCHEMA.fields_dict["auditHeader"].default
         self.entityType = str()
         self.entityUrn = self.RECORD_SCHEMA.fields_dict["entityUrn"].default
         self.entityKeyAspect = self.RECORD_SCHEMA.fields_dict["entityKeyAspect"].default
         self.changeType = ChangeTypeClass.UPSERT
         self.aspectName = self.RECORD_SCHEMA.fields_dict["aspectName"].default
         self.aspect = self.RECORD_SCHEMA.fields_dict["aspect"].default
         self.systemMetadata = self.RECORD_SCHEMA.fields_dict["systemMetadata"].default
     
     
     @property
     def auditHeader(self) -> Union[None, "KafkaAuditHeaderClass"]:
-        """Kafka audit header. Currently remains unused in the open source."""
+        """Getter: Kafka audit header. Currently remains unused in the open source."""
         return self._inner_dict.get('auditHeader')  # type: ignore
     
     @auditHeader.setter
     def auditHeader(self, value: Union[None, "KafkaAuditHeaderClass"]) -> None:
+        """Setter: Kafka audit header. Currently remains unused in the open source."""
         self._inner_dict['auditHeader'] = value
     
     
     @property
     def entityType(self) -> str:
-        """Type of the entity being written to"""
+        """Getter: Type of the entity being written to"""
         return self._inner_dict.get('entityType')  # type: ignore
     
     @entityType.setter
     def entityType(self, value: str) -> None:
+        """Setter: Type of the entity being written to"""
         self._inner_dict['entityType'] = value
     
     
     @property
     def entityUrn(self) -> Union[None, str]:
-        """Urn of the entity being written"""
+        """Getter: Urn of the entity being written"""
         return self._inner_dict.get('entityUrn')  # type: ignore
     
     @entityUrn.setter
     def entityUrn(self, value: Union[None, str]) -> None:
+        """Setter: Urn of the entity being written"""
         self._inner_dict['entityUrn'] = value
     
     
     @property
     def entityKeyAspect(self) -> Union[None, "GenericAspectClass"]:
-        """Key aspect of the entity being written"""
+        """Getter: Key aspect of the entity being written"""
         return self._inner_dict.get('entityKeyAspect')  # type: ignore
     
     @entityKeyAspect.setter
     def entityKeyAspect(self, value: Union[None, "GenericAspectClass"]) -> None:
+        """Setter: Key aspect of the entity being written"""
         self._inner_dict['entityKeyAspect'] = value
     
     
     @property
     def changeType(self) -> Union[str, "ChangeTypeClass"]:
-        """Type of change being proposed"""
+        """Getter: Type of change being proposed"""
         return self._inner_dict.get('changeType')  # type: ignore
     
     @changeType.setter
     def changeType(self, value: Union[str, "ChangeTypeClass"]) -> None:
+        """Setter: Type of change being proposed"""
         self._inner_dict['changeType'] = value
     
     
     @property
     def aspectName(self) -> Union[None, str]:
-        """Aspect of the entity being written to
+        """Getter: Aspect of the entity being written to
     Not filling this out implies that the writer wants to affect the entire entity
     Note: This is only valid for CREATE, UPSERT, and DELETE operations."""
         return self._inner_dict.get('aspectName')  # type: ignore
     
     @aspectName.setter
     def aspectName(self, value: Union[None, str]) -> None:
+        """Setter: Aspect of the entity being written to
+    Not filling this out implies that the writer wants to affect the entire entity
+    Note: This is only valid for CREATE, UPSERT, and DELETE operations."""
         self._inner_dict['aspectName'] = value
     
     
     @property
     def aspect(self) -> Union[None, "GenericAspectClass"]:
-        """The value of the new aspect."""
+        """Getter: The value of the new aspect."""
         return self._inner_dict.get('aspect')  # type: ignore
     
     @aspect.setter
     def aspect(self, value: Union[None, "GenericAspectClass"]) -> None:
+        """Setter: The value of the new aspect."""
         self._inner_dict['aspect'] = value
     
     
     @property
     def systemMetadata(self) -> Union[None, "SystemMetadataClass"]:
-        """A string->string map of custom properties that one might want to attach to an event"""
+        """Getter: A string->string map of custom properties that one might want to attach to an event"""
         return self._inner_dict.get('systemMetadata')  # type: ignore
     
     @systemMetadata.setter
     def systemMetadata(self, value: Union[None, "SystemMetadataClass"]) -> None:
+        """Setter: A string->string map of custom properties that one might want to attach to an event"""
         self._inner_dict['systemMetadata'] = value
     
     
 class PlatformEventClass(DictWrapper):
     """A DataHub Platform Event."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.mxe.PlatformEvent")
@@ -13282,72 +15390,90 @@
     ):
         super().__init__()
         
         self.header = header
         self.name = name
         self.payload = payload
     
+    @classmethod
+    def construct_with_defaults(cls) -> "PlatformEventClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
-        self.header = PlatformEventHeaderClass._construct_with_defaults()
+        self.header = PlatformEventHeaderClass.construct_with_defaults()
         self.name = str()
-        self.payload = GenericPayloadClass._construct_with_defaults()
+        self.payload = GenericPayloadClass.construct_with_defaults()
     
     
     @property
     def header(self) -> "PlatformEventHeaderClass":
-        """Header information stored with the event."""
+        """Getter: Header information stored with the event."""
         return self._inner_dict.get('header')  # type: ignore
     
     @header.setter
     def header(self, value: "PlatformEventHeaderClass") -> None:
+        """Setter: Header information stored with the event."""
         self._inner_dict['header'] = value
     
     
     @property
     def name(self) -> str:
-        """The name of the event, e.g. the type of event. For example, 'notificationRequestEvent', 'entityChangeEvent'"""
+        """Getter: The name of the event, e.g. the type of event. For example, 'notificationRequestEvent', 'entityChangeEvent'"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: The name of the event, e.g. the type of event. For example, 'notificationRequestEvent', 'entityChangeEvent'"""
         self._inner_dict['name'] = value
     
     
     @property
     def payload(self) -> "GenericPayloadClass":
-        """The event payload."""
+        """Getter: The event payload."""
         return self._inner_dict.get('payload')  # type: ignore
     
     @payload.setter
     def payload(self, value: "GenericPayloadClass") -> None:
+        """Setter: The event payload."""
         self._inner_dict['payload'] = value
     
     
 class PlatformEventHeaderClass(DictWrapper):
     """A header included with each DataHub platform event."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.mxe.PlatformEventHeader")
     def __init__(self,
         timestampMillis: int,
     ):
         super().__init__()
         
         self.timestampMillis = timestampMillis
     
+    @classmethod
+    def construct_with_defaults(cls) -> "PlatformEventHeaderClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMillis = int()
     
     
     @property
     def timestampMillis(self) -> int:
-        """The event timestamp field as epoch at UTC in milli seconds."""
+        """Getter: The event timestamp field as epoch at UTC in milli seconds."""
         return self._inner_dict.get('timestampMillis')  # type: ignore
     
     @timestampMillis.setter
     def timestampMillis(self, value: int) -> None:
+        """Setter: The event timestamp field as epoch at UTC in milli seconds."""
         self._inner_dict['timestampMillis'] = value
     
     
 class SystemMetadataClass(DictWrapper):
     """Metadata associated with each metadata change that is processed by the system"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.mxe.SystemMetadata")
@@ -13370,69 +15496,81 @@
             self.runId = self.RECORD_SCHEMA.fields_dict["runId"].default
         else:
             self.runId = runId
         self.registryName = registryName
         self.registryVersion = registryVersion
         self.properties = properties
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SystemMetadataClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.lastObserved = self.RECORD_SCHEMA.fields_dict["lastObserved"].default
         self.runId = self.RECORD_SCHEMA.fields_dict["runId"].default
         self.registryName = self.RECORD_SCHEMA.fields_dict["registryName"].default
         self.registryVersion = self.RECORD_SCHEMA.fields_dict["registryVersion"].default
         self.properties = self.RECORD_SCHEMA.fields_dict["properties"].default
     
     
     @property
     def lastObserved(self) -> Union[int, None]:
-        """The timestamp the metadata was observed at"""
+        """Getter: The timestamp the metadata was observed at"""
         return self._inner_dict.get('lastObserved')  # type: ignore
     
     @lastObserved.setter
     def lastObserved(self, value: Union[int, None]) -> None:
+        """Setter: The timestamp the metadata was observed at"""
         self._inner_dict['lastObserved'] = value
     
     
     @property
     def runId(self) -> Union[str, None]:
-        """The run id that produced the metadata. Populated in case of batch-ingestion."""
+        """Getter: The run id that produced the metadata. Populated in case of batch-ingestion."""
         return self._inner_dict.get('runId')  # type: ignore
     
     @runId.setter
     def runId(self, value: Union[str, None]) -> None:
+        """Setter: The run id that produced the metadata. Populated in case of batch-ingestion."""
         self._inner_dict['runId'] = value
     
     
     @property
     def registryName(self) -> Union[None, str]:
-        """The model registry name that was used to process this event"""
+        """Getter: The model registry name that was used to process this event"""
         return self._inner_dict.get('registryName')  # type: ignore
     
     @registryName.setter
     def registryName(self, value: Union[None, str]) -> None:
+        """Setter: The model registry name that was used to process this event"""
         self._inner_dict['registryName'] = value
     
     
     @property
     def registryVersion(self) -> Union[None, str]:
-        """The model registry version that was used to process this event"""
+        """Getter: The model registry version that was used to process this event"""
         return self._inner_dict.get('registryVersion')  # type: ignore
     
     @registryVersion.setter
     def registryVersion(self, value: Union[None, str]) -> None:
+        """Setter: The model registry version that was used to process this event"""
         self._inner_dict['registryVersion'] = value
     
     
     @property
     def properties(self) -> Union[None, Dict[str, str]]:
-        """Additional properties"""
+        """Getter: Additional properties"""
         return self._inner_dict.get('properties')  # type: ignore
     
     @properties.setter
     def properties(self, value: Union[None, Dict[str, str]]) -> None:
+        """Setter: Additional properties"""
         self._inner_dict['properties'] = value
     
     
 class ChartCellClass(DictWrapper):
     """Chart cell in a notebook, which will present content in chart format"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.notebook.ChartCell")
@@ -13443,47 +15581,57 @@
     ):
         super().__init__()
         
         self.cellTitle = cellTitle
         self.cellId = cellId
         self.changeAuditStamps = changeAuditStamps
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ChartCellClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.cellTitle = self.RECORD_SCHEMA.fields_dict["cellTitle"].default
         self.cellId = str()
-        self.changeAuditStamps = ChangeAuditStampsClass._construct_with_defaults()
+        self.changeAuditStamps = ChangeAuditStampsClass.construct_with_defaults()
     
     
     @property
     def cellTitle(self) -> Union[None, str]:
-        """Title of the cell"""
+        """Getter: Title of the cell"""
         return self._inner_dict.get('cellTitle')  # type: ignore
     
     @cellTitle.setter
     def cellTitle(self, value: Union[None, str]) -> None:
+        """Setter: Title of the cell"""
         self._inner_dict['cellTitle'] = value
     
     
     @property
     def cellId(self) -> str:
-        """Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'"""
+        """Getter: Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'"""
         return self._inner_dict.get('cellId')  # type: ignore
     
     @cellId.setter
     def cellId(self, value: str) -> None:
+        """Setter: Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'"""
         self._inner_dict['cellId'] = value
     
     
     @property
     def changeAuditStamps(self) -> "ChangeAuditStampsClass":
-        """Captures information about who created/last modified/deleted this Notebook cell and when"""
+        """Getter: Captures information about who created/last modified/deleted this Notebook cell and when"""
         return self._inner_dict.get('changeAuditStamps')  # type: ignore
     
     @changeAuditStamps.setter
     def changeAuditStamps(self, value: "ChangeAuditStampsClass") -> None:
+        """Setter: Captures information about who created/last modified/deleted this Notebook cell and when"""
         self._inner_dict['changeAuditStamps'] = value
     
     
 class EditableNotebookPropertiesClass(_Aspect):
     """Stores editable changes made to properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines
     Note: This is IN BETA version"""
@@ -13510,58 +15658,69 @@
             # default: {'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'time': 0, 'message': None}
             self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         else:
             self.lastModified = lastModified
         self.deleted = deleted
         self.description = description
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableNotebookPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.created = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["created"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["created"].type)
         self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         self.deleted = self.RECORD_SCHEMA.fields_dict["deleted"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def deleted(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
+        """Getter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         return self._inner_dict.get('deleted')  # type: ignore
     
     @deleted.setter
     def deleted(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         self._inner_dict['deleted'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Edited documentation of the Notebook"""
+        """Getter: Edited documentation of the Notebook"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Edited documentation of the Notebook"""
         self._inner_dict['description'] = value
     
     
 class NotebookCellClass(DictWrapper):
     """A record of all supported cells for a Notebook. Only one type of cell will be non-null."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.notebook.NotebookCell")
@@ -13574,58 +15733,69 @@
         super().__init__()
         
         self.textCell = textCell
         self.queryCell = queryCell
         self.chartCell = chartCell
         self.type = type
     
+    @classmethod
+    def construct_with_defaults(cls) -> "NotebookCellClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.textCell = self.RECORD_SCHEMA.fields_dict["textCell"].default
         self.queryCell = self.RECORD_SCHEMA.fields_dict["queryCell"].default
         self.chartCell = self.RECORD_SCHEMA.fields_dict["chartCell"].default
         self.type = NotebookCellTypeClass.TEXT_CELL
     
     
     @property
     def textCell(self) -> Union[None, "TextCellClass"]:
-        """The text cell content. The will be non-null only when all other cell field is null."""
+        """Getter: The text cell content. The will be non-null only when all other cell field is null."""
         return self._inner_dict.get('textCell')  # type: ignore
     
     @textCell.setter
     def textCell(self, value: Union[None, "TextCellClass"]) -> None:
+        """Setter: The text cell content. The will be non-null only when all other cell field is null."""
         self._inner_dict['textCell'] = value
     
     
     @property
     def queryCell(self) -> Union[None, "QueryCellClass"]:
-        """The query cell content. The will be non-null only when all other cell field is null."""
+        """Getter: The query cell content. The will be non-null only when all other cell field is null."""
         return self._inner_dict.get('queryCell')  # type: ignore
     
     @queryCell.setter
     def queryCell(self, value: Union[None, "QueryCellClass"]) -> None:
+        """Setter: The query cell content. The will be non-null only when all other cell field is null."""
         self._inner_dict['queryCell'] = value
     
     
     @property
     def chartCell(self) -> Union[None, "ChartCellClass"]:
-        """The chart cell content. The will be non-null only when all other cell field is null."""
+        """Getter: The chart cell content. The will be non-null only when all other cell field is null."""
         return self._inner_dict.get('chartCell')  # type: ignore
     
     @chartCell.setter
     def chartCell(self, value: Union[None, "ChartCellClass"]) -> None:
+        """Setter: The chart cell content. The will be non-null only when all other cell field is null."""
         self._inner_dict['chartCell'] = value
     
     
     @property
     def type(self) -> Union[str, "NotebookCellTypeClass"]:
-        """The type of this Notebook cell"""
+        """Getter: The type of this Notebook cell"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "NotebookCellTypeClass"]) -> None:
+        """Setter: The type of this Notebook cell"""
         self._inner_dict['type'] = value
     
     
 class NotebookCellTypeClass(object):
     """Type of Notebook Cell"""
     
     
@@ -13655,25 +15825,33 @@
         
         if cells is None:
             # default: []
             self.cells = list()
         else:
             self.cells = cells
     
+    @classmethod
+    def construct_with_defaults(cls) -> "NotebookContentClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.cells = list()
     
     
     @property
     def cells(self) -> List["NotebookCellClass"]:
-        """The content of a Notebook which is composed by a list of NotebookCell"""
+        """Getter: The content of a Notebook which is composed by a list of NotebookCell"""
         return self._inner_dict.get('cells')  # type: ignore
     
     @cells.setter
     def cells(self, value: List["NotebookCellClass"]) -> None:
+        """Setter: The content of a Notebook which is composed by a list of NotebookCell"""
         self._inner_dict['cells'] = value
     
     
 class NotebookInfoClass(_Aspect):
     """Information about a Notebook
     Note: This is IN BETA version"""
 
@@ -13697,69 +15875,81 @@
         else:
             self.customProperties = customProperties
         self.externalUrl = externalUrl
         self.title = title
         self.description = description
         self.changeAuditStamps = changeAuditStamps
     
+    @classmethod
+    def construct_with_defaults(cls) -> "NotebookInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.customProperties = dict()
         self.externalUrl = self.RECORD_SCHEMA.fields_dict["externalUrl"].default
         self.title = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
-        self.changeAuditStamps = ChangeAuditStampsClass._construct_with_defaults()
+        self.changeAuditStamps = ChangeAuditStampsClass.construct_with_defaults()
     
     
     @property
     def customProperties(self) -> Dict[str, str]:
-        """Custom property bag."""
+        """Getter: Custom property bag."""
         return self._inner_dict.get('customProperties')  # type: ignore
     
     @customProperties.setter
     def customProperties(self, value: Dict[str, str]) -> None:
+        """Setter: Custom property bag."""
         self._inner_dict['customProperties'] = value
     
     
     @property
     def externalUrl(self) -> Union[None, str]:
-        """URL where the reference exist"""
+        """Getter: URL where the reference exist"""
         return self._inner_dict.get('externalUrl')  # type: ignore
     
     @externalUrl.setter
     def externalUrl(self, value: Union[None, str]) -> None:
+        """Setter: URL where the reference exist"""
         self._inner_dict['externalUrl'] = value
     
     
     @property
     def title(self) -> str:
-        """Title of the Notebook"""
+        """Getter: Title of the Notebook"""
         return self._inner_dict.get('title')  # type: ignore
     
     @title.setter
     def title(self, value: str) -> None:
+        """Setter: Title of the Notebook"""
         self._inner_dict['title'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Detailed description about the Notebook"""
+        """Getter: Detailed description about the Notebook"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Detailed description about the Notebook"""
         self._inner_dict['description'] = value
     
     
     @property
     def changeAuditStamps(self) -> "ChangeAuditStampsClass":
-        """Captures information about who created/last modified/deleted this Notebook and when"""
+        """Getter: Captures information about who created/last modified/deleted this Notebook and when"""
         return self._inner_dict.get('changeAuditStamps')  # type: ignore
     
     @changeAuditStamps.setter
     def changeAuditStamps(self, value: "ChangeAuditStampsClass") -> None:
+        """Setter: Captures information about who created/last modified/deleted this Notebook and when"""
         self._inner_dict['changeAuditStamps'] = value
     
     
 class QueryCellClass(DictWrapper):
     """Query cell in a Notebook, which will present content in query format"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.notebook.QueryCell")
@@ -13774,69 +15964,81 @@
         
         self.cellTitle = cellTitle
         self.cellId = cellId
         self.changeAuditStamps = changeAuditStamps
         self.rawQuery = rawQuery
         self.lastExecuted = lastExecuted
     
+    @classmethod
+    def construct_with_defaults(cls) -> "QueryCellClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.cellTitle = self.RECORD_SCHEMA.fields_dict["cellTitle"].default
         self.cellId = str()
-        self.changeAuditStamps = ChangeAuditStampsClass._construct_with_defaults()
+        self.changeAuditStamps = ChangeAuditStampsClass.construct_with_defaults()
         self.rawQuery = str()
         self.lastExecuted = self.RECORD_SCHEMA.fields_dict["lastExecuted"].default
     
     
     @property
     def cellTitle(self) -> Union[None, str]:
-        """Title of the cell"""
+        """Getter: Title of the cell"""
         return self._inner_dict.get('cellTitle')  # type: ignore
     
     @cellTitle.setter
     def cellTitle(self, value: Union[None, str]) -> None:
+        """Setter: Title of the cell"""
         self._inner_dict['cellTitle'] = value
     
     
     @property
     def cellId(self) -> str:
-        """Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'"""
+        """Getter: Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'"""
         return self._inner_dict.get('cellId')  # type: ignore
     
     @cellId.setter
     def cellId(self, value: str) -> None:
+        """Setter: Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'"""
         self._inner_dict['cellId'] = value
     
     
     @property
     def changeAuditStamps(self) -> "ChangeAuditStampsClass":
-        """Captures information about who created/last modified/deleted this Notebook cell and when"""
+        """Getter: Captures information about who created/last modified/deleted this Notebook cell and when"""
         return self._inner_dict.get('changeAuditStamps')  # type: ignore
     
     @changeAuditStamps.setter
     def changeAuditStamps(self, value: "ChangeAuditStampsClass") -> None:
+        """Setter: Captures information about who created/last modified/deleted this Notebook cell and when"""
         self._inner_dict['changeAuditStamps'] = value
     
     
     @property
     def rawQuery(self) -> str:
-        """Raw query to explain some specific logic in a Notebook"""
+        """Getter: Raw query to explain some specific logic in a Notebook"""
         return self._inner_dict.get('rawQuery')  # type: ignore
     
     @rawQuery.setter
     def rawQuery(self, value: str) -> None:
+        """Setter: Raw query to explain some specific logic in a Notebook"""
         self._inner_dict['rawQuery'] = value
     
     
     @property
     def lastExecuted(self) -> Union[None, "AuditStampClass"]:
-        """Captures information about who last executed this query cell and when"""
+        """Getter: Captures information about who last executed this query cell and when"""
         return self._inner_dict.get('lastExecuted')  # type: ignore
     
     @lastExecuted.setter
     def lastExecuted(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: Captures information about who last executed this query cell and when"""
         self._inner_dict['lastExecuted'] = value
     
     
 class TextCellClass(DictWrapper):
     """Text cell in a Notebook, which will present content in text format"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.notebook.TextCell")
@@ -13849,58 +16051,69 @@
         super().__init__()
         
         self.cellTitle = cellTitle
         self.cellId = cellId
         self.changeAuditStamps = changeAuditStamps
         self.text = text
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TextCellClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.cellTitle = self.RECORD_SCHEMA.fields_dict["cellTitle"].default
         self.cellId = str()
-        self.changeAuditStamps = ChangeAuditStampsClass._construct_with_defaults()
+        self.changeAuditStamps = ChangeAuditStampsClass.construct_with_defaults()
         self.text = str()
     
     
     @property
     def cellTitle(self) -> Union[None, str]:
-        """Title of the cell"""
+        """Getter: Title of the cell"""
         return self._inner_dict.get('cellTitle')  # type: ignore
     
     @cellTitle.setter
     def cellTitle(self, value: Union[None, str]) -> None:
+        """Setter: Title of the cell"""
         self._inner_dict['cellTitle'] = value
     
     
     @property
     def cellId(self) -> str:
-        """Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'"""
+        """Getter: Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'"""
         return self._inner_dict.get('cellId')  # type: ignore
     
     @cellId.setter
     def cellId(self, value: str) -> None:
+        """Setter: Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'"""
         self._inner_dict['cellId'] = value
     
     
     @property
     def changeAuditStamps(self) -> "ChangeAuditStampsClass":
-        """Captures information about who created/last modified/deleted this Notebook cell and when"""
+        """Getter: Captures information about who created/last modified/deleted this Notebook cell and when"""
         return self._inner_dict.get('changeAuditStamps')  # type: ignore
     
     @changeAuditStamps.setter
     def changeAuditStamps(self, value: "ChangeAuditStampsClass") -> None:
+        """Setter: Captures information about who created/last modified/deleted this Notebook cell and when"""
         self._inner_dict['changeAuditStamps'] = value
     
     
     @property
     def text(self) -> str:
-        """The actual text in a TextCell in a Notebook"""
+        """Getter: The actual text in a TextCell in a Notebook"""
         return self._inner_dict.get('text')  # type: ignore
     
     @text.setter
     def text(self, value: str) -> None:
+        """Setter: The actual text in a TextCell in a Notebook"""
         self._inner_dict['text'] = value
     
     
 class EntityChangeEventClass(DictWrapper):
     """Shared fields for all entity change events."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.platform.event.v1.EntityChangeEvent")
@@ -13921,114 +16134,136 @@
         self.category = category
         self.operation = operation
         self.modifier = modifier
         self.parameters = parameters
         self.auditStamp = auditStamp
         self.version = version
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EntityChangeEventClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.entityType = str()
         self.entityUrn = str()
         self.category = str()
         self.operation = str()
         self.modifier = self.RECORD_SCHEMA.fields_dict["modifier"].default
         self.parameters = self.RECORD_SCHEMA.fields_dict["parameters"].default
-        self.auditStamp = AuditStampClass._construct_with_defaults()
+        self.auditStamp = AuditStampClass.construct_with_defaults()
         self.version = int()
     
     
     @property
     def entityType(self) -> str:
-        """The type of the entity affected. Corresponds to the entity registry, e.g. 'dataset', 'chart', 'dashboard', etc."""
+        """Getter: The type of the entity affected. Corresponds to the entity registry, e.g. 'dataset', 'chart', 'dashboard', etc."""
         return self._inner_dict.get('entityType')  # type: ignore
     
     @entityType.setter
     def entityType(self, value: str) -> None:
+        """Setter: The type of the entity affected. Corresponds to the entity registry, e.g. 'dataset', 'chart', 'dashboard', etc."""
         self._inner_dict['entityType'] = value
     
     
     @property
     def entityUrn(self) -> str:
-        """The urn of the entity which was affected."""
+        """Getter: The urn of the entity which was affected."""
         return self._inner_dict.get('entityUrn')  # type: ignore
     
     @entityUrn.setter
     def entityUrn(self, value: str) -> None:
+        """Setter: The urn of the entity which was affected."""
         self._inner_dict['entityUrn'] = value
     
     
     @property
     def category(self) -> str:
-        """The category type (TAG, GLOSSARY_TERM, OWNERSHIP, TECHNICAL_SCHEMA, etc). This is used to determine what the rest of the schema will look like."""
+        """Getter: The category type (TAG, GLOSSARY_TERM, OWNERSHIP, TECHNICAL_SCHEMA, etc). This is used to determine what the rest of the schema will look like."""
         return self._inner_dict.get('category')  # type: ignore
     
     @category.setter
     def category(self, value: str) -> None:
+        """Setter: The category type (TAG, GLOSSARY_TERM, OWNERSHIP, TECHNICAL_SCHEMA, etc). This is used to determine what the rest of the schema will look like."""
         self._inner_dict['category'] = value
     
     
     @property
     def operation(self) -> str:
-        """The operation type. This is used to determine what the rest of the schema will look like."""
+        """Getter: The operation type. This is used to determine what the rest of the schema will look like."""
         return self._inner_dict.get('operation')  # type: ignore
     
     @operation.setter
     def operation(self, value: str) -> None:
+        """Setter: The operation type. This is used to determine what the rest of the schema will look like."""
         self._inner_dict['operation'] = value
     
     
     @property
     def modifier(self) -> Union[None, str]:
-        """The urn of the entity which was affected."""
+        """Getter: The urn of the entity which was affected."""
         return self._inner_dict.get('modifier')  # type: ignore
     
     @modifier.setter
     def modifier(self, value: Union[None, str]) -> None:
+        """Setter: The urn of the entity which was affected."""
         self._inner_dict['modifier'] = value
     
     
     @property
     def parameters(self) -> Union[None, "ParametersClass"]:
-        """Arbitrary key-value parameters corresponding to the event."""
+        """Getter: Arbitrary key-value parameters corresponding to the event."""
         return self._inner_dict.get('parameters')  # type: ignore
     
     @parameters.setter
     def parameters(self, value: Union[None, "ParametersClass"]) -> None:
+        """Setter: Arbitrary key-value parameters corresponding to the event."""
         self._inner_dict['parameters'] = value
     
     
     @property
     def auditStamp(self) -> "AuditStampClass":
-        """Audit stamp of the operation"""
+        """Getter: Audit stamp of the operation"""
         return self._inner_dict.get('auditStamp')  # type: ignore
     
     @auditStamp.setter
     def auditStamp(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp of the operation"""
         self._inner_dict['auditStamp'] = value
     
     
     @property
     def version(self) -> int:
-        """The version of the event type, incremented in integers."""
+        """Getter: The version of the event type, incremented in integers."""
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: int) -> None:
+        """Setter: The version of the event type, incremented in integers."""
         self._inner_dict['version'] = value
     
     
 class ParametersClass(DictWrapper):
     """Arbitrary key-value parameters for an Entity Change Event. (any record)."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.platform.event.v1.Parameters")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ParametersClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class DataHubActorFilterClass(DictWrapper):
     """Information used to filter DataHub actors."""
     
@@ -14058,81 +16293,95 @@
         if allGroups is None:
             # default: False
             self.allGroups = self.RECORD_SCHEMA.fields_dict["allGroups"].default
         else:
             self.allGroups = allGroups
         self.roles = roles
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubActorFilterClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.users = self.RECORD_SCHEMA.fields_dict["users"].default
         self.groups = self.RECORD_SCHEMA.fields_dict["groups"].default
         self.resourceOwners = self.RECORD_SCHEMA.fields_dict["resourceOwners"].default
         self.allUsers = self.RECORD_SCHEMA.fields_dict["allUsers"].default
         self.allGroups = self.RECORD_SCHEMA.fields_dict["allGroups"].default
         self.roles = self.RECORD_SCHEMA.fields_dict["roles"].default
     
     
     @property
     def users(self) -> Union[None, List[str]]:
-        """A specific set of users to apply the policy to (disjunctive)"""
+        """Getter: A specific set of users to apply the policy to (disjunctive)"""
         return self._inner_dict.get('users')  # type: ignore
     
     @users.setter
     def users(self, value: Union[None, List[str]]) -> None:
+        """Setter: A specific set of users to apply the policy to (disjunctive)"""
         self._inner_dict['users'] = value
     
     
     @property
     def groups(self) -> Union[None, List[str]]:
-        """A specific set of groups to apply the policy to (disjunctive)"""
+        """Getter: A specific set of groups to apply the policy to (disjunctive)"""
         return self._inner_dict.get('groups')  # type: ignore
     
     @groups.setter
     def groups(self, value: Union[None, List[str]]) -> None:
+        """Setter: A specific set of groups to apply the policy to (disjunctive)"""
         self._inner_dict['groups'] = value
     
     
     @property
     def resourceOwners(self) -> bool:
-        """Whether the filter should return true for owners of a particular resource.
+        """Getter: Whether the filter should return true for owners of a particular resource.
     Only applies to policies of type 'Metadata', which have a resource associated with them."""
         return self._inner_dict.get('resourceOwners')  # type: ignore
     
     @resourceOwners.setter
     def resourceOwners(self, value: bool) -> None:
+        """Setter: Whether the filter should return true for owners of a particular resource.
+    Only applies to policies of type 'Metadata', which have a resource associated with them."""
         self._inner_dict['resourceOwners'] = value
     
     
     @property
     def allUsers(self) -> bool:
-        """Whether the filter should apply to all users."""
+        """Getter: Whether the filter should apply to all users."""
         return self._inner_dict.get('allUsers')  # type: ignore
     
     @allUsers.setter
     def allUsers(self, value: bool) -> None:
+        """Setter: Whether the filter should apply to all users."""
         self._inner_dict['allUsers'] = value
     
     
     @property
     def allGroups(self) -> bool:
-        """Whether the filter should apply to all groups."""
+        """Getter: Whether the filter should apply to all groups."""
         return self._inner_dict.get('allGroups')  # type: ignore
     
     @allGroups.setter
     def allGroups(self, value: bool) -> None:
+        """Setter: Whether the filter should apply to all groups."""
         self._inner_dict['allGroups'] = value
     
     
     @property
     def roles(self) -> Union[None, List[str]]:
-        """A specific set of roles to apply the policy to (disjunctive)."""
+        """Getter: A specific set of roles to apply the policy to (disjunctive)."""
         return self._inner_dict.get('roles')  # type: ignore
     
     @roles.setter
     def roles(self, value: Union[None, List[str]]) -> None:
+        """Setter: A specific set of roles to apply the policy to (disjunctive)."""
         self._inner_dict['roles'] = value
     
     
 class DataHubPolicyInfoClass(_Aspect):
     """Information about a DataHub (UI) access policy."""
 
 
@@ -14163,113 +16412,129 @@
         if editable is None:
             # default: True
             self.editable = self.RECORD_SCHEMA.fields_dict["editable"].default
         else:
             self.editable = editable
         self.lastUpdatedTimestamp = lastUpdatedTimestamp
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubPolicyInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.displayName = str()
         self.description = str()
         self.type = str()
         self.state = str()
         self.resources = self.RECORD_SCHEMA.fields_dict["resources"].default
         self.privileges = list()
-        self.actors = DataHubActorFilterClass._construct_with_defaults()
+        self.actors = DataHubActorFilterClass.construct_with_defaults()
         self.editable = self.RECORD_SCHEMA.fields_dict["editable"].default
         self.lastUpdatedTimestamp = self.RECORD_SCHEMA.fields_dict["lastUpdatedTimestamp"].default
     
     
     @property
     def displayName(self) -> str:
-        """Display name of the Policy"""
+        """Getter: Display name of the Policy"""
         return self._inner_dict.get('displayName')  # type: ignore
     
     @displayName.setter
     def displayName(self, value: str) -> None:
+        """Setter: Display name of the Policy"""
         self._inner_dict['displayName'] = value
     
     
     @property
     def description(self) -> str:
-        """Description of the Policy"""
+        """Getter: Description of the Policy"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: str) -> None:
+        """Setter: Description of the Policy"""
         self._inner_dict['description'] = value
     
     
     @property
     def type(self) -> str:
-        """The type of policy"""
+        """Getter: The type of policy"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: str) -> None:
+        """Setter: The type of policy"""
         self._inner_dict['type'] = value
     
     
     @property
     def state(self) -> str:
-        """The state of policy, ACTIVE or INACTIVE"""
+        """Getter: The state of policy, ACTIVE or INACTIVE"""
         return self._inner_dict.get('state')  # type: ignore
     
     @state.setter
     def state(self, value: str) -> None:
+        """Setter: The state of policy, ACTIVE or INACTIVE"""
         self._inner_dict['state'] = value
     
     
     @property
     def resources(self) -> Union[None, "DataHubResourceFilterClass"]:
-        """The resource that the policy applies to. Not required for some 'Platform' privileges."""
+        """Getter: The resource that the policy applies to. Not required for some 'Platform' privileges."""
         return self._inner_dict.get('resources')  # type: ignore
     
     @resources.setter
     def resources(self, value: Union[None, "DataHubResourceFilterClass"]) -> None:
+        """Setter: The resource that the policy applies to. Not required for some 'Platform' privileges."""
         self._inner_dict['resources'] = value
     
     
     @property
     def privileges(self) -> List[str]:
-        """The privileges that the policy grants."""
+        """Getter: The privileges that the policy grants."""
         return self._inner_dict.get('privileges')  # type: ignore
     
     @privileges.setter
     def privileges(self, value: List[str]) -> None:
+        """Setter: The privileges that the policy grants."""
         self._inner_dict['privileges'] = value
     
     
     @property
     def actors(self) -> "DataHubActorFilterClass":
-        """The actors that the policy applies to."""
+        """Getter: The actors that the policy applies to."""
         return self._inner_dict.get('actors')  # type: ignore
     
     @actors.setter
     def actors(self, value: "DataHubActorFilterClass") -> None:
+        """Setter: The actors that the policy applies to."""
         self._inner_dict['actors'] = value
     
     
     @property
     def editable(self) -> bool:
-        """Whether the policy should be editable via the UI"""
+        """Getter: Whether the policy should be editable via the UI"""
         return self._inner_dict.get('editable')  # type: ignore
     
     @editable.setter
     def editable(self, value: bool) -> None:
+        """Setter: Whether the policy should be editable via the UI"""
         self._inner_dict['editable'] = value
     
     
     @property
     def lastUpdatedTimestamp(self) -> Union[None, int]:
-        """Timestamp when the policy was last updated"""
+        """Getter: Timestamp when the policy was last updated"""
         return self._inner_dict.get('lastUpdatedTimestamp')  # type: ignore
     
     @lastUpdatedTimestamp.setter
     def lastUpdatedTimestamp(self, value: Union[None, int]) -> None:
+        """Setter: Timestamp when the policy was last updated"""
         self._inner_dict['lastUpdatedTimestamp'] = value
     
     
 class DataHubResourceFilterClass(DictWrapper):
     """Information used to filter DataHub resource."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.policy.DataHubResourceFilter")
@@ -14286,60 +16551,73 @@
         if allResources is None:
             # default: False
             self.allResources = self.RECORD_SCHEMA.fields_dict["allResources"].default
         else:
             self.allResources = allResources
         self.filter = filter
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubResourceFilterClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = self.RECORD_SCHEMA.fields_dict["type"].default
         self.resources = self.RECORD_SCHEMA.fields_dict["resources"].default
         self.allResources = self.RECORD_SCHEMA.fields_dict["allResources"].default
         self.filter = self.RECORD_SCHEMA.fields_dict["filter"].default
     
     
     @property
     def type(self) -> Union[None, str]:
-        """The type of resource that the policy applies to. This will most often be a data asset entity name, for
+        """Getter: The type of resource that the policy applies to. This will most often be a data asset entity name, for
     example 'dataset'. It is not strictly required because in the future we will want to support filtering a resource
     by domain, as well."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[None, str]) -> None:
+        """Setter: The type of resource that the policy applies to. This will most often be a data asset entity name, for
+    example 'dataset'. It is not strictly required because in the future we will want to support filtering a resource
+    by domain, as well."""
         self._inner_dict['type'] = value
     
     
     @property
     def resources(self) -> Union[None, List[str]]:
-        """A specific set of resources to apply the policy to, e.g. asset urns"""
+        """Getter: A specific set of resources to apply the policy to, e.g. asset urns"""
         return self._inner_dict.get('resources')  # type: ignore
     
     @resources.setter
     def resources(self, value: Union[None, List[str]]) -> None:
+        """Setter: A specific set of resources to apply the policy to, e.g. asset urns"""
         self._inner_dict['resources'] = value
     
     
     @property
     def allResources(self) -> bool:
-        """Whether the policy should be applied to all assets matching the filter."""
+        """Getter: Whether the policy should be applied to all assets matching the filter."""
         return self._inner_dict.get('allResources')  # type: ignore
     
     @allResources.setter
     def allResources(self, value: bool) -> None:
+        """Setter: Whether the policy should be applied to all assets matching the filter."""
         self._inner_dict['allResources'] = value
     
     
     @property
     def filter(self) -> Union[None, "PolicyMatchFilterClass"]:
-        """Filter to apply privileges to"""
+        """Getter: Filter to apply privileges to"""
         return self._inner_dict.get('filter')  # type: ignore
     
     @filter.setter
     def filter(self, value: Union[None, "PolicyMatchFilterClass"]) -> None:
+        """Setter: Filter to apply privileges to"""
         self._inner_dict['filter'] = value
     
     
 class DataHubRoleInfoClass(_Aspect):
     """Information about a DataHub Role."""
 
 
@@ -14358,47 +16636,57 @@
         self.description = description
         if editable is None:
             # default: False
             self.editable = self.RECORD_SCHEMA.fields_dict["editable"].default
         else:
             self.editable = editable
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubRoleInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.description = str()
         self.editable = self.RECORD_SCHEMA.fields_dict["editable"].default
     
     
     @property
     def name(self) -> str:
-        """Name of the Role"""
+        """Getter: Name of the Role"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the Role"""
         self._inner_dict['name'] = value
     
     
     @property
     def description(self) -> str:
-        """Description of the Role"""
+        """Getter: Description of the Role"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: str) -> None:
+        """Setter: Description of the Role"""
         self._inner_dict['description'] = value
     
     
     @property
     def editable(self) -> bool:
-        """Whether the role should be editable via the UI"""
+        """Getter: Whether the role should be editable via the UI"""
         return self._inner_dict.get('editable')  # type: ignore
     
     @editable.setter
     def editable(self, value: bool) -> None:
+        """Setter: Whether the role should be editable via the UI"""
         self._inner_dict['editable'] = value
     
     
 class PolicyMatchConditionClass(object):
     """The matching condition in a filter criterion"""
     
     
@@ -14421,72 +16709,90 @@
         self.values = values
         if condition is None:
             # default: 'EQUALS'
             self.condition = self.RECORD_SCHEMA.fields_dict["condition"].default
         else:
             self.condition = condition
     
+    @classmethod
+    def construct_with_defaults(cls) -> "PolicyMatchCriterionClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.field = str()
         self.values = list()
         self.condition = self.RECORD_SCHEMA.fields_dict["condition"].default
     
     
     @property
     def field(self) -> str:
-        """The name of the field that the criterion refers to"""
+        """Getter: The name of the field that the criterion refers to"""
         return self._inner_dict.get('field')  # type: ignore
     
     @field.setter
     def field(self, value: str) -> None:
+        """Setter: The name of the field that the criterion refers to"""
         self._inner_dict['field'] = value
     
     
     @property
     def values(self) -> List[str]:
-        """Values. Matches criterion if any one of the values matches condition (OR-relationship)"""
+        """Getter: Values. Matches criterion if any one of the values matches condition (OR-relationship)"""
         return self._inner_dict.get('values')  # type: ignore
     
     @values.setter
     def values(self, value: List[str]) -> None:
+        """Setter: Values. Matches criterion if any one of the values matches condition (OR-relationship)"""
         self._inner_dict['values'] = value
     
     
     @property
     def condition(self) -> Union[str, "PolicyMatchConditionClass"]:
-        """The condition for the criterion"""
+        """Getter: The condition for the criterion"""
         return self._inner_dict.get('condition')  # type: ignore
     
     @condition.setter
     def condition(self, value: Union[str, "PolicyMatchConditionClass"]) -> None:
+        """Setter: The condition for the criterion"""
         self._inner_dict['condition'] = value
     
     
 class PolicyMatchFilterClass(DictWrapper):
     """The filter for specifying the resource or actor to apply privileges to"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.policy.PolicyMatchFilter")
     def __init__(self,
         criteria: List["PolicyMatchCriterionClass"],
     ):
         super().__init__()
         
         self.criteria = criteria
     
+    @classmethod
+    def construct_with_defaults(cls) -> "PolicyMatchFilterClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.criteria = list()
     
     
     @property
     def criteria(self) -> List["PolicyMatchCriterionClass"]:
-        """A list of criteria to apply conjunctively (so all criteria must pass)"""
+        """Getter: A list of criteria to apply conjunctively (so all criteria must pass)"""
         return self._inner_dict.get('criteria')  # type: ignore
     
     @criteria.setter
     def criteria(self, value: List["PolicyMatchCriterionClass"]) -> None:
+        """Setter: A list of criteria to apply conjunctively (so all criteria must pass)"""
         self._inner_dict['criteria'] = value
     
     
 class PostContentClass(DictWrapper):
     """Content stored inside a Post."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.post.PostContent")
@@ -14501,69 +16807,81 @@
         
         self.title = title
         self.type = type
         self.description = description
         self.link = link
         self.media = media
     
+    @classmethod
+    def construct_with_defaults(cls) -> "PostContentClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.title = str()
         self.type = PostContentTypeClass.TEXT
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.link = self.RECORD_SCHEMA.fields_dict["link"].default
         self.media = self.RECORD_SCHEMA.fields_dict["media"].default
     
     
     @property
     def title(self) -> str:
-        """Title of the post."""
+        """Getter: Title of the post."""
         return self._inner_dict.get('title')  # type: ignore
     
     @title.setter
     def title(self, value: str) -> None:
+        """Setter: Title of the post."""
         self._inner_dict['title'] = value
     
     
     @property
     def type(self) -> Union[str, "PostContentTypeClass"]:
-        """Type of content held in the post."""
+        """Getter: Type of content held in the post."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "PostContentTypeClass"]) -> None:
+        """Setter: Type of content held in the post."""
         self._inner_dict['type'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Optional description of the post."""
+        """Getter: Optional description of the post."""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Optional description of the post."""
         self._inner_dict['description'] = value
     
     
     @property
     def link(self) -> Union[None, str]:
-        """Optional link that the post is associated with."""
+        """Getter: Optional link that the post is associated with."""
         return self._inner_dict.get('link')  # type: ignore
     
     @link.setter
     def link(self, value: Union[None, str]) -> None:
+        """Setter: Optional link that the post is associated with."""
         self._inner_dict['link'] = value
     
     
     @property
     def media(self) -> Union[None, "MediaClass"]:
-        """Optional media that the post is storing"""
+        """Getter: Optional media that the post is storing"""
         return self._inner_dict.get('media')  # type: ignore
     
     @media.setter
     def media(self, value: Union[None, "MediaClass"]) -> None:
+        """Setter: Optional media that the post is storing"""
         self._inner_dict['media'] = value
     
     
 class PostContentTypeClass(object):
     """Enum defining the type of content held in a Post."""
     
     
@@ -14591,58 +16909,69 @@
         super().__init__()
         
         self.type = type
         self.content = content
         self.created = created
         self.lastModified = lastModified
     
+    @classmethod
+    def construct_with_defaults(cls) -> "PostInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = PostTypeClass.HOME_PAGE_ANNOUNCEMENT
-        self.content = PostContentClass._construct_with_defaults()
+        self.content = PostContentClass.construct_with_defaults()
         self.created = int()
         self.lastModified = int()
     
     
     @property
     def type(self) -> Union[str, "PostTypeClass"]:
-        """Type of the Post."""
+        """Getter: Type of the Post."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "PostTypeClass"]) -> None:
+        """Setter: Type of the Post."""
         self._inner_dict['type'] = value
     
     
     @property
     def content(self) -> "PostContentClass":
-        """Content stored in the post."""
+        """Getter: Content stored in the post."""
         return self._inner_dict.get('content')  # type: ignore
     
     @content.setter
     def content(self, value: "PostContentClass") -> None:
+        """Setter: Content stored in the post."""
         self._inner_dict['content'] = value
     
     
     @property
     def created(self) -> int:
-        """The time at which the post was initially created"""
+        """Getter: The time at which the post was initially created"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: int) -> None:
+        """Setter: The time at which the post was initially created"""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> int:
-        """The time at which the post was last modified"""
+        """Getter: The time at which the post was last modified"""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: int) -> None:
+        """Setter: The time at which the post was last modified"""
         self._inner_dict['lastModified'] = value
     
     
 class PostTypeClass(object):
     """Enum defining types of Posts."""
     
     
@@ -14679,80 +17008,93 @@
         self.statement = statement
         self.source = source
         self.name = name
         self.description = description
         self.created = created
         self.lastModified = lastModified
     
+    @classmethod
+    def construct_with_defaults(cls) -> "QueryPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
-        self.statement = QueryStatementClass._construct_with_defaults()
+        self.statement = QueryStatementClass.construct_with_defaults()
         self.source = QuerySourceClass.MANUAL
         self.name = self.RECORD_SCHEMA.fields_dict["name"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
-        self.created = AuditStampClass._construct_with_defaults()
-        self.lastModified = AuditStampClass._construct_with_defaults()
+        self.created = AuditStampClass.construct_with_defaults()
+        self.lastModified = AuditStampClass.construct_with_defaults()
     
     
     @property
     def statement(self) -> "QueryStatementClass":
-        """The Query Statement."""
+        """Getter: The Query Statement."""
         return self._inner_dict.get('statement')  # type: ignore
     
     @statement.setter
     def statement(self, value: "QueryStatementClass") -> None:
+        """Setter: The Query Statement."""
         self._inner_dict['statement'] = value
     
     
     @property
     def source(self) -> Union[str, "QuerySourceClass"]:
-        """The source of the Query"""
+        """Getter: The source of the Query"""
         return self._inner_dict.get('source')  # type: ignore
     
     @source.setter
     def source(self, value: Union[str, "QuerySourceClass"]) -> None:
+        """Setter: The source of the Query"""
         self._inner_dict['source'] = value
     
     
     @property
     def name(self) -> Union[None, str]:
-        """Optional display name to identify the query."""
+        """Getter: Optional display name to identify the query."""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: Union[None, str]) -> None:
+        """Setter: Optional display name to identify the query."""
         self._inner_dict['name'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """The Query description."""
+        """Getter: The Query description."""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: The Query description."""
         self._inner_dict['description'] = value
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """Audit stamp capturing the time and actor who created the Query."""
+        """Getter: Audit stamp capturing the time and actor who created the Query."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp capturing the time and actor who created the Query."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """Audit stamp capturing the time and actor who last modified the Query."""
+        """Getter: Audit stamp capturing the time and actor who last modified the Query."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp capturing the time and actor who last modified the Query."""
         self._inner_dict['lastModified'] = value
     
     
 class QuerySourceClass(object):
     # No docs available.
     
     
@@ -14773,36 +17115,45 @@
         self.value = value
         if language is None:
             # default: 'SQL'
             self.language = self.RECORD_SCHEMA.fields_dict["language"].default
         else:
             self.language = language
     
+    @classmethod
+    def construct_with_defaults(cls) -> "QueryStatementClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.value = str()
         self.language = self.RECORD_SCHEMA.fields_dict["language"].default
     
     
     @property
     def value(self) -> str:
-        """The query text"""
+        """Getter: The query text"""
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: str) -> None:
+        """Setter: The query text"""
         self._inner_dict['value'] = value
     
     
     @property
     def language(self) -> Union[str, "QueryLanguageClass"]:
-        """The language of the Query, e.g. SQL."""
+        """Getter: The language of the Query, e.g. SQL."""
         return self._inner_dict.get('language')  # type: ignore
     
     @language.setter
     def language(self, value: Union[str, "QueryLanguageClass"]) -> None:
+        """Setter: The language of the Query, e.g. SQL."""
         self._inner_dict['language'] = value
     
     
 class QuerySubjectClass(DictWrapper):
     """A single subject of a particular query.
     In the future, we may evolve this model to include richer details
     about the Query Subject in relation to the query."""
@@ -14811,25 +17162,33 @@
     def __init__(self,
         entity: str,
     ):
         super().__init__()
         
         self.entity = entity
     
+    @classmethod
+    def construct_with_defaults(cls) -> "QuerySubjectClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.entity = str()
     
     
     @property
     def entity(self) -> str:
-        """An entity which is the subject of a query."""
+        """Getter: An entity which is the subject of a query."""
         return self._inner_dict.get('entity')  # type: ignore
     
     @entity.setter
     def entity(self, value: str) -> None:
+        """Setter: An entity which is the subject of a query."""
         self._inner_dict['entity'] = value
     
     
 class QuerySubjectsClass(_Aspect):
     """Information about the subjects of a particular Query, i.e. the assets
     being queried."""
 
@@ -14841,31 +17200,45 @@
     def __init__(self,
         subjects: List["QuerySubjectClass"],
     ):
         super().__init__()
         
         self.subjects = subjects
     
+    @classmethod
+    def construct_with_defaults(cls) -> "QuerySubjectsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.subjects = list()
     
     
     @property
     def subjects(self) -> List["QuerySubjectClass"]:
-        """One or more subjects of the query.
+        """Getter: One or more subjects of the query.
     
     In single-asset queries (e.g. table select), this will contain the Table reference
     and optionally schema field references.
     
     In multi-asset queries (e.g. table joins), this may contain multiple Table references
     and optionally schema field references."""
         return self._inner_dict.get('subjects')  # type: ignore
     
     @subjects.setter
     def subjects(self, value: List["QuerySubjectClass"]) -> None:
+        """Setter: One or more subjects of the query.
+    
+    In single-asset queries (e.g. table select), this will contain the Table reference
+    and optionally schema field references.
+    
+    In multi-asset queries (e.g. table joins), this may contain multiple Table references
+    and optionally schema field references."""
         self._inner_dict['subjects'] = value
     
     
 class DataHubRetentionConfigClass(_Aspect):
     # No docs available.
 
 
@@ -14876,25 +17249,33 @@
     def __init__(self,
         retention: "RetentionClass",
     ):
         super().__init__()
         
         self.retention = retention
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubRetentionConfigClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
-        self.retention = RetentionClass._construct_with_defaults()
+        self.retention = RetentionClass.construct_with_defaults()
     
     
     @property
     def retention(self) -> "RetentionClass":
         # No docs available.
         return self._inner_dict.get('retention')  # type: ignore
     
     @retention.setter
     def retention(self, value: "RetentionClass") -> None:
+        # No docs available.
         self._inner_dict['retention'] = value
     
     
 class RetentionClass(DictWrapper):
     """Base class that encapsulates different retention policies.
     Only one of the fields should be set"""
     
@@ -14904,161 +17285,216 @@
         time: Union[None, "TimeBasedRetentionClass"]=None,
     ):
         super().__init__()
         
         self.version = version
         self.time = time
     
+    @classmethod
+    def construct_with_defaults(cls) -> "RetentionClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.version = self.RECORD_SCHEMA.fields_dict["version"].default
         self.time = self.RECORD_SCHEMA.fields_dict["time"].default
     
     
     @property
     def version(self) -> Union[None, "VersionBasedRetentionClass"]:
         # No docs available.
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: Union[None, "VersionBasedRetentionClass"]) -> None:
+        # No docs available.
         self._inner_dict['version'] = value
     
     
     @property
     def time(self) -> Union[None, "TimeBasedRetentionClass"]:
         # No docs available.
         return self._inner_dict.get('time')  # type: ignore
     
     @time.setter
     def time(self, value: Union[None, "TimeBasedRetentionClass"]) -> None:
+        # No docs available.
         self._inner_dict['time'] = value
     
     
 class TimeBasedRetentionClass(DictWrapper):
     """Keep records that are less than X seconds old"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.retention.TimeBasedRetention")
     def __init__(self,
         maxAgeInSeconds: int,
     ):
         super().__init__()
         
         self.maxAgeInSeconds = maxAgeInSeconds
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TimeBasedRetentionClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.maxAgeInSeconds = int()
     
     
     @property
     def maxAgeInSeconds(self) -> int:
         # No docs available.
         return self._inner_dict.get('maxAgeInSeconds')  # type: ignore
     
     @maxAgeInSeconds.setter
     def maxAgeInSeconds(self, value: int) -> None:
+        # No docs available.
         self._inner_dict['maxAgeInSeconds'] = value
     
     
 class VersionBasedRetentionClass(DictWrapper):
     """Keep max N latest records"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.retention.VersionBasedRetention")
     def __init__(self,
         maxVersions: int,
     ):
         super().__init__()
         
         self.maxVersions = maxVersions
     
+    @classmethod
+    def construct_with_defaults(cls) -> "VersionBasedRetentionClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.maxVersions = int()
     
     
     @property
     def maxVersions(self) -> int:
         # No docs available.
         return self._inner_dict.get('maxVersions')  # type: ignore
     
     @maxVersions.setter
     def maxVersions(self, value: int) -> None:
+        # No docs available.
         self._inner_dict['maxVersions'] = value
     
     
 class ArrayTypeClass(DictWrapper):
     """Array field type."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.ArrayType")
     def __init__(self,
         nestedType: Union[None, List[str]]=None,
     ):
         super().__init__()
         
         self.nestedType = nestedType
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ArrayTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.nestedType = self.RECORD_SCHEMA.fields_dict["nestedType"].default
     
     
     @property
     def nestedType(self) -> Union[None, List[str]]:
-        """List of types this array holds."""
+        """Getter: List of types this array holds."""
         return self._inner_dict.get('nestedType')  # type: ignore
     
     @nestedType.setter
     def nestedType(self, value: Union[None, List[str]]) -> None:
+        """Setter: List of types this array holds."""
         self._inner_dict['nestedType'] = value
     
     
 class BinaryJsonSchemaClass(DictWrapper):
     """Schema text of binary JSON schema."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.BinaryJsonSchema")
     def __init__(self,
         schema: str,
     ):
         super().__init__()
         
         self.schema = schema
     
+    @classmethod
+    def construct_with_defaults(cls) -> "BinaryJsonSchemaClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.schema = str()
     
     
     @property
     def schema(self) -> str:
-        """The native schema text for binary JSON file format."""
+        """Getter: The native schema text for binary JSON file format."""
         return self._inner_dict.get('schema')  # type: ignore
     
     @schema.setter
     def schema(self, value: str) -> None:
+        """Setter: The native schema text for binary JSON file format."""
         self._inner_dict['schema'] = value
     
     
 class BooleanTypeClass(DictWrapper):
     """Boolean field type."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.BooleanType")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "BooleanTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class BytesTypeClass(DictWrapper):
     """Bytes field type."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.BytesType")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "BytesTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class DatasetFieldForeignKeyClass(DictWrapper):
     """For non-urn based foregin keys."""
     
@@ -15070,59 +17506,76 @@
     ):
         super().__init__()
         
         self.parentDataset = parentDataset
         self.currentFieldPaths = currentFieldPaths
         self.parentField = parentField
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DatasetFieldForeignKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.parentDataset = str()
         self.currentFieldPaths = list()
         self.parentField = str()
     
     
     @property
     def parentDataset(self) -> str:
-        """dataset that stores the resource."""
+        """Getter: dataset that stores the resource."""
         return self._inner_dict.get('parentDataset')  # type: ignore
     
     @parentDataset.setter
     def parentDataset(self, value: str) -> None:
+        """Setter: dataset that stores the resource."""
         self._inner_dict['parentDataset'] = value
     
     
     @property
     def currentFieldPaths(self) -> List[str]:
-        """List of fields in hosting(current) SchemaMetadata that conform a foreign key. List can contain a single entry or multiple entries if several entries in hosting schema conform a foreign key in a single parent dataset."""
+        """Getter: List of fields in hosting(current) SchemaMetadata that conform a foreign key. List can contain a single entry or multiple entries if several entries in hosting schema conform a foreign key in a single parent dataset."""
         return self._inner_dict.get('currentFieldPaths')  # type: ignore
     
     @currentFieldPaths.setter
     def currentFieldPaths(self, value: List[str]) -> None:
+        """Setter: List of fields in hosting(current) SchemaMetadata that conform a foreign key. List can contain a single entry or multiple entries if several entries in hosting schema conform a foreign key in a single parent dataset."""
         self._inner_dict['currentFieldPaths'] = value
     
     
     @property
     def parentField(self) -> str:
-        """SchemaField@fieldPath that uniquely identify field in parent dataset that this field references."""
+        """Getter: SchemaField@fieldPath that uniquely identify field in parent dataset that this field references."""
         return self._inner_dict.get('parentField')  # type: ignore
     
     @parentField.setter
     def parentField(self, value: str) -> None:
+        """Setter: SchemaField@fieldPath that uniquely identify field in parent dataset that this field references."""
         self._inner_dict['parentField'] = value
     
     
 class DateTypeClass(DictWrapper):
     """Date field type."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.DateType")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DateTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class EditableSchemaFieldInfoClass(DictWrapper):
     """SchemaField to describe metadata related to dataset schema."""
     
@@ -15136,58 +17589,69 @@
         super().__init__()
         
         self.fieldPath = fieldPath
         self.description = description
         self.globalTags = globalTags
         self.glossaryTerms = glossaryTerms
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableSchemaFieldInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.fieldPath = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.globalTags = self.RECORD_SCHEMA.fields_dict["globalTags"].default
         self.glossaryTerms = self.RECORD_SCHEMA.fields_dict["glossaryTerms"].default
     
     
     @property
     def fieldPath(self) -> str:
-        """FieldPath uniquely identifying the SchemaField this metadata is associated with"""
+        """Getter: FieldPath uniquely identifying the SchemaField this metadata is associated with"""
         return self._inner_dict.get('fieldPath')  # type: ignore
     
     @fieldPath.setter
     def fieldPath(self, value: str) -> None:
+        """Setter: FieldPath uniquely identifying the SchemaField this metadata is associated with"""
         self._inner_dict['fieldPath'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Description"""
+        """Getter: Description"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Description"""
         self._inner_dict['description'] = value
     
     
     @property
     def globalTags(self) -> Union[None, "GlobalTagsClass"]:
-        """Tags associated with the field"""
+        """Getter: Tags associated with the field"""
         return self._inner_dict.get('globalTags')  # type: ignore
     
     @globalTags.setter
     def globalTags(self, value: Union[None, "GlobalTagsClass"]) -> None:
+        """Setter: Tags associated with the field"""
         self._inner_dict['globalTags'] = value
     
     
     @property
     def glossaryTerms(self) -> Union[None, "GlossaryTermsClass"]:
-        """Glossary terms associated with the field"""
+        """Getter: Glossary terms associated with the field"""
         return self._inner_dict.get('glossaryTerms')  # type: ignore
     
     @glossaryTerms.setter
     def glossaryTerms(self, value: Union[None, "GlossaryTermsClass"]) -> None:
+        """Setter: Glossary terms associated with the field"""
         self._inner_dict['glossaryTerms'] = value
     
     
 class EditableSchemaMetadataClass(_Aspect):
     """EditableSchemaMetadata stores editable changes made to schema metadata. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines."""
 
@@ -15213,70 +17677,88 @@
             # default: {'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'time': 0, 'message': None}
             self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         else:
             self.lastModified = lastModified
         self.deleted = deleted
         self.editableSchemaFieldInfo = editableSchemaFieldInfo
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EditableSchemaMetadataClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.created = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["created"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["created"].type)
         self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         self.deleted = self.RECORD_SCHEMA.fields_dict["deleted"].default
         self.editableSchemaFieldInfo = list()
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def deleted(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
+        """Getter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         return self._inner_dict.get('deleted')  # type: ignore
     
     @deleted.setter
     def deleted(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         self._inner_dict['deleted'] = value
     
     
     @property
     def editableSchemaFieldInfo(self) -> List["EditableSchemaFieldInfoClass"]:
-        """Client provided a list of fields from document schema."""
+        """Getter: Client provided a list of fields from document schema."""
         return self._inner_dict.get('editableSchemaFieldInfo')  # type: ignore
     
     @editableSchemaFieldInfo.setter
     def editableSchemaFieldInfo(self, value: List["EditableSchemaFieldInfoClass"]) -> None:
+        """Setter: Client provided a list of fields from document schema."""
         self._inner_dict['editableSchemaFieldInfo'] = value
     
     
 class EnumTypeClass(DictWrapper):
     """Enum field type."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.EnumType")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EnumTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class EspressoSchemaClass(DictWrapper):
     """Schema text of an espresso table schema."""
     
@@ -15286,48 +17768,64 @@
         tableSchema: str,
     ):
         super().__init__()
         
         self.documentSchema = documentSchema
         self.tableSchema = tableSchema
     
+    @classmethod
+    def construct_with_defaults(cls) -> "EspressoSchemaClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.documentSchema = str()
         self.tableSchema = str()
     
     
     @property
     def documentSchema(self) -> str:
-        """The native espresso document schema."""
+        """Getter: The native espresso document schema."""
         return self._inner_dict.get('documentSchema')  # type: ignore
     
     @documentSchema.setter
     def documentSchema(self, value: str) -> None:
+        """Setter: The native espresso document schema."""
         self._inner_dict['documentSchema'] = value
     
     
     @property
     def tableSchema(self) -> str:
-        """The espresso table schema definition."""
+        """Getter: The espresso table schema definition."""
         return self._inner_dict.get('tableSchema')  # type: ignore
     
     @tableSchema.setter
     def tableSchema(self, value: str) -> None:
+        """Setter: The espresso table schema definition."""
         self._inner_dict['tableSchema'] = value
     
     
 class FixedTypeClass(DictWrapper):
     """Fixed field type."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.FixedType")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "FixedTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class ForeignKeyConstraintClass(DictWrapper):
     """Description of a foreign key constraint in a schema."""
     
@@ -15341,83 +17839,102 @@
         super().__init__()
         
         self.name = name
         self.foreignFields = foreignFields
         self.sourceFields = sourceFields
         self.foreignDataset = foreignDataset
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ForeignKeyConstraintClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.foreignFields = list()
         self.sourceFields = list()
         self.foreignDataset = str()
     
     
     @property
     def name(self) -> str:
-        """Name of the constraint, likely provided from the source"""
+        """Getter: Name of the constraint, likely provided from the source"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Name of the constraint, likely provided from the source"""
         self._inner_dict['name'] = value
     
     
     @property
     def foreignFields(self) -> List[str]:
-        """Fields the constraint maps to on the foreign dataset"""
+        """Getter: Fields the constraint maps to on the foreign dataset"""
         return self._inner_dict.get('foreignFields')  # type: ignore
     
     @foreignFields.setter
     def foreignFields(self, value: List[str]) -> None:
+        """Setter: Fields the constraint maps to on the foreign dataset"""
         self._inner_dict['foreignFields'] = value
     
     
     @property
     def sourceFields(self) -> List[str]:
-        """Fields the constraint maps to on the source dataset"""
+        """Getter: Fields the constraint maps to on the source dataset"""
         return self._inner_dict.get('sourceFields')  # type: ignore
     
     @sourceFields.setter
     def sourceFields(self, value: List[str]) -> None:
+        """Setter: Fields the constraint maps to on the source dataset"""
         self._inner_dict['sourceFields'] = value
     
     
     @property
     def foreignDataset(self) -> str:
-        """Reference to the foreign dataset for ease of lookup"""
+        """Getter: Reference to the foreign dataset for ease of lookup"""
         return self._inner_dict.get('foreignDataset')  # type: ignore
     
     @foreignDataset.setter
     def foreignDataset(self, value: str) -> None:
+        """Setter: Reference to the foreign dataset for ease of lookup"""
         self._inner_dict['foreignDataset'] = value
     
     
 class ForeignKeySpecClass(DictWrapper):
     """Description of a foreign key in a schema."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.ForeignKeySpec")
     def __init__(self,
         foreignKey: Union["DatasetFieldForeignKeyClass", "UrnForeignKeyClass"],
     ):
         super().__init__()
         
         self.foreignKey = foreignKey
     
+    @classmethod
+    def construct_with_defaults(cls) -> "ForeignKeySpecClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
-        self.foreignKey = DatasetFieldForeignKeyClass._construct_with_defaults()
+        self.foreignKey = DatasetFieldForeignKeyClass.construct_with_defaults()
     
     
     @property
     def foreignKey(self) -> Union["DatasetFieldForeignKeyClass", "UrnForeignKeyClass"]:
-        """Foreign key definition in metadata schema."""
+        """Getter: Foreign key definition in metadata schema."""
         return self._inner_dict.get('foreignKey')  # type: ignore
     
     @foreignKey.setter
     def foreignKey(self, value: Union["DatasetFieldForeignKeyClass", "UrnForeignKeyClass"]) -> None:
+        """Setter: Foreign key definition in metadata schema."""
         self._inner_dict['foreignKey'] = value
     
     
 class KafkaSchemaClass(DictWrapper):
     """Schema holder for kafka schema."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.KafkaSchema")
@@ -15426,36 +17943,45 @@
         keySchema: Union[None, str]=None,
     ):
         super().__init__()
         
         self.documentSchema = documentSchema
         self.keySchema = keySchema
     
+    @classmethod
+    def construct_with_defaults(cls) -> "KafkaSchemaClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.documentSchema = str()
         self.keySchema = self.RECORD_SCHEMA.fields_dict["keySchema"].default
     
     
     @property
     def documentSchema(self) -> str:
-        """The native kafka document schema. This is a human readable avro document schema."""
+        """Getter: The native kafka document schema. This is a human readable avro document schema."""
         return self._inner_dict.get('documentSchema')  # type: ignore
     
     @documentSchema.setter
     def documentSchema(self, value: str) -> None:
+        """Setter: The native kafka document schema. This is a human readable avro document schema."""
         self._inner_dict['documentSchema'] = value
     
     
     @property
     def keySchema(self) -> Union[None, str]:
-        """The native kafka key schema as retrieved from Schema Registry"""
+        """Getter: The native kafka key schema as retrieved from Schema Registry"""
         return self._inner_dict.get('keySchema')  # type: ignore
     
     @keySchema.setter
     def keySchema(self, value: Union[None, str]) -> None:
+        """Setter: The native kafka key schema as retrieved from Schema Registry"""
         self._inner_dict['keySchema'] = value
     
     
 class KeyValueSchemaClass(DictWrapper):
     """Schema text of a key-value store schema."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.KeyValueSchema")
@@ -15464,36 +17990,45 @@
         valueSchema: str,
     ):
         super().__init__()
         
         self.keySchema = keySchema
         self.valueSchema = valueSchema
     
+    @classmethod
+    def construct_with_defaults(cls) -> "KeyValueSchemaClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.keySchema = str()
         self.valueSchema = str()
     
     
     @property
     def keySchema(self) -> str:
-        """The raw schema for the key in the key-value store."""
+        """Getter: The raw schema for the key in the key-value store."""
         return self._inner_dict.get('keySchema')  # type: ignore
     
     @keySchema.setter
     def keySchema(self, value: str) -> None:
+        """Setter: The raw schema for the key in the key-value store."""
         self._inner_dict['keySchema'] = value
     
     
     @property
     def valueSchema(self) -> str:
-        """The raw schema for the value in the key-value store."""
+        """Getter: The raw schema for the value in the key-value store."""
         return self._inner_dict.get('valueSchema')  # type: ignore
     
     @valueSchema.setter
     def valueSchema(self, value: str) -> None:
+        """Setter: The raw schema for the value in the key-value store."""
         self._inner_dict['valueSchema'] = value
     
     
 class MapTypeClass(DictWrapper):
     """Map field type."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.MapType")
@@ -15502,86 +18037,117 @@
         valueType: Union[None, str]=None,
     ):
         super().__init__()
         
         self.keyType = keyType
         self.valueType = valueType
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MapTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.keyType = self.RECORD_SCHEMA.fields_dict["keyType"].default
         self.valueType = self.RECORD_SCHEMA.fields_dict["valueType"].default
     
     
     @property
     def keyType(self) -> Union[None, str]:
-        """Key type in a map"""
+        """Getter: Key type in a map"""
         return self._inner_dict.get('keyType')  # type: ignore
     
     @keyType.setter
     def keyType(self, value: Union[None, str]) -> None:
+        """Setter: Key type in a map"""
         self._inner_dict['keyType'] = value
     
     
     @property
     def valueType(self) -> Union[None, str]:
-        """Type of the value in a map"""
+        """Getter: Type of the value in a map"""
         return self._inner_dict.get('valueType')  # type: ignore
     
     @valueType.setter
     def valueType(self, value: Union[None, str]) -> None:
+        """Setter: Type of the value in a map"""
         self._inner_dict['valueType'] = value
     
     
 class MySqlDDLClass(DictWrapper):
     """Schema holder for MySql data definition language that describes an MySql table."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.MySqlDDL")
     def __init__(self,
         tableSchema: str,
     ):
         super().__init__()
         
         self.tableSchema = tableSchema
     
+    @classmethod
+    def construct_with_defaults(cls) -> "MySqlDDLClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.tableSchema = str()
     
     
     @property
     def tableSchema(self) -> str:
-        """The native schema in the dataset's platform. This is a human readable (json blob) table schema."""
+        """Getter: The native schema in the dataset's platform. This is a human readable (json blob) table schema."""
         return self._inner_dict.get('tableSchema')  # type: ignore
     
     @tableSchema.setter
     def tableSchema(self, value: str) -> None:
+        """Setter: The native schema in the dataset's platform. This is a human readable (json blob) table schema."""
         self._inner_dict['tableSchema'] = value
     
     
 class NullTypeClass(DictWrapper):
     """Null field type."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.NullType")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "NullTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class NumberTypeClass(DictWrapper):
     """Number data type: long, integer, short, etc.."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.NumberType")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "NumberTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class OracleDDLClass(DictWrapper):
     """Schema holder for oracle data definition language that describes an oracle table."""
     
@@ -15589,112 +18155,151 @@
     def __init__(self,
         tableSchema: str,
     ):
         super().__init__()
         
         self.tableSchema = tableSchema
     
+    @classmethod
+    def construct_with_defaults(cls) -> "OracleDDLClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.tableSchema = str()
     
     
     @property
     def tableSchema(self) -> str:
-        """The native schema in the dataset's platform. This is a human readable (json blob) table schema."""
+        """Getter: The native schema in the dataset's platform. This is a human readable (json blob) table schema."""
         return self._inner_dict.get('tableSchema')  # type: ignore
     
     @tableSchema.setter
     def tableSchema(self, value: str) -> None:
+        """Setter: The native schema in the dataset's platform. This is a human readable (json blob) table schema."""
         self._inner_dict['tableSchema'] = value
     
     
 class OrcSchemaClass(DictWrapper):
     """Schema text of an ORC schema."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.OrcSchema")
     def __init__(self,
         schema: str,
     ):
         super().__init__()
         
         self.schema = schema
     
+    @classmethod
+    def construct_with_defaults(cls) -> "OrcSchemaClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.schema = str()
     
     
     @property
     def schema(self) -> str:
-        """The native schema for ORC file format."""
+        """Getter: The native schema for ORC file format."""
         return self._inner_dict.get('schema')  # type: ignore
     
     @schema.setter
     def schema(self, value: str) -> None:
+        """Setter: The native schema for ORC file format."""
         self._inner_dict['schema'] = value
     
     
 class OtherSchemaClass(DictWrapper):
     """Schema holder for undefined schema types."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.OtherSchema")
     def __init__(self,
         rawSchema: str,
     ):
         super().__init__()
         
         self.rawSchema = rawSchema
     
+    @classmethod
+    def construct_with_defaults(cls) -> "OtherSchemaClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.rawSchema = str()
     
     
     @property
     def rawSchema(self) -> str:
-        """The native schema in the dataset's platform."""
+        """Getter: The native schema in the dataset's platform."""
         return self._inner_dict.get('rawSchema')  # type: ignore
     
     @rawSchema.setter
     def rawSchema(self, value: str) -> None:
+        """Setter: The native schema in the dataset's platform."""
         self._inner_dict['rawSchema'] = value
     
     
 class PrestoDDLClass(DictWrapper):
     """Schema holder for presto data definition language that describes a presto view."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.PrestoDDL")
     def __init__(self,
         rawSchema: str,
     ):
         super().__init__()
         
         self.rawSchema = rawSchema
     
+    @classmethod
+    def construct_with_defaults(cls) -> "PrestoDDLClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.rawSchema = str()
     
     
     @property
     def rawSchema(self) -> str:
-        """The raw schema in the dataset's platform. This includes the DDL and the columns extracted from DDL."""
+        """Getter: The raw schema in the dataset's platform. This includes the DDL and the columns extracted from DDL."""
         return self._inner_dict.get('rawSchema')  # type: ignore
     
     @rawSchema.setter
     def rawSchema(self, value: str) -> None:
+        """Setter: The raw schema in the dataset's platform. This includes the DDL and the columns extracted from DDL."""
         self._inner_dict['rawSchema'] = value
     
     
 class RecordTypeClass(DictWrapper):
     """Record field type."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.RecordType")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "RecordTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class SchemaFieldClass(DictWrapper):
     """SchemaField to describe metadata related to dataset schema."""
     
@@ -15742,208 +18347,242 @@
             # default: False
             self.isPartOfKey = self.RECORD_SCHEMA.fields_dict["isPartOfKey"].default
         else:
             self.isPartOfKey = isPartOfKey
         self.isPartitioningKey = isPartitioningKey
         self.jsonProps = jsonProps
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SchemaFieldClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.fieldPath = str()
         self.jsonPath = self.RECORD_SCHEMA.fields_dict["jsonPath"].default
         self.nullable = self.RECORD_SCHEMA.fields_dict["nullable"].default
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.label = self.RECORD_SCHEMA.fields_dict["label"].default
         self.created = self.RECORD_SCHEMA.fields_dict["created"].default
         self.lastModified = self.RECORD_SCHEMA.fields_dict["lastModified"].default
-        self.type = SchemaFieldDataTypeClass._construct_with_defaults()
+        self.type = SchemaFieldDataTypeClass.construct_with_defaults()
         self.nativeDataType = str()
         self.recursive = self.RECORD_SCHEMA.fields_dict["recursive"].default
         self.globalTags = self.RECORD_SCHEMA.fields_dict["globalTags"].default
         self.glossaryTerms = self.RECORD_SCHEMA.fields_dict["glossaryTerms"].default
         self.isPartOfKey = self.RECORD_SCHEMA.fields_dict["isPartOfKey"].default
         self.isPartitioningKey = self.RECORD_SCHEMA.fields_dict["isPartitioningKey"].default
         self.jsonProps = self.RECORD_SCHEMA.fields_dict["jsonProps"].default
     
     
     @property
     def fieldPath(self) -> str:
-        """Flattened name of the field. Field is computed from jsonPath field."""
+        """Getter: Flattened name of the field. Field is computed from jsonPath field."""
         return self._inner_dict.get('fieldPath')  # type: ignore
     
     @fieldPath.setter
     def fieldPath(self, value: str) -> None:
+        """Setter: Flattened name of the field. Field is computed from jsonPath field."""
         self._inner_dict['fieldPath'] = value
     
     
     @property
     def jsonPath(self) -> Union[None, str]:
-        """Flattened name of a field in JSON Path notation."""
+        """Getter: Flattened name of a field in JSON Path notation."""
         return self._inner_dict.get('jsonPath')  # type: ignore
     
     @jsonPath.setter
     def jsonPath(self, value: Union[None, str]) -> None:
+        """Setter: Flattened name of a field in JSON Path notation."""
         self._inner_dict['jsonPath'] = value
     
     
     @property
     def nullable(self) -> bool:
-        """Indicates if this field is optional or nullable"""
+        """Getter: Indicates if this field is optional or nullable"""
         return self._inner_dict.get('nullable')  # type: ignore
     
     @nullable.setter
     def nullable(self, value: bool) -> None:
+        """Setter: Indicates if this field is optional or nullable"""
         self._inner_dict['nullable'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Description"""
+        """Getter: Description"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Description"""
         self._inner_dict['description'] = value
     
     
     @property
     def label(self) -> Union[None, str]:
-        """Label of the field. Provides a more human-readable name for the field than field path. Some sources will
+        """Getter: Label of the field. Provides a more human-readable name for the field than field path. Some sources will
     provide this metadata but not all sources have the concept of a label. If just one string is associated with
     a field in a source, that is most likely a description."""
         return self._inner_dict.get('label')  # type: ignore
     
     @label.setter
     def label(self, value: Union[None, str]) -> None:
+        """Setter: Label of the field. Provides a more human-readable name for the field than field path. Some sources will
+    provide this metadata but not all sources have the concept of a label. If just one string is associated with
+    a field in a source, that is most likely a description."""
         self._inner_dict['label'] = value
     
     
     @property
     def created(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the creation of this schema field."""
+        """Getter: An AuditStamp corresponding to the creation of this schema field."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the creation of this schema field."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the last modification of this schema field."""
+        """Getter: An AuditStamp corresponding to the last modification of this schema field."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the last modification of this schema field."""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def type(self) -> "SchemaFieldDataTypeClass":
-        """Platform independent field type of the field."""
+        """Getter: Platform independent field type of the field."""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: "SchemaFieldDataTypeClass") -> None:
+        """Setter: Platform independent field type of the field."""
         self._inner_dict['type'] = value
     
     
     @property
     def nativeDataType(self) -> str:
-        """The native type of the field in the dataset's platform as declared by platform schema."""
+        """Getter: The native type of the field in the dataset's platform as declared by platform schema."""
         return self._inner_dict.get('nativeDataType')  # type: ignore
     
     @nativeDataType.setter
     def nativeDataType(self, value: str) -> None:
+        """Setter: The native type of the field in the dataset's platform as declared by platform schema."""
         self._inner_dict['nativeDataType'] = value
     
     
     @property
     def recursive(self) -> bool:
-        """There are use cases when a field in type B references type A. A field in A references field of type B. In such cases, we will mark the first field as recursive."""
+        """Getter: There are use cases when a field in type B references type A. A field in A references field of type B. In such cases, we will mark the first field as recursive."""
         return self._inner_dict.get('recursive')  # type: ignore
     
     @recursive.setter
     def recursive(self, value: bool) -> None:
+        """Setter: There are use cases when a field in type B references type A. A field in A references field of type B. In such cases, we will mark the first field as recursive."""
         self._inner_dict['recursive'] = value
     
     
     @property
     def globalTags(self) -> Union[None, "GlobalTagsClass"]:
-        """Tags associated with the field"""
+        """Getter: Tags associated with the field"""
         return self._inner_dict.get('globalTags')  # type: ignore
     
     @globalTags.setter
     def globalTags(self, value: Union[None, "GlobalTagsClass"]) -> None:
+        """Setter: Tags associated with the field"""
         self._inner_dict['globalTags'] = value
     
     
     @property
     def glossaryTerms(self) -> Union[None, "GlossaryTermsClass"]:
-        """Glossary terms associated with the field"""
+        """Getter: Glossary terms associated with the field"""
         return self._inner_dict.get('glossaryTerms')  # type: ignore
     
     @glossaryTerms.setter
     def glossaryTerms(self, value: Union[None, "GlossaryTermsClass"]) -> None:
+        """Setter: Glossary terms associated with the field"""
         self._inner_dict['glossaryTerms'] = value
     
     
     @property
     def isPartOfKey(self) -> bool:
-        """For schema fields that are part of complex keys, set this field to true
+        """Getter: For schema fields that are part of complex keys, set this field to true
     We do this to easily distinguish between value and key fields"""
         return self._inner_dict.get('isPartOfKey')  # type: ignore
     
     @isPartOfKey.setter
     def isPartOfKey(self, value: bool) -> None:
+        """Setter: For schema fields that are part of complex keys, set this field to true
+    We do this to easily distinguish between value and key fields"""
         self._inner_dict['isPartOfKey'] = value
     
     
     @property
     def isPartitioningKey(self) -> Union[None, bool]:
-        """For Datasets which are partitioned, this determines the partitioning key."""
+        """Getter: For Datasets which are partitioned, this determines the partitioning key."""
         return self._inner_dict.get('isPartitioningKey')  # type: ignore
     
     @isPartitioningKey.setter
     def isPartitioningKey(self, value: Union[None, bool]) -> None:
+        """Setter: For Datasets which are partitioned, this determines the partitioning key."""
         self._inner_dict['isPartitioningKey'] = value
     
     
     @property
     def jsonProps(self) -> Union[None, str]:
-        """For schema fields that have other properties that are not modeled explicitly,
+        """Getter: For schema fields that have other properties that are not modeled explicitly,
     use this field to serialize those properties into a JSON string"""
         return self._inner_dict.get('jsonProps')  # type: ignore
     
     @jsonProps.setter
     def jsonProps(self, value: Union[None, str]) -> None:
+        """Setter: For schema fields that have other properties that are not modeled explicitly,
+    use this field to serialize those properties into a JSON string"""
         self._inner_dict['jsonProps'] = value
     
     
 class SchemaFieldDataTypeClass(DictWrapper):
     """Schema field data types"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.SchemaFieldDataType")
     def __init__(self,
         type: Union["BooleanTypeClass", "FixedTypeClass", "StringTypeClass", "BytesTypeClass", "NumberTypeClass", "DateTypeClass", "TimeTypeClass", "EnumTypeClass", "NullTypeClass", "MapTypeClass", "ArrayTypeClass", "UnionTypeClass", "RecordTypeClass"],
     ):
         super().__init__()
         
         self.type = type
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SchemaFieldDataTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
-        self.type = BooleanTypeClass._construct_with_defaults()
+        self.type = BooleanTypeClass.construct_with_defaults()
     
     
     @property
     def type(self) -> Union["BooleanTypeClass", "FixedTypeClass", "StringTypeClass", "BytesTypeClass", "NumberTypeClass", "DateTypeClass", "TimeTypeClass", "EnumTypeClass", "NullTypeClass", "MapTypeClass", "ArrayTypeClass", "UnionTypeClass", "RecordTypeClass"]:
-        """Data platform specific types"""
+        """Getter: Data platform specific types"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union["BooleanTypeClass", "FixedTypeClass", "StringTypeClass", "BytesTypeClass", "NumberTypeClass", "DateTypeClass", "TimeTypeClass", "EnumTypeClass", "NullTypeClass", "MapTypeClass", "ArrayTypeClass", "UnionTypeClass", "RecordTypeClass"]) -> None:
+        """Setter: Data platform specific types"""
         self._inner_dict['type'] = value
     
     
 class SchemaMetadataClass(_Aspect):
     """SchemaMetadata to describe metadata related to store schema"""
 
 
@@ -15988,206 +18627,248 @@
         self.hash = hash
         self.platformSchema = platformSchema
         self.fields = fields
         self.primaryKeys = primaryKeys
         self.foreignKeysSpecs = foreignKeysSpecs
         self.foreignKeys = foreignKeys
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SchemaMetadataClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.schemaName = str()
         self.platform = str()
         self.version = int()
         self.created = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["created"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["created"].type)
         self.lastModified = _json_converter.from_json_object(self.RECORD_SCHEMA.fields_dict["lastModified"].default, writers_schema=self.RECORD_SCHEMA.fields_dict["lastModified"].type)
         self.deleted = self.RECORD_SCHEMA.fields_dict["deleted"].default
         self.dataset = self.RECORD_SCHEMA.fields_dict["dataset"].default
         self.cluster = self.RECORD_SCHEMA.fields_dict["cluster"].default
         self.hash = str()
-        self.platformSchema = EspressoSchemaClass._construct_with_defaults()
+        self.platformSchema = EspressoSchemaClass.construct_with_defaults()
         self.fields = list()
         self.primaryKeys = self.RECORD_SCHEMA.fields_dict["primaryKeys"].default
         self.foreignKeysSpecs = self.RECORD_SCHEMA.fields_dict["foreignKeysSpecs"].default
         self.foreignKeys = self.RECORD_SCHEMA.fields_dict["foreignKeys"].default
     
     
     @property
     def schemaName(self) -> str:
-        """Schema name e.g. PageViewEvent, identity.Profile, ams.account_management_tracking"""
+        """Getter: Schema name e.g. PageViewEvent, identity.Profile, ams.account_management_tracking"""
         return self._inner_dict.get('schemaName')  # type: ignore
     
     @schemaName.setter
     def schemaName(self, value: str) -> None:
+        """Setter: Schema name e.g. PageViewEvent, identity.Profile, ams.account_management_tracking"""
         self._inner_dict['schemaName'] = value
     
     
     @property
     def platform(self) -> str:
-        """Standardized platform urn where schema is defined. The data platform Urn (urn:li:platform:{platform_name})"""
+        """Getter: Standardized platform urn where schema is defined. The data platform Urn (urn:li:platform:{platform_name})"""
         return self._inner_dict.get('platform')  # type: ignore
     
     @platform.setter
     def platform(self, value: str) -> None:
+        """Setter: Standardized platform urn where schema is defined. The data platform Urn (urn:li:platform:{platform_name})"""
         self._inner_dict['platform'] = value
     
     
     @property
     def version(self) -> int:
-        """Every change to SchemaMetadata in the resource results in a new version. Version is server assigned. This version is differ from platform native schema version."""
+        """Getter: Every change to SchemaMetadata in the resource results in a new version. Version is server assigned. This version is differ from platform native schema version."""
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: int) -> None:
+        """Setter: Every change to SchemaMetadata in the resource results in a new version. Version is server assigned. This version is differ from platform native schema version."""
         self._inner_dict['version'] = value
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
+        """Getter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data."""
         self._inner_dict['lastModified'] = value
     
     
     @property
     def deleted(self) -> Union[None, "AuditStampClass"]:
-        """An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
+        """Getter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         return self._inner_dict.get('deleted')  # type: ignore
     
     @deleted.setter
     def deleted(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics."""
         self._inner_dict['deleted'] = value
     
     
     @property
     def dataset(self) -> Union[None, str]:
-        """Dataset this schema metadata is associated with."""
+        """Getter: Dataset this schema metadata is associated with."""
         return self._inner_dict.get('dataset')  # type: ignore
     
     @dataset.setter
     def dataset(self, value: Union[None, str]) -> None:
+        """Setter: Dataset this schema metadata is associated with."""
         self._inner_dict['dataset'] = value
     
     
     @property
     def cluster(self) -> Union[None, str]:
-        """The cluster this schema metadata resides from"""
+        """Getter: The cluster this schema metadata resides from"""
         return self._inner_dict.get('cluster')  # type: ignore
     
     @cluster.setter
     def cluster(self, value: Union[None, str]) -> None:
+        """Setter: The cluster this schema metadata resides from"""
         self._inner_dict['cluster'] = value
     
     
     @property
     def hash(self) -> str:
-        """the SHA1 hash of the schema content"""
+        """Getter: the SHA1 hash of the schema content"""
         return self._inner_dict.get('hash')  # type: ignore
     
     @hash.setter
     def hash(self, value: str) -> None:
+        """Setter: the SHA1 hash of the schema content"""
         self._inner_dict['hash'] = value
     
     
     @property
     def platformSchema(self) -> Union["EspressoSchemaClass", "OracleDDLClass", "MySqlDDLClass", "PrestoDDLClass", "KafkaSchemaClass", "BinaryJsonSchemaClass", "OrcSchemaClass", "SchemalessClass", "KeyValueSchemaClass", "OtherSchemaClass"]:
-        """The native schema in the dataset's platform."""
+        """Getter: The native schema in the dataset's platform."""
         return self._inner_dict.get('platformSchema')  # type: ignore
     
     @platformSchema.setter
     def platformSchema(self, value: Union["EspressoSchemaClass", "OracleDDLClass", "MySqlDDLClass", "PrestoDDLClass", "KafkaSchemaClass", "BinaryJsonSchemaClass", "OrcSchemaClass", "SchemalessClass", "KeyValueSchemaClass", "OtherSchemaClass"]) -> None:
+        """Setter: The native schema in the dataset's platform."""
         self._inner_dict['platformSchema'] = value
     
     
     @property
     def fields(self) -> List["SchemaFieldClass"]:
-        """Client provided a list of fields from document schema."""
+        """Getter: Client provided a list of fields from document schema."""
         return self._inner_dict.get('fields')  # type: ignore
     
     @fields.setter
     def fields(self, value: List["SchemaFieldClass"]) -> None:
+        """Setter: Client provided a list of fields from document schema."""
         self._inner_dict['fields'] = value
     
     
     @property
     def primaryKeys(self) -> Union[None, List[str]]:
-        """Client provided list of fields that define primary keys to access record. Field order defines hierarchical espresso keys. Empty lists indicates absence of primary key access patter. Value is a SchemaField@fieldPath."""
+        """Getter: Client provided list of fields that define primary keys to access record. Field order defines hierarchical espresso keys. Empty lists indicates absence of primary key access patter. Value is a SchemaField@fieldPath."""
         return self._inner_dict.get('primaryKeys')  # type: ignore
     
     @primaryKeys.setter
     def primaryKeys(self, value: Union[None, List[str]]) -> None:
+        """Setter: Client provided list of fields that define primary keys to access record. Field order defines hierarchical espresso keys. Empty lists indicates absence of primary key access patter. Value is a SchemaField@fieldPath."""
         self._inner_dict['primaryKeys'] = value
     
     
     @property
     def foreignKeysSpecs(self) -> Union[None, Dict[str, "ForeignKeySpecClass"]]:
-        """Map captures all the references schema makes to external datasets. Map key is ForeignKeySpecName typeref."""
+        """Getter: Map captures all the references schema makes to external datasets. Map key is ForeignKeySpecName typeref."""
         return self._inner_dict.get('foreignKeysSpecs')  # type: ignore
     
     @foreignKeysSpecs.setter
     def foreignKeysSpecs(self, value: Union[None, Dict[str, "ForeignKeySpecClass"]]) -> None:
+        """Setter: Map captures all the references schema makes to external datasets. Map key is ForeignKeySpecName typeref."""
         self._inner_dict['foreignKeysSpecs'] = value
     
     
     @property
     def foreignKeys(self) -> Union[None, List["ForeignKeyConstraintClass"]]:
-        """List of foreign key constraints for the schema"""
+        """Getter: List of foreign key constraints for the schema"""
         return self._inner_dict.get('foreignKeys')  # type: ignore
     
     @foreignKeys.setter
     def foreignKeys(self, value: Union[None, List["ForeignKeyConstraintClass"]]) -> None:
+        """Setter: List of foreign key constraints for the schema"""
         self._inner_dict['foreignKeys'] = value
     
     
 class SchemalessClass(DictWrapper):
     """The dataset has no specific schema associated with it"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.Schemaless")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "SchemalessClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class StringTypeClass(DictWrapper):
     """String field type."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.StringType")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "StringTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class TimeTypeClass(DictWrapper):
     """Time field type. This should also be used for datetimes."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.TimeType")
     def __init__(self,
     ):
         super().__init__()
         
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TimeTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         pass
     
     
 class UnionTypeClass(DictWrapper):
     """Union field type."""
     
@@ -16195,50 +18876,66 @@
     def __init__(self,
         nestedTypes: Union[None, List[str]]=None,
     ):
         super().__init__()
         
         self.nestedTypes = nestedTypes
     
+    @classmethod
+    def construct_with_defaults(cls) -> "UnionTypeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.nestedTypes = self.RECORD_SCHEMA.fields_dict["nestedTypes"].default
     
     
     @property
     def nestedTypes(self) -> Union[None, List[str]]:
-        """List of types in union type."""
+        """Getter: List of types in union type."""
         return self._inner_dict.get('nestedTypes')  # type: ignore
     
     @nestedTypes.setter
     def nestedTypes(self, value: Union[None, List[str]]) -> None:
+        """Setter: List of types in union type."""
         self._inner_dict['nestedTypes'] = value
     
     
 class UrnForeignKeyClass(DictWrapper):
     """If SchemaMetadata fields make any external references and references are of type com.linkedin.pegasus2avro.common.Urn or any children, this models can be used to mark it."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.UrnForeignKey")
     def __init__(self,
         currentFieldPath: str,
     ):
         super().__init__()
         
         self.currentFieldPath = currentFieldPath
     
+    @classmethod
+    def construct_with_defaults(cls) -> "UrnForeignKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.currentFieldPath = str()
     
     
     @property
     def currentFieldPath(self) -> str:
-        """Field in hosting(current) SchemaMetadata."""
+        """Getter: Field in hosting(current) SchemaMetadata."""
         return self._inner_dict.get('currentFieldPath')  # type: ignore
     
     @currentFieldPath.setter
     def currentFieldPath(self, value: str) -> None:
+        """Setter: Field in hosting(current) SchemaMetadata."""
         self._inner_dict['currentFieldPath'] = value
     
     
 class DataHubSecretValueClass(_Aspect):
     """The value of a DataHub Secret"""
 
 
@@ -16255,58 +18952,69 @@
         super().__init__()
         
         self.name = name
         self.value = value
         self.description = description
         self.created = created
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubSecretValueClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.value = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.created = self.RECORD_SCHEMA.fields_dict["created"].default
     
     
     @property
     def name(self) -> str:
-        """The display name for the secret"""
+        """Getter: The display name for the secret"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: The display name for the secret"""
         self._inner_dict['name'] = value
     
     
     @property
     def value(self) -> str:
-        """The AES-encrypted value of the DataHub secret."""
+        """Getter: The AES-encrypted value of the DataHub secret."""
         return self._inner_dict.get('value')  # type: ignore
     
     @value.setter
     def value(self, value: str) -> None:
+        """Setter: The AES-encrypted value of the DataHub secret."""
         self._inner_dict['value'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Description of the secret"""
+        """Getter: Description of the secret"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Description of the secret"""
         self._inner_dict['description'] = value
     
     
     @property
     def created(self) -> Union[None, "AuditStampClass"]:
-        """Created Audit stamp"""
+        """Getter: Created Audit stamp"""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: Union[None, "AuditStampClass"]) -> None:
+        """Setter: Created Audit stamp"""
         self._inner_dict['created'] = value
     
     
 class GlobalSettingsInfoClass(_Aspect):
     """DataHub Global platform settings. Careful - these should not be modified by the outside world!"""
 
 
@@ -16317,50 +19025,66 @@
     def __init__(self,
         views: Union[None, "GlobalViewsSettingsClass"]=None,
     ):
         super().__init__()
         
         self.views = views
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlobalSettingsInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.views = self.RECORD_SCHEMA.fields_dict["views"].default
     
     
     @property
     def views(self) -> Union[None, "GlobalViewsSettingsClass"]:
-        """Settings related to the Views Feature"""
+        """Getter: Settings related to the Views Feature"""
         return self._inner_dict.get('views')  # type: ignore
     
     @views.setter
     def views(self, value: Union[None, "GlobalViewsSettingsClass"]) -> None:
+        """Setter: Settings related to the Views Feature"""
         self._inner_dict['views'] = value
     
     
 class GlobalViewsSettingsClass(DictWrapper):
     """Settings for DataHub Views feature."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.settings.global.GlobalViewsSettings")
     def __init__(self,
         defaultView: Union[None, str]=None,
     ):
         super().__init__()
         
         self.defaultView = defaultView
     
+    @classmethod
+    def construct_with_defaults(cls) -> "GlobalViewsSettingsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.defaultView = self.RECORD_SCHEMA.fields_dict["defaultView"].default
     
     
     @property
     def defaultView(self) -> Union[None, str]:
-        """The default View for the instance, or organization."""
+        """Getter: The default View for the instance, or organization."""
         return self._inner_dict.get('defaultView')  # type: ignore
     
     @defaultView.setter
     def defaultView(self, value: Union[None, str]) -> None:
+        """Setter: The default View for the instance, or organization."""
         self._inner_dict['defaultView'] = value
     
     
 class DataHubStepStatePropertiesClass(_Aspect):
     """The properties associated with a DataHub step state"""
 
 
@@ -16377,36 +19101,45 @@
         if properties is None:
             # default: {}
             self.properties = dict()
         else:
             self.properties = properties
         self.lastModified = lastModified
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubStepStatePropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.properties = dict()
-        self.lastModified = AuditStampClass._construct_with_defaults()
+        self.lastModified = AuditStampClass.construct_with_defaults()
     
     
     @property
     def properties(self) -> Dict[str, str]:
-        """Description of the secret"""
+        """Getter: Description of the secret"""
         return self._inner_dict.get('properties')  # type: ignore
     
     @properties.setter
     def properties(self, value: Dict[str, str]) -> None:
+        """Setter: Description of the secret"""
         self._inner_dict['properties'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """Audit stamp describing the last person to update it."""
+        """Getter: Audit stamp describing the last person to update it."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp describing the last person to update it."""
         self._inner_dict['lastModified'] = value
     
     
 class TagPropertiesClass(_Aspect):
     """Properties associated with a Tag"""
 
 
@@ -16421,47 +19154,57 @@
     ):
         super().__init__()
         
         self.name = name
         self.description = description
         self.colorHex = colorHex
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TagPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.colorHex = self.RECORD_SCHEMA.fields_dict["colorHex"].default
     
     
     @property
     def name(self) -> str:
-        """Display name of the tag"""
+        """Getter: Display name of the tag"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: Display name of the tag"""
         self._inner_dict['name'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Documentation of the tag"""
+        """Getter: Documentation of the tag"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Documentation of the tag"""
         self._inner_dict['description'] = value
     
     
     @property
     def colorHex(self) -> Union[None, str]:
-        """The color associated with the Tag in Hex. For example #FFFFFF."""
+        """Getter: The color associated with the Tag in Hex. For example #FFFFFF."""
         return self._inner_dict.get('colorHex')  # type: ignore
     
     @colorHex.setter
     def colorHex(self, value: Union[None, str]) -> None:
+        """Setter: The color associated with the Tag in Hex. For example #FFFFFF."""
         self._inner_dict['colorHex'] = value
     
     
 class TelemetryClientIdClass(_Aspect):
     """A simple wrapper around a String to persist the client ID for telemetry in DataHub's backend DB"""
 
 
@@ -16472,25 +19215,33 @@
     def __init__(self,
         clientId: str,
     ):
         super().__init__()
         
         self.clientId = clientId
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TelemetryClientIdClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.clientId = str()
     
     
     @property
     def clientId(self) -> str:
-        """A string representing the telemetry client ID"""
+        """Getter: A string representing the telemetry client ID"""
         return self._inner_dict.get('clientId')  # type: ignore
     
     @clientId.setter
     def clientId(self, value: str) -> None:
+        """Setter: A string representing the telemetry client ID"""
         self._inner_dict['clientId'] = value
     
     
 class TestDefinitionClass(DictWrapper):
     # No docs available.
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.test.TestDefinition")
@@ -16499,36 +19250,45 @@
         json: Union[None, str]=None,
     ):
         super().__init__()
         
         self.type = type
         self.json = json
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TestDefinitionClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = TestDefinitionTypeClass.JSON
         self.json = self.RECORD_SCHEMA.fields_dict["json"].default
     
     
     @property
     def type(self) -> Union[str, "TestDefinitionTypeClass"]:
-        """The Test Definition Type"""
+        """Getter: The Test Definition Type"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "TestDefinitionTypeClass"]) -> None:
+        """Setter: The Test Definition Type"""
         self._inner_dict['type'] = value
     
     
     @property
     def json(self) -> Union[None, str]:
-        """JSON format configuration for the test"""
+        """Getter: JSON format configuration for the test"""
         return self._inner_dict.get('json')  # type: ignore
     
     @json.setter
     def json(self, value: Union[None, str]) -> None:
+        """Setter: JSON format configuration for the test"""
         self._inner_dict['json'] = value
     
     
 class TestDefinitionTypeClass(object):
     # No docs available.
     
     
@@ -16553,58 +19313,69 @@
         super().__init__()
         
         self.name = name
         self.category = category
         self.description = description
         self.definition = definition
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TestInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.category = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
-        self.definition = TestDefinitionClass._construct_with_defaults()
+        self.definition = TestDefinitionClass.construct_with_defaults()
     
     
     @property
     def name(self) -> str:
-        """The name of the test"""
+        """Getter: The name of the test"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: The name of the test"""
         self._inner_dict['name'] = value
     
     
     @property
     def category(self) -> str:
-        """Category of the test"""
+        """Getter: Category of the test"""
         return self._inner_dict.get('category')  # type: ignore
     
     @category.setter
     def category(self, value: str) -> None:
+        """Setter: Category of the test"""
         self._inner_dict['category'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Description of the test"""
+        """Getter: Description of the test"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Description of the test"""
         self._inner_dict['description'] = value
     
     
     @property
     def definition(self) -> "TestDefinitionClass":
-        """Configuration for the Test"""
+        """Getter: Configuration for the Test"""
         return self._inner_dict.get('definition')  # type: ignore
     
     @definition.setter
     def definition(self, value: "TestDefinitionClass") -> None:
+        """Setter: Configuration for the Test"""
         self._inner_dict['definition'] = value
     
     
 class TestResultClass(DictWrapper):
     """Information about a Test Result"""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.test.TestResult")
@@ -16613,36 +19384,45 @@
         type: Union[str, "TestResultTypeClass"],
     ):
         super().__init__()
         
         self.test = test
         self.type = type
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TestResultClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.test = str()
         self.type = TestResultTypeClass.SUCCESS
     
     
     @property
     def test(self) -> str:
-        """The urn of the test"""
+        """Getter: The urn of the test"""
         return self._inner_dict.get('test')  # type: ignore
     
     @test.setter
     def test(self, value: str) -> None:
+        """Setter: The urn of the test"""
         self._inner_dict['test'] = value
     
     
     @property
     def type(self) -> Union[str, "TestResultTypeClass"]:
-        """The type of the result"""
+        """Getter: The type of the result"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "TestResultTypeClass"]) -> None:
+        """Setter: The type of the result"""
         self._inner_dict['type'] = value
     
     
 class TestResultTypeClass(object):
     # No docs available.
     
     
@@ -16666,36 +19446,45 @@
         passing: List["TestResultClass"],
     ):
         super().__init__()
         
         self.failing = failing
         self.passing = passing
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TestResultsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.failing = list()
         self.passing = list()
     
     
     @property
     def failing(self) -> List["TestResultClass"]:
-        """Results that are failing"""
+        """Getter: Results that are failing"""
         return self._inner_dict.get('failing')  # type: ignore
     
     @failing.setter
     def failing(self, value: List["TestResultClass"]) -> None:
+        """Setter: Results that are failing"""
         self._inner_dict['failing'] = value
     
     
     @property
     def passing(self) -> List["TestResultClass"]:
-        """Results that are passing"""
+        """Getter: Results that are passing"""
         return self._inner_dict.get('passing')  # type: ignore
     
     @passing.setter
     def passing(self, value: List["TestResultClass"]) -> None:
+        """Setter: Results that are passing"""
         self._inner_dict['passing'] = value
     
     
 class CalendarIntervalClass(object):
     # No docs available.
     
     SECOND = "SECOND"
@@ -16723,47 +19512,57 @@
             # default: 'PARTITION'
             self.type = self.RECORD_SCHEMA.fields_dict["type"].default
         else:
             self.type = type
         self.partition = partition
         self.timePartition = timePartition
     
+    @classmethod
+    def construct_with_defaults(cls) -> "PartitionSpecClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.type = self.RECORD_SCHEMA.fields_dict["type"].default
         self.partition = str()
         self.timePartition = self.RECORD_SCHEMA.fields_dict["timePartition"].default
     
     
     @property
     def type(self) -> Union[str, "PartitionTypeClass"]:
         # No docs available.
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "PartitionTypeClass"]) -> None:
+        # No docs available.
         self._inner_dict['type'] = value
     
     
     @property
     def partition(self) -> str:
-        """String representation of the partition"""
+        """Getter: String representation of the partition"""
         return self._inner_dict.get('partition')  # type: ignore
     
     @partition.setter
     def partition(self, value: str) -> None:
+        """Setter: String representation of the partition"""
         self._inner_dict['partition'] = value
     
     
     @property
     def timePartition(self) -> Union[None, "TimeWindowClass"]:
-        """Time window of the partition if applicable"""
+        """Getter: Time window of the partition if applicable"""
         return self._inner_dict.get('timePartition')  # type: ignore
     
     @timePartition.setter
     def timePartition(self, value: Union[None, "TimeWindowClass"]) -> None:
+        """Setter: Time window of the partition if applicable"""
         self._inner_dict['timePartition'] = value
     
     
 class PartitionTypeClass(object):
     # No docs available.
     
     FULL_TABLE = "FULL_TABLE"
@@ -16780,36 +19579,45 @@
         length: "TimeWindowSizeClass",
     ):
         super().__init__()
         
         self.startTimeMillis = startTimeMillis
         self.length = length
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TimeWindowClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.startTimeMillis = int()
-        self.length = TimeWindowSizeClass._construct_with_defaults()
+        self.length = TimeWindowSizeClass.construct_with_defaults()
     
     
     @property
     def startTimeMillis(self) -> int:
-        """Start time as epoch at UTC."""
+        """Getter: Start time as epoch at UTC."""
         return self._inner_dict.get('startTimeMillis')  # type: ignore
     
     @startTimeMillis.setter
     def startTimeMillis(self, value: int) -> None:
+        """Setter: Start time as epoch at UTC."""
         self._inner_dict['startTimeMillis'] = value
     
     
     @property
     def length(self) -> "TimeWindowSizeClass":
-        """The length of the window."""
+        """Getter: The length of the window."""
         return self._inner_dict.get('length')  # type: ignore
     
     @length.setter
     def length(self, value: "TimeWindowSizeClass") -> None:
+        """Setter: The length of the window."""
         self._inner_dict['length'] = value
     
     
 class TimeWindowSizeClass(DictWrapper):
     """Defines the size of a time window."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.timeseries.TimeWindowSize")
@@ -16822,36 +19630,45 @@
         self.unit = unit
         if multiple is None:
             # default: 1
             self.multiple = self.RECORD_SCHEMA.fields_dict["multiple"].default
         else:
             self.multiple = multiple
     
+    @classmethod
+    def construct_with_defaults(cls) -> "TimeWindowSizeClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.unit = CalendarIntervalClass.SECOND
         self.multiple = self.RECORD_SCHEMA.fields_dict["multiple"].default
     
     
     @property
     def unit(self) -> Union[str, "CalendarIntervalClass"]:
-        """Interval unit such as minute/hour/day etc."""
+        """Getter: Interval unit such as minute/hour/day etc."""
         return self._inner_dict.get('unit')  # type: ignore
     
     @unit.setter
     def unit(self, value: Union[str, "CalendarIntervalClass"]) -> None:
+        """Setter: Interval unit such as minute/hour/day etc."""
         self._inner_dict['unit'] = value
     
     
     @property
     def multiple(self) -> int:
-        """How many units. Defaults to 1."""
+        """Getter: How many units. Defaults to 1."""
         return self._inner_dict.get('multiple')  # type: ignore
     
     @multiple.setter
     def multiple(self, value: int) -> None:
+        """Setter: How many units. Defaults to 1."""
         self._inner_dict['multiple'] = value
     
     
 class DataHubUpgradeRequestClass(_Aspect):
     """Information collected when kicking off a DataHubUpgrade"""
 
 
@@ -16864,36 +19681,45 @@
         version: str,
     ):
         super().__init__()
         
         self.timestampMs = timestampMs
         self.version = version
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubUpgradeRequestClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMs = int()
         self.version = str()
     
     
     @property
     def timestampMs(self) -> int:
-        """Timestamp when we started this DataHubUpgrade"""
+        """Getter: Timestamp when we started this DataHubUpgrade"""
         return self._inner_dict.get('timestampMs')  # type: ignore
     
     @timestampMs.setter
     def timestampMs(self, value: int) -> None:
+        """Setter: Timestamp when we started this DataHubUpgrade"""
         self._inner_dict['timestampMs'] = value
     
     
     @property
     def version(self) -> str:
-        """Version of this upgrade"""
+        """Getter: Version of this upgrade"""
         return self._inner_dict.get('version')  # type: ignore
     
     @version.setter
     def version(self, value: str) -> None:
+        """Setter: Version of this upgrade"""
         self._inner_dict['version'] = value
     
     
 class DataHubUpgradeResultClass(_Aspect):
     """Information collected when a DataHubUpgrade successfully finishes"""
 
 
@@ -16906,36 +19732,45 @@
         result: Union[None, Dict[str, str]]=None,
     ):
         super().__init__()
         
         self.timestampMs = timestampMs
         self.result = result
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubUpgradeResultClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.timestampMs = int()
         self.result = self.RECORD_SCHEMA.fields_dict["result"].default
     
     
     @property
     def timestampMs(self) -> int:
-        """Timestamp when we started this DataHubUpgrade"""
+        """Getter: Timestamp when we started this DataHubUpgrade"""
         return self._inner_dict.get('timestampMs')  # type: ignore
     
     @timestampMs.setter
     def timestampMs(self, value: int) -> None:
+        """Setter: Timestamp when we started this DataHubUpgrade"""
         self._inner_dict['timestampMs'] = value
     
     
     @property
     def result(self) -> Union[None, Dict[str, str]]:
-        """Result map to place helpful information about this upgrade job"""
+        """Getter: Result map to place helpful information about this upgrade job"""
         return self._inner_dict.get('result')  # type: ignore
     
     @result.setter
     def result(self, value: Union[None, Dict[str, str]]) -> None:
+        """Setter: Result map to place helpful information about this upgrade job"""
         self._inner_dict['result'] = value
     
     
 class FieldUsageCountsClass(DictWrapper):
     """ Records field-level usage counts for a given resource """
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.usage.FieldUsageCounts")
@@ -16944,36 +19779,45 @@
         count: int,
     ):
         super().__init__()
         
         self.fieldName = fieldName
         self.count = count
     
+    @classmethod
+    def construct_with_defaults(cls) -> "FieldUsageCountsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.fieldName = str()
         self.count = int()
     
     
     @property
     def fieldName(self) -> str:
         # No docs available.
         return self._inner_dict.get('fieldName')  # type: ignore
     
     @fieldName.setter
     def fieldName(self, value: str) -> None:
+        # No docs available.
         self._inner_dict['fieldName'] = value
     
     
     @property
     def count(self) -> int:
         # No docs available.
         return self._inner_dict.get('count')  # type: ignore
     
     @count.setter
     def count(self, value: int) -> None:
+        # No docs available.
         self._inner_dict['count'] = value
     
     
 class UsageAggregationClass(DictWrapper):
     """Usage data for a given resource, rolled up into a bucket."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.usage.UsageAggregation")
@@ -16986,58 +19830,69 @@
         super().__init__()
         
         self.bucket = bucket
         self.duration = duration
         self.resource = resource
         self.metrics = metrics
     
+    @classmethod
+    def construct_with_defaults(cls) -> "UsageAggregationClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.bucket = int()
         self.duration = WindowDurationClass.YEAR
         self.resource = str()
-        self.metrics = UsageAggregationMetricsClass._construct_with_defaults()
+        self.metrics = UsageAggregationMetricsClass.construct_with_defaults()
     
     
     @property
     def bucket(self) -> int:
-        """ Bucket start time in milliseconds """
+        """Getter:  Bucket start time in milliseconds """
         return self._inner_dict.get('bucket')  # type: ignore
     
     @bucket.setter
     def bucket(self, value: int) -> None:
+        """Setter:  Bucket start time in milliseconds """
         self._inner_dict['bucket'] = value
     
     
     @property
     def duration(self) -> Union[str, "WindowDurationClass"]:
-        """ Bucket duration """
+        """Getter:  Bucket duration """
         return self._inner_dict.get('duration')  # type: ignore
     
     @duration.setter
     def duration(self, value: Union[str, "WindowDurationClass"]) -> None:
+        """Setter:  Bucket duration """
         self._inner_dict['duration'] = value
     
     
     @property
     def resource(self) -> str:
-        """ Resource associated with these usage stats """
+        """Getter:  Resource associated with these usage stats """
         return self._inner_dict.get('resource')  # type: ignore
     
     @resource.setter
     def resource(self, value: str) -> None:
+        """Setter:  Resource associated with these usage stats """
         self._inner_dict['resource'] = value
     
     
     @property
     def metrics(self) -> "UsageAggregationMetricsClass":
-        """ Metrics associated with this bucket """
+        """Getter:  Metrics associated with this bucket """
         return self._inner_dict.get('metrics')  # type: ignore
     
     @metrics.setter
     def metrics(self, value: "UsageAggregationMetricsClass") -> None:
+        """Setter:  Metrics associated with this bucket """
         self._inner_dict['metrics'] = value
     
     
 class UsageAggregationMetricsClass(DictWrapper):
     """Metrics for usage data for a given resource and bucket. Not all fields
     make sense for all buckets, so every field is optional."""
     
@@ -17053,69 +19908,81 @@
         
         self.uniqueUserCount = uniqueUserCount
         self.users = users
         self.totalSqlQueries = totalSqlQueries
         self.topSqlQueries = topSqlQueries
         self.fields = fields
     
+    @classmethod
+    def construct_with_defaults(cls) -> "UsageAggregationMetricsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.uniqueUserCount = self.RECORD_SCHEMA.fields_dict["uniqueUserCount"].default
         self.users = self.RECORD_SCHEMA.fields_dict["users"].default
         self.totalSqlQueries = self.RECORD_SCHEMA.fields_dict["totalSqlQueries"].default
         self.topSqlQueries = self.RECORD_SCHEMA.fields_dict["topSqlQueries"].default
         self.fields = self.RECORD_SCHEMA.fields_dict["fields"].default
     
     
     @property
     def uniqueUserCount(self) -> Union[None, int]:
-        """ Unique user count """
+        """Getter:  Unique user count """
         return self._inner_dict.get('uniqueUserCount')  # type: ignore
     
     @uniqueUserCount.setter
     def uniqueUserCount(self, value: Union[None, int]) -> None:
+        """Setter:  Unique user count """
         self._inner_dict['uniqueUserCount'] = value
     
     
     @property
     def users(self) -> Union[None, List["UserUsageCountsClass"]]:
-        """ Users within this bucket, with frequency counts """
+        """Getter:  Users within this bucket, with frequency counts """
         return self._inner_dict.get('users')  # type: ignore
     
     @users.setter
     def users(self, value: Union[None, List["UserUsageCountsClass"]]) -> None:
+        """Setter:  Users within this bucket, with frequency counts """
         self._inner_dict['users'] = value
     
     
     @property
     def totalSqlQueries(self) -> Union[None, int]:
-        """ Total SQL query count """
+        """Getter:  Total SQL query count """
         return self._inner_dict.get('totalSqlQueries')  # type: ignore
     
     @totalSqlQueries.setter
     def totalSqlQueries(self, value: Union[None, int]) -> None:
+        """Setter:  Total SQL query count """
         self._inner_dict['totalSqlQueries'] = value
     
     
     @property
     def topSqlQueries(self) -> Union[None, List[str]]:
-        """ Frequent SQL queries; mostly makes sense for datasets in SQL databases """
+        """Getter:  Frequent SQL queries; mostly makes sense for datasets in SQL databases """
         return self._inner_dict.get('topSqlQueries')  # type: ignore
     
     @topSqlQueries.setter
     def topSqlQueries(self, value: Union[None, List[str]]) -> None:
+        """Setter:  Frequent SQL queries; mostly makes sense for datasets in SQL databases """
         self._inner_dict['topSqlQueries'] = value
     
     
     @property
     def fields(self) -> Union[None, List["FieldUsageCountsClass"]]:
-        """ Field-level usage stats """
+        """Getter:  Field-level usage stats """
         return self._inner_dict.get('fields')  # type: ignore
     
     @fields.setter
     def fields(self, value: Union[None, List["FieldUsageCountsClass"]]) -> None:
+        """Setter:  Field-level usage stats """
         self._inner_dict['fields'] = value
     
     
 class UserUsageCountsClass(DictWrapper):
     """ Records a single user's usage counts for a given resource """
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.usage.UserUsageCounts")
@@ -17126,47 +19993,57 @@
     ):
         super().__init__()
         
         self.user = user
         self.count = count
         self.userEmail = userEmail
     
+    @classmethod
+    def construct_with_defaults(cls) -> "UserUsageCountsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.user = self.RECORD_SCHEMA.fields_dict["user"].default
         self.count = int()
         self.userEmail = self.RECORD_SCHEMA.fields_dict["userEmail"].default
     
     
     @property
     def user(self) -> Union[None, str]:
         # No docs available.
         return self._inner_dict.get('user')  # type: ignore
     
     @user.setter
     def user(self, value: Union[None, str]) -> None:
+        # No docs available.
         self._inner_dict['user'] = value
     
     
     @property
     def count(self) -> int:
         # No docs available.
         return self._inner_dict.get('count')  # type: ignore
     
     @count.setter
     def count(self, value: int) -> None:
+        # No docs available.
         self._inner_dict['count'] = value
     
     
     @property
     def userEmail(self) -> Union[None, str]:
-        """ If user_email is set, we attempt to resolve the user's urn upon ingest """
+        """Getter:  If user_email is set, we attempt to resolve the user's urn upon ingest """
         return self._inner_dict.get('userEmail')  # type: ignore
     
     @userEmail.setter
     def userEmail(self, value: Union[None, str]) -> None:
+        """Setter:  If user_email is set, we attempt to resolve the user's urn upon ingest """
         self._inner_dict['userEmail'] = value
     
     
 class DataHubViewDefinitionClass(DictWrapper):
     """A View definition."""
     
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.view.DataHubViewDefinition")
@@ -17175,36 +20052,45 @@
         filter: "FilterClass",
     ):
         super().__init__()
         
         self.entityTypes = entityTypes
         self.filter = filter
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubViewDefinitionClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.entityTypes = list()
-        self.filter = FilterClass._construct_with_defaults()
+        self.filter = FilterClass.construct_with_defaults()
     
     
     @property
     def entityTypes(self) -> List[str]:
-        """The Entity Types in the scope of the View."""
+        """Getter: The Entity Types in the scope of the View."""
         return self._inner_dict.get('entityTypes')  # type: ignore
     
     @entityTypes.setter
     def entityTypes(self, value: List[str]) -> None:
+        """Setter: The Entity Types in the scope of the View."""
         self._inner_dict['entityTypes'] = value
     
     
     @property
     def filter(self) -> "FilterClass":
-        """The filter criteria, which represents the view itself"""
+        """Getter: The filter criteria, which represents the view itself"""
         return self._inner_dict.get('filter')  # type: ignore
     
     @filter.setter
     def filter(self, value: "FilterClass") -> None:
+        """Setter: The filter criteria, which represents the view itself"""
         self._inner_dict['filter'] = value
     
     
 class DataHubViewInfoClass(_Aspect):
     """Information about a DataHub View. -- TODO: Understand whether an entity type filter is required."""
 
 
@@ -17225,80 +20111,93 @@
         self.name = name
         self.description = description
         self.type = type
         self.definition = definition
         self.created = created
         self.lastModified = lastModified
     
+    @classmethod
+    def construct_with_defaults(cls) -> "DataHubViewInfoClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
     def _restore_defaults(self) -> None:
         self.name = str()
         self.description = self.RECORD_SCHEMA.fields_dict["description"].default
         self.type = DataHubViewTypeClass.PERSONAL
-        self.definition = DataHubViewDefinitionClass._construct_with_defaults()
-        self.created = AuditStampClass._construct_with_defaults()
-        self.lastModified = AuditStampClass._construct_with_defaults()
+        self.definition = DataHubViewDefinitionClass.construct_with_defaults()
+        self.created = AuditStampClass.construct_with_defaults()
+        self.lastModified = AuditStampClass.construct_with_defaults()
     
     
     @property
     def name(self) -> str:
-        """The name of the View"""
+        """Getter: The name of the View"""
         return self._inner_dict.get('name')  # type: ignore
     
     @name.setter
     def name(self, value: str) -> None:
+        """Setter: The name of the View"""
         self._inner_dict['name'] = value
     
     
     @property
     def description(self) -> Union[None, str]:
-        """Description of the view"""
+        """Getter: Description of the view"""
         return self._inner_dict.get('description')  # type: ignore
     
     @description.setter
     def description(self, value: Union[None, str]) -> None:
+        """Setter: Description of the view"""
         self._inner_dict['description'] = value
     
     
     @property
     def type(self) -> Union[str, "DataHubViewTypeClass"]:
-        """The type of View"""
+        """Getter: The type of View"""
         return self._inner_dict.get('type')  # type: ignore
     
     @type.setter
     def type(self, value: Union[str, "DataHubViewTypeClass"]) -> None:
+        """Setter: The type of View"""
         self._inner_dict['type'] = value
     
     
     @property
     def definition(self) -> "DataHubViewDefinitionClass":
-        """The view itself"""
+        """Getter: The view itself"""
         return self._inner_dict.get('definition')  # type: ignore
     
     @definition.setter
     def definition(self, value: "DataHubViewDefinitionClass") -> None:
+        """Setter: The view itself"""
         self._inner_dict['definition'] = value
     
     
     @property
     def created(self) -> "AuditStampClass":
-        """Audit stamp capturing the time and actor who created the View."""
+        """Getter: Audit stamp capturing the time and actor who created the View."""
         return self._inner_dict.get('created')  # type: ignore
     
     @created.setter
     def created(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp capturing the time and actor who created the View."""
         self._inner_dict['created'] = value
     
     
     @property
     def lastModified(self) -> "AuditStampClass":
-        """Audit stamp capturing the time and actor who last modified the View."""
+        """Getter: Audit stamp capturing the time and actor who last modified the View."""
         return self._inner_dict.get('lastModified')  # type: ignore
     
     @lastModified.setter
     def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp capturing the time and actor who last modified the View."""
         self._inner_dict['lastModified'] = value
     
     
 class DataHubViewTypeClass(object):
     # No docs available.
     
     
@@ -17331,17 +20230,15 @@
     'com.linkedin.pegasus2avro.chart.ChartQueryType': ChartQueryTypeClass,
     'com.linkedin.pegasus2avro.chart.ChartType': ChartTypeClass,
     'com.linkedin.pegasus2avro.chart.ChartUsageStatistics': ChartUsageStatisticsClass,
     'com.linkedin.pegasus2avro.chart.ChartUserUsageCounts': ChartUserUsageCountsClass,
     'com.linkedin.pegasus2avro.chart.EditableChartProperties': EditableChartPropertiesClass,
     'com.linkedin.pegasus2avro.common.AccessLevel': AccessLevelClass,
     'com.linkedin.pegasus2avro.common.AuditStamp': AuditStampClass,
-    'com.linkedin.pegasus2avro.common.BrowsePathEntry': BrowsePathEntryClass,
     'com.linkedin.pegasus2avro.common.BrowsePaths': BrowsePathsClass,
-    'com.linkedin.pegasus2avro.common.BrowsePathsV2': BrowsePathsV2Class,
     'com.linkedin.pegasus2avro.common.ChangeAuditStamps': ChangeAuditStampsClass,
     'com.linkedin.pegasus2avro.common.Cost': CostClass,
     'com.linkedin.pegasus2avro.common.CostCost': CostCostClass,
     'com.linkedin.pegasus2avro.common.CostCostDiscriminator': CostCostDiscriminatorClass,
     'com.linkedin.pegasus2avro.common.CostType': CostTypeClass,
     'com.linkedin.pegasus2avro.common.DataPlatformInstance': DataPlatformInstanceClass,
     'com.linkedin.pegasus2avro.common.Deprecation': DeprecationClass,
@@ -17668,17 +20565,15 @@
     'ChartQueryType': ChartQueryTypeClass,
     'ChartType': ChartTypeClass,
     'ChartUsageStatistics': ChartUsageStatisticsClass,
     'ChartUserUsageCounts': ChartUserUsageCountsClass,
     'EditableChartProperties': EditableChartPropertiesClass,
     'AccessLevel': AccessLevelClass,
     'AuditStamp': AuditStampClass,
-    'BrowsePathEntry': BrowsePathEntryClass,
     'BrowsePaths': BrowsePathsClass,
-    'BrowsePathsV2': BrowsePathsV2Class,
     'ChangeAuditStamps': ChangeAuditStampsClass,
     'Cost': CostClass,
     'CostCost': CostCostClass,
     'CostCostDiscriminator': CostCostDiscriminatorClass,
     'CostType': CostTypeClass,
     'DataPlatformInstance': DataPlatformInstanceClass,
     'Deprecation': DeprecationClass,
@@ -17988,378 +20883,208 @@
 
 _json_converter = avrojson.AvroJsonConverter(use_logical_types=False, schema_types=__SCHEMA_TYPES)
 
 
     
 
 ASPECT_CLASSES: List[Type[_Aspect]] = [
-    GlobalSettingsInfoClass,
-    DataHubAccessTokenInfoClass,
-    MLHyperParamClass,
+    EditableDataJobPropertiesClass,
+    DataFlowInfoClass,
+    EditableDataFlowPropertiesClass,
+    DataJobInputOutputClass,
+    DataJobInfoClass,
+    VersionInfoClass,
+    DatahubIngestionRunSummaryClass,
+    DatahubIngestionCheckpointClass,
+    EmbedClass,
+    InputFieldsClass,
+    CostClass,
+    SubTypesClass,
+    GlobalTagsClass,
+    SiblingsClass,
+    OperationClass,
+    BrowsePathsClass,
+    GlossaryTermsClass,
+    OriginClass,
+    DataPlatformInstanceClass,
+    InstitutionalMemoryClass,
+    DeprecationClass,
+    OwnershipClass,
+    StatusClass,
+    SourceCodeClass,
+    EditableMLModelPropertiesClass,
+    EditableMLPrimaryKeyPropertiesClass,
     EvaluationDataClass,
+    EditableMLFeatureTablePropertiesClass,
     MLPrimaryKeyPropertiesClass,
+    MLHyperParamClass,
+    MLModelDeploymentPropertiesClass,
+    CaveatsAndRecommendationsClass,
     MLMetricClass,
-    EditableMLFeaturePropertiesClass,
-    MetricsClass,
-    EditableMLFeatureTablePropertiesClass,
-    SourceCodeClass,
-    IntendedUseClass,
-    EditableMLPrimaryKeyPropertiesClass,
-    MLModelPropertiesClass,
     TrainingDataClass,
-    QuantitativeAnalysesClass,
-    MLModelDeploymentPropertiesClass,
-    EthicalConsiderationsClass,
-    MLModelFactorPromptsClass,
     MLModelGroupPropertiesClass,
-    CaveatsAndRecommendationsClass,
-    EditableMLModelPropertiesClass,
-    EditableMLModelGroupPropertiesClass,
+    MetricsClass,
+    MLModelFactorPromptsClass,
+    EthicalConsiderationsClass,
     MLFeaturePropertiesClass,
+    MLModelPropertiesClass,
+    EditableMLFeaturePropertiesClass,
+    EditableMLModelGroupPropertiesClass,
     MLFeatureTablePropertiesClass,
-    DataHubViewInfoClass,
-    DataProcessInstanceOutputClass,
-    DataProcessInstanceRunEventClass,
-    DataProcessInstanceRelationshipsClass,
-    DataProcessInstancePropertiesClass,
-    DataProcessInfoClass,
-    DataProcessInstanceInputClass,
-    ChartInfoClass,
-    EditableChartPropertiesClass,
-    ChartQueryClass,
-    ChartUsageStatisticsClass,
-    NotebookInfoClass,
-    NotebookContentClass,
-    EditableNotebookPropertiesClass,
-    RoleMembershipClass,
-    InviteTokenClass,
-    GroupMembershipClass,
-    NativeGroupMembershipClass,
-    CorpGroupEditableInfoClass,
-    CorpUserEditableInfoClass,
-    CorpUserInfoClass,
-    CorpUserCredentialsClass,
-    CorpUserSettingsClass,
-    CorpUserStatusClass,
-    CorpGroupInfoClass,
-    DataPlatformInstancePropertiesClass,
-    AssertionRunEventClass,
-    AssertionInfoClass,
+    QuantitativeAnalysesClass,
+    IntendedUseClass,
+    DataHubPolicyInfoClass,
+    DataHubRoleInfoClass,
     DataHubStepStatePropertiesClass,
-    DataHubSecretValueClass,
-    TagPropertiesClass,
+    DataPlatformInfoClass,
     ExecutionRequestResultClass,
     ExecutionRequestInputClass,
     ExecutionRequestSignalClass,
-    DashboardInfoClass,
-    DashboardUsageStatisticsClass,
-    EditableDashboardPropertiesClass,
-    EditableDataJobPropertiesClass,
-    DataJobInputOutputClass,
-    EditableDataFlowPropertiesClass,
-    VersionInfoClass,
-    DataFlowInfoClass,
-    DataJobInfoClass,
-    DatahubIngestionRunSummaryClass,
-    DatahubIngestionCheckpointClass,
-    DataHubRetentionConfigClass,
-    EditableSchemaMetadataClass,
-    SchemaMetadataClass,
-    DataPlatformInfoClass,
-    TelemetryClientIdClass,
-    DataHubIngestionSourceInfoClass,
+    ChartQueryClass,
+    EditableChartPropertiesClass,
+    ChartUsageStatisticsClass,
+    ChartInfoClass,
+    EditableContainerPropertiesClass,
+    ContainerPropertiesClass,
+    ContainerClass,
+    GlossaryNodeInfoClass,
     GlossaryTermInfoClass,
     GlossaryRelatedTermsClass,
-    GlossaryNodeInfoClass,
-    DomainsClass,
+    TagPropertiesClass,
+    QuerySubjectsClass,
+    QueryPropertiesClass,
+    TelemetryClientIdClass,
     DomainPropertiesClass,
-    PostInfoClass,
+    DomainsClass,
+    EditableSchemaMetadataClass,
+    SchemaMetadataClass,
+    DataHubIngestionSourceInfoClass,
+    ViewPropertiesClass,
     DatasetDeprecationClass,
-    EditableDatasetPropertiesClass,
+    DatasetUpstreamLineageClass,
     DatasetUsageStatisticsClass,
+    DatasetPropertiesClass,
     UpstreamLineageClass,
     DatasetProfileClass,
-    DatasetPropertiesClass,
-    DatasetUpstreamLineageClass,
-    ViewPropertiesClass,
-    EditableContainerPropertiesClass,
-    ContainerClass,
-    ContainerPropertiesClass,
-    TestInfoClass,
-    TestResultsClass,
-    QuerySubjectsClass,
-    QueryPropertiesClass,
-    DataHubPolicyInfoClass,
-    DataHubRoleInfoClass,
-    DashboardKeyClass,
-    GlobalSettingsKeyClass,
-    DataHubSecretKeyClass,
+    EditableDatasetPropertiesClass,
+    EditableDashboardPropertiesClass,
+    DashboardUsageStatisticsClass,
+    DashboardInfoClass,
+    DataHubRetentionConfigClass,
+    CorpUserStatusClass,
+    CorpUserCredentialsClass,
+    CorpUserEditableInfoClass,
+    InviteTokenClass,
+    CorpUserInfoClass,
+    RoleMembershipClass,
+    GroupMembershipClass,
+    CorpUserSettingsClass,
+    CorpGroupEditableInfoClass,
+    NativeGroupMembershipClass,
+    CorpGroupInfoClass,
+    DataHubAccessTokenInfoClass,
+    DataProcessInstanceRunEventClass,
+    DataProcessInstanceInputClass,
+    DataProcessInstancePropertiesClass,
+    DataProcessInstanceOutputClass,
+    DataProcessInfoClass,
+    DataProcessInstanceRelationshipsClass,
+    AssertionInfoClass,
+    AssertionRunEventClass,
+    NotebookContentClass,
+    NotebookInfoClass,
+    EditableNotebookPropertiesClass,
+    PostInfoClass,
+    DataHubUpgradeResultClass,
+    DataHubUpgradeRequestClass,
     MLFeatureTableKeyClass,
-    TelemetryKeyClass,
-    DataHubAccessTokenKeyClass,
-    DataHubPolicyKeyClass,
-    TestKeyClass,
-    DataHubIngestionSourceKeyClass,
-    AssertionKeyClass,
-    PostKeyClass,
-    MLFeatureKeyClass,
-    TagKeyClass,
-    DomainKeyClass,
-    DataHubViewKeyClass,
-    MLPrimaryKeyKeyClass,
-    DataHubStepStateKeyClass,
-    DataHubUpgradeKeyClass,
-    GlossaryNodeKeyClass,
-    MLModelDeploymentKeyClass,
-    DataHubRoleKeyClass,
-    DataPlatformKeyClass,
     ContainerKeyClass,
-    DataPlatformInstanceKeyClass,
+    DataHubRetentionKeyClass,
     DataJobKeyClass,
-    ExecutionRequestKeyClass,
-    DataProcessKeyClass,
-    CorpUserKeyClass,
-    NotebookKeyClass,
+    MLModelDeploymentKeyClass,
     SchemaFieldKeyClass,
+    DataProcessKeyClass,
+    CorpGroupKeyClass,
+    MLPrimaryKeyKeyClass,
+    DataFlowKeyClass,
+    DashboardKeyClass,
     MLModelGroupKeyClass,
+    TelemetryKeyClass,
+    GlossaryTermKeyClass,
+    GlossaryNodeKeyClass,
     DataProcessInstanceKeyClass,
-    QueryKeyClass,
+    DataHubRoleKeyClass,
     MLModelKeyClass,
-    GlossaryTermKeyClass,
-    DatasetKeyClass,
-    InviteTokenKeyClass,
-    CorpGroupKeyClass,
-    DataHubRetentionKeyClass,
-    DataFlowKeyClass,
+    DataHubViewKeyClass,
+    QueryKeyClass,
+    CorpUserKeyClass,
+    DataPlatformInstanceKeyClass,
+    NotebookKeyClass,
+    MLFeatureKeyClass,
     ChartKeyClass,
-    GlossaryTermsClass,
-    EmbedClass,
-    DataPlatformInstanceClass,
-    InputFieldsClass,
-    InstitutionalMemoryClass,
-    OwnershipClass,
-    SubTypesClass,
-    OriginClass,
-    BrowsePathsV2Class,
-    GlobalTagsClass,
-    StatusClass,
-    OperationClass,
-    SiblingsClass,
-    BrowsePathsClass,
-    DeprecationClass,
-    CostClass,
-    DataHubUpgradeResultClass,
-    DataHubUpgradeRequestClass
+    AssertionKeyClass,
+    DataHubSecretKeyClass,
+    DataHubPolicyKeyClass,
+    TestKeyClass,
+    DataHubAccessTokenKeyClass,
+    DataHubUpgradeKeyClass,
+    DataPlatformKeyClass,
+    InviteTokenKeyClass,
+    PostKeyClass,
+    ExecutionRequestKeyClass,
+    DataHubIngestionSourceKeyClass,
+    DomainKeyClass,
+    DatasetKeyClass,
+    DataHubStepStateKeyClass,
+    GlobalSettingsKeyClass,
+    TagKeyClass,
+    GlobalSettingsInfoClass,
+    TestResultsClass,
+    TestInfoClass,
+    DataPlatformInstancePropertiesClass,
+    DataHubViewInfoClass,
+    DataHubSecretValueClass
 ]
 
-ASPECT_NAME_MAP: Dict[str, Type[_Aspect]] = {
-    aspect.get_aspect_name(): aspect
-    for aspect in ASPECT_CLASSES
-}
-
-from typing_extensions import TypedDict
-
-class AspectBag(TypedDict, total=False):
-    globalSettingsInfo: GlobalSettingsInfoClass
-    dataHubAccessTokenInfo: DataHubAccessTokenInfoClass
-    mlHyperParam: MLHyperParamClass
-    mlModelEvaluationData: EvaluationDataClass
-    mlPrimaryKeyProperties: MLPrimaryKeyPropertiesClass
-    mlMetric: MLMetricClass
-    editableMlFeatureProperties: EditableMLFeaturePropertiesClass
-    mlModelMetrics: MetricsClass
-    editableMlFeatureTableProperties: EditableMLFeatureTablePropertiesClass
-    sourceCode: SourceCodeClass
-    intendedUse: IntendedUseClass
-    editableMlPrimaryKeyProperties: EditableMLPrimaryKeyPropertiesClass
-    mlModelProperties: MLModelPropertiesClass
-    mlModelTrainingData: TrainingDataClass
-    mlModelQuantitativeAnalyses: QuantitativeAnalysesClass
-    mlModelDeploymentProperties: MLModelDeploymentPropertiesClass
-    mlModelEthicalConsiderations: EthicalConsiderationsClass
-    mlModelFactorPrompts: MLModelFactorPromptsClass
-    mlModelGroupProperties: MLModelGroupPropertiesClass
-    mlModelCaveatsAndRecommendations: CaveatsAndRecommendationsClass
-    editableMlModelProperties: EditableMLModelPropertiesClass
-    editableMlModelGroupProperties: EditableMLModelGroupPropertiesClass
-    mlFeatureProperties: MLFeaturePropertiesClass
-    mlFeatureTableProperties: MLFeatureTablePropertiesClass
-    dataHubViewInfo: DataHubViewInfoClass
-    dataProcessInstanceOutput: DataProcessInstanceOutputClass
-    dataProcessInstanceRunEvent: DataProcessInstanceRunEventClass
-    dataProcessInstanceRelationships: DataProcessInstanceRelationshipsClass
-    dataProcessInstanceProperties: DataProcessInstancePropertiesClass
-    dataProcessInfo: DataProcessInfoClass
-    dataProcessInstanceInput: DataProcessInstanceInputClass
-    chartInfo: ChartInfoClass
-    editableChartProperties: EditableChartPropertiesClass
-    chartQuery: ChartQueryClass
-    chartUsageStatistics: ChartUsageStatisticsClass
-    notebookInfo: NotebookInfoClass
-    notebookContent: NotebookContentClass
-    editableNotebookProperties: EditableNotebookPropertiesClass
-    roleMembership: RoleMembershipClass
-    inviteToken: InviteTokenClass
-    groupMembership: GroupMembershipClass
-    nativeGroupMembership: NativeGroupMembershipClass
-    corpGroupEditableInfo: CorpGroupEditableInfoClass
-    corpUserEditableInfo: CorpUserEditableInfoClass
-    corpUserInfo: CorpUserInfoClass
-    corpUserCredentials: CorpUserCredentialsClass
-    corpUserSettings: CorpUserSettingsClass
-    corpUserStatus: CorpUserStatusClass
-    corpGroupInfo: CorpGroupInfoClass
-    dataPlatformInstanceProperties: DataPlatformInstancePropertiesClass
-    assertionRunEvent: AssertionRunEventClass
-    assertionInfo: AssertionInfoClass
-    dataHubStepStateProperties: DataHubStepStatePropertiesClass
-    dataHubSecretValue: DataHubSecretValueClass
-    tagProperties: TagPropertiesClass
-    dataHubExecutionRequestResult: ExecutionRequestResultClass
-    dataHubExecutionRequestInput: ExecutionRequestInputClass
-    dataHubExecutionRequestSignal: ExecutionRequestSignalClass
-    dashboardInfo: DashboardInfoClass
-    dashboardUsageStatistics: DashboardUsageStatisticsClass
-    editableDashboardProperties: EditableDashboardPropertiesClass
-    editableDataJobProperties: EditableDataJobPropertiesClass
-    dataJobInputOutput: DataJobInputOutputClass
-    editableDataFlowProperties: EditableDataFlowPropertiesClass
-    versionInfo: VersionInfoClass
-    dataFlowInfo: DataFlowInfoClass
-    dataJobInfo: DataJobInfoClass
-    datahubIngestionRunSummary: DatahubIngestionRunSummaryClass
-    datahubIngestionCheckpoint: DatahubIngestionCheckpointClass
-    dataHubRetentionConfig: DataHubRetentionConfigClass
-    editableSchemaMetadata: EditableSchemaMetadataClass
-    schemaMetadata: SchemaMetadataClass
-    dataPlatformInfo: DataPlatformInfoClass
-    telemetryClientId: TelemetryClientIdClass
-    dataHubIngestionSourceInfo: DataHubIngestionSourceInfoClass
-    glossaryTermInfo: GlossaryTermInfoClass
-    glossaryRelatedTerms: GlossaryRelatedTermsClass
-    glossaryNodeInfo: GlossaryNodeInfoClass
-    domains: DomainsClass
-    domainProperties: DomainPropertiesClass
-    postInfo: PostInfoClass
-    datasetDeprecation: DatasetDeprecationClass
-    editableDatasetProperties: EditableDatasetPropertiesClass
-    datasetUsageStatistics: DatasetUsageStatisticsClass
-    upstreamLineage: UpstreamLineageClass
-    datasetProfile: DatasetProfileClass
-    datasetProperties: DatasetPropertiesClass
-    datasetUpstreamLineage: DatasetUpstreamLineageClass
-    viewProperties: ViewPropertiesClass
-    editableContainerProperties: EditableContainerPropertiesClass
-    container: ContainerClass
-    containerProperties: ContainerPropertiesClass
-    testInfo: TestInfoClass
-    testResults: TestResultsClass
-    querySubjects: QuerySubjectsClass
-    queryProperties: QueryPropertiesClass
-    dataHubPolicyInfo: DataHubPolicyInfoClass
-    dataHubRoleInfo: DataHubRoleInfoClass
-    dashboardKey: DashboardKeyClass
-    globalSettingsKey: GlobalSettingsKeyClass
-    dataHubSecretKey: DataHubSecretKeyClass
-    mlFeatureTableKey: MLFeatureTableKeyClass
-    telemetryKey: TelemetryKeyClass
-    dataHubAccessTokenKey: DataHubAccessTokenKeyClass
-    dataHubPolicyKey: DataHubPolicyKeyClass
-    testKey: TestKeyClass
-    dataHubIngestionSourceKey: DataHubIngestionSourceKeyClass
-    assertionKey: AssertionKeyClass
-    postKey: PostKeyClass
-    mlFeatureKey: MLFeatureKeyClass
-    tagKey: TagKeyClass
-    domainKey: DomainKeyClass
-    dataHubViewKey: DataHubViewKeyClass
-    mlPrimaryKeyKey: MLPrimaryKeyKeyClass
-    dataHubStepStateKey: DataHubStepStateKeyClass
-    dataHubUpgradeKey: DataHubUpgradeKeyClass
-    glossaryNodeKey: GlossaryNodeKeyClass
-    mlModelDeploymentKey: MLModelDeploymentKeyClass
-    dataHubRoleKey: DataHubRoleKeyClass
-    dataPlatformKey: DataPlatformKeyClass
-    containerKey: ContainerKeyClass
-    dataPlatformInstanceKey: DataPlatformInstanceKeyClass
-    dataJobKey: DataJobKeyClass
-    dataHubExecutionRequestKey: ExecutionRequestKeyClass
-    dataProcessKey: DataProcessKeyClass
-    corpUserKey: CorpUserKeyClass
-    notebookKey: NotebookKeyClass
-    schemaFieldKey: SchemaFieldKeyClass
-    mlModelGroupKey: MLModelGroupKeyClass
-    dataProcessInstanceKey: DataProcessInstanceKeyClass
-    queryKey: QueryKeyClass
-    mlModelKey: MLModelKeyClass
-    glossaryTermKey: GlossaryTermKeyClass
-    datasetKey: DatasetKeyClass
-    inviteTokenKey: InviteTokenKeyClass
-    corpGroupKey: CorpGroupKeyClass
-    dataHubRetentionKey: DataHubRetentionKeyClass
-    dataFlowKey: DataFlowKeyClass
-    chartKey: ChartKeyClass
-    glossaryTerms: GlossaryTermsClass
-    embed: EmbedClass
-    dataPlatformInstance: DataPlatformInstanceClass
-    inputFields: InputFieldsClass
-    institutionalMemory: InstitutionalMemoryClass
-    ownership: OwnershipClass
-    subTypes: SubTypesClass
-    origin: OriginClass
-    browsePathsV2: BrowsePathsV2Class
-    globalTags: GlobalTagsClass
-    status: StatusClass
-    operation: OperationClass
-    siblings: SiblingsClass
-    browsePaths: BrowsePathsClass
-    deprecation: DeprecationClass
-    cost: CostClass
-    dataHubUpgradeResult: DataHubUpgradeResultClass
-    dataHubUpgradeRequest: DataHubUpgradeRequestClass
-
-
 KEY_ASPECTS: Dict[str, Type[_Aspect]] = {
-    'dashboard': DashboardKeyClass,
-    'globalSettings': GlobalSettingsKeyClass,
-    'dataHubSecret': DataHubSecretKeyClass,
     'mlFeatureTable': MLFeatureTableKeyClass,
-    'telemetry': TelemetryKeyClass,
-    'dataHubAccessToken': DataHubAccessTokenKeyClass,
-    'dataHubPolicy': DataHubPolicyKeyClass,
-    'test': TestKeyClass,
-    'dataHubIngestionSource': DataHubIngestionSourceKeyClass,
-    'assertion': AssertionKeyClass,
-    'post': PostKeyClass,
-    'mlFeature': MLFeatureKeyClass,
-    'tag': TagKeyClass,
-    'domain': DomainKeyClass,
-    'dataHubView': DataHubViewKeyClass,
-    'mlPrimaryKey': MLPrimaryKeyKeyClass,
-    'dataHubStepState': DataHubStepStateKeyClass,
-    'dataHubUpgrade': DataHubUpgradeKeyClass,
-    'glossaryNode': GlossaryNodeKeyClass,
-    'mlModelDeployment': MLModelDeploymentKeyClass,
-    'dataHubRole': DataHubRoleKeyClass,
-    'dataPlatform': DataPlatformKeyClass,
     'container': ContainerKeyClass,
-    'dataPlatformInstance': DataPlatformInstanceKeyClass,
+    'dataHubRetention': DataHubRetentionKeyClass,
     'dataJob': DataJobKeyClass,
-    'dataHubExecutionRequest': ExecutionRequestKeyClass,
-    'dataProcess': DataProcessKeyClass,
-    'corpuser': CorpUserKeyClass,
-    'notebook': NotebookKeyClass,
     'schemaField': SchemaFieldKeyClass,
+    'corpGroup': CorpGroupKeyClass,
+    'mlPrimaryKey': MLPrimaryKeyKeyClass,
+    'dataFlow': DataFlowKeyClass,
+    'dashboard': DashboardKeyClass,
     'mlModelGroup': MLModelGroupKeyClass,
+    'telemetry': TelemetryKeyClass,
+    'glossaryTerm': GlossaryTermKeyClass,
+    'glossaryNode': GlossaryNodeKeyClass,
     'dataProcessInstance': DataProcessInstanceKeyClass,
-    'query': QueryKeyClass,
+    'dataHubRole': DataHubRoleKeyClass,
     'mlModel': MLModelKeyClass,
-    'glossaryTerm': GlossaryTermKeyClass,
-    'dataset': DatasetKeyClass,
+    'dataHubView': DataHubViewKeyClass,
+    'query': QueryKeyClass,
+    'corpuser': CorpUserKeyClass,
+    'dataPlatformInstance': DataPlatformInstanceKeyClass,
+    'notebook': NotebookKeyClass,
+    'mlFeature': MLFeatureKeyClass,
+    'chart': ChartKeyClass,
+    'assertion': AssertionKeyClass,
+    'dataHubSecret': DataHubSecretKeyClass,
+    'dataHubPolicy': DataHubPolicyKeyClass,
+    'test': TestKeyClass,
+    'dataHubAccessToken': DataHubAccessTokenKeyClass,
+    'dataHubUpgrade': DataHubUpgradeKeyClass,
+    'dataPlatform': DataPlatformKeyClass,
     'inviteToken': InviteTokenKeyClass,
-    'corpGroup': CorpGroupKeyClass,
-    'dataHubRetention': DataHubRetentionKeyClass,
-    'dataFlow': DataFlowKeyClass,
-    'chart': ChartKeyClass
+    'post': PostKeyClass,
+    'dataHubExecutionRequest': ExecutionRequestKeyClass,
+    'dataHubIngestionSource': DataHubIngestionSourceKeyClass,
+    'domain': DomainKeyClass,
+    'dataset': DatasetKeyClass,
+    'dataHubStepState': DataHubStepStateKeyClass,
+    'globalSettings': GlobalSettingsKeyClass,
+    'tag': TagKeyClass
 }
 
 # fmt: on
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schemas/MetadataChangeEvent.avsc` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/MetadataChangeEvent.avsc`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9999999995026112%*

 * *Differences: {"'fields'": "{1: {'type': {0: {'fields': {1: {'type': {'items': {1: {'fields': {1: {delete: "*

 * *             "['Searchable']}, 5: {delete: ['Searchable']}}}}}}}}, 3: {'fields': {1: {'type': "*

 * *             "{'items': {1: {'fields': {1: {delete: ['Searchable']}, 9: {delete: "*

 * *             "['Searchable']}}}}}}}}, 4: {'fields': {1: {'type': {'items': {1: {'fields': {1: "*

 * *             "{delete: ['Searchable']}}}}}}}}, 5: {'fields': {1: {'type': {'items': {1: {'fields': "*

 * *             "{1: {delete: ['Searchable'] []*

```diff
@@ -158,17 +158,14 @@
                                                 "name": "customProperties",
                                                 "type": {
                                                     "type": "map",
                                                     "values": "string"
                                                 }
                                             },
                                             {
-                                                "Searchable": {
-                                                    "fieldType": "KEYWORD"
-                                                },
                                                 "default": null,
                                                 "doc": "URL where the reference exist",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                                                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                                                 },
                                                 "name": "externalUrl",
@@ -275,17 +272,14 @@
                                                     ],
                                                     "name": "ChangeAuditStamps",
                                                     "namespace": "com.linkedin.pegasus2avro.common",
                                                     "type": "record"
                                                 }
                                             },
                                             {
-                                                "Searchable": {
-                                                    "fieldType": "KEYWORD"
-                                                },
                                                 "default": null,
                                                 "doc": "URL for the chart. This could be used as an external link on DataHub to allow users access/view the chart",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                                                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                                                 },
                                                 "name": "chartUrl",
@@ -1620,17 +1614,14 @@
                                                 "name": "customProperties",
                                                 "type": {
                                                     "type": "map",
                                                     "values": "string"
                                                 }
                                             },
                                             {
-                                                "Searchable": {
-                                                    "fieldType": "KEYWORD"
-                                                },
                                                 "default": null,
                                                 "doc": "URL where the reference exist",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                                                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                                                 },
                                                 "name": "externalUrl",
@@ -1754,17 +1745,14 @@
                                             },
                                             {
                                                 "doc": "Captures information about who created/last modified/deleted this dashboard and when",
                                                 "name": "lastModified",
                                                 "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
                                             },
                                             {
-                                                "Searchable": {
-                                                    "fieldType": "KEYWORD"
-                                                },
                                                 "default": null,
                                                 "doc": "URL for the dashboard. This could be used as an external link on DataHub to allow users access/view the dashboard",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                                                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                                                 },
                                                 "name": "dashboardUrl",
@@ -1946,17 +1934,14 @@
                                                 "name": "customProperties",
                                                 "type": {
                                                     "type": "map",
                                                     "values": "string"
                                                 }
                                             },
                                             {
-                                                "Searchable": {
-                                                    "fieldType": "KEYWORD"
-                                                },
                                                 "default": null,
                                                 "doc": "URL where the reference exist",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                                                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                                                 },
                                                 "name": "externalUrl",
@@ -2210,17 +2195,14 @@
                                                 "name": "customProperties",
                                                 "type": {
                                                     "type": "map",
                                                     "values": "string"
                                                 }
                                             },
                                             {
-                                                "Searchable": {
-                                                    "fieldType": "KEYWORD"
-                                                },
                                                 "default": null,
                                                 "doc": "URL where the reference exist",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                                                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                                                 },
                                                 "name": "externalUrl",
@@ -2606,15 +2588,15 @@
                                                         "type": "array"
                                                     }
                                                 ],
                                                 "urn_is_array": true
                                             },
                                             {
                                                 "default": null,
-                                                "doc": "Fine-grained column-level lineages\nNot currently supported in the UI\nUse UpstreamLineage aspect for datasets to express Column Level Lineage for the UI",
+                                                "doc": "Fine-grained column-level lineages",
                                                 "name": "fineGrainedLineages",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": {
                                                             "doc": "A fine-grained lineage from upstream fields/datasets to downstream field(s)",
                                                             "fields": [
@@ -2897,17 +2879,14 @@
                                                 "name": "customProperties",
                                                 "type": {
                                                     "type": "map",
                                                     "values": "string"
                                                 }
                                             },
                                             {
-                                                "Searchable": {
-                                                    "fieldType": "KEYWORD"
-                                                },
                                                 "default": null,
                                                 "doc": "URL where the reference exist",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                                                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                                                 },
                                                 "name": "externalUrl",
@@ -3580,16 +3559,15 @@
                                                     "items": {
                                                         "doc": "SchemaField to describe metadata related to dataset schema.",
                                                         "fields": [
                                                             {
                                                                 "Searchable": {
                                                                     "boostScore": 5.0,
                                                                     "fieldName": "fieldPaths",
-                                                                    "fieldType": "TEXT",
-                                                                    "queryByDefault": "true"
+                                                                    "fieldType": "TEXT"
                                                                 },
                                                                 "doc": "Flattened name of the field. Field is computed from jsonPath field.",
                                                                 "name": "fieldPath",
                                                                 "type": "string"
                                                             },
                                                             {
                                                                 "Deprecated": true,
@@ -4581,17 +4559,14 @@
                                                 "name": "customProperties",
                                                 "type": {
                                                     "type": "map",
                                                     "values": "string"
                                                 }
                                             },
                                             {
-                                                "Searchable": {
-                                                    "fieldType": "KEYWORD"
-                                                },
                                                 "default": null,
                                                 "doc": "URL where the reference exist",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                                                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                                                 },
                                                 "name": "externalUrl",
@@ -6058,17 +6033,14 @@
                                                 "name": "customProperties",
                                                 "type": {
                                                     "type": "map",
                                                     "values": "string"
                                                 }
                                             },
                                             {
-                                                "Searchable": {
-                                                    "fieldType": "KEYWORD"
-                                                },
                                                 "default": null,
                                                 "doc": "URL where the reference exist",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.url.Url",
                                                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                                                 },
                                                 "name": "externalUrl",
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schemas/MetadataChangeProposal.avsc` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/MetadataChangeProposal.avsc`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/metadata/schemas/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/specific/custom_properties.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/specific/custom_properties.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/specific/dataset.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/specific/dataset.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/telemetry/stats.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/stats.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/telemetry/telemetry.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/telemetry.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/upgrade/upgrade.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/upgrade/upgrade.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/bigquery_sql_parser.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/bigquery_sql_parser.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/checkpoint_state_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/checkpoint_state_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/delayed_iter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/delayed_iter.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/file_backed_collections.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/file_backed_collections.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 import collections
-import gzip
 import logging
 import pathlib
 import pickle
 import sqlite3
 import tempfile
 from dataclasses import dataclass, field
 from datetime import datetime
@@ -132,15 +131,15 @@
 
 
 def _default_deserializer(value: Any) -> Any:
     return pickle.loads(value)
 
 
 @dataclass(eq=False)
-class FileBackedDict(MutableMapping[str, _VT], Closeable, Generic[_VT]):
+class FileBackedDict(MutableMapping[str, _VT], Generic[_VT], Closeable):
     """
     A dict-like object that stores its data in a temporary SQLite database.
     This is useful for storing large amounts of data that don't fit in memory.
 
     This class is not thread-safe.
     """
 
@@ -150,19 +149,16 @@
 
     serializer: Callable[[_VT], SqliteValue] = _default_serializer
     deserializer: Callable[[Any], _VT] = _default_deserializer
     extra_columns: Dict[str, Callable[[_VT], SqliteValue]] = field(default_factory=dict)
 
     cache_max_size: int = _DEFAULT_MEMORY_CACHE_MAX_SIZE
     cache_eviction_batch_size: int = _DEFAULT_MEMORY_CACHE_EVICTION_BATCH_SIZE
-    delay_index_creation: bool = False
-    should_compress_value: bool = False
 
     _conn: ConnectionWrapper = field(init=False, repr=False)
-    indexes_created: bool = field(init=False, default=False)
 
     # To improve performance, we maintain an in-memory LRU cache using an OrderedDict.
     # Maintains a dirty bit marking whether the value has been modified since it was persisted.
     _active_object_cache: OrderedDict[str, Tuple[_VT, bool]] = field(
         init=False, repr=False
     )
 
@@ -190,32 +186,20 @@
             f"""CREATE TABLE {self.tablename} (
                 key TEXT PRIMARY KEY,
                 value BLOB
                 {''.join(f', {column_name} BLOB' for column_name in self.extra_columns.keys())}
             )"""
         )
 
-        if not self.delay_index_creation:
-            self.create_indexes()
-
-        if self.should_compress_value:
-            serializer = self.serializer
-            self.serializer = lambda value: gzip.compress(serializer(value))  # type: ignore
-            deserializer = self.deserializer
-            self.deserializer = lambda value: deserializer(gzip.decompress(value))
-
-    def create_indexes(self) -> None:
-        if self.indexes_created:
-            return
-        # The key column will automatically be indexed, but we need indexes for the extra columns.
+        # The key column will automatically be indexed, but we need indexes
+        # for the extra columns.
         for column_name in self.extra_columns.keys():
             self._conn.execute(
                 f"CREATE INDEX {self.tablename}_{column_name} ON {self.tablename} ({column_name})"
             )
-        self.indexes_created = True
 
     def _add_to_cache(self, key: str, value: _VT, dirty: bool) -> None:
         self._active_object_cache[key] = value, dirty
 
         if len(self._active_object_cache) > self.cache_max_size:
             # Try to prune in batches rather than one at a time.
             num_items_to_prune = min(
@@ -389,15 +373,15 @@
         serializer: Callable[[_VT], SqliteValue] = _default_serializer,
         deserializer: Callable[[Any], _VT] = _default_deserializer,
         extra_columns: Optional[Dict[str, Callable[[_VT], SqliteValue]]] = None,
         cache_max_size: Optional[int] = None,
         cache_eviction_batch_size: Optional[int] = None,
     ) -> None:
         self._len = 0
-        self._dict = FileBackedDict[_VT](
+        self._dict = FileBackedDict(
             shared_connection=connection,
             serializer=serializer,
             deserializer=deserializer,
             tablename=tablename,
             extra_columns=extra_columns or {},
             cache_max_size=cache_max_size or _DEFAULT_MEMORY_CACHE_MAX_SIZE,
             cache_eviction_batch_size=cache_eviction_batch_size
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/hive_schema_to_avro.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/hive_schema_to_avro.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/logging_manager.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/logging_manager.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/lossy_collections.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/lossy_collections.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/mapping.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/mapping.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/memory_footprint.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/memory_footprint.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/memory_leak_detector.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/memory_leak_detector.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/parsing_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/parsing_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/perf_timer.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/perf_timer.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/registries/domain_registry.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/registries/domain_registry.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sample_data.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sample_data.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/server_config_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/server_config_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/source_helpers.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/source_helpers.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sql_formatter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_formatter.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sql_lineage_parser_impl.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_lineage_parser_impl.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sql_parser.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_parser.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sqlalchemy_query_combiner.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sqlalchemy_query_combiner.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/sqllineage_patch.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sqllineage_patch.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/stats_collections.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/stats_collections.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/tee_io.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/tee_io.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/type_annotations.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/type_annotations.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urn_encoder.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urn_encoder.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/corp_group_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/corp_group_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/corpuser_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/corpuser_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/data_flow_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_flow_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/data_job_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_job_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/data_platform_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_platform_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/data_process_instance_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_process_instance_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/dataset_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/dataset_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/domain_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/domain_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/notebook_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/notebook_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/tag_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/tag_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub/utilities/urns/urn_iter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/urn_iter.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/_airflow_shims.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_airflow_shims.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/_lineage_core.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_lineage_core.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/_plugin.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_plugin.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/client/airflow_generator.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/client/airflow_generator.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/entities.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/entities.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/generic_recipe_sample_dag.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/generic_recipe_sample_dag.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/lineage_backend_demo.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_backend_demo.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/lineage_emission_dag.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_emission_dag.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/mysql_sample_dag.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/mysql_sample_dag.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/example_dags/snowflake_sample_dag.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/snowflake_sample_dag.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/hooks/datahub.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/hooks/datahub.py`

 * *Files 1% similar despite different names*

```diff
@@ -52,22 +52,16 @@
             },
         }
 
     def _get_config(self) -> Tuple[str, Optional[str], Optional[int]]:
         conn: "Connection" = self.get_connection(self.datahub_rest_conn_id)
 
         host = conn.host
-        if not host:
+        if host is None:
             raise AirflowException("host parameter is required")
-        if conn.port:
-            if ":" in host:
-                raise AirflowException(
-                    "host parameter should not contain a port number if the port is specified separately"
-                )
-            host = f"{host}:{conn.port}"
         password = conn.password
         timeout_sec = conn.extra_dejson.get("timeout_sec")
         return (host, password, timeout_sec)
 
     def make_emitter(self) -> "DatahubRestEmitter":
         import datahub.emitter.rest_emitter
```

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/lineage/datahub.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/lineage/datahub.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/datahub.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/datahub_assertion_operator.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_assertion_operator.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/datahub_assertion_sensor.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_assertion_sensor.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/datahub_operation_operator.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_operation_operator.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.2.3/src/datahub_provider/operators/datahub_operation_sensor.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_operation_sensor.py`

 * *Files identical despite different names*

