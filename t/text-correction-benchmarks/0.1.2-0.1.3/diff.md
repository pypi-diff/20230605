# Comparing `tmp/text_correction_benchmarks-0.1.2-py2.py3-none-any.whl.zip` & `tmp/text_correction_benchmarks-0.1.3-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,17 +1,18 @@
-Zip file size: 20342 bytes, number of entries: 15
--rw-r--r--  2.0 unx        0 b- defN 23-Feb-21 16:06 text_correction_benchmarks/__init__.py
--rw-r--r--  2.0 unx     4683 b- defN 23-Feb-21 16:06 text_correction_benchmarks/baselines/__init__.py
--rw-r--r--  2.0 unx      282 b- defN 23-Feb-21 16:06 text_correction_benchmarks/baselines/sec.py
--rw-r--r--  2.0 unx      735 b- defN 23-Feb-21 16:06 text_correction_benchmarks/baselines/seds.py
--rw-r--r--  2.0 unx      939 b- defN 23-Feb-21 16:06 text_correction_benchmarks/baselines/sedw.py
--rw-r--r--  2.0 unx      800 b- defN 23-Feb-21 16:06 text_correction_benchmarks/baselines/wsc.py
--rw-r--r--  2.0 unx        0 b- defN 23-Feb-21 16:06 text_correction_benchmarks/cli/__init__.py
--rw-r--r--  2.0 unx     1678 b- defN 23-Feb-21 16:06 text_correction_benchmarks/cli/download.py
--rw-r--r--  2.0 unx    15187 b- defN 23-Feb-21 16:06 text_correction_benchmarks/cli/evaluate.py
--rw-r--r--  2.0 unx    11357 b- defN 23-Feb-21 16:07 text_correction_benchmarks-0.1.2.dist-info/LICENSE
--rw-r--r--  2.0 unx    20315 b- defN 23-Feb-21 16:07 text_correction_benchmarks-0.1.2.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Feb-21 16:07 text_correction_benchmarks-0.1.2.dist-info/WHEEL
--rw-r--r--  2.0 unx      195 b- defN 23-Feb-21 16:07 text_correction_benchmarks-0.1.2.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       27 b- defN 23-Feb-21 16:07 text_correction_benchmarks-0.1.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1489 b- defN 23-Feb-21 16:07 text_correction_benchmarks-0.1.2.dist-info/RECORD
-15 files, 57797 bytes uncompressed, 17782 bytes compressed:  69.2%
+Zip file size: 21659 bytes, number of entries: 16
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-05 10:40 text_correction_benchmarks/__init__.py
+-rw-r--r--  2.0 unx     5592 b- defN 23-Jun-05 10:40 text_correction_benchmarks/baselines/__init__.py
+-rw-r--r--  2.0 unx     2660 b- defN 23-Jun-05 10:40 text_correction_benchmarks/baselines/general.py
+-rw-r--r--  2.0 unx      282 b- defN 23-Jun-05 10:40 text_correction_benchmarks/baselines/sec.py
+-rw-r--r--  2.0 unx      735 b- defN 23-Jun-05 10:40 text_correction_benchmarks/baselines/seds.py
+-rw-r--r--  2.0 unx      939 b- defN 23-Jun-05 10:40 text_correction_benchmarks/baselines/sedw.py
+-rw-r--r--  2.0 unx      800 b- defN 23-Jun-05 10:40 text_correction_benchmarks/baselines/wsc.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-05 10:40 text_correction_benchmarks/cli/__init__.py
+-rw-r--r--  2.0 unx     1678 b- defN 23-Jun-05 10:40 text_correction_benchmarks/cli/download.py
+-rw-r--r--  2.0 unx    15310 b- defN 23-Jun-05 10:40 text_correction_benchmarks/cli/evaluate.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-Jun-05 10:41 text_correction_benchmarks-0.1.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx    20400 b- defN 23-Jun-05 10:41 text_correction_benchmarks-0.1.3.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-Jun-05 10:41 text_correction_benchmarks-0.1.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx      195 b- defN 23-Jun-05 10:41 text_correction_benchmarks-0.1.3.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       27 b- defN 23-Jun-05 10:41 text_correction_benchmarks-0.1.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1593 b- defN 23-Jun-05 10:41 text_correction_benchmarks-0.1.3.dist-info/RECORD
+16 files, 61678 bytes uncompressed, 18929 bytes compressed:  69.3%
```

## zipnote {}

```diff
@@ -1,13 +1,16 @@
 Filename: text_correction_benchmarks/__init__.py
 Comment: 
 
 Filename: text_correction_benchmarks/baselines/__init__.py
 Comment: 
 
+Filename: text_correction_benchmarks/baselines/general.py
+Comment: 
+
 Filename: text_correction_benchmarks/baselines/sec.py
 Comment: 
 
 Filename: text_correction_benchmarks/baselines/seds.py
 Comment: 
 
 Filename: text_correction_benchmarks/baselines/sedw.py
@@ -21,26 +24,26 @@
 
 Filename: text_correction_benchmarks/cli/download.py
 Comment: 
 
 Filename: text_correction_benchmarks/cli/evaluate.py
 Comment: 
 
-Filename: text_correction_benchmarks-0.1.2.dist-info/LICENSE
+Filename: text_correction_benchmarks-0.1.3.dist-info/LICENSE
 Comment: 
 
-Filename: text_correction_benchmarks-0.1.2.dist-info/METADATA
+Filename: text_correction_benchmarks-0.1.3.dist-info/METADATA
 Comment: 
 
-Filename: text_correction_benchmarks-0.1.2.dist-info/WHEEL
+Filename: text_correction_benchmarks-0.1.3.dist-info/WHEEL
 Comment: 
 
-Filename: text_correction_benchmarks-0.1.2.dist-info/entry_points.txt
+Filename: text_correction_benchmarks-0.1.3.dist-info/entry_points.txt
 Comment: 
 
-Filename: text_correction_benchmarks-0.1.2.dist-info/top_level.txt
+Filename: text_correction_benchmarks-0.1.3.dist-info/top_level.txt
 Comment: 
 
-Filename: text_correction_benchmarks-0.1.2.dist-info/RECORD
+Filename: text_correction_benchmarks-0.1.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## text_correction_benchmarks/baselines/__init__.py

```diff
@@ -27,14 +27,18 @@
     # deep learning based
     SEC_NEUSPELL_BERT = "sec_neuspell_bert"
 
     # Whitespace correction
     WSC_DUMMY = "wsc_dummy"
     WSC_WORDSEGMENT = "wsc_wordsegment"
 
+    # Genral models able to do all tasks (usually
+    # language models)
+    CHAT_GPT = "chat_gpt"
+
 
 class Baseline:
     def __init__(self, seed: Optional[int] = None):
         self.seed = seed
 
     def run(self, sequences: Iterable[str], **kwargs: Any) -> Iterable[str]:
         raise NotImplementedError
@@ -66,14 +70,19 @@
         return Dummy()
     elif baseline == Baselines.WSC_DUMMY:
         from text_correction_benchmarks.baselines.wsc import Dummy
         return Dummy()
     elif baseline == Baselines.WSC_WORDSEGMENT:
         from text_correction_benchmarks.baselines.wsc import Wordsegment
         return Wordsegment()
+    elif baseline == Baselines.CHAT_GPT:
+        from text_correction_benchmarks.baselines.general import ChatGPT
+        assert kwargs["task"] is not None, \
+            "task must be specified for the chat gpt baseline"
+        return ChatGPT(task=kwargs["task"])
     else:
         raise ValueError(f"unknown baseline {baseline}")
 
 
 def parse_args() -> argparse.Namespace:
     parser = argparse.ArgumentParser(
         "Text correction baselines",
@@ -85,14 +94,29 @@
             baseline
             for key, baseline in vars(Baselines).items()
             if not key.startswith("_")
         ],
         help="The baseline to run"
     )
     parser.add_argument(
+        "--task",
+        choices=["seds", "sedw", "sec", "wsc", "gec"],
+        default=None,
+        help="Specify the task to run the baseline on, "
+        "only required for general models that can do all tasks"
+    )
+    input_group = parser.add_mutually_exclusive_group()
+    input_group.add_argument(
+        "-t",
+        "--text",
+        type=str,
+        default=None,
+        help="Run baseline on this text instead of stdin"
+    )
+    input_group.add_argument(
         "-f",
         "--file",
         type=str,
         default=None,
         help="Run baseline on this file instead of stdin"
     )
     parser.add_argument(
@@ -128,18 +152,20 @@
             s = unicode.normalize(s, normalization)
         yield s
 
 
 def run(args: argparse.Namespace):
     # prepare baseline and input
     baseline = get_baseline(**vars(args))
-    if args.file is None:
-        sequences = sys.stdin
-    else:
+    if args.text is not None:
+        sequences = [args.text]
+    elif args.file is not None:
         sequences = open(args.file, "r", encoding="utf8")
+    else:
+        sequences = sys.stdin
 
     if args.out is not None:
         of = open(args.out, "w", encoding="utf8")
     else:
         of = None
 
     # run the baseline
```

## text_correction_benchmarks/cli/evaluate.py

```diff
@@ -139,15 +139,15 @@
         elif metric == "seq_acc":
             accuracy = M.accuracy(predictions, groundtruths)
             outputs.append(
                 (metric_name, f"{accuracy * 100:.2f}", accuracy)
             )
 
         elif metric == "mned":
-            mned = M.mean_normalized_sequence_edit_distance(
+            mned = M.mean_normalized_edit_distance(
                 predictions, groundtruths
             )
             outputs.append((mned, f"{mned:.4f}", mned))
 
         elif metric == "sec_f1_micro" or metric == "sec_f1_seq":
             (f1, _, _), _ = M.spelling_correction_f1(
                 corrupted,
@@ -288,14 +288,17 @@
             prediction_dir = os.path.join(benchmark, "predictions")
             assert os.path.exists(prediction_dir) and os.path.isdir(prediction_dir), \
                 f"expecting a subdirectory 'predictions' in each benchmark directory when " \
                 f"evaluating on multiple benchmarks, but got none in {benchmark}"
             predictions = list_dir(prediction_dir)
             benchmark_predictions.append(predictions)
 
+    if all(len(predictions) == 0 for predictions in benchmark_predictions):
+        raise RuntimeError("no benchmark predictions")
+
     try:
         benchmark_evaluations = []
         for (benchmark, *_), predictions in zip(benchmarks, benchmark_predictions):
             in_file = os.path.join(benchmark, "corrupt.txt")
             gt_file = os.path.join(benchmark, "correct.txt")
             lc_file = os.path.join(benchmark, "lowercase.txt")
             evaluations = []
```

## Comparing `text_correction_benchmarks-0.1.2.dist-info/LICENSE` & `text_correction_benchmarks-0.1.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `text_correction_benchmarks-0.1.2.dist-info/METADATA` & `text_correction_benchmarks-0.1.3.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: text-correction-benchmarks
-Version: 0.1.2
+Version: 0.1.3
 Summary: Benchmarks and evaluation tools for text correction tasks
 Author-email: Sebastian Walter <swalter@cs.uni-freiburg.de>
 License: Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
         
            TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
@@ -212,28 +212,35 @@
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Text Processing
 Classifier: Topic :: Utilities
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: text-correction-utils (>=0.1.4)
+Requires-Dist: text-correction-utils (==0.1.4)
 Requires-Dist: requests (>=2.0.0)
+Requires-Dist: openai (>=0.27.0)
 Provides-Extra: baselines
 Requires-Dist: jamspell ; extra == 'baselines'
 Requires-Dist: wordsegment ; extra == 'baselines'
 
 # Text correction benchmarks
 
 Benchmarks and baselines for various text correction tasks. Light-weight and 
 easy to use.
 
 ### Installation
 
 ```
+pip install text-correction-benchmarks
+```
+
+or
+
+```
 git clone https://github.com/ad-freiburg/text-correction-benchmarks
 cd text-correction-benchmarks && pip install .
 ```
 
 After installation you will have two commands available to you:
 - `tcb.evaluate` for evaluating model predictions on benchmarks
 - `tcb.baseline` for running baselines on benchmarks
```

## Comparing `text_correction_benchmarks-0.1.2.dist-info/RECORD` & `text_correction_benchmarks-0.1.3.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 text_correction_benchmarks/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-text_correction_benchmarks/baselines/__init__.py,sha256=n-G56wfG5P8vt7e14XqDPwmkZ00JVQdgbaRHx4sg3oo,4683
+text_correction_benchmarks/baselines/__init__.py,sha256=j2wLNAP_aMmSJ-7LLnYSd20LxcVxgz0tov1xXW2_Bko,5592
+text_correction_benchmarks/baselines/general.py,sha256=XXQs9CbX6SNnb8bMBjROitPLLLVTtnAb4SAvGwAAYz8,2660
 text_correction_benchmarks/baselines/sec.py,sha256=tSMv1E9TdhuZmKHwRsuTJtw2_bFhW3iV8VvnjFd-BRo,282
 text_correction_benchmarks/baselines/seds.py,sha256=iJoinWGOFob4iw02t6_N5Cz0IYH5scnRFekBJbBe9v0,735
 text_correction_benchmarks/baselines/sedw.py,sha256=yRptfy7uL4Kqbw06J6uOOt9vCPj_xOancdFZ0nG8n0M,939
 text_correction_benchmarks/baselines/wsc.py,sha256=GS1AZVOSe9LASji0fIJuT8mBKXlZ8WoZraBWT6gkejI,800
 text_correction_benchmarks/cli/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 text_correction_benchmarks/cli/download.py,sha256=FvqSsEN1VdPp23R9uaniQc2jIwNSDVf_2zCCd5FMrxg,1678
-text_correction_benchmarks/cli/evaluate.py,sha256=lOCEfJFtyQUHFoYjK_4ldyCNiHMtcd7TGJRjbAORL3I,15187
-text_correction_benchmarks-0.1.2.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-text_correction_benchmarks-0.1.2.dist-info/METADATA,sha256=caVBexmUViqjYWNEo1Hf6150-olabOEDexLTTAaOXWo,20315
-text_correction_benchmarks-0.1.2.dist-info/WHEEL,sha256=bb2Ot9scclHKMOLDEHY6B2sicWOgugjFKaJsT7vwMQo,110
-text_correction_benchmarks-0.1.2.dist-info/entry_points.txt,sha256=8XlGMQPslYCgbpi8oqDWLDEHmBCcTdVN1DO0rQkRIr4,195
-text_correction_benchmarks-0.1.2.dist-info/top_level.txt,sha256=ShNBaFrYwDLBDA2bzRnYiawcrd1mMUg9jK9IyARVEQ4,27
-text_correction_benchmarks-0.1.2.dist-info/RECORD,,
+text_correction_benchmarks/cli/evaluate.py,sha256=jVaXHnl9h1FgyzlVj7DpF4V7pM3FQbvTn9PIcBJT52M,15310
+text_correction_benchmarks-0.1.3.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+text_correction_benchmarks-0.1.3.dist-info/METADATA,sha256=M7rXdmIDpDC52LHZXMG0GhioiohFjZE_aqKfmoM7doo,20400
+text_correction_benchmarks-0.1.3.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+text_correction_benchmarks-0.1.3.dist-info/entry_points.txt,sha256=8XlGMQPslYCgbpi8oqDWLDEHmBCcTdVN1DO0rQkRIr4,195
+text_correction_benchmarks-0.1.3.dist-info/top_level.txt,sha256=ShNBaFrYwDLBDA2bzRnYiawcrd1mMUg9jK9IyARVEQ4,27
+text_correction_benchmarks-0.1.3.dist-info/RECORD,,
```

